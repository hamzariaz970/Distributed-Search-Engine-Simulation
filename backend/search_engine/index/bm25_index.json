{
  "corpus": [
    "1\nA Survey on Efficient Inference for Large\nLanguage Models\nZixuan Zhou*, Xuefei Ning*, Ke Hong*, Tianyu Fu, Jiaming Xu, Shiyao Li,\nYuming Lou, Luning Wang, Zhihang Yuan, Xiuhong Li, Shengen Yan, Guohao Dai,\nXiao-Ping Zhang Fellow, IEEE, Huazhong Yang Fellow, IEEE, Yuhan Dong, Yu Wang Fellow, IEEE\n\u2726\nAbstract\u2014Large Language Models (LLMs) have attracted extensive\nattention due to their remarkable performance across various tasks.\nHowever, the substantial computational and memory requirements of\nLLM inference pose challenges for deployment in resource-constrained\nscenarios. Efforts within the field have been directed towards developing\ntechniques aimed at enhancing the efficiency of LLM inference. This\npaper presents a comprehensive survey of the existing literature on\nefficient LLM inference. We start by analyzing the primary causes of\nthe inefficient LLM inference, i.e., the large model size, the quadratic-\ncomplexity attention operation, and the auto-regressive decoding ap-\nproach. Then, we introduce a comprehensive taxonomy that organizes\nthe current literature into data-level, model-level, and system-level op-\ntimization. Moreover, the paper includes comparative experiments on\nrepresentative methods within critical sub-fields to provide quantitative\ninsights. Last but not least, we provide some knowledge summary and\ndiscuss future research directions.\n1\nINTRODUCTION\nLarge Language Models (LLMs) have garnered substantial\nattention from both academia and industry in recent years.\nThe field of LLMs has experienced notable growth and sig-\nnificant achievements. Numerous open-source LLMs have\nemerged, including the GPT-series (GPT-1 [1], GPT-2 [2],\nand GPT-3 [3]), OPT [4], LLaMA-series (LLaMA [5], LLaMA\n2 [5], Baichuan 2 [6], Vicuna [7], LongChat [8]), BLOOM [9],\nFALCON [10], GLM [11], and Mistral [12], which are used\nfor both academic research and commercial purposes. The\nsuccess of LLMs stems from their robust capability in han-\ndling diverse tasks such as neural language understanding\n\u2022\nZ. Zhou, K. Hong, T. Fu, S. Li, L. Wang are with Infinigence-AI and the\nDepartment of Electronic Engineering, Tsinghua University, China.\nE-mail: zhouzx21@mails.tsinghua.edu.cn (Z. Zhou)\n\u2022\nX. Ning, Y. Lou, H. Yang, Y. Wang are with the Department of Electronic\nEngineering, Tsinghua University, China.\nE-mail: foxdoraame@gmail.com (X. Ning), yu-wang@tsinghua.edu.cn (Y.\nWang)\n\u2022\nJ. Xu, G. Dai are with Infinigence-AI and the Department of Electronic\nEngineering, Shanghai Jiaotong University, China.\nE-mail: daiguohao@sjtu.edu.cn (G. Dai)\n\u2022\nX.-P. Zhang, Y. Dong are with Tsinghua Shenzhen International Graduate\nSchool.\nE-mail: xpzhang@ieee.org (X.-P. Zhang), dongyuhan@sz.tsinghua.edu.cn\n(Y. Dong)\n\u2022\nZ. Yuan, S. Yan are with Infinigence-AI.\n\u2022\nX. Li is with Peking University.\n\u2022\nCorresponding authors: Yu Wang, Xuefei Ning, Guohao Dai.\n\u2022\n*Equal contribution.\n(NLU), neural language generation (NLG), reasoning [13],\n[14], and code generation [15], consequently enabling im-\npactful applications like ChatGPT, Copilot, and Bing. There\nis a growing belief [16] that the rise and achievements of\nLLMs signify a significant stride towards Artificial General\nIntelligence (AGI) for humanity.\nHigher Computational \nCost\nHigher Memory Access \nCost\nHigher Memory Cost\nHigher Latency\nLower Throughput\nHigher Power\nConsumption\nHigher Storage\nFig. 1. The challenges of LLM deployment.\nHowever, the deployment of LLMs is not always going\nsmoothly. As shown in Fig. 1, LLMs typically demand\nhigher computational cost, memory access cost and memory\nusage in their inference process (we will analyse the root\ncauses in the Sec. 2.3), which deteriorates the efficiency\nindicators (e.g., latency, throughput, power consumption\nand storage) in the resource-constrained scenarios. This\nposes challenges for the application of LLMs in both edge\nand cloud scenarios. For example, the immense storage re-\nquirements render the deployment of a 70-billion-parameter\nmodel impractical on personal laptops for tasks such as\ndevelopment assistance. Additionally, the low throughput\nwould result in significant costs if LLMs are used for every\nsearch engine request, leading to a considerable reduction\nin the profits of the search engine.\nFortunately, a substantial array of techniques has been\nproposed to enable efficient inference for LLMs. To gain\na comprehensive understanding of existing studies and\ninspire further research, this survey employs a hierarchical\nclassification and systematic summarization of the current\nlandscape of efficient LLM inference. Specifically, we cat-\negorize relevant studies into three levels: data-level opti-\nmization, model-level optimization, and system-level op-\ntimization (refer to Sec. 3 for elaboration). Moreover, we\nconduct experimental analyses on representative methods\narXiv:2404.14294v3  [cs.CL]  19 Jul 2024\n\n2\nwithin critical sub-fields to consolidate knowledge, offer\npractical recommendations, and provide guidance for future\nresearch endeavors.\nTABLE 1\nComparison of existing surveys.\nSurvey\nOptimization Levels\nExperimental\nAnalysis\nData-level\nModel-level\nSystem-level\n[17], [18], [19]\n\u2713\n[20]\n\u2713\n\u2713\n[21]\n\u2713\n\u2713\n[22]\n\u2713\n\u2713\n[23], [24]\n\u2713\n\u2713\n\u2713\nOurs\n\u2713\n\u2713\n\u2713\n\u2713\nCurrently, several surveys [17], [18], [19], [20], [21], [22],\n[23] have been conducted in the field of efficient LLMs.\nThese surveys primarily focus on different aspects of LLM\nefficiency but offer opportunities for further improvement.\nZhu et al. [17], Park et al. [18], Wang et al. [19] and Tang\net al. [20] concentrate on model compression techniques\nwithin model-level optimization. Ding et al. [21] center\non efficiency research considering both data and model\narchitecture perspectives. Miao et al. [22] approach efficient\nLLM inference from a machine learning system (MLSys) re-\nsearch perspective. In contrast, our survey provides a more\ncomprehensive research scope, addressing optimization at\nthree levels: data-level, model-level, and system-level, with\nthe inclusion of recent advancements. While Wan et al. [23]\nand Xu et al. [24] also deliver comprehensive review of\nefficient LLM research, our work extends by incorporating\ncomparative experiments and offering practical insights and\nrecommendations based on experimental analyses in sev-\neral critical sub-fields like model quantization and serving\nsystems. A comparison of these surveys is summarized in\nTable 1.\nThe remainder of this survey is organized as follows:\nSec. 2 introduces the basic concept and knowledge about\nLLMs and presents a detailed analysis of the efficiency\nbottlenecks during the inference process of LLMs. Sec. 3\ndemonstrates our taxonomy. Sec. 4 to Sec. 6 respectively\npresent and discuss studies on efficiency optimization at\nthree distinct levels. Sec. 7 offers broader discussions for\nseveral key application scenarios. Sec. 8 concludes the key\ncontributions provided by this survey.\n2\nPRELIMINARIES\n2.1\nTransformer-Style LLMs\nLanguage modeling, as the fundamental function of lan-\nguage models (LMs), involves modeling the likelihood of\nthe word sequence and predicting the distribution of subse-\nquent words. Over recent years, researchers have discovered\nthat scaling up language models not only enhances their\nlanguage modeling ability but also engenders emergent\ncapabilities for tackling more intricate tasks beyond conven-\ntional NLP tasks [25]. These scaled-up language models are\nreferred to as large language models (LLMs).\nThe mainstream LLMs are designed based on the Trans-\nformer architecture [26]. Specifically, a typical Transformer\narchitecture is composed of several stacked Transformer\nblocks. Typically, a Transformer block consists of a Multi-\nHead Self-Attention (MHSA) block, a Feed Forward Net-\nwork (FFN), and a LayerNorm (LN) operation. For each\nblock, it receives the output features of the previous one\nas the input, and passes the features through each sub-\nmodule to obtain the output. Specially, before the first block,\na tokenizer is used to convert the original input sentence\ninto a sequence of tokens, and a following embedding layer\nserves to convert the tokens into the input features. Then,\nthe additional position embeddings are added into the input\nfeatures to encode the sequential order of each input token.\nThe core concept of the Transformer architecture is the\nself-attention mechanism, which is adopted in the MHSA\nblock. Specifically, denoted the input features as X\n=\n[x1, x2, ..., xn], the MHSA block applies linear projection to\nthem and obtains a set of queries Q, keys K and values V as\nEq. 1:\nQi = XW Qi, Ki = XW Ki, Vi = XW Vi,\n(1)\nwhere W Qi, W Ki and W Vi are the projection matrices\ncorresponding to the i-th attention head. Then the self-\nattention operation is applied to each tuple of (Qi, Ki, Vi)\nand get the feature of the i-th attention head Zi as Eq. 2:\nZi = Attention(Qi, Ki, Vi) = Softmax(QiKT\ni\n\u221adk\n)Vi,\n(2)\nwhere dk is the dimension of the queries (keys). Note that\nthe self-attention operation contains the matrix multipli-\ncation operation, its computation complexity is quadratic\nin the input length. Finally, the MHSA block concatenates\nthe features of all the attention heads and applies a linear\nprojection to them to form its output Z as Eq. 3:\nZ = Concat(Z1, Z2, ..., Zh)W O,\n(3)\nwhere WO is the projection matrix. As can be seen, the\nself-attention mechanism allows the model to identify the\nimportance of different input parts regardless of the dis-\ntance, and thus can capture the long-range dependencies\nand complex relationships in the input sentence.\nAnother important module in the Transformer block is\nthe FFN. Typically, FFN is placed after the MHSA block\nand consists of two linear transformation layers with a non-\nlinear activation function. It receives the output features X\nfrom the MHSA block and processes them as Eq 4:\nFFN(X) = W2\u03c3(W1X),\n(4)\nwhere W1 and W2 denote the weight matrices of the two\nlinear layers, and \u03c3(\u00b7) denotes the activation function.\n2.2\nInference Process of LLMs\nThe most popular LLMs, i.e., decoder-only LLMs, often\nadopt the auto-regressive method to generate the output\nsentence. Specifically, the auto-regressive method generates\nthe tokens one by one. In each generation step, the LLM\ntakes as input the whole token sequences, including the in-\nput tokens and previously generated tokens, and generates\nthe next token. With the increase in sequence length, the\ntime cost of the generation process grows rapidly. To ad-\ndress this challenge, a crucial technique, namely key-value\n\n3\nOutput: ['Processing'] (1*dim) \nInput: ['I', 'like', 'natural', 'language']\nWQ\nWV\nWO\nSelf-Attention\nWK\nAdd & LayerNorm\nAdd & LayerNorm\nFC1\nFC2\nActivation\nQ\nK\nV\nSoftmax \ud835\udc78\ud835\udc72\ud835\udc7b\n\ud835\udc85\ud835\udc8c\n\ud835\udc7d\n where \ud835\udc44, \ud835\udc3e, \ud835\udc49\u2208\ud835\udc45\ud835\udc41\u00d7\ud835\udc51 \nInput: ['I', 'like', 'natural', 'language', 'Processing']\nOutput: ['! '] (1*dim) \nWQ\nWV\nWO\nSelf-Attention\nWK\nK Cache\nV Cache\nAdd & LayerNorm\nAdd & LayerNorm\nFC1\nFC2\nActivation\nQ\nK\nV\nSoftmax \ud835\udc78\ud835\udc72\ud835\udc7b\n\ud835\udc85\ud835\udc8c\n\ud835\udc7d\n where \ud835\udc44\u2208\ud835\udc451\u00d7\ud835\udc51\n\ud835\udc3e, \ud835\udc49\u2208\ud835\udc45\ud835\udc41\u00d7\ud835\udc51 \nMHSA\nFFN\nFFN\nMHSA\n(a) The prefilling stage\n(b) The decoding stage\nFig. 2. Demonstration of the prefilling stage (a) and decoding stage (b).\n(KV) cache, has been introduced to expedite the generation\nprocess. The KV cache technique, as its name suggests,\ninvolves storing and reusing previous key (K) and value (V)\npairs within the Multi-Head Self-Attention (MHSA) block.\nThis technique has been widely adopted in LLM inference\nengines and systems due to its substantial optimization\nof generation latency. Based on the above methods and\ntechniques, the inference process of LLMs can be divided\ninto two stages:\n\u2022 Prefilling Stage: The LLM calculates and stores the KV\ncache of the initial input tokens, and generates the first\noutput token, as shown in Fig. 2(a).\n\u2022 Decoding Stage: The LLM generates the output tokens\none by one with the KV cache, and then updates it with\nthe key (K) and value (V) pairs of the newly generated\ntoken, as shown in Fig. 2(b).\nAs shown in Fig. 3, we illustrate some critical efficiency\nindicators. As for the latency, we denote first token latency\nas the latency to generate the first output token in the\nprefilling stage, while we denote per-output token latency\nas the average latency to generate one output token in\nthe decoding stage. Besides, we use generation latency\nto denote the latency to generate the whole output token\nsequences. As for the memory, we use model size to denote\nthe memory to store the model weights, and use KV cache\nsize to denote the memory to store the KV cache. Addition-\nally, peak memory denotes the maximum memory usage\nduring the generation process, which is approximately equal\nto the memory sum of model weights and KV cache. Apart\nfrom the latency and memory, throughput is also a widely-\nused indicator in the LLM serving system. We use token\nthroughput to denote the number of generated tokens per\nsecond, and use request throughput to denote the number\nof completed requests per second.\n2.3\nEfficiency Analysis\nDeploying LLMs on resource-constrained scenarios while\npreserving their powerful capabilities poses a significant\nchallenge for both practitioners and researchers. For in-\nstance, let\u2019s consider to deploy a LLaMA-2-70B model,\nwhich contains 70 billion parameters. Storing its weights\nin FP16 format necessitates 140 GB of VRAM, requiring\nat least 6 RTX 3090Ti GPUs (each with 24 GB VRAM)\nFirst Token \nLatency\nPer-output Token \nLatency\nModel \nSize\nPeak\nMemory\nGeneration Latency\nKV Cache \nSize\nLatency\nMemory\nFig. 3. Illustration of the memory variation through time (latency) during\none generation process. Note that we ignore the activation size in this\nfigure for a simplification.\nor 2 NVIDIA A100 GPUs (each with 80 GB VRAM) for\ninference. As for latency, generating one token on 2 NVIDIA\nA100 GPUs requires approximately 100 milliseconds. Con-\nsequently, generating a sequence with hundreds of tokens\nrequires more than 10 seconds. In addition to storage and\nlatency, the efficiency indicators, such as throughput, energy\nand power consumption, also need to be considered. During\nthe LLM inference process, three important factors would\nlargely affect these indicators, i.e., the computational cost,\nthe memory access cost and the memory usage. Yuan et\nal. [27] provide a more systematic analysis to demonstrate\nhow these factors affect the inference inefficiency with a\nroofline model. In the following, we further analyze three\nroot causes of inefficiency in the LLM inference process,\nfocusing on the above three key factors:\n\u2022 Model Size: Mainstream LLMs typically incorporate\nbillions or even trillions of parameters. For instance,\nthe LLaMA-70B model comprises 70 billion parame-\nters, while the GPT-3 model scales up to 175 billion\nparameters. This considerable model size contributes\nsignificantly to the elevated computational cost, mem-\nory access cost, and memory usage during the LLM\ninference process.\n\u2022 Attention Operation: As illustrated in Sec. 2.1 and\nSec. 2.2, in the prefilling stage, the self-attention oper-\nation exhibits quadratic computational complexity in\nthe input length. Consequently, as the input length\nincreases, the computational cost, memory access cost,\nand memory usage of the attention operation escalate\nrapidly.\n\u2022 Decoding Approach: The auto-regressive decoding ap-\nproach generates the tokens one by one. In each decod-\ning step, all the model weights are loaded from the off-\nchip HBM to the GPU chip, leading to a large memory\naccess cost. In addition, the size of KV cache increases\nwith the growth in the input length, potentially leading\nto fragmented memory and irregular memory access\npatterns.\n3\nTAXONOMY\nIn the aforementioned discussion, we identify key factors\n(i.e., computational cost, memory access cost and mem-\nory usage) that significantly impact the efficiency during\n\n4\nEfficient Inference for Large Language Models\nSystem-level\nOptimization\n(Sec. 6)\nServing System\n(Sec. 6.2)\nDistributed Systems\nScheduling\nBatching\nMemory Management\nInference Engine\n(Sec. 6.1)\nSpeculative Decoding\nOffloading\nGraph and Operator\nOptimization\nModel-level\nOptimization\n(Sec. 5)\nModel Compression\n(Sec. 5.2)\nDynamic Inference\nKnowledge Distillation\nBlack-box KD\nWhite-box KD\nStructure Optimization\nNeural Architecture Search\nStructure Factorization\nSparsification\nSparse Attention\nWeight Pruning\nQuantization\nQuantization-\naware Training\nPost-Training Quantization\nEfficient Structure Design\n(Sec. 5.1)\nTransformer Alternate\nEfficient Attention Design\nMulti/Group-\nQuery Attention\nLow-Complexity Attention\nEfficient FFN Design\nData-level\nOptimization\n(Sec. 4)\nOutput Organization\n(Sec. 4.2)\nInput Compression\n(Sec. 4.1)\nRetrieval-Augmented\nGeneration\nSoft Prompt-based\nCompression\nPrompt Summary\nPrompt Pruning\nFig. 4. Taxonomy of efficient inference methods for Large Language Models.\nthe LLM inference process, and further analyze three root\ncauses (i.e., model size, attention operation and decoding\napproach). Many efforts have been made to optimize the\ninference efficiency from different perspectives. By carefully\nreviewing and summarizing these studies, we classify them\ninto three levels, i.e., data-level optimization, model-level\noptimization and system-level optimization (as shown in\nFig. 4):\n\u2022 Data-level Optimization refers to improving the ef-\nficiency via optimizing the input prompts (i.e., input\ncompression) or better organizing the output content\n(i.e., output organization). This line of optimization typ-\nically does not change the original model, thus is free\nof costly model training cost (note that a small amount\nof training for auxiliary models might be required, but\nthis cost can be ignored compared with the training cost\nfor original LLMs).\n\u2022 Model-level Optimization refers to designing an ef-\nficient model structure (i.e., efficient structure design)\nor compressing the pre-trained models (i.e., model\ncompression) in the inference process to improve its\nefficiency. This line of optimization (1) often requires\ncostly pre-training or a smaller amount of fine-tuning\ncost to retain or recover the model ability, and (2) is\ntypically lossy in the model performance.\n\u2022 System-level Optimization refers to optimizing the\ninference engine or the serving system. This line of opti-\nmization (1) does not involve the costly model training,\n\n5\nand (2) is typically lossless in model performance1. In\naddition, we provide a brief introduction for hardware\naccelerator design in Sec. 6.3.\n4\nDATA-LEVEL OPTIMIZATION\nIn the data level, prior studies can be divided into two\ncategories, i.e., input compression and output organization.\nInput compression techniques directly shorten the model in-\nput to reduce the inference cost. While output organization\ntechniques enable batch (parallel) inference via organizing\nthe structure of output content, which can improve the\nhardware utilization and reduce the generation latency.\n4.1\nInput Compression\nIn the practical application of LLMs, prompts are crucial.\nNumerous studies suggest new ways to design prompts\neffectively and show in practice that well-designed prompts\ncan unleash the capabilities of LLMs. For instance, In-\nContext Learning (ICL) [45] suggests to include multiple\nrelevant examples within the prompt. This approach en-\ncourages LLMs to learn through analogy. Chain-of-Thought\n(CoT) [14] proposes to incorporate a sequence of intermedi-\nate reasoning steps within the in-context examples, which\nhelp LLMs to conduct complex reasoning. However, these\nprompting techniques inevitably lead to longer prompts,\nwhich poses a challenge because the computational cost and\nmemory usage increase quadratically during the prefilling\nstage (as illustrated in Sec. 2.3).\nTo address this challenge, input prompt compres-\nsion [33] has been proposed to shorten prompts without\nsignificantly impacting the quality of answers from LLMs.\nWithin this field, relevant studies are categorized into four\ngroups, as depicted in Figure 5: prompt pruning, prompt\nsummary, soft prompt-based compression, and retrieval-\naugmented generation.\n4.1.1\nPrompt Pruning\nThe core idea behind the prompt pruning is to remove\nunimportant tokens, sentences, or documents online from\neach input prompt based on predefined or learnable impor-\ntance indicators. DYNAICL [38] proposes to dynamically\ndecide the optimal number of in-context examples for a\ngiven input based on the computational budget via a well-\ntrained LLM-based meta controller. Selective Context [39]\nproposes to merge tokens into units, and then applies a\nunit-level prompt pruning based on the self-information\nindicator (i.e., negative log likelihood). STDC [40] prunes\nthe prompts based on the parse tree, which iteratively\nremoves phrase nodes that cause the smallest performance\ndrop after pruning it. PCRL [41] introduces a token-level\npruning scheme based on reinforcement learning. The main\nidea behind PCRL is to train a policy LLM by combining\nfaithfulness and compression ratio into the reward func-\ntion. Faithfulness is measured as the output similarity be-\ntween the compressed prompt and the original prompt.\nRECOMP [36] implements a sentence-level pruning strategy\n1. A recent study [28] shows that FlashAttention, a common-used\nsystem-level optimization technique, might cause the numeric devia-\ntion.\nto compress prompts for Retrieval-Augmented Language\nModels (RALMs). The approach involves encoding the in-\nput question and documents into latent embeddings using\na pre-trained encoder. Then, it decides which documents\nto remove based on the similarity of their embeddings\nwith the question\u2019s embedding. LLMLingua [42] introduces\na coarse-to-fine pruning scheme for prompt compression.\nInitially, it performs a demonstration-level pruning followed\nby token-level pruning based on perplexity. To enhance\nperformance, LLMLingua proposes a budget controller that\ndynamically allocates the pruning budget across different\nparts of prompts. Additionally, it utilizes an iterative token-\nlevel compression algorithm to address inaccuracies in-\ntroduced by conditional independence assumptions. Fur-\nthermore, LLMLingua incorporates a distribution align-\nment strategy to align the output distribution of the target\nLLM with a smaller LLM used for perplexity calculation.\nLongLLMLingua [43] builds upon LLMLingua with several\nenhancements: (1) It utilizes perplexity conditioned on the\ninput question as the indicator for prompt pruning. (2) It\nallocates varying pruning ratios to different demonstrations\nand reorders the demonstrations within the final prompt\nbased on their indicator values. (3) It restores the original\ncontent based on the response. CoT-Influx [44] introduces\na coarse-to-grained pruning method for Chain-of-Thought\n(CoT) prompts using reinforcement learning. Specifically, it\nprunes unimportant examples, followed by pruning unim-\nportant tokens within the remaining examples.\n4.1.2\nPrompt Summary\nThe core idea of prompt summary is to condense the\noriginal prompt into a shorter summary while preserving\nsimilar semantic information. These techniques also serve as\nonline compression methods for prompts. In contrast to the\naforementioned prompt pruning techniques that preserve\nthe unpruned tokens unchanged, this line of methods con-\nverts the entire prompt into its summation. RECOMP [36]\nintroduces an Abstractive Compressor that takes an input\nquestion and retrieved documents as input, and produces\na concise summary. Specifically, it distills a lightweight\ncompressor from the extreme-scale LLMs to perform the\nsummary. SemanticCompression [37] proposes a semantic\ncompression method. It starts by breaking down the text\ninto sentences. Next, it groups sentences together by topic\nand then summarizes the sentences within each group.\n4.1.3\nSoft Prompt-based Compression\nThe core idea of this kind of compression techniques is to\ndesign a soft prompt, significantly shorter than the orig-\ninal prompt, for use as input to LLMs. The soft prompt\nis defined as a sequence of learnable continuous tokens.\nSome techniques adopt offline compression for the fixed\nprefix prompt (e.g., system prompt, task-specific prompt).\nFor example, PromptCompression [33] trains a soft prompt\nto emulate a predetermined system prompt. The approach\ninvolves adding several soft tokens before the input tokens\nand enabling these soft tokens to be adjusted during back-\npropagation. Following fine-tuning on the prompt dataset,\nthe sequence of soft tokens serves as the soft prompt.\nGisting [34] introduces a method to condense task-specific\n\n6\nInput\nCompression\nRetrieval-Augmented\nGeneration\nRAG [29], FLARE [30], REPLUG [31], Self-\nRAG [32]\nSoft Prompt-based\nCompression\nPromptCompression [33], Gisting [34], Auto-\nCompressors [30], ICAE [35]\nPrompt Summary\nRECOMP [36], SemanticCompression [37]\nPrompt Pruning\nDYNAICL [38], Selective Context [39],\nSTDC [40], PCRL [41], RECOMP [36], LLM-\nLingua [42], LongLLMLingua [43], CoT-\nInflux [44]\nFig. 5. Taxonomy of the input compression methods for Large Language Models.\nprompts into a concise set of gist tokens using prefix-\ntuning [46]. Given that task-specific prompts differ across\ntasks, prefix-tuning is applied individually for each task.\nTo enhance efficiency, Gisting further introduces a meta-\nlearning approach that predicts gist tokens for new unseen\ntasks based on the gist tokens of previous tasks.\nOther techniques adopt online compression for every\nnew input prompts. For instance, AutoCompressors [30]\ntrain a pre-trained LM to compress the prompts into sum-\nmary vectors via unsupervised learning. ICAE [35] trains\nan autoencoder to compress the original context into short\nmemory slots. Specifically, ICAE employs a LoRA-adapted\nLLM as the encoder, and uses the target LLM as the decoder.\nA set of memory tokens is added before the input tokens\nand encoded into memory slots.\n4.1.4\nRetrieval-Augmented Generation\nRetrieval-Augmented Generation (RAG) [29] aims to im-\nprove the quality of LLMs\u2019 responses by incorporating exter-\nnal knowledge sources. RAG can be also viewed as a tech-\nnique to improve the inference efficiency when handling a\nlarge amount of data. Instead of merging all information\ninto an excessively long prompt, RAG only adds relevant\nretrieved information to the original prompt, ensuring that\nthe model receives necessary information while reducing\nprompt length significantly. FLARE [30] uses predictions of\nupcoming sentences to proactively decide when and what\ninformation to retrieve. REPLUG [31] treats the LLM as a\nblack box and augments it with a tuneable retrieval model.\nIt prepends retrieved documents to the input for the frozen\nblack-box LLM, and further utilizes the LLM to supervise\nthe retrieval model. Self-RAG [32] enhances LLM\u2019s quality\nand factuality through retrieval and self-reflection. It intro-\nduces reflection tokens to make the LLM controllable during\nthe inference phase.\n4.2\nOutput Organization\nThe traditional generation process of LLMs is entirely se-\nquential, leading to significant time consumption. Output\norganization techniques aim to (partially) parallelize gener-\nation via organizing the structure of output content.\nSkeleton-of-Thought (SoT) [47] is pioneering in this di-\nrection. The core idea behind SoT is to leverage the emerg-\ning ability of LLMs to plan the output content\u2019s struc-\nture. Specifically, SoT consists of two main phases. In the\nfirst phase (i.e., skeleton phase), SoT instructs the LLM to\ngenerate a concise skeleton of the answer using a prede-\nfined \u201dskeleton prompt.\u201d For instance, given a question like\n\u201dWhat are the typical types of Chinese dishes?\u201d, the output\nat this stage would be a list of dishes (e.g., noodles, hot\npot, rice) without elaborate descriptions. Then, in the second\nphase (i.e., point-expanding phase), SoT instructs the LLM\nto expand each point in the skeleton simultaneously using\na \u201dpoint-expanding prompt,\u201d and then concatenates these\nexpansions to form the final answer. When applied to open-\nsource models, point-expanding can be performed through\nbatch inference, which optimizes hardware utilization and\nreduces overall generation latency using the same compu-\ntational resources. To mitigate the additional computation\nWhat are the \ntypical types of \nChinese dishes?\n1. Noodles\n2. Hot pot\n3. Rice\n\u2026\n1. Noodles: Various noodle \ndishes, such as \u2026\n2. Hot pot: A communal pot \nof simmering broth at the \ncenter of the table \u2026\n3. Rice: Fried Rice, \nYangzhou Fried Rice, and \nother rice-based dishes \u2026\n\u2026\n(a) The skeleton stage\n(b) The point-expanding stage\nFig. 6. Demonstration of the inference process of SoT.\noverhead brought by the extra prompt (i.e., skeleton prompt\nand point-expanding prompt), SoT discusses the possibility of\nsharing the KV cache of the common prompt prefix across\nmultiple points in the point expansion phase. Additionally,\nSoT uses a router model to decide whether applying SoT is\nappropriate for specific questions, aiming to limit its use to\nsuitable cases. As a result, SoT achieves up to a 2.39\u00d7 speed-\nup on 12 recently released LLMs, and improves the answer\nquality for many questions by improving the diversity and\nrelevance of their answer.\nSGD [48] further extends the idea of SoT by organizing\nsub-problem points into a Directed Acyclic Graph (DAG)\nand answering the logic-independent sub-problems in par-\nallel in one turn. Similar to SoT, SGD also leverages the\nemerging ability of LLMs to generate the output structure\nby providing manually-crafted prompts along with several\nexamples. SGD relaxes the strict independence assumption\namong different points to enhance the quality of answers,\nespecially for math and coding problems. Compared with\nSoT, SGD prioritizes answer quality over speed. Addition-\nally, SGD introduces an adaptive model selection approach,\n\n7\nassigning an optimal model size to handle each sub-problem\nbased on its estimated complexity, thus further improving\nefficiency.\nAPAR [49] adopts a similar idea with SoT, leveraging\nLLMs to output special control tokens (i.e., [fork]) for auto-\nmatically and dynamically triggering the parallel decoding.\nTo effectively exploit the inherent parallelizable structure\nwithin the output content and accurately generate control\ntokens, APAR fine-tunes the LLMs on carefully-designed\ndata that formed in specific tree structure. As a result, APAR\nachieves an average 1.4\u223c2.0\u00d7 speed-up on benchmarks and\ncases a negligible impact on the answer quality. Further-\nmore, APAR combines their decoding approach with the\nspeculative decoding technique (i.e., Medusa [50]) and serv-\ning system (i.e. vLLM [51]) to further improve the inference\nlatency and system throughput, respectively.\nSGLang [52] introduces a domain-specific language\n(DSL) in Python featuring primitives that flexibly facili-\ntate LLM programming. The core idea behind SGLang is\nto analyze dependencies among various generation calls\nautomatically, and perform batch inference and KV cache\nsharing based on this analysis. With this language, users can\nimplement various prompting strategies easily and benefit\nfrom the automatic efficiency optimization of SGLang (e.g.,\nSoT [47], ToT [53]). Furthermore, SGLang introduces and\ncombines several system-level compilation techniques, such\nas code movement and prefetching annotations.\n4.3\nKnowledge, Suggestions and Future Direction\nThe growing demand for LLMs to handle longer inputs and\ngenerate longer outputs highlights the importance of the\ndata-level optimization techniques. Within these techniques,\ninput compression methods primarily target enhancing the\nprefilling stage by diminishing the computational and mem-\nory cost resulting from the attention operation. Additionally,\nfor API-based LLMs, these methods can reduce the API cost\nassociated with input tokens. In contrast, output organiza-\ntion methods concentrate on optimizing the decoding stage\nby alleviating the substantial memory access cost associated\nwith auto-regressive decoding approach.\nAs LLMs become more and more capable, there is poten-\ntial to utilize them to compress the input prompts or struc-\nture the output content. Recent advancements in output\norganization methods [47], [48], [49] demonstrate the effec-\ntiveness of leveraging LLMs to organize the output content\ninto independent points or a dependency graph, facilitating\nbatch inference for improving generation latency. These\nmethods capitalize on the inherent parallelizable structure\nwithin output content, enabling LLMs to perform parallel\ndecoding to enhance hardware utilization and thereby re-\nduce end-to-end generation latency.\nRecently, diverse prompting pipelines (e.g., ToT [53],\nGoT [54]) and agent frameworks [55], [56], [57] are emerg-\ning. While these innovations enhance LLMs\u2019 capabilities,\nthey also extend the length of inputs, leading to increased\ncomputational cost. To address this challenge, adopting\ninput compression techniques to reduce input length shows\npromise as a solution. Simultaneously, these pipelines and\nframeworks naturally introduce more parallelism into out-\nput structures, offering increased potential for parallel de-\ncoding and key-value (KV) cache sharing across different\ndecoding threads. SGLang [52] supports flexible LLM pro-\ngramming and offers opportunities for front-end and back-\nend co-optimization, laying the groundwork for further\nextensions and improvements in this area. In summary,\ndata-level optimization, including input compression and\noutput organization techniques, would become increasingly\nnecessary to enhance efficiency in the foreseeable future.\nIn addition to optimizing the efficiency of existing frame-\nworks, certain studies focus on designing more efficient\nagent frameworks directly. For example, FrugalGPT [58]\nproposes a model cascade comprising LLMs of varying\nsizes, with the inference process being halted early if the\nmodel reaches a sufficient level of certainty regarding the\nanswer. This approach aims to achieve efficiency by leverag-\ning a tiered model architecture and intelligent inference ter-\nmination based on model confidence estimation. Compared\nwith model-level dynamic inference techniques (Sec. 5.2.5),\nFrugalGPT performs dynamic inference at the pipeline level.\n5\nMODEL-LEVEL OPTIMIZATION\nThe model-level optimization for LLM efficient inference\nmainly concentrates on optimizing the model structure or\ndata representation. Model structure optimization involves\ndirectly designing efficient model structure, modifying the\noriginal model and adjusting the inference-time architec-\nture. In terms of data representation optimization, the model\nquantization technique is commonly employed.\nIn this section, we categorize model-level optimization\ntechniques based on the additional training overhead they\nrequire. The first category involves designing more efficient\nmodel structures (referred to as efficient structure design).\nModels developed using this approach typically require\ntraining from scratch. The second category focuses on com-\npressing pre-trained models (referred to as model compres-\nsion). Compressed models in this category generally require\nonly minimal fine-tuning to restore their performance.\n5.1\nEfficient Structure Design\nCurrently, state-of-the-art LLMs commonly employ the\nTransformer architecture, as discussed in Section 2.1. How-\never, the key components of Transformer-based LLMs, in-\ncluding the Feed Forward Network (FFN) and attention\noperation, present efficiency challenges during inference.\nWe identify the causes as follows:\n\u2022 The FFN contributes a substantial portion of the model\nparameters in Transformer-based LLMs, resulting in\nsignificant memory access cost and memory usage,\nparticularly during the decoding stage. For instance, the\nFFN module accounts for 63.01% of the parameters in\nthe LLaMA-7B model and 71.69% in the LLaMA-70B\nmodel.\n\u2022 The attention operation demonstrates quadratic com-\nplexity in the input length, leading to substantial com-\nputational cost and memory usage, especially when\ndealing with longer input contexts.\nTo tackle these efficiency challenges, several studies have\nconcentrated on developing more efficient model structures.\nWe categorize these studies into three groups (as depicted in\nFig. 7): efficient FFN design, efficient attention design, and\nTransformer alternates.\n\n8\nEfficient\nStructure\nDesign\nTransformer Alternates\nOthers\nSGConv [59], CKConv [60], Hyena [61],\nRWKV [62], RetNet [63]\nSSM\nHiPPO [64], LSSL [65], S4 [66], DSS [67],\nS4D [68], GSS [69], H3 [70], Liquid S4 [71],\nS5 [72], BST [73], BiGS [74], Mamba [75],\nMambaFormer [76]\nEfficient Attention Design\nMulti/Group-\nQuery Attention\nMQA [77], GQA [78]\nLow-Complexity Attention\nLow-Rank\nAttention\nLinformer [79], LRT [80],\nFLuRKA [81],Luna [82],\nSet Transformer [83]\nKernel-based\nAttention\nLinear Transformer [84],\nPerformers [85], RFA [86],\nPolySketchFormer [87]\nEfficient FFN Design\nSwitch Transformers [88], MoEfication [89], MPOE [90], Sparse Upcy-\ncling [91], BASE [92], Expert Choice [93], SE-MoE [94], StableMoE [95], SMoE-\nDropout [96], GLaM [97], Mixtral 8x7B [12]\nFig. 7. Taxonomy of the efficient structure design for Large Language Models.\n5.1.1\nEfficient FFN Design\nIn this field, many studies concentrate on integrating the\nMixture-of-Experts (MoE) technique [98] into LLMs to en-\nhance their performance while maintaining the computa-\ntional cost. The core idea of MoE is to dynamically allocate\nvarying computational budgets to different input tokens. In\nMoE-based Transformers, multiple parallel Feed Forward\nNetworks (FFNs), namely experts, are utilized alongside a\ntrainable routing module. During inference, the model se-\nlectively activates specific experts for each token controlled\nby the routing module.\nSome researches concentrate on the construction of FFN\nexpert, which mainly focus on optimizing the process of\nacquiring expert weights or making these experts more\nlightweight for efficiency. For instance, MoEfication [89] de-\nvises a method to transform a non-MoE LLM into the MoE\nversion using its pre-trained weights. This approach elimi-\nnates the need for expensive pre-training of the MoE model.\nTo accomplish this, MoEfication first divides FFN neurons\nof the pre-trained LLM into multiple groups. Within each\ngroup, the neurons are commonly activated simultaneously\nby the activation function. Then, it restructures each group\nof neurons as an expert. Sparse Upcycling [91] introduces a\nmethod to initialize the weights of MoE-based LLM directly\nfrom a dense model\u2019s checkpoint. In this approach, the\nexperts within the MoE-based LLM are exact replicas of\nthe FFN from the dense model. By employing this straight-\nforward initialization, Sparse Upcycling can efficiently train\nthe MoE model to achieve high performance. MPOE [90]\nproposes to reduce the parameters of MoE-based LLMs\nthrough Matrix Product Operators (MPO) decomposition.\nThis method involves decomposing each weight matrix of\nthe FFN into a global shared tensor containing common\ninformation and a set of local auxiliary tensors that capture\nspecialized features.\nAnother line of researches focuses on improving the\ndesign of the routing module (or strategy) within MoE\nmodels. In previous MoE models, the routing module often\ncauses the load imbalance problem, which denotes that\nsome experts are assigned a large number of tokens while\nthe others handle only a few. This imbalance not only\nwastes the capacities of the under-utilized experts, which\ndegrades model performance, but also degrades the infer-\nence efficiency. Current MoE implementations [88], [99],\n[100] often use batched matrix multiplication to compute\nall FFN experts simultaneously. This requires that the input\nmatrices of each expert must have the same shape. However,\nsince the load imbalance problem exists, input token sets\nfor these under-utilized experts are needed to be padded to\nmeet the shape constraint, resulting in a waste of compu-\ntation. Therefore, the major aim of routing module design\nis achieving better balance in token assignment for MoE\nexperts. Switch Transformers [88] introduces an additional\nloss, namely the load balancing loss, into the final loss\nfunction to penalize imbalanced assignments by the routing\nmodule. This loss is formulated as the scaled dot-product\nbetween the token assignment fraction vector and a uniform\ndistribution vector. As a result, the loss is minimized only\nwhen the token assignment is balanced across all experts.\nThis approach encourages the routing module to distribute\ntokens evenly among experts, promoting load balance and\nultimately improving model performance and efficiency.\nBASE [92] learns an embedding for each expert in an end-\nto-end manner and then assigns experts to tokens based on\nthe similarity of their embeddings. To ensure load balance,\nBASE formulates a linear assignment problem and utilizes\nthe auction algorithm [101] to solve this problem efficiently.\nExpert Choice [93] introduces a simple yet effective strategy\nto ensure perfect load balance within MoE-based models.\nUnlike previous methods that assign experts to tokens,\nExpert Choice allows each expert to independently select\nthe top-k tokens based on their embedding similarities. This\napproach ensures that each expert handles a fixed number\nof tokens, even though each token might be assigned to a\ndifferent number of experts.\nIn addition to the aforementioned researches focusing\n\n9\non the model architecture itself, there are also studies that\nconcentrate on improving the training methods for MoE-\nbased models. SE-MoE [94] introduces a new auxiliary loss\ncalled the router z-loss, which aims to enhance the stability\nof model training without compromising performance. SE-\nMoE identifies that the exponential functions introduced by\nsoftmax operations in the routing module can exacerbate\nroundoff errors, leading to training instability. To address\nthis issue, the router z-loss penalizes large logits that are in-\nput into exponential functions, thereby minimizing roundoff\nerrors during training. StableMoE [95] points out the routing\nfluctuation problem existing in the MoE-based LLMs, which\ndenotes the inconsistency of the expert assignment in the\ntraining and inference stage. For the same input token, it is\nassigned to different experts along with training, but only\nactivates one expert at inference time. To address this issue,\nStableMoE suggests a more consistent training approach.\nIt first learns a routing strategy and then keeps it fixed\nduring both the model backbone training and the inference\nstage. SMoE-Dropout [96] designs a novel training method\nfor MoE-based LLMs, which proposes to gradually increase\nthe number of activated experts during the training process.\nThis approach enhances the scalability of MoE-based mod-\nels for inference and downstream fine-tuning. GLaM [97]\npre-trains and releases a series of models with various pa-\nrameter sizes, demonstrating their comparable performance\nto dense LLMs on few-shot tasks. The largest model in this\nfamily has a parameter size of up to 1.2 trillion. Mixtral\n8x7B [12] is a remarkable recently released open-source\nmodel. During inference, it utilizes only 13 billion active\nparameters and achieves superior performance compared\nto the LLaMA-2-70B model across different benchmarks.\nMixtral 8x7B consists of 8 Feed-Forward Network (FFN)\nexperts in each layer, with each token assigned to two\nexperts during inference.\n5.1.2\nEfficient Attention Design\nThe attention operation is a critical component in the Trans-\nformer architecture. However, its quadratic complexity in\nrelation to input length leads to substantial computational\ncost, memory access cost, and memory usage, especially\nwhen dealing with long contexts. To address this issue,\nresearchers are exploring more efficient approaches to ap-\nproximate the functionality of the original attention oper-\nation. These studies can be broadly categorized into two\nmain branches: multi-query attention and low-complexity\nattention.\nMulti-Query Attention. Multi-query attention (MQA) [77]\noptimizes the attention operation by sharing the key (K)\nand value (V) cache across different attention heads. This\nstrategy effectively reduces both memory access cost and\nmemory usage during inference, contributing to improved\nefficiency in Transformer models. As introduced in Sec. 2.2,\nthe Transformer-style LLMs typically adopts multi-head\nattention (MHA) operation. This operation requires stor-\ning and retrieving K and V pairs for each attention head\nduring the decoding stage, leading to substantial increases\nin memory access cost and memory usage. MQA tackles\nthis challenge by using the same K and V pairs across\ndifferent heads while maintaining distinct query (Q) values.\nThrough extensive testing, it has been demonstrated that\nMQA significantly reduces memory requirements with only\na minimal impact on model performance, making it a cru-\ncial strategy for enhancing inference efficiency. The concept\nof MQA is further extended by Grouped-query attention\n(GQA) [78], which can be seen as a blend of MHA and\nMQA. Specifically, GQA segments the attention heads into\ngroups, storing a single set of K and V values for each\ngroup. This method not only sustains the benefits of MQA\nin reducing memory overhead but also offers an enhanced\nbalance between inference speed and output quality.\nLow-Complexity\nAttention.\nLow-complexity\nattention\nmethods aim to design new mechanisms that reduce the\ncomputational complexity of each attention head. To sim-\nplify the discussion, we assume that the dimensions of the\nQ (query), K (key), and V (value) matrices are identical,\nwith Q, K, V \u2208Rn\u00d7d. Since the following work does not\ninvolve altering the number of attention heads like MQA,\nour discussions focus on the attention mechanism within\neach head. As introduced in Section 2.2, the computational\ncomplexity of the conventional attention mechanism scales\nas O(n2), exhibiting quadratic growth with respect to the in-\nput length n. To address the inefficiency issue, kernel-based\nattention and low-rank attention methods are proposed to\nreduce the complexity to O(n).\n\u2022 Kernel-based Attention. Kernel-based attention designs\nkernel \u03d5 to approximate the non-linear softmax oper-\nation of Softmax(QKT ) with a linear dot product be-\ntween kernel-transformed feature maps, i.e., \u03d5(Q)\u03d5(K)T .\nIt avoids the conventional quadratic computation associ-\nated with QKT \u2208Rn\u00d7n by prioritizing the computation\nof \u03d5(K)T V \u2208Rd\u00d7d, followed by its multiplication with\n\u03d5(Q) \u2208Rn\u00d7d. Specifically, the input Q and K matrices are\nfirst mapped into kernel space using a kernel function \u03d5,\nwhile maintaining their original dimensions. Leveraging\nthe associative property of matrix multiplication allows\nfor the multiplication of K and V prior to their interaction\nwith Q. The attention mechanism is reformulated as:\nSoftmax(QKT )V \u2248\u03d5(Q)(\u03d5(K)T V ),\n(5)\nwhere \u03d5(Q), \u03d5(K) \u2208Rn\u00d7d. This strategy effectively re-\nduces the computational complexity to O(nd2), render-\ning it linear with respect to the input length. Linear\nTransformer [84] is the first work to propose the kernel-\nbased attention. It adopts \u03d5(x) = elu(x) + 1 as the ker-\nnel function, where elu(\u00b7) denotes the exponential linear\nunit activation function. Performers [85] and RFA [86]\nproposes to use random feature projection to better ap-\nproximate the softmax function. PolySketchFormer [87]\nemploys polynomial functions and sketching techniques\nto approximate the softmax function.\n\u2022 Low-Rank Attention. Low-Rank Attention technique em-\nploys compression on the token dimensions (i.e., n) of the\nK and V matrices to a smaller, fixed length (i.e., k) before\nperforming the attention computation. The approach is\nbased on the insight that the n \u00d7 n attention matrix\noften exhibits a low-rank property, making it feasible to\ncompress it in the token dimension. The main focus of\nthis line of researches is to design effective methods for\nthe compression, where X can be context matrix or K and\nV matrices:\n\n10\nX \u2208Rn\u00d7d \u2192X\u2032 \u2208Rk\u00d7d.\n(6)\nOne line of work uses linear projection to compress the\ntoken dimension. It is done by multiplying K and V matri-\nces with projection matrices Pk, Pv \u2208Rk\u00d7n. In this way,\nthe computational complexity of the attention operation\nis reduced to O(nkd), which is linear to the input length.\nLinformer [79] first observes and analyses the low-rank\nproperty of the attention map, and proposes the low-rank\nattention framework. LRT [80] proposes to simultaneously\napply low-rank transformation to both attention block\nand FFN to further improve the computational efficiency.\nFLuRKA [81] combines the low-rank transformation and\nkernalization to the attention matrices to further improve\nthe efficiency. Specifically, it first reduces the token dimen-\nsion of K and V matrices, and then applies kernel function\nto the Q and low-rank K matrices.\nAside from linear projection, other token-dimension\ncompression methods are also proposed. Luna [82] and\nSet Transformer [83] leverage additional attention compu-\ntations alongside smaller queries to effectively compress\nthe K and V matrices. Luna [82] involves an extra query\nmatrix of fixed length k. The small query performs at-\ntention with the original context matrix, termed as pack\nattention, to compress the context matrix to size Rk\u00d7d.\nSubsequently, the regular attention, termed unpack atten-\ntion, applies attention to the original Q matrices and the\ncompressed K and V matrices. The extra query matrix\ncan be learnable parameters or acquired from previous\nlayers. Set Transformer [83] designs the similar technique\nby introducing an inducing points vector with fixed length.\nUnlike previous works that compress K and V, Funnel-\nTransformer [102] uses pooling operation to gradually\ncompress the sequence length of the Q matrix.\n5.1.3\nTransformer Alternates\nIn addition to applying efficient techniques to the attention\noperation, recent studies have also innovated to design\nsequence modeling architectures that are efficient yet ef-\nfective. Table 2 compares the efficiency of some represen-\ntative non-Transformer models. These architectures exhibit\nsub-quadratic computational complexity with respect to se-\nquence length during both training and inference, enabling\nLLMs to significantly increase their context length.\nWithin this research field, two prominent lines of study\nhave garnered significant attention. One line of studies con-\ncentrates on the State Space Model (SSM), which formulates\nsequence modeling as a recurrence transformation based\non the HiPPO theory [64]. Additionally, other studies pri-\nmarily focus on employing long convolutions or designing\nattention-like formulations to model sequences.\nState Space Model. The State Space Model (SSM) has\ndemonstrated competitive modeling capabilities in certain\nNatural Language Processing (NLP) [75] and and Computer\nVision (CV) [103] tasks. Compared to attention-based Trans-\nformers, SSM exhibits linear computational and memory\ncomplexity with respect to the input sequence length, which\nenhances its efficiency in handling long-context sequences.\nIn this survey, SSM refers to a series of model architectures\nthat satisfy the following two properties: (1) They model\nsequence based on the following formulation proposed by\nHiPPO [64] and LSSL [65]:\nxk = Axk\u22121 + Buk,\nyk = Cxk,\n(7)\nwhere A, B and C denote the transition matrices, x denotes\nthe intermediate state and u denotes the input sequence. (2)\nThey design the transition matrix A based on the HiPPO\ntheory [64]. Specifically, HiPPO proposes to compress the\ninput sequence into a sequence of coefficients (namely state)\nby projecting it onto a set of polynomial bases.\nBuilding upon the aforementioned framework, several\nstudies concentrate on improving the parameterization or\ninitialization of the transition matrix A. This involves re-\nfining how the matrix is formulated or initialized within\nthe SSM to enhance its effectiveness and performance in\nsequence modeling tasks. LSSL [65] firstly proposes to ini-\ntialize A with the optimal transition matrix HiPPO-LegS\ndesigned by HiPPO. In addition, LSSL also trains the SSM\nin a convolution manner by unrolling the Eq. 7. Specifically,\nthrough a convolution kernel defined as KL(A, B, C) =\n(CAiB)i\u2208[L] = (CB, CAB, ..., CAL\u22121B), the Eq. 7 can\nbe rewritten as y = KL(A, B, C) \u2217u and also can be\ncomputed efficiently via Fast Fourier Transform (FFT). How-\never, computing this convolution kernel is expensive, since\nit requires multiple times of multiplication by A. To this\nend, S4 [66], DSS [67] and S4D [68] propose to diagonalize\nthe matrix A, which can accelerate the computing. This can\nbe seen as a parameterization technique to the transition\nmatrix A. Previous SSMs processed each input dimension\nindependently, resulting in a large number of trainable\nparameters. To enhance efficiency, S5 [72] proposes to simul-\ntaneously process all input dimensions using a single set of\nparameters. Building upon this structure, S5 introduces a\nparameterization and initialization method for A based on\nthe standard HiPPO matrix. Liquid S4 [71] and Mamba [75]\nparameterize the transition matrices in a input-dependent\nmanner, which further enhances the modeling capability of\nSSM. Additionally, both S5 [72] and Mamba [75] adopt a\nparallel scan technique for efficient model training without\nthe need for convolution operations. This technique offers\nadvantages in implementation and deployment on modern\nGPU hardware.\nAnother line of research aim to design better model\narchitecture based on SSMs. GSS [69] and BiGS [74] com-\nbines the Gated Attention Unit (GAU) [104] with SSM.\nSpecifically, they replace the attention operation in GAU\nwith SSM operation. BST [73] combines the SSM model\nwith the proposed Block Transformer which introduces a\nstrong local inductive bias. H3 [70] observes that SSM is\nweak in recalling the earlier tokens and comparing a token\nacross the sequence. To this end, it proposes to add a shift\nSSM operation before the standard SSM operation, which\nis used to directly shift the input tokens into the state.\nMambaFormer [76] combines the standard Transformer and\nSSM model by substituting the FFN layer in the Trans-\nformer with an SSM layer. Jamba [105] introduces another\napproach to combining the Transformer and SSM models by\nadding four Transformer layers into an SSM model. Dense-\nMamba [106] explores the issue of hidden state degradation\n\n11\nTABLE 2\nEfficiency comparison of some novel non-Transformer models. Note that we denote n as the input length and d as the input dimension.\nModel\nTraining Form\nTraining Computational\nComplexity\nTraining Memory\nComplexity\nInference Form\nInference Computational Complexity\nPrefilling\nDecoding (per token)\nTransformer [26]\nTransformer-like\nO(n2d)\nO(n2 + nd)\nTransformer-like\nO(n2d)\nO(nd)\nS4 [66]\nConvolution\nO(nd2 log n)\nO(nd)\nRecurrence\nO(nd2)\nO(d2)\nMamba [75]\nRecurrence\nO(nd2 log n)\nO(nd)\nRecurrence\nO(nd2)\nO(d2)\nHyena [61]\nConvolution\nO(nd log n)\nO(nd)\nConvolution\nO(nd log n)\nO(nd log n)\nRetNet [63]\nTransformer-like\nO(n2d)\nO(n2 + nd)\nRecurrence\nO(nd2)\nO(d2)\nRWKV [62]\nRecurrence\nO(nd2)\nO(nd)\nRecurrence\nO(nd2)\nO(d2)\nin traditional SSMs and introduces dense connections within\nthe SSM architecture to preserve fine-grained information\nacross deeper layers of the model. BlackMamba [107] and\nMoE-Mamba [108] propose to enhance SSM models with the\nMixture-of-Experts (MoE) technique to optimize the train-\ning and inference efficiency while maintaining the model\nperformance.\nOther Alternates. In addition to SSMs, several other efficient\nalternates have also garnered significant attention, including\nlong convolution and attention-like recurrence operation.\nSeveral recent studies have applied long convolution\nin the context of modeling long sequences [59], [60], [61].\nThese investigations primarily concentrate on refining the\nparameterization of the convolution kernel. For instance,\nHyena [61] employs an data-dependent parameterization\nmethod for long convolutions using a shallow feed-forward\nneural network (FFN).\nOther studies [62], [63] aim to design the operation that\nhas a similar form as the attention operation but can be\nenrolled to the recurrent manner, enabling both efficient\ntraining and efficient inference. For instance, RWKV [62]\nbuilds upon AFT [109], which proposes to substitute the\nattention operation in the Transformer model with the fol-\nlowing equation:\nYt = \u03c3q(Qt) \u2299\nPT\nt\u2032=1 exp(Kt\u2032 + wt,t\u2032) \u2299Vt\u2032\nPT\nt\u2032=1 exp(Kt\u2032 + wt,t\u2032)\n,\n(8)\nwhere Q, K, and V are the query, key, and value matrices\nas in Transformer, w \u2208RT \u00d7T denotes a learnable pair-\nwise position bias and \u03c3q(\u00b7) denotes a non-linear function.\nSpecifically, it further reparameterizes the position bias as\nwt,t\u2032 = \u2212(t \u2212t\n\u2032)w, and thus can rewrite Eq. 8 in a recursive\nform. In this way, RWKV can combine the effective paral-\nlelizable training feature of Transformer and the efficient\ninference ability of RNN.\nEfficiency Analysis. We analyze and compare the com-\nputational and memory complexity of several innovative\nand representative non-transformer architectures in Table 2.\nIn terms of training time, many studies (e.g., S4, Hyena,\nRetNet) aim to preserve training parallelism by adopting\ntraining forms such as the convolution or attention. Notably,\nMamba utilizes parallel scan techniques for processing input\nsequences, thereby leveraging training parallelism as well.\nOn the other hand, during inference, most studies opt\nfor recurrent architectures to maintain linear computational\ncomplexity in the prefilling stage and to remain context\nlength-agnostic in the decoding stage. Furthermore, in the\ndecoding phase, these novel architectures eliminate the need\nto cache and load features of previous tokens (similar to the\nkey-value cache in Transformer-based language models),\nresulting in significant memory access cost savings.\n5.2\nModel Compression\nModel compression encompasses a range of techniques de-\nsigned to enhance the inference efficiency of a pre-trained\nmodel by modifying its data representation (e.g., quantiza-\ntion) or altering its architecture (e.g., sparsification, struc-\ntural optimization, and dynamic inference), as depicted in\nFig. 8.\n5.2.1\nQuantization\nQuantization is a widely employed technique that reduces\nthe computational and memory cost of LLMs by converting\nthe models\u2019 weights and activations from high bit-width to\nlow bit-width representations. Specifically, many methods\ninvolve quantizing FP16 tensors into low-bit integer tensors,\nwhich can be represented as follows:\nXINT =\n\u0014XFP16 \u2212Z\nS\n\u0015\n,\n(9)\nS = max(XFP16) \u2212min(XFP16)\n2N\u22121 \u22121\n,\n(10)\nwhere XFP16 denotes the 16-bit floating-point (FP16) value,\nXINT denotes the low-precision integer value, N denotes\nthe number of bits, and S and Z denote the scaling factor\nand zero-point.\nIn the following, we start with an efficiency analysis to\nillustrate how quantization techniques reduce the end-to-\nend inference latency of LLMs. Subsequently, we offer a de-\ntailed introduction to two distinct quantization workflows:\nPost-Training Quantization (PTQ) and Quantization-Aware\nTraining (QAT), respectively.\nEfficiency Analysis. As discussed in Section 2.2, the infer-\nence process of LLMs involves two stages: the prefilling\nstage and the decoding stage. During the prefilling stage,\nLLMs typically handle long token sequences, and the pri-\nmary operation is general matrix multiplication (GEMM).\nThe latency of the prefilling stage is primarily constrained\nby the computation performed by high-precision CUDA\nCores. To address this challenge, existing methods quan-\ntize both weights and activations to accelerate computation\nusing low-precision Tensor Cores. As illustrated in Figure 9\n(b), activation quantization is performed online before each\nGEMM operation, allowing computation with low-precision\n\n12\nModel\nCompression\nDynamic Inference\nToken-level\nCALM [110], SkipDecode [111]\nSample-level\nFastBERT [112], MPEE [113], Global Past-\nFuture Early Exit [114], DeeBERT [115],\nPABEE [116],HASHEE [117]\nKnowledge Distillation\nBlack-box KD\nMultitask-ICT [118], MCKD [119], Distilling\nStep-by-Step [120], SCoTD [121], CoT Prompt-\ning [122], MCC-KD [123], Fine-tune-CoT [124],\nSocratic CoT [125], PaD [126], SCOTT [127],\nDISCO [128], LaMini-LM [129], Lion [130]\nWhite-box KD\nMiniLLM [131], GKD [132], TED [133], BabyL-\nlama [134], MiniMoE [135], DynaBERT [136],\nKPTD [137]\nStructure Optimization\nNeural Architecture Search\nAutoTinyBERT [138], NAS-BERT [139], Struc-\nture pruning via NAS [140], LiteTransform-\nerSearch [141], AutoDistil [142]\nStructure Factorization\nLoRD [143], TensorGPT [144], LoSparse [145],\nLPLR [146], ZeroQuant-V2 [147], DS-\nFormer [148], ASVD [149]\nSparsification\nSparse Attention\nSparse Transformer [150], StreamingLLM [151],\nLongformer [152], Bigbird [153], Structured\nSparse Attention [154], SemSA [155], Spat-\nten [156], SeqBoat [157], Adaptively Sparse\nAttention [158], Reformer [159], Sparse Flash\nAttention [160], Routing Transformer [161],\nSparse Sinkhorn Attention [162], H2O [163],\nDiffuser [164]\nWeight Pruning\nSparseGPT [165], Wanda [166], ISC [167],\nPrune and Tune [168], OWL [169], BESA [170],\noBERT [171], FastPruning [172], RIA [173],\nLLM-Pruner [174], Sheared LLaMA [175],\nZipLM [176], LoRAPrune [177], LoRAS-\nhear [178], SliceGPT [179], PLATON [180],\nCoFi [181], SIMPLE [182], ExpertSpar-\nsity [183], SEER-MoE [184], Pruner-Zero [185],\nDS\u00d8T [186]\nQuantization\nQuantization-\naware Training\nLLM-QAT [187], Norm Tweaking [188],\nQLoRA [189], QA-LoRA [190], LoftQ [191]\nPost-Training Quantization\nGPTQ [192], LUT-GEMM [193], AWQ [194],\nOWQ [195], SpQR [196], SqueezeLLM [197],\nQuIP [198], FineQuant [199], QuantEase [200],\nLLM-MQ [201], ZeroQuant [202], Flex-\nGen [203], LLM.int8() [204], Smoothquant\n[205], ZeroQuant-V2 [206], RPTQ [207],\nOliVe [208], OS+ [169], ZeroQuant-FP [209],\nOmniquant [210], QLLM [211], ATOM [212],\nLLM-FP4 [187], BiLLM [213], Li et.al. [214],\nAffineQuant [215], QuIP [198], QuIP# [216],\nQuaRot [217], SpinQuant [218]\nFig. 8. Taxonomy of model compression methods for Large Language Models.\nTensor Cores (e.g., INT8). This quantization approach is\nreferred to as Weight-Activation Quantization.\nIn contrast, during the decoding stage, LLMs process\nonly one token at each generation step using general matrix-\nvector multiplication (GEMV) as the core operation. The\nlatency of the decoding stage is mainly influenced by the\nloading of large weight tensors. To tackle this challenge,\nexisting methods focus on quantizing only the weights to\naccelerate memory access. This method, known as Weight-\nonly Quantization, involves offline quantization of weights,\nfollowed by de-quantization of the low-precision weights\ninto FP16 format for computation, as shown in Figure 9 (a).\nPost-Training\nQuantization.\nPost-training\nquantization\n(PTQ) involves quantizing pre-trained models without the\nneed for retraining, which can be a costly process. While\nPTQ methods have been well-explored for smaller mod-\nels, applying existing quantization techniques directly to\nLLMs presents challenges. This is primarily because the\nweights and activations of LLMs often exhibit more outliers\nand have a wider distribution range compared to smaller\n\n13\nTABLE 3\nSummary of the representative studies on Post-Training Quantization. Quantized Tensor Type denotes which parts of tensors are quantized. Data\nFormat denotes whether to adopt uniform or non-uniform quantization. Quantization Parameter Determination Scheme denotes the how to decide\nthe parameters (e.g., scaling factor, zero-point). Quantized Value Update denotes whether to change the model weight (e.g., compensation,\nre-parameterization) during the quantization process.\nModel\nQuantized Tensor Type\nData\nFormat\nQuantization Parameter\nDetermination Scheme\nQuantized\nValue Update\nWeight\nActivation\nKV Cache\nGPTQ [192]\n\u2713\nUniform\nStatistic-based\n\u2713\nLUT-GEMM [193]\n\u2713\nNon-uniform\nStatistic-based\nAWQ [194]\n\u2713\nUniform\nSearch-based\n\u2713\nSqueezeLLM [197]\n\u2713\nNon-uniform\nStatistic-based\nLLM.int8() [204]\n\u2713\n\u2713\nUniform\nStatistic-based\nSmoothQuant [205]\n\u2713\n\u2713\nUniform\nStatistic-based\n\u2713\nRPTQ [207]\n\u2713\n\u2713\nUniform\nStatistic-based\nOmniQuant [210]\n\u2713\n\u2713\nUniform\nSearch-based\nFlexGen [203]\n\u2713\n\u2713\nUniform\nStatistic-based\nAtom [212]\n\u2713\n\u2713\n\u2713\nUniform\nStatistic-based\nKVQuant [219]\n\u2713\nNon-uniform\nStatistic-based\nKIVI [220]\n\u2713\nUniform\nStatistic-based\nFP16\nGEMM/GEMV\nFP16\nAccumulator\nActivation\n(FP16)\n(a) Weight-only Quantization\nWeight\n(INT8)\nActivation\n(FP16)\nDe-quantize\nBias\n(INT8)\nDe-quantize\nINT8\nGEMM/GEMV\nINT8\nAccumulator\nActivation\n(FP16)\n(b) Weight-Activation Quantization\nWeight\n(INT8)\nActivation\n(FP16)\nQuantization\nDe-quantize\nINT32\nBias\n(INT8)\nFig. 9. (a) The inference workflow of Weight-only Quantization. (b) The\ninference workflow of Weight-Activation Quantization.\nmodels, making their quantization more challenging. In\nsummary, the complex nature of LLMs, characterized by\ntheir size and complexity, requires specialized approaches\nto effectively handle the quantization process. The presence\nof outliers and wider distribution ranges in LLMs necessi-\ntates the development of tailored quantization techniques\nthat can account for these unique characteristics without\ncompromising model performance or efficiency.\nNumerous studies have concentrated on developing\neffective quantization algorithms to compress LLMs. We\nprovide a synthesis of representative algorithms categorized\nacross four dimensions in Tab. 3. Regarding the types of\nquantized tensors, certain studies [192], [193], [194], [197]\nconcentrate on weight-only quantization, whereas many\nothers [204], [205], [207] focus on quantizing both weights\nand activations. Notably, in LLMs, the KV cache represents\na distinctive component that impacts memory and memory\naccess. Consequently, some investigations [203], [212], [219]\npropose KV cache quantization. Regarding data formats,\nthe majority of algorithms adopt a uniform format for\nstraightforward hardware implementation. Concerning the\ndetermination of quantized parameters (e.g., scale, zero-\npoint), most studies rely on statistics derived from weight or\nactivation values. Nevertheless, some research efforts [194],\n[210] advocate for searching optimal parameters based on\nreconstruction loss. Furthermore, certain studies [192], [194],\n[205] suggest updating unquantized weights (referred to as\nQuantized Value Update) before or during the quantization\nprocess to enhance performance.\nIn weight-only quantization, GPTQ [192] represents an\nearly advancement in LLM quantization, building upon the\ntraditional algorithm OBQ [221]. OBQ utilizes an optimal\nquantization order per row of the weight matrix, guided\nby the reconstruction error relative to the Hessian matrix\nof unquantized weights. After each quantization step, OBQ\niteratively adjusts the unquantized weights to mitigate re-\nconstruction errors. However, the frequent updating of the\nHessian matrix during quantization escalates computational\ncomplexity. GPTQ streamlines this process by adopting a\nuniform left-to-right order for quantizing each row, thus cir-\ncumventing the need for extensive Hessian matrix updates.\nThis strategy substantially reduces computational demands\nby computing the Hessian matrix solely during the quanti-\nzation of one row, then leveraging the computing results\nfor subsequent rows, expediting the overall quantization\nprocedure. LUT-GEMM [193] presents a novel dequantiza-\ntion method utilizing a Look-Up Table (LUT), aiming to\naccelerate the inference process of quantized LLMs by re-\nducing the dequantization overhead. Additionally, it adopts\na non-uniform quantization approach known as Binary-\nCoding Quantization (BCQ), which incorporates learnable\nquantization intervals. AWQ [194] observes that weight\nchannels vary in importance for performance, particularly\nemphasizing those aligned with input channels exhibiting\noutliers in activations. To enhance the preservation of crit-\nical weight channels, AWQ utilizes a reparameterization\nmethod. This technique selects reparameterization coeffi-\ncients via grid search to minimize reconstruction errors\neffectively. OWQ [195] observes the difficulty of quantiz-\ning weights associated with activation outliers. To address\nthis challenge, OWQ employs a mixed-precision quanti-\nzation strategy. This method identifies weak columns in\n\n14\nthe weight matrix and allocates higher precision to these\nspecific weights, while quantizing the rest of the weights at a\nlower precision level. SpQR [196] introduces a methodology\nwhere weight outliers are identified and allocated higher\nprecision during quantization, while the rest of the weights\nare quantized to 3 bits. SqueezeLLM [197] proposes to store\nthe outliers in a full-precision sparse matrix, and apply\nnon-uniform quantization to the remaining weights. The\nvalues for non-uniform quantization are determined based\non quantization sensitivity, which contributes to improved\nperformance of the quantized model. QuIP [198] introduces\nLDLQ, an optimal adaptive method for a quadratic proxy\nobjective. The study reveals that ensuring incoherence be-\ntween weight and Hessian matrices can enhance the ef-\nfectiveness of LDLQ. QuIP utilizes LDLQ and achieves\nincoherence by employing random orthogonal matrix mul-\ntiplication. FineQuant [199] utilizes a heuristic approach\nto determine the granularity of quantization per column,\ncombining empirical insights gained from experiments to\ndesign a quantization scheme. QuantEase [200] builds upon\nGPTQ. When quantizing each layer, it proposes a method\nbased on Coordinate Descent to compensate for the un-\nquantized weights more precisely. Additionally, QuantEase\ncan leverage quantized weights from GPTQ as an initial-\nization and further refine the compensation process. LLM-\nMQ [201] protects the weight outliers with FP16 format,\nand stores them in Compressed Sparse Row (CSR) format\nfor efficient computation. Besides, LLM-MQ models the bit-\nwidth assignment to each layer as an integer programming\nproblem, and employs an efficient solver to solve it within a\nfew seconds. Moveover, LLM-MQ designs a efficient CUDA\nkernel to integrate dequantization operators, thereby reduc-\ning memory access cost during computation. Inspired by\nthe equivalent transformations used in the previous PTQ\nmethods, AffineQuant [215] firstly introduces equivalent\naffine transformations in quantization, which extends the\noptimization scope and further reduces the quantization er-\nrors. Recently, many studies [198], [216], [217], [218] follows\nthe computational invariance idea, by multiplying rotation\nmatrices to the weight matrices and activation matrices. In\nthis way, they can effectively eliminate the outliers in the\nweights and activations, thus help to quantize the LLMs.\nThese studies use different rotation matrices. For example,\nQuaRot [217] applies randomize Hadamard transformations\nto the weights and activations. SpinQuant [218] finds the\noptimal rotation matrices by training on a small validation\ndataset.\nFor weight-activation quantization, ZeroQuant [202] em-\nploys finer-grained quantization for weights and activa-\ntions, leveraging kernel fusion to minimize the memory\naccess cost during quantization and conducting layer-by-\nlayer knowledge distillation to recover the performance.\nFlexGen [203] quantizes weights and KV cache directly\ninto INT4 to reduce the memory footprint during infer-\nence with large batch sizes. LLM.int8() [204] identifies that\noutliers in activations are concentrated within a small sub-\nset of channels. Leveraging this insight, LLM.int8() splits\nactivations and weights into two distinct parts based on\nthe outlier distribution within input channels to minimize\nquantization errors in activations. Channels containing out-\nlier data in both activations and weights are stored in\nFP16 format, while other channels are stored in INT8\nformat. SmoothQuant [205] employs a reparameterization\ntechnique to address the challenges of quantizing activa-\ntion values. This method introduces a scaling factor that\nexpands the data range of weight channels while shrinking\nthe data range of corresponding activation channels. Zero-\nQuant [202] introduces a group-wise quantization strategy\nfor weights and a token-wise quantization approach for\nactivations. Building upon this methodology, ZeroQuant-\nV2 [206] presents the LoRC (Low-Rank Compensation) tech-\nnique, employing low-rank matrices to mitigate quantiza-\ntion inaccuracies. RPTQ [207] identifies substantial varia-\ntions in the distribution of different activation channels,\nwhich present challenges for quantization. To mitigate this\nissue, RPTQ reorganizes channels with similar activation\ndistributions into clusters and independently applies quan-\ntization within each cluster. OliVe [208] observes that the\nnormal values neighboring to the outliers are less critical.\nTherefore, it pairs each outlier with a normal value, sacri-\nficing the latter to achieve a broader representation range\nfor outliers. OS+ [169] observes that the distribution of out-\nliers is concentrated and asymmetrical, posing a challenge\nto LLM quantization. To address this, OS+ introduces a\nchannel-wise shifting and scaling technique aimed at allevi-\nating these challenges. The shifting and scaling parameters\nare determined through a search process to effectively han-\ndle the concentrated and asymmetrical outlier distribution.\nZeroQuant-FP [209] investigates the feasibility of quantizing\nweight and activation values into FP4 and FP8 formats.\nThe study reveals that quantizing activations into floating-\npoint types (FP4 and FP8) produces superior results com-\npared to integer types. Omniquant [210] diverges from prior\napproaches that rely on empirical design of quantization\nparameters. Instead, it optimizes the boundaries for weight\nclipping and the scaling factor for equivalent transformation\nto minimize quantization errors. QLLM [211] addresses\nthe impact of outliers on quantization by implementing\nchannel reassembly. Additionally, it introduces learnable\nlow-rank parameters to minimize quantization errors in\nthe post-quantized model. Atom [212] employs a strategy\ninvolving mixed-precision and dynamic quantization for\nactivations. Notably, it extends this approach to quantize the\nKV cache into INT4 to enhance throughput performance.\nLLM-FP4 [187] endeavors to quantize the entire model\ninto FP4 format and introduces a pre-shifted exponent\nbias technique. This approach combines the scaling factor\nof activation values with weights to address quantization\nchallenges posed by outliers. BiLLM [213] represents one\nof the lowest-bit PTQ efforts to date. BiLLM identified the\nbell-shaped distribution of weights and the exceptionally\nlong-tail distribution of weights\u2019 Hessian matrix. Based on\nthis, it proposes to categorize weights into salient and non-\nsalient values structurally based on the Hessian matrix\nand binarizes them separately. As a result, BiLLM can\nextensively quantize LLMs to 1.08 bits without significant\ndegradation in perplexity. KVQuant [219] proposes a non-\nuniform quantization scheme for KV cache quantization, by\nderiving the optimal datatypes offline on a calibration set.\nKIVI [220] proposes a tuning-free 2bit KV cache quantiza-\ntion algorithm, which utilizes per-channel quantization for\nkey cache and per-token quantization for value cache in a\n\n15\nTABLE 4\nComparison of speed-ups in different scenarios (e.g., model size, batch size, input context length, inference framework) with W4A16 quantization\nbased on TensorRT-LLM [222] and LMDeploy [223] framework, respectively. We test the speed-ups of prefilling/decoding/end-to-end latency on a\nsingle NVIDIA A100 GPU. OOM denotes \u201cOut Of Memory\u201d.\nTensorRT-LLM\nB\n128\n256\n512\n1024\n2048\nLLaMA-2-7B\n1\n1.06/2.40/2.37\n0.90/2.38/2.34\n0.92/2.30/2.28\n0.88/2.19/2.17\n0.91/2.00/1.98\n2\n0.88/2.10/2.05\n0.91/2.07/2.04\n0.89/2.01/1.98\n0.91/1.92/1.89\n0.88/1.78/1.76\n4\n0.92/1.72/1.67\n0.89/1.67/1.64\n0.90/1.61/1.58\n0.87/1.53/1.51\n0.84/1.42/1.40\n8\n0.91/1.43/1.36\n0.88/1.38/1.33\n0.83/1.33/1.28\n0.77/1.25/1.21\n0.78/1.16/1.14\n16\n0.91/1.43/1.36\n0.88/1.38/1.33\n0.83/1.33/1.28\n0.77/1.25/1.21\n0.78/1.16/1.14\nB\n128\n256\n512\n1024\n2048\nLLaMA-2-13B\n1\n1.24/2.51/2.50\n0.89/2.45/2.47\n0.94/2.34/2.42\n0.90/2.18/2.32\n0.83/1.94/2.16\n2\n0.90/2.51/2.50\n0.95/2.45/2.47\n0.90/2.34/2.42\n0.83/2.18/2.32\n0.80/1.94/2.16\n4\n0.96/1.80/1.76\n0.91/1.78/1.74\n0.83/1.73/1.69\n0.80/1.65/1.62\n0.83/1.54/1.52\n8\n0.91/1.86/1.77\n0.83/1.81/1.73\n0.80/1.73/1.66\n0.82/1.62/1.56\n0.75/1.46/1.41\n16\n0.84/1.84/1.69\n0.81/1.77/1.63\n0.82/1.63/1.53\n0.78/1.46/1.39\nOOM\nLMDeploy\nB\n128\n256\n512\n1024\n2048\nLLaMA-2-7B\n1\n1.30/2.11/2.09\n0.94/2.07/2.05\n0.90/2.03/2.02\n0.88/1.97/1.96\n0.94/1.92/1.91\n2\n1.03/2.24/2.20\n0.90/2.19/2.15\n0.88/2.11/2.08\n0.93/1.97/1.95\n0.85/1.78/1.76\n4\n0.90/2.18/2.10\n0.87/2.12/2.05\n0.93/2.01/1.96\n0.92/1.86/1.83\n0.92/1.64/1.62\n8\n0.92/1.92/1.77\n0.91/1.82/1.71\n0.92/1.65/1.57\n0.93/1.45/1.41\n0.94/1.28/1.26\n16\n0.92/1.92/1.77\n0.91/1.82/1.71\n0.92/1.65/1.57\n0.93/1.45/1.41\n0.94/1.28/1.26\nB\n128\n256\n512\n1024\n2048\nLLaMA-2-13B\n1\n1.32/2.34/2.32\n0.94/2.31/2.28\n0.92/2.22/2.20\n0.94/2.15/2.13\n0.94/2.01/1.99\n2\n1.06/2.42/2.36\n0.92/2.37/2.32\n0.94/2.29/2.25\n0.94/2.15/2.12\n0.95/1.95/1.93\n4\n0.93/2.36/2.26\n0.94/2.29/2.21\n0.94/2.18/2.12\n0.95/2.01/1.97\n0.96/1.78/1.75\n8\n0.92/2.24/2.10\n0.93/1.93/2.02\n0.94/1.81/1.89\n0.94/1.65/1.71\n0.95/1.45/1.49\n16\n0.93/2.02/1.85\n0.94/1.90/1.76\n0.94/1.73/1.63\n0.95/1.50/1.45\nOOM\ngroup-wise manner. Li et al. [214] conducted a thorough\nevaluation to assess the impact of quantization on different\ntensor types (including KV Cache), various tasks, 11 LLM\nfamilies, and SOTA quantization methods.\nQuantization-Aware Training. Quantization-aware training\n(QAT) incorporates the influence of quantization within the\nmodel training procedure. By integrating layers that repli-\ncate quantization effects, this approach facilitates weight\nadaptation to quantization-induced errors, leading to en-\nhanced task performance. Nevertheless, training LLMs typ-\nically demands substantial training data and considerable\ncomputational resources, posing potential bottlenecks for\nQAT implementation. Consequently, current research en-\ndeavors focus on strategies to reduce the training data re-\nquirements or alleviate the computational burden associated\nwith QAT implementation.\nTo reduce the data requirements, LLM-QAT [187] intro-\nduces a data-free method to generate the training data by\nusing the original FP16 LLMs. Specifically, LLM-QAT uses\nevery token in the tokenization vocabulary as a starting\ntoken to generate sentences. Based on the generated training\ndata, LLM-QAT applies a distillation-based workflow to\ntrain the quantized LLM to match the output distribution\nof the original FP16 LLM. Norm Tweaking [188] limits\nthe selection of the starting token to only those language\ncategories listed among the top languages with the highest\nproportion. This strategy can effectively improve the gener-\nalization of the quantized model on different tasks.\nTo reduce the computation cost, many methods apply\nparameter-efficient tuning (PEFT) strategies to accelerate\nQAT. QLoRA [189] quantizes the weights of LLMs into\n4-bit and subsequently employs LoRA [224] in BF16 for\neach 4-bit weight matrix to fine-tune the quantized model.\nQLoRA allows for the efficient fine-tuning of a 65B param-\neter LLM on one GPU with only 30GB of memory. QA-\nLoRA [190] proposes to incorporate group-wise quantiza-\ntion into QLoRA. The authors observe that the number of\nquantization parameters in QLoRA is significantly smaller\nthan the number of LoRA parameters, leading to an imbal-\nance between quantization and low-rank adaptation. They\nsuggest that group-wise operations can address this issue by\nincreasing the number of parameters dedicated to quantiza-\ntion. In addition, QA-LoRA can merge the LoRA terms into\nthe corresponding quantized weight matrices. LoftQ [191]\nidentifies that initializing LoRA matrices with zeros in\nQLoRA is inefficient for downstream tasks. As an alterna-\ntive, LoftQ suggests initializing the LoRA matrices using\nthe Singular Value Decomposition (SVD) of the difference\nbetween the original FP16 weights and quantized weights.\nLoftQ iteratively applies quantization and SVD to achieve a\nmore accurate approximation of the original weights. Norm\nTweaking [188] proposes to train the LayerNorm layer after\nquantization and use knowledge distillation to match the\noutput distribution of the quantized model with that of the\nFP16 model, achieving effects similar to LLM-QAT while\navoiding high training costs.\nComparative Experiments and Analysis. In this section, we\nconduct experiments to evaluate the speed-ups achieved\nby employing the weight-only quantization technique in\nvarious scenarios. Specifically, we focus on two widely-used\n\n16\nlarge language models (LLMs), LLaMA-2-7B and LLaMA-2-\n13B, and quantize their weights to 4-bit using the AWQ [194]\nalgorithm. Subsequently, we deploy these quantized models\non a single NVIDIA A100 GPU using two different inference\nframeworks: TensorRT-LLM [222] and LMDeploy [223]. We\nthen evaluate the speed-ups achieved by these frameworks\nacross different input sequences characterized by varying\nbatch sizes and context lengths.\nWe present the speed-ups of prefilling latency, decoding\nlatency, and end-to-end latency, as summarized in Tab. 4.\nFrom the results, several key observations can be made:\n(1) Weight-only quantization can substantially accelerate the\ndecoding stage, leading to improvements in end-to-end la-\ntency. This enhancement primarily stems from the capability\nof loading the quantized model with low-precision weight\ntensors much more swiftly from the High Bandwidth Mem-\nory (HBM), as illustrated in the preceding \u201cEfficient Analy-\nsis\u201d part. Consequently, this approach markedly diminishes\nthe memory access overhead. (2) Regarding the prefilling\nstage, weight-only quantization may actually increase the\nlatency. This is due to the fact that the bottleneck in the\nprefilling stage is the computational cost rather than the\nmemory access cost. Therefore, quantizing only the weights\nwithout the activations has minimal impact on latency. Ad-\nditionally, as illustrated in Fig. 9, weight-only quantization\nnecessitates the de-quantization of low-precision weights\nto FP16, leading to additional computational overhead and\nconsequently slowing down the prefilling stage. (3) As the\nbatch size and input length increase, the extent of speed-up\nachieved by weight-only quantization gradually diminishes.\nThis is primarily because, with larger batch sizes and input\nlengths, the computational cost constitutes a larger propor-\ntion of latency. While weight-only quantization predomi-\nnantly reduces memory access cost, its impact on latency\nbecomes less significant as the computational demands\nbecome more prominent with larger batch sizes and input\nlengths. (4) Weight-only quantization offers greater benefits\nfor larger models due to the significant memory access over-\nhead associated with larger model sizes. As models grow\nin complexity and size, the amount of memory required\nto store and access weights increases proportionally. By\nquantizing the model weights, weight-only quantization ef-\nfectively reduces this memory footprint and memory access\noverhead.\n5.2.2\nSparsification\nSparsification is a compression technique that increases the\nproportion of zero-valued elements in data structures such\nas model parameters or activations. This method aims to\ndecrease computational complexity and memory usage by\nefficiently ignoring zero elements during computation. In\nthe context of LLMs, sparsification is commonly applied\nto weight parameters and attention activations. It leads to\nthe development of weight pruning strategies and sparse\nattention mechanisms.\nWeight Pruning. Weight pruning systematically removes\nless critical weights and structures from models, aiming\nto reduce computational and memory cost during both\nprefilling stages and decoding stages without significantly\ncompromising performance. This sparsification approach is\ncategorized into two main types: unstructured pruning and\nstructured pruning. The categorization is based on the gran-\nularity of the pruning process, as illustrated in Figure 10.\nUnstructured Pruning\nGranularity: Weight\nStructured Pruning\nGranularity: Channel/Group/Layer\nFig. 10. Illustration of Unstructured Pruning (left) and Structured Pruning\n(right).\nUnstructured pruning prunes individual weight values\nwith fine granularity. Compared with structured pruning, it\ntypically achieves a greater level of sparsity with minimal\nimpact on model prediction. However, the sparse pattern\nachieved through unstructured pruning lacks high-level\nregularity, leading to irregular memory access and compu-\ntation patterns. This irregularity can significantly hinder the\npotential for hardware acceleration, as modern computing\narchitectures are optimized for dense, regular data patterns.\nConsequently, despite achieving higher sparsity levels, the\npractical benefits of unstructured pruning in terms of hard-\nware efficiency and computational speedup may be limited.\nThe common focus of this line of work is the pruning\ncriterion, including the weight importance and pruning\nratio. Considering the huge parameter size of LLMs, im-\nproving the pruning efficiency is also crucial. One pruning\ncriterion is to minimize the reconstruction loss of the model.\nSparseGPT [165] is a representative approach in this field. It\nfollows the idea of OBS [225], which considers the impact of\nremoving each weight on the network\u2019s reconstruction loss.\nOBS iteratively decides a pruning mask to prune the weights\nand reconstructs the unpruned weights to compensate for\nthe pruning loss. SparseGPT overcomes the efficiency bot-\ntleneck of OBS via the Optimal Partial Updates technique,\nand designs an adaptive mask selection technique based\non the OBS reconstruction error. Prune and Tune [168]\nimproves upon SparseGPT by fine-tuning the LLMs with\nminimal training steps during pruning. ISC [167] designs a\nnovel pruning criterion by combining the saliency criteria\nin OBS [225] and OBD [226]. It further assigns non-uniform\npruning ratios to each layer based on Hessian information.\noBERT [171] and FastPruning [172] utilizes the second-\norder information of the loss function to decide the pruned\nweights. BESA [170] learns a differentiable binary mask via\ngradient descent of the reconstruction loss. The pruning ra-\ntio for each layer is sequentially decided by minimizing the\nreconstruction error. The other popular pruning criterion is\nmagnitude-based. Wanda [166] proposes to use the element-\nwise product between the weight magnitude and the norm\nof input activation as the pruning criterion. RIA [173] jointly\nconsiders the weights and activations by using the metric of\nRelative Importance and Activations, which evaluates the\nimportance of each weight element based on all its con-\nnected weights. In addition, RIA converts the unstructured\nsparsity pattern to a structured N:M sparsity pattern, which\ncan enjoy the actual speed-up on NVIDIA GPUs. The recent\nstudy, Pruner-Zero [185], proposes to automatically identify\n\n17\nthe optimal pruning metric for LLMs, going beyond the\nhand-designed matrics. As a result, the optimal metric tai-\nlored for LLaMA and LLaMA-2 is W\nW\nW \u2299W\nW\nW \u2299\u03c3(GGG), where W\nW\nW\nand GGG represent the weights and gradients, and \u03c3(\u00b7) scales\na tensor to [0,1] using its mininum and maximum value.\nAdditionally, OWL [169] focuses on deciding the pruning\nratio of each layer. It assigns the pruning ratios to each layer\nbased on its activation outlier ratios. DS\u00d8T [186] proposes\na training-free appoarch to fine-tune the pruned LLMs. It\nbuilds upon the \u201cpruning-and-growing\u201d workflow adopted\nin Dynamic Sparse Training [227], which first prunes the\nmodel and then iteratively adjusts the network topology\nwithout training or weight update. DS\u00d8T further designs\nthe pruning and growing metrics tailored for LLMs.\nStructured pruning prunes larger structural units of\nthe model, such as entire channels or layers, operating at\na coarser granularity compared to unstructured pruning.\nThese methods directly facilitate inference speed-up on con-\nventional hardware platforms due to their alignment with\nthe dense, regular data patterns these systems are optimized\nto process. However, the coarse granularity of structured\npruning often results in a more pronounced impact on\nmodel performance. The pruning criterion of this line of\nwork additionally enforces the structured pruning pattern.\nLLM-Pruner [174] proposes a task-agnostic structured prun-\ning algorithm. Specifically, it first identifies the couple struc-\ntures in the LLM, based on the connection dependencies\nbetween neurons. Then, it decides which structure groups\nto remove based on a well-designed group-wise pruning\nmetric. After pruning, it further proposes to recover the\nmodel performance by a parameter-efficient training tech-\nnique, i.e., LoRA [224]. Sheared LLaMA [175] proposes to\nprune the original LLM to a specific target architecture of\nexisting pre-trained LLMs. In addition, it designs dynamic\nbatch-loading techniques to improve post-training perfor-\nmance. ZipLM [176] iteratively identifies and prunes the\nstructural components with the worst trade-off between\nloss and runtime. LoRAPrune [177] proposes a structured\npruning framework for the pre-trained LLMs with LoRA\nmodules to enable fast inference of LoRA-based models.\nIt designs a LoRA-guided pruning criterion that uses the\nweights and gradients of LoRA, and an iterative pruning\nscheme to remove the unimportant weights based on the\ncriterion. LoRAShear [178] also designs a pruning method\nfor LoRA-based LLMs with (1) a graph algorithm to identify\nthe minimal removal structures, (2) a progressive structured\npruning algorithm LHSPG, and (3) a dynamic knowledge\nrecovery mechanism to recover the model performance.\nSliceGPT [179] builds on the idea of computational invari-\nance of RMSNorm operation. It proposes to structurally\narrange the sparsity in each weight matrix, and to slice\nout the entire rows or columns. PLATON [180] proposes\nto prune the weights by considering both their importance\nand uncertainty. It uses the exponential moving average\n(EMA) of the importance scores to estimate the importance,\nand adopts the upper confidence bound (UCB) for the\nuncertainty. CoFi [181] and SIMPLE [182] propose to prune\nthe attention head, FFN neurons and hidden dimension via\nlearning the corresponding sparsity masks. After pruning,\nthey further adopt knowledge distillation to fine-tune the\npruned models for performance recovery. MoE techniques\n(Sec. 5.1.1) have attracted much attention in the field of\nefficient LLMs. Recent studies tend to explore the expert\npruning methods for MoE-based LLMs. For example, Ex-\npertSparsity [183] proposes to prune some less important\nFFN experts in each model layer. Specifically, it utilizes\nthe Frobenius norm of the difference between the original\noutput and the output of the pruned layer to quantify the\nloss of pruned experts. In constrast, SEER-MoE [184] uses\nthe total number of times that one expert gets activated on\na calibration dataset, to quantify this expert\u2019s importance.\n(a)\n(b)\n(c)\nlocal\nglobal\nrandom\ndilated rate 1/2/8\n(d)\nattended\npruned\nbucket 0/1\nFig. 11. Examples of different sparse attention masks. (a) Static mask\nwith local, global, and random attention pattern. (b) Static mask with\ndilated attention pattern of different dilated rate. (c) Dynamic token\npruning. (d) Dynamic attention pruning.\nSparse Attention. Sparse attention techniques in Multi-\nHead Self-Attention (MHSA) components of transformer\nmodels strategically omit certain attention calculations to\nenhance computational efficiency of the attention operation\nmainly in the prefilling stage. These mechanisms diverge\ninto static and dynamic categories based on their reliance\non specific input data.\nStatic sparse attention removes activation values inde-\npendently of specific inputs [150], [152], [153], [154]. These\nmethods pre-determine the sparse attention mask and en-\nforce it on the attention matrix during inference. Previous\nstudies combine different sparse patterns to preserve the\nmost essential elements within each attention matrix. As\nshown in Figure 11(a), the most common sparse attention\npatterns are the local and global attention patterns. The local\nattention pattern captures the local context of each token\nwith a fixed-size window attention surrounding each token.\nThe global attention pattern captures the correlation of spe-\ncific tokens to all other tokens by computing and attending\nto all tokens across the sequence. Note that leveraging global\npatterns can eliminate the need to store key-value (KV)\npairs for unused tokens, thereby reducing memory access\ncost and memory usage during the decoding stage. Sparse\nTransformer [150] combines these patterns to capture the\n\n18\nlocal context with a local pattern, and then aggregates the\ninformation with the global pattern for every few words.\nStreamingLLM [151] applies the local pattern, along with\nthe global pattern only for the first few tokens. It shows\nthat such a global pattern serves as the attention sink to\nkeep the strong attention scores toward initial tokens. It\nhelps the LLMs to generalize to infinite input sequence\nlength. Bigbird [153] also uses the random pattern, where\nall tokens attend to a set of random tokens. The combi-\nnation of local, global and random patterns is proven to\nencapsulate all continuous sequence-to-sequence functions,\naffirming its Turing completeness. As shown in Figure 11(b),\nLongformer [152] additionally introduces the dilated sliding\nwindow pattern. It is analogous to dilated CNNs and makes\nthe sliding window \u201cdilated\u201d to increase the receptive\nfield. To adapt the model to the sparse setting, Structured\nSparse Attention [154] advocates an entropy-aware training\nmethod that congregates high-probability attention values\ninto denser regions. Unlike previous studies that manually\ndesign sparse patterns, SemSA [155] uses gradient-based\nprofiling to identify important attention patterns and au-\ntomatically optimizes the attention density distribution to\nfurther improve model efficiency.\nIn contrast, Dynamic sparse attention adaptively elim-\ninates activation values based on varying inputs, employ-\ning real-time monitoring of neuronal activation values to\nbypass computations for neurons with negligible impact,\nthereby achieving pruning. Most dynamic sparse attention\nmethods employ the dynamic token-pruning methods, as\nFigure 11(c) shows. Spatten [156], SeqBoat [157] and Adap-\ntively Sparse Attention [158] leverage the inherent redun-\ndancy in linguistic constructs to propose dynamic token-\nlevel pruning strategies. Spatten [156] assesses the cumula-\ntive importance of each word by aggregating the attention\nmatrix columns, subsequently pruning tokens with minimal\ncumulative significance from the input in subsequent layers.\nSeqBoat [157] trains a linear State Space Model (SSM) with a\nsparse sigmoid function to determine which token to prune\nfor each attention head. Both Spatten and SeqBoat prune\nthe uninformative tokens for the whole input. Adaptively\nSparse Attention [158] gradually prunes the tokens during\nthe generation process. It drops parts of the context that are\nno longer required for future generation.\nIn addition to dynamic token pruning, dynamic atten-\ntion pruning strategies are also employed [159], [160], [161],\n[162], [163]. As Figure 11(d) shows, instead of pruning all\nthe attention values of certain tokens, these methods dy-\nnamically prune the selective part of the attention based on\nthe input. A prominent approach within this domain is dy-\nnamically segmenting input tokens into groups, known as\nbuckets, and strategically omitting the attention calculations\nfor tokens that reside in separate buckets. The challenge\nand focus of these methods lie in the way to cluster related\ntokens together, thereby facilitating attention computations\nsolely among them to enhance efficiency. Reformer [159]\nleverages locality-sensitive hashing to cluster keys and\nqueries that share identical hash codes into the same bucket.\nFollowing this, Sparse Flash Attention [160] introduces spe-\ncialized GPU kernels optimized for this hash-based sparse\nattention mechanism, further improving computational effi-\nciency. Meanwhile, the Routing Transformer [161] employs a\nspherical k-means clustering algorithm to aggregate tokens\ninto buckets, optimizing the selection process for attention\ncomputations. Sparse Sinkhorn Attention [162] adopts a\nlearned sorting network to align keys with their relevant\nquery buckets, ensuring that attention is computed only\nbetween the corresponding query-key pairs. Diverging from\nthe bucket-level operation, H2O [163] introduces the token-\nlevel dynamic attention pruning mechanism. It combines\nstatic local attention with dynamic computations between\nthe current query and a set of dynamically identified key\ntokens, termed heavy-hitters (H2). These heavy-hitters are\ndynamically adjusted with an eviction policy aimed at re-\nmoving the least significant keys at each generation step,\neffectively managing the size and relevance of the heavy-\nhitter set.\nMoreover, viewing each token as a graph node and\nattention between tokens as edges offers an extended per-\nspective on static sparse attention [153], [164]. The original,\nfull attention mechanism equates to a complete graph with\na uniform shortest path distance of 1. Sparse attention,\nwith its random mask, introduces random edges, effectively\nreducing the shortest path distance between any two nodes\nto O(log n), thus maintaining efficient information flow\nakin to full attention. Diffuser [164] utilizes the perspective\nof graph theory to expand the receptive field of sparse\nattention with multi-hop token correlations. It also takes\ninspiration from the expander graph properties to design\nbetter sparse patterns that approximate the information flow\nof full attention.\nBeyond the attention-level and token-level sparsity, the\nscope of attention pruning extends to various granularities.\nSpatten [156] also extends pruning beyond token granular-\nity to attention head granularity, eliminating computations\nfor inessential attention heads to further reduce computa-\ntional and memory demands.\n5.2.3\nStructure Optimization\nThe objective of structure optimization is to refine model\narchitecture or structure with the goal of enhancing the\nbalance between model efficiency and performance. Within\nthis field of research, two prominent techniques stand out:\nNeural Architecture Search (NAS) and Low Rank Factoriza-\ntion (LRF).\nNeural Architecture Search. Neural Architecture Search\n(NAS) [228] aims to automatically search the optimal neu-\nral architectures that strike an optimized balance between\nefficiency and performance. AutoTinyBERT [138] utilizes\none-shot Neural Architecture Search (NAS) to discover the\nhyper-parameters of the Transformer architecture. Notably,\nit introduces a compelling batch-wise training approach\nto train a Super Pre-trained Language Model (SuperPLM)\nand subsequently employs an evolutionary algorithm to\nidentify the optimal sub-models. NAS-BERT [139] trains a\nlarge super-net on conventional self-supervised pre-training\ntasks using several innovative techniques, such as block-\nwise search, search space pruning, and performance ap-\nproximation. This approach allows NAS-BERT to be applied\nefficiently across various downstream tasks without requir-\ning extensive re-training. Structure pruning via NAS [140]\ntreats structural pruning as a multi-objective NAS problem,\n\n19\nand solves it via the one-shot NAS method. LiteTransform-\nerSearch [141] proposes to use a training-free indicator, i.e.,\nthe number of parameters, as a proxy indicator to guide the\nsearch. This method enables efficient exploration and selec-\ntion of the optimal architectures without the need for actual\ntraining during the search phase. AutoDistil [142] presents a\nfully task-agnostic few-shot NAS algorithm featuring three\nprimary techniques: search space partitioning, task-agnostic\nSuperLM training, and task-agnostic search. This approach\naims to facilitate efficient architecture discovery across vari-\nous tasks with minimal task-specific adaptations. Typically,\nNAS algorithms necessitate evaluating the performance of\neach sampled architecture, which can incur significant train-\ning cost. Consequently, these techniques are challenging to\napply to LLMs.\nLow Rank Factorization. Low Rank Factorization (LRF), or\nLow Rank Decomposition, aims to approximate a matrix\nAm\u00d7n with two low-rank matrices Bm\u00d7r and Cr\u00d7n by:\nAm\u00d7n \u2248Bm\u00d7r \u00d7 Cr\u00d7n,\n(11)\nwhere r is much smaller than m and n. In this way, LRF\ncan diminish memory usage and enhance computational\nefficiency. Furthermore, during the decoding stage of LLM\ninference, memory access cost presents a bottleneck to the\ndecoding speed. Therefore, LRF can reduce the number\nof parameters that need to be loaded, thereby accelerat-\ning the decoding speed. LoRD [143] shows the potential\nof compressing the LLMs without largely degrading the\nperformance via LRF. Specifically, it adopts Singular Value\nDecomposition (SVD) to factorize the weight matrices, and\nsuccessfully compresses a LLM with 16B parameters to\n12.3B with minimal performance drop. TensorGPT [144]\nintroduces a method to compress the embedding layer\nusing Tensor-Train Decomposition. Each token embedding\nis treated as a Matrix Product State (MPS) and efficiently\ncomputed in a distributed manner. LoSparse [145] combines\nthe benefits of LRF and weight pruning for LLM com-\npression. By leveraging low-rank approximation, LoSparse\nmitigates the risk of losing too many expressive neurons that\ntypically occurs with direct model pruning. LPLR [146] and\nZeroQuant-V2 [147] both propose to compress the weight\nmatrix by simultaneously applying LRF and quantization to\nit. DSFormer [148] proposes to factorize the weight matrix\ninto the product of a semi-structured sparse matrix and\na small dense matrix. ASVD [149] designs an activation-\naware SVD method. This approach involves scaling the\nweight matrix based on activation distribution prior to ap-\nplying SVD for matrix decomposition. ASVD also involves\ndetermining an appropriate truncation rank for each layer\nthrough a search process. SVD-LLM [229] analyses the re-\nlationship between the singular values of the transformed\nweight matrices and the compression loss. Then, it designs\na truncation-aware data whitening technique to identify the\nsingular value that causes the minimal loss after removing\nit. Additionally, SVD-LLM develops a layer-wise closed-\nform update strategy to recover the task performance after\nthe factorization.\n5.2.4\nKnowledge Distillation\nKnowledge Distillation (KD) is a well-established technique\nfor model compression, wherein knowledge from large\nmodels (referred to as teacher models) is transferred to\nsmaller models (referred to as student models). In the\ncontext of LLMs, KD involves using the original LLMs as\nteacher models to distill smaller LMs. Numerous studies\nhave focused on effectively transferring various abilities of\nLLMs to smaller models. In this domain, methods can be\ncategorized into two main types: white-box KD and black-\nbox KD (as illustrated in Fig. 12).\nTeacher Model\nStudent Model\nFeatures\nLogits\nOutputs\nWhite-Box KD\nAPI-based Teacher \nModel\nOutputs\nBlack-Box KD\nICL ability\nCoT ability\nIF ability\nFig. 12. Illustration of White-Box KD (left) and Black-Box KD (right).\nWhite-box KD. White-box KD refers to distillation methods\nthat leverage access to the structure and parameters of the\nteacher models. This approach enables KD to effectively\nutilize the intermediate features and output logits of the\nteacher models for enhanced performance of the student\nmodels. MiniLLM [131] proposes to adopt the standard\nwhite-box KD approach but replace the forward Kullback-\nLeibler divergence (KLD) with the reverse KLD. GKD [132]\nintroduces the use of on-policy data, which includes output\nsequences generated by the student model itself, to further\ndistill the student model. This method focuses on aligning\nthe output logits between the teacher and student models\nusing these on-policy data. TED [133] presents a task-aware\nlayer-wise KD method. This approach involves adding fil-\nters after each layer in both the teacher and student models,\ntraining these task-specific filters, and subsequently freez-\ning the teacher model\u2019s filters while training the student\nfilters to align their output features with the correspond-\ning teacher filters. MiniMoE [135] mitigates the capacity\ngap by utilizing a Mixture-of-Experts (MoE) model as the\nstudent model. DynaBERT [136] proposes to progressively\ndecrease the models\u2019 width and depth, and uses knowledge\ndistillation to train the smaller models. For newly emerging\nentities, pre-trained language models (LLMs) may lack up-\nto-date information. To address this, one solution involves\nincorporating additional retrieved texts into prompts, albeit\nat an increased inference cost. Alternatively, KPTD [137]\nsuggests transferring knowledge from entity definitions into\nLLM parameters via knowledge distillation. This method\ngenerates a transfer set based on entity definitions and\ndistills the student model to match output distributions with\nthe teacher model based on these definitions.\nBlack-box KD. Black-box KD refers to the knowledge dis-\ntillation methods in which the structure and parameters of\nteacher models are not available. Typically, black-box KD\nonly uses the final results obtained by the teacher models\nto distill the student models. In the field of LLMs, black-\nbox KD mainly guides the student models to learn LLMs\u2019\ngeneralization ability and emergent ability, including In-\nContext Learning (ICL) ability [45], Chain-of-Thought (CoT)\nreasoning ability [14] and Instruction Following (IF) abil-\nity [230].\nRegarding the ICL ability, Multitask-ICT [118] introduces\nin-context learning distillation to transfer the multitask few-\n\n20\nshot ability of Large Language Models (LLMs), leveraging\nboth in-context learning and language modeling proficiency.\nMCKD [119] observes that student models distilled from in-\ncontext learned teacher models often exhibit superior per-\nformance on unseen input prompts. Building on this obser-\nvation, MCKD devises a multi-stage distillation paradigm\nwhere the student model from previous stages is employed\nto generate distillation data for subsequent stages, enhanc-\ning the effectiveness of the distillation method.\nTo distill the Chain-of-Thought (CoT) reasoning ability,\nseveral techniques such as Distilling Step-by-Step [120],\nSCoTD [121], CoT Prompting [122], MCC-KD [123], and\nFine-tune-CoT [124] propose distillation methods that in-\ncorporate responses and rationales extracted from LLMs\nto train student models. Socratic CoT [125] also targets\nreasoning ability transfer to smaller models. Specifically,\nit fine-tunes a pair of student models, namely a Question\nGeneration (QG) model and a Question Answering (QA)\nmodel. The QG model is trained to generate intermediate\nquestions based on input questions, guiding the QA model\nin producing the final response. PaD [126] observes that\nfaulty reasoning (i.e., correct final answer but incorrect\nreasoning steps) can be detrimental to student models. To\naddress this, PaD proposes generating synthetic programs\nfor reasoning problems, which can then be automatically\nchecked by an additional interpreter. This approach helps in\nremoving distillation data with faulty reasoning, enhancing\nthe quality of the training data for student models.\nFor the IF ability, several methods have been proposed\nto transfer this capability to smaller models. DISCO [128]\nintroduces a technique where phrasal perturbations are gen-\nerated using a LLM. These perturbations are then filtered by\na task-specific teacher model to distill high-quality counter-\nfactual data. LaMini-LM [129] aims to transfer instruction\nfollowing ability by designing a diverse instruction set for\ndistilling student models. Lion [130] utilizes the teacher\nmodel to identify difficult instructions, and generates new\nand complex instructions to distill the small model.\n5.2.5\nDynamic Inference\nDynamic inference involves the adaptive selection of model\nsub-structures during the inference process, conditioned\non input data. This section focuses on early exiting tech-\nniques, which enable a LLM to halt its inference at different\nmodel layers depending on specific samples or tokens.\nNotably, while MoE techniques (discussed in Sec. 5.1.1)\nalso adjust model structure during inference, they typically\ninvolve expensive pre-training cost. In contrast, early ex-\niting techniques only require training a small module to\ndetermine when to conclude the inference. Some previous\nsurveys [231], [232] have reviewed dynamic inference tech-\nniques for traditional language models (e.g., RNN, LSTM).\nIn this paper, we categorize studies on early exiting tech-\nniques for LLMs into two main types: sample-level early\nexiting and token-level early exiting (illustrated in Fig. 13).\nSample-level. Sample-level early exiting techniques focus\non determining the optimal size and structure of Language\nModels (LLMs) for individual input samples. A common\napproach is to augment LLMs with additional modules after\neach layer, leveraging these modules to decide whether to\nInput 1\nInput 2\nOutput 1\nOutput 2\nPrompt\nToken 1\nToken 2\nSample-level Dynamic \nInference\nToken-level Dynamic \nInference\nFig. 13. Illustration of Token-level (up) and Sample-level (down) dynamic\ninference.\nterminate inference at a specific layer. FastBERT [112], Dee-\nBERT [115], MP [233], and MPEE [113] train these modules\ndirectly to make decisions (e.g., outputting 0 to continue or\n1 to stop) based on features from the current layer. Global\nPast-Future Early Exit [114] proposes a method that enriches\nthe input to these modules with linguistic information from\nboth preceding and subsequent layers. Given that future\nlayer features are not directly accessible during inference, a\nsimple feed-forward layer is trained to estimate these future\nfeatures. PABEE [116] trains the modules as output heads\nfor direct prediction, suggesting inference termination when\npredictions remain consistent. HASHEE [117] employs a\nnon-parametric decision-making approach based on the hy-\npothesis that similar samples should exit inference at the\nsame layer.\nToken-level. In the decoding stage of LLM inference, where\ntokens are generated sequentially, token-level early exiting\ntechniques aim to optimize the size and structure of LLMs\nfor each output token. CALM [110] introduces early exit\nclassifiers after each Transformer layer, training them to\noutput confidence scores that determine whether to halt\ninference at a specific layer. Notably, in the self-attention\nblock, computing the current token\u2019s feature at each layer\nrelies on all previous tokens\u2019 features (i.e., KV cache) in\nthe same layer. To address the issue of missing KV cache\ndue to early exiting of previous tokens, CALM proposes\ndirectly copying the feature from the exiting layer to subse-\nquent layers, with experimental results showing only minor\nperformance degradation. SkipDecode [111] addresses lim-\nitations of previous early exiting methods that hinder their\napplicability to batch inference and KV caching, thereby lim-\niting actual speed-up gains. For batch inference, SkipDecode\nproposes a unified exit point for all tokens within a batch.\nRegarding KV caching, SkipDecode ensures a monotonic\ndecrease in exit points to prevent recomputation of KV\ncache, facilitating efficiency gains during inference.\n5.3\nKnowledge, Suggestions and Future Direction\nIn the field of efficient structure design, the pursuit of\nalternative architectures to Transformers is a burgeoning\narea of research. Examples such as Mamba [75], RWKV [62],\nand their respective variants [103], [106] have demonstrated\ncompetitive performance across various tasks, garnering\n\n21\nincreasing attention in recent times. Nevertheless, it remains\npertinent to investigate whether these non-Transformer\nmodels may exhibit certain shortcomings compared to\nTransformer models. Concurrently, exploring the integra-\ntion of non-Transformer architectures with the attention\noperation [76], [105], [234] represents another promising\navenue for future research.\nIn the realm of model compression, quantization stands\nout as the predominant method employed in Large Lan-\nguage Model (LLM) deployment, primarily due to two key\nfactors. Firstly, quantization presents a convenient means of\ncompressing LLMs. For instance, employing Post-Training\nQuantization (PTQ) methods can reduce the parameter\ncount of an LLM with seven billion parameters to a com-\npressed form within a matter of minutes. Secondly, quan-\ntization holds the potential to achieve substantial reduc-\ntions in memory consumption and inference speed, while\nintroducing only minor performance trade-offs. This com-\npromise is generally deemed acceptable for numerous real-\nworld applications. However, it\u2019s worth noting that quan-\ntization may still compromise certain emergent abilities\nof LLMs, such as self-calibration or multi-step reasoning.\nAdditionally, in specific scenarios like dealing with long\ncontexts, quantization could lead to significant performance\ndegradation [214]. Consequently, it is required to carefully\nselect appropriate quantization methods to mitigate the risk\nof such degradation in these specialized cases.\nExtensive literature has devoted into studying sparse at-\ntention techniques for efficient long-context processing. For\nexample, a recent representative work, StreamingLLM [151],\ncan process 4 million tokens by only restoring several\nattention sink tokens. Nonetheless, these approaches of-\nten sacrifice critical information, resulting in performance\ndegradation. Therefore, the challenge of preserving essen-\ntial information while efficiently managing long contexts\nremains an important area for future exploration. As for\nthe weight pruning techniques, LLM-KICK [235] notes that\ncurrent state-of-the-art (SOTA) methods experience con-\nsiderable performance degradation even at relatively low\nsparsity ratios. Consequently, developing effective weight\npruning methods to maintain LLM performance remains an\nemerging and critical research direction.\nThe optimization of model structures often involves the\nuse of Neural Architecture Search (NAS), which typically\ndemands extensive computational resources, posing a po-\ntential barrier to its practical application in compressing\nLLMs. Therefore, investigating the feasibility of employ-\ning automatic structure optimization for LLM compression\nwarrants further exploration. Additionally, the challenge\nremains for techniques like low-rank factorization (LRF) to\nachieve an optimal balance between compression ratio and\ntask performance. For instance, ASVD [149] achieves only a\nmodest 10% to 20% compression ratio without compromis-\ning the reasoning capabilities of LLMs.\nIn addition to employing individual model compres-\nsion techniques, several studies explore the combination\nof different methods to compress LLMs, leveraging their\nrespective advantages for improved efficiency. For instance,\nMPOE [90] applies weight matrix factorization specifically\nto the expert Feed-Forward Networks (FFNs) in MoE-based\nLLMs, with the goal of further reducing memory require-\nments. LLM-MQ [201] utilizes weight sparsity techniques to\nprotect weight outliers during model quantization, thereby\nminimizing quantization errors. LPLR [146] focuses on\nquantizing low-rank factorized weight matrices to further\ndecrease memory footprint and memory access cost during\nLLM inference. Furthermore, LoSparse [145] combines low-\nrank factorization with weight pruning, leveraging pruning\nto enhance the diversity of low-rank approximation while\nusing low-rank factorization to retain important weights\nand prevent loss of critical information. These approaches\nhighlight the potential of integrating multiple compression\ntechniques to achieve better optimization of LLMs.\n6\nSYSTEM-LEVEL OPTIMIZATION\nThe system-level optimization for LLM inference primarily\ninvolves enhancing the model forward pass. Considering\nthe computational graph of an LLM, there exist multiple\noperators, with attention and linear operators dominating\nmost of the runtime. As mentioned in Sec. 2.3, system-level\noptimization primarily considers the distinctive characteris-\ntics of the attention operator and the decoding approach\nwithin LLM. In particular, to address the specific issues\nrelated to the decoding approach of LLMs, the linear opera-\ntor requires special tiling designs, and speculative decoding\nmethods are proposed to improve the utilization. The sub-\nstantial memory demand of LLMs leads to the offloading\nof parameters or KV cache to the CPU. Furthermore, in\nthe context of online serving, requests come from multiple\nusers. Therefore, beyond the optimizations discussed earlier,\nonline serving faces challenges related to memory, batching,\nand scheduling arising from asynchronous requests.\n6.1\nInference Engine\nThe optimizations for inference engines are dedicated to\naccelerating the model forward process. The main operators\nand the computational graph in LLM inference are highly\noptimized. Besides, the speculative decoding technique is\nproposed to accelerate the inference speed without perfor-\nmance degradation, and the offloading technique is intro-\nduced to mitigate the memory pressure.\n6.1.1\nGraph and Operator Optimization\nRuntime Profiling. Using HuggingFace [260] implementa-\ntion, we profile the inference runtime with different mod-\nels and context lengths. The profiling results in Fig. 15\ndemonstrate that attention operators and linear operators\ncollectively dominate runtime, with their combined dura-\ntion often exceeding 75% of the inference duration. Conse-\nquently, a significant portion of optimization efforts at the\noperator level is dedicated to enhancing the performance of\nthe two operators. Furthermore, there are multiple operators\noccupying a small proportion of runtime, which fragments\nthe operator execution timeline and increases the cost of\nkernel launch on the CPU side. To address this issue, at\nthe computational graph level, current optimized inference\nengines implement highly fused operators.\nAttention Operator Optimization. The standard attention\ncomputation (e.g., using Pytorch) involves the multiplica-\ntion of the Query matrix (Q) with the Key matrix (K),\n\n22\nInference\nEngine\nSpeculative Decoding\nSpeculative decoding [236], Speculative sampling [237], DistillSpec [238], Self-\nspeculative decoding [239], OSD [240], PaSS [241], REST [242], SpecInfer [243],\nStage speculative decoding [244], Cascade Speculative Drafting [245], Looka-\nhead decoding [246], Medusa [50], Eagle [247], Spectr [248], Kangaroo [249]\nOffloading\nFlexGen [203], llama.cpp [250], powerinfer [251], FastDecode [252]\nGraph and Operator\nOptimization\nLinear Operator\nOptimization\nTensorRT-LLM [222], FlashDecoding++ [253],\nMegaBlocks [254], vLLM [51]\nGraph-Level Optimization\nFlashAttention [255], [256], ByteTrans-\nformer [257], DeepSpeed [258], FlashDecod-\ning++ [253]\nAttention Operator\nOptimization\nFlashAttention [255], [256], FlashDecod-\ning [259], FlashDecoding++ [253]\nFig. 14. Taxonomy of the optimization for LLM inference engine.\nattention\n35.82%\nlinear\n40.55%\nothers\n23.63%\nothers\n21.68%\nattention\n39.50%\nlinear\n38.82%\nothers\n27.36%\nattention\n19.27%\nlinear\n53.37%\nothers\n24.64%\nattention\n29.94%\nlinear\n50.42%\nattention\n1.84%\nlinear\n91.44%\nothers\n6.72%\nattention\n2.33%\nlinear\n89.72%\nothers\n7.95%\n(a) Llama2-7B, \n128 context length\n(b) Llama2-7B, \n2k context length\n(c) Baichuan2-13B, \n128 context length\n(d) Baichuan2-13B, \n2k context length\n(e) Mixtral-8x7B, \n128 context length\n(f) Mixtral-8x7B, \n2k context length\nFig. 15. Inference runtime breakdown over multiple LLMs.\nresulting in quadratic time and space complexity in relation\nto the input sequence length. As shown in Fig. 15, the\ntime proportion of the attention operator increases as the\ncontext length grows. This translates to high demands on\nmemory size and computational capability, especially when\ndealing with long sequences. To address the computational\nand memory overhead of standard attention computation\non GPUs, customized attention operators are essential.\nFlashAttention [255], [256] fuses the entire attention oper-\nation into a single, memory-efficient operator to alleviate\nmemory access overhead. The input matrices (Q, K, V) and\nattention matrix are tiled into multiple blocks, which elimi-\nnates the need for complete data loading. Built upon Flash\nAttention, FlashDecoding [259] aims to maximize compu-\ntational parallelism for decoding. Due to the application of\nthe decoding approach, the Q matrix degrades into a batch\nof vectors during decoding, which makes it challenging\nto fill the computational units if the parallelism is limited\nto the batch size dimension. FlashDecoding addresses this\nby introducing parallel computation along the sequence\ndimension. While this introduces some synchronization\noverhead to softmax computation, it leads to noticeable\nimprovements in parallelism, particularly for small batch\nsizes and long sequences. The subsequent work, FlashDe-\ncoding++ [253], observes that in previous works [255], [256],\n[259], the maximum value within the softmax only serves as\na scaling factor to prevent data overflow. However, the dy-\nnamical maximum value incurs significant synchronization\noverhead. Moreover, extensive experiments indicate that\nin typical LLM (e.g., Llama2 [261], ChatGLM [262]), over\n99.99% of the softmax inputs fall within a certain range.\nThus, FlashDecoding++ proposes to determine the scaling\nfactor based on statistics in advance. This eliminates the\nsynchronization overhead in softmax computation, enabling\nparallel execution of subsequent operations alongside the\nsoftmax computation.\nLinear Operator Optimization The linear operator plays\na pivotal role in LLM inference, performing in feature\nprojection and Feedforward Neural Networks (FFNs). In\ntraditional neural networks, linear operators can be ab-\nstracted into General Matrix-Matrix Multiplication (GEMM)\noperations. However, in the case of LLM, the application of\nthe decoding approach results in a notably reduced dimen-\nsion, diverging from the conventional GEMM workload.\nThe low-level implementation of traditional GEMM has\nbeen highly optimized, and mainstream LLM frameworks\n(e.g., DeepSpeed [258], vLLM [51], OpenPPL [263] and etc.)\nprimarily call the GEMM APIs offered by cuBLAS [264]\nfor linear operators. Without an explicitly tailored imple-\nmentation for GEMMs with a reduced dimension, the lin-\near operators during decoding suffer inefficiency. A no-\ntable trend to address the issue is observed in the latest\nrelease of TensorRT-LLM [222]. It introduces a dedicated\nGeneral Matrix-Vector Multiplication (GEMV) implemen-\ntation, potentially improving efficiency for the decoding\nstep. Recent research FlashDecoding++ [253] makes a fur-\nther step, addressing the inefficiency of cuBLAS [264] and\nCUTLASS [265] libraries when dealing with small batch\nsizes during the decode step. The authors first introduce\nthe concept of the FlatGEMM operation to represent the\n\n23\nworkload of GEMM with a highly reduced dimension (di-\nmension size < 8 in FlashDecoding++). As FlatGEMM poses\nnew computational characteristics, the tiling strategy for\ntraditional GEMMs necessitates modification to be applied.\nThe authors observe that two challenges exist as the work-\nload varies: low parallelism and memory access bottleneck.\nTo tackle the challenges, FlashDecoding++ adopts a fine-\ngrained tiling strategy to improve parallelism, and leverages\nthe double buffering technique to hide memory access la-\ntency. Furthermore, recognizing that the linear operations in\ntypical LLM (e.g., Llama2 [261], ChatGLM [262]) often have\nfixed shapes, FlashDecoding++ establishes a heuristic selec-\ntion mechanism. This mechanism dynamically chooses be-\ntween different linear operators based on the input size. The\noptions include FastGEMV [266], FlatGEMM, and GEMM\nprovided by cuBLAS [264], [265] libraries. This approach\nensures the selection of the most efficient operator for the\ngiven linear workload, potentially leading to better end-to-\nend performance.\nRecently, the application of the MoE FFN to enhance the\nmodel capability has become a trend in LLMs [12]. This\nmodel structure also puts forward new requirements for\noperator optimization. As shown in Fig. 15, in the Mixtral\nmodel with MoE FFN, the linear operator dominates the\nruntime due to the non-optimized FFN computation in the\nHuggingFace implementation. Besides, Mixtral\u2019s adoption\nof the GQA attention structure decreases the attention op-\nerator\u2019s runtime proportion, which further points out the\nurgent need to optimize the FFN layer. MegaBlocks [254]\nis the first to optimize the computation for MoE FFN lay-\ners. The work formulates the MoE FFN computation into\nblock-sparse operations and proposes tailored GPU kernels\nfor acceleration. However, MegaBlocks concentrates on the\nefficient training of the MoE models and hence ignores the\ncharacteristics of inference (e.g., the decoding approach).\nExisting frameworks are working hard to optimize the\ncomputations of the MoE FFN inference stage. The official\nrepository of vLLM [51] integrates the fused kernels for\nMoE FFN in Triton [267], seamlessly removing the index\noverhead.\nGraph-Level Optimization. Kernel fusion stands out as a\nprevalent graph-level optimization because of its capabil-\nity to reduce runtime. There are three main advantages\nof applying kernel fusion: (1) To reduce memory access.\nThe fused kernel inherently removes the memory access of\nintermediate results, mitigating the memory bottleneck for\noperators. (2) To mitigate kernel launching overhead. For\nsome lightweight operators (e.g., residual adding), the ker-\nnel launching time occupies most of the latency, and kernel\nfusion reduces individual kernel launchings. (3) To enhance\nparallelism. For those operators without data dependency,\nwhen one-by-one kernel execution fails to fill the hardware\ncapacity, it is beneficial to parallel the kernels via fusion.\nThe technique of kernel fusion proves effective with\nLLM inference, with all of the aforementioned benefits.\nFlashAttention [255] formulates the attention operator into\none single kernel, removing the overhead of accessing the at-\ntention results. Based on the fact that the attention operator\nis memory-bounded, the reduction of memory access effec-\ntively transfers to runtime speed-up. ByteTransformer [257]\nand DeepSpeed [258] propose to fuse lightweight operators\nincluding residual adding, layernorm, and activation func-\ntions, into the former linear operators to reduce the kernel\nlaunching overhead. As a result, those lightweight operators\ndisappear in the timeline with nearly no extra latency. More-\nover, kernel fusion is also adopted to enhance the utilization\nof LLM inference. The projections of Query, Key, and Value\nmatrices are originally three individual linear operations,\nand are fused into one linear operator to deploy on mod-\nern GPUs. Currently, the kernel fusion technique has been\nexploited in LLM inference practice, and highly optimized\ninference engines employ only a few fused kernels within\nthe runtime. For example, in FlashDecoding++ [253] im-\nplementation, a transformer block integrates merely seven\nfused kernels. Leveraging the aforementioned operators and\nkernel fusion optimization, FlashDecoding++ achieves up to\n4.86\u00d7 speed-up over the HuggingFace implementation.\n6.1.2\nSpeculative Decoding\nSpeculative decoding [268], [269] is an innovative decoding\ntechnique for auto-regressive LLMs designed to enhance\ndecoding efficiency without compromising the fidelity of\noutputs. The core idea of this approach involves employing\na smaller model, termed a draft model, to predict several\nsubsequent tokens efficiently, followed by validation of\nthese predictions using the target LLM in parallel. This\nmethodology aims to enable the LLM to generate multiple\ntokens within the time frame typically required for a sin-\ngle inference. Fig. 16 demonstrates the comparison of the\ntraditional auto-regressive decoding method and the spec-\nulative decoding approach. Formally, speculative decoding\napproach consists of two steps:\n1) Draft Construction: It employs the draft model to gen-\nerate several subsequent tokens, namely draft tokens,\nin parallel or in the auto-regressive manner.\n2) Draft Verification: It employs the target model to com-\npute the conditional probabilities of all the draft tokens\nin a single LLM inference step, subsequently determin-\ning the acceptance of each draft token sequentially. The\nacceptance rate, representing the average number of\naccepted draft tokens per inference step, serves as a key\nmetric for evaluating the performance of a speculative\ndecoding algorithm.\nLLM\nDraft Model\nOptional\nLLM \nGenerated \nToken\nDraft Token\nToken Accept\nToken Reject\n(a) Auto-regressive Decoding\n(b) Speculative Decoding\nFig. 16. Comparison of auto-regressive decoding (a) and speculative\ndecoding (b).\nSpeculative decoding ensures output equivalence with\nstandard auto-regressive decoding methods. Traditional de-\ncoding techniques typically employ two primary sampling\n\n24\nTABLE 5\nComparison of several open-source implementations of speculative decoding. In this table, we also show the additional overhead of constructing\ndraft models. Note that for SpD [236], [237], LADE [246], Medusa [50] and Eagle [247], we report the training cost from their original papers. And\nfor SSD [239] and REST [29], we run the sub-LLM search and datastore construction with the code they provide, and report the time cost.\nBesides, for Medusa, we use Medusa-1 [50] which does not fine-tune the original LLM backbone.\nMethod\nDraft Model\nDraft Construction\nDraft Verifier\nAdditional Overhead\n(GPU hours)\nAcceptance Rate\nSpeed-up\nSpD [236], [237]\nsmall speculative model\none draft sequence\nspeculative sampling\n275\n1.77\u223c2.02\u00d7\n1.05\u223c1.77\u00d7\nLADE [246]\nLLM + N grams\none draft sequence\ngreedy sampling\n0\n1.92\u223c2.14\u00d7\n1.12\u223c1.30\u00d7\nSSD [239]\nsub-LLM\none draft sequence\nspeculative sampling\n4\n1.64\u223c1.74\u00d7\n1.01\u223c1.23\u00d7\nREST [29]\ndatastore\ntoken tree\nspeculative sampling\n1.5\n2.18\u223c2.31\u00d7\n1.72\u223c2.27\u00d7\nMedusa-1 [50]\nfour LLM heads\ntoken tree\nspeculative sampling\n\u223c24\n2.52\u223c2.62\u00d7\n2.04\u223c2.86\u00d7\nEagle [247]\none Transformer Layer\ntoken tree\nspeculative sampling\n96\u223c192\n3.47\u223c3.72\u00d7\n2.77\u223c3.74\u00d7\nstrategies: greedy sampling and nucleus sampling. Greedy\nsampling involves selecting the token with the highest\nprobability at each decoding step to generate a specific\noutput sequence. The initial attempt at speculative decod-\ning, known as Blockwise Parallel Decoding [270], aims to\nensure that the draft tokens precisely match the tokens\nsampled via greedy sampling, thus preserving output to-\nken equivalence. In contrast, nucleus sampling involves\nsampling tokens from a probability distribution, resulting\nin diverse token sequences with each run. This diversity\nmakes nucleus sampling popular. To accommodate nucleus\nsampling within speculative decoding frameworks, specu-\nlative sampling techniques [236], [237] have been proposed.\nSpeculative sampling maintains output distribution equiv-\nalence, aligning with the probabilistic nature of nucleus\nsampling to generate varied token sequences. Formally,\ngiven a sequence of tokens x1, x2, ..., xn and a sequence of\ndraft tokens \u02c6xn+1, \u02c6xn+2, ..., \u02c6xn+k, the speculative sampling\nstrategy accepts the i-th draft token with the following\nprobabilities:\nmin\n\u0012\n1, p(\u02c6xi|x1, x2, ..., xi\u22121)\nq(\u02c6xi|x1, x2, ..., xi\u22121)\n\u0013\n,\n(12)\nwhere p(\u00b7|\u00b7) and q(\u00b7|\u00b7) denote the conditional probabilities\nfrom the target LLM and the draft model, respectively. If\nthe i-th draft token is accepted, it sets xi \u2190\u2212\u02c6xi. Otherwise,\nit quits the verification of the following draft tokens, and\nresamples xi from the following distribution:\nnorm(max(0, p(\u00b7|x1, x2, ..., xi\u22121) \u2212q(\u00b7|x1, x2, ..., xi\u22121))).\n(13)\nBuilding upon speculative sampling, several variants [243],\n[248] have emerged, aimed at validating multiple draft\ntoken sequences. Notably, the token tree verifier [243] has\nbecome a widely adopted verification strategy within this\ncontext. This approach utilizes a tree-structured represen-\ntation of draft token sets and employs a tree attention\nmechanism to efficiently perform the verification process.\nIn the speculative decoding approach, the acceptance\nrate of draft tokens is significantly influenced by the degree\nto which the output distributions of draft models align\nwith those of original LLMs. As a result, considerable re-\nsearch efforts have been directed towards improving the\ndesign of draft models. DistillSpec [238] directly distills a\nsmaller draft model from the target LLM. SSD [239] involves\nautomatically identifying a sub-model (a subset of model\nlayers) from the target LLM to serve as the draft model,\neliminating the need for separate training of the draft model.\nOSD [240] dynamically adjusts the output distribution of the\ndraft model to match the user query distribution in online\nLLM services. It achieves this by monitoring rejected draft\ntokens from the LLM and using this data to refine the draft\nmodel through distillation. PaSS [241] proposes utilizing the\ntarget LLM itself as the draft model, incorporating trainable\ntokens (look-ahead tokens) into the input sequence to enable\nsimultaneous generation of subsequent tokens. REST [242]\nintroduces a retrieval-based speculative decoding approach,\nemploying a non-parametric retrieval data store as the draft\nmodel. SpecInfer [243] introduces a collective boost-tuning\ntechnique to align the output distribution of a group of\ndraft models with that of the target LLM. Lookahead decod-\ning [246] involves generating n-grams of the target LLM in\nparallel to aid in generating draft tokens. Medusa [50] fine-\ntunes several heads of the LLM specifically for generating\nsubsequent draft tokens. Eagle [247] adopts a lightweight\ntransformer layer called an auto-regression head to gener-\nate draft tokens in an auto-regressive manner, integrating\nrich contextual features from the target LLM into the draft\nmodel\u2019s input. Kangaroo [249] uses a fixed shallow sub-\nnetwork as the draft model, and trains a lightweight adapter\non the top of the sub-network. In this way, it does not need\nto train a separate draft model.\nAnother line of studies focuses on designing more effec-\ntive draft construction strategies. Conventional approaches\noften yield single draft token sequences, posing challenges\nfor passing verification. In response, Spectr [248] advocates\ngenerating multiple draft token sequences and employs a\nk-sequential draft selection technique to concurrently verify\nk sequences. This method leverages speculative sampling,\nensuring equivalence in output distributions. Similarly,\nSpecInfer [243] adopts a comparable approach. However,\nunlike Spectr, SpecInfer merges draft token sequences into a\n\u201ctoken tree\u201d and introduces a tree attention mechanism for\nvalidation. This strategy is called the \u201dtoken tree verifier\u201d.\nDue to its efficacy, token tree verifier has been widely em-\nbraced in numerous speculative decoding algorithms [50],\n[242], [244], [247]. In addition to these efforts, Stage Spec-\nulative Decoding [244] and Cascade Speculative Drafting\n(CS Drafting) [245] propose accelerating draft construction\nby integrating speculative decoding directly into the token\ngeneration process.\nComparative\nExperiments\nand\nAnalysis. We conduct\nan experiment to evaluate the speed-up performance of\n\n25\nthe speculative decoding methods. Specifically, we thor-\noughly review the studies of this field, and select six\nof them that have open-sourced their codes, i.e., Spec-\nulative Decoding (SpD) [236], [237], Lookahead Decod-\ning (LADE) [246], REST [242], Self-speculative Decoding\n(SSD) [239], Medusa [50] and Eagle [247]. As for the eval-\nuation dataset, we use Vicuna-80 [7] to evaluate the above\nmethods, which contains 80 questions that classified into\n10 categories. We report the average results on these 80\nquestions. As for target LLMs, we adopt five fashion open-\nsource LLMs, i.e., Vicuna-7B-V1.3 [7], Vicuna-13B-V1.3 [7],\nVicuna-33B-V1.3 [7], LLaMA-2-7B [5] and LLaMA-2-13B [5].\nWe report the range of evaluation metrics across these 5\nLLMs. As for draft models, we adopt two well-trained\ndraft models, i.e., LLaMA-68M and LLaMA-160M [243] for\nSpD. For other speculative decoding methods, we follow\ntheir proposed draft construction approach and use the\ncheckpoints they provide. As for the evaluation metrics, we\nadopt acceptance rate, which denotes the ratio of the number\nof accepted tokens to the number of generation steps, and\nspeed-up, which denotes the ratio of the latency of original\nauto-regressive decoding to the latency of speculative de-\ncoding when fixing the total length of output.\nTab. 5 provides a comparison of various speculative\ndecoding methods, highlighting several key observations:\n(1) Eagle demonstrates exceptional performance, achieving\na notable 3.47\u223c3.72\u00d7 end-to-end speed-up across multiple\nLLMs. To understand its success, a deeper analysis of Eagle\nreveals two key factors. Firstly, Eagle employs an auto-\nregressive approach for decoding draft tokens, leveraging\ninformation from previously generated tokens directly. Sec-\nondly, Eagle integrates rich features from previous tokens of\nboth original LLMs and draft models to enhance the accu-\nracy of the next draft token generation. (2) The token tree\nverifier proves to be an effective technique in enhancing the\nperformance of speculative decoding methods. (3) The end-\nto-end speed-up achieved by these methods is often lower\nthan the acceptance rate. This difference arises due to the\npractical consideration that the generation cost associated\nwith draft models cannot be overlooked.\n6.1.3\nOffloading\nCurrent research investigates the potential of offloading to\naccommodate the substantial memory demand of LLMs (see\nSec. 2.3) in resource-constrained environments. The essence\nof offloading is to offload part of the storage from the GPU to\nthe CPU when it is free of use. Intuitively, the focus of such\nkind of research lies in hiding the expensive data move-\nment latency between the GPU and the CPU. FlexGen [203]\nenables the offloading of weights, activations, and the KV\ncache, and further formulates a graph traversal problem for\noffloading to maximize the throughput. The data loading of\nthe next batch and the data storing of the previous batch can\nbe overlapped with the computation of the current batch.\nAnother work llama.cpp [250] also assigns computational\ntasks to the CPU, mitigating the data transfer overhead at\nthe cost of computing with the low-powered CPU. Powerin-\nfer [251] exploits the sparsity in activations using ReLU [271]\nin LLMs, and divides the activations into subsets of cold and\nhot neurons representing the frequency of computation. The\ncold neurons are offloaded to the CPU for both storage and\ncomputation in Powerinfer. Leveraging adaptive predictors\nand sparse operators, Powerinfer significantly improves the\ncomputational efficiency with offloading. FastDecode [252]\nproposes to offload the storage and the computation of the\nentire attention operator to the CPU. Since the attention\noperation is computed on the CPU, the data movement\nof KV cache is reduced to merely some activations. The\nnumber of CPUs is selected to match the workload latency\non GPUs so that the bubbles in the heterogeneous pipeline\nare mitigated.\n6.2\nServing System\nThe optimizations for serving systemworks are dedicated\nto improving the efficiency in handling asynchronous re-\nquests. The memory management is optimized to hold more\nrequests, and efficient batching and scheduling strategies\nare integrated to enhance the system throughput. Besides,\noptimizations specific to distributed systems are proposed\nto exploit distributed computational resources.\n6.2.1\nMemory Management\nThe storage of KV cache dominates the memory usage in\nLLM serving, especially when the context length is long\n(see Sec. 2.3). Since the generation length is uncertain, it\nis challenging to allocate the space for KV cache storage\nin advance. Earlier implementations [286] usually allocate\nstorage space in advance based on the preset maximum\nlength of each request. However, in instances where re-\nquest generation is terminated early, this approach incurs\nsignificant wastage of storage resources. To address the\nissue, S3 [284] proposes to predict an upper bound of the\ngeneration length for each request, in order to reduce the\nwaste of the pre-allocated space. However, the static way\nof KV cache memory allocation still fails when no such\nlarge contiguous space exists. To deal with the fragmented\nstorage, vLLM [51] proposes to store the KV cache in a\npaged manner following the operating system. vLLM first\nallocates a memory space as large as possible and divides\nit equally into multiple physical blocks. When a request\ncomes, vLLM dynamically maps the generated KV cache to\nthe pre-allocated physical blocks in a discontinuous fashion.\nIn this way, vLLM significantly reduces storage fragmenta-\ntion and achieves a higher throughput in LLM serving. On\nthe basis of vLLM, LightLLM [278] uses a more fine-grained\nKV cache storage to cut down the waste happening with\nthe irregular boundary. Instead of a block, LightLLM treats\nthe KV cache of a token as a unit, so that the generated KV\ncache always saturates the pre-allocated space.\nCurrent optimized service systems commonly employ\nthis paged approach to manage the KV cache storage,\nthereby mitigating the waste of redundant KV cache mem-\nory. However, the paged storage leads to irregular memory\naccess in the attention operator. For the attention operator\nusing the paged KV cache, this necessitates the consider-\nation of the mapping relationship between the virtual ad-\ndress space of the KV cache and its corresponding physical\naddress space. To enhance the efficiency of the attention\noperator, the loading pattern of the KV cache must be tai-\nlored to facilitate contiguous memory access. For instance,\nin the case of the PagedAttention by vLLM [51], the storage\n\n26\nServing System\nDistributed Systems\nSplitwise [272], TetriInfer [273], Dist-\nServe [274], SpotServe [275], Infinite-LLM [276]\nScheduling\nORCA [277], vLLM [51], LightLLM [278],\nDeepSpeed-FastGen [279], FastServe [280],\nVTC [281]\nBatching\nORCA [277], vLLM [51], Sarathi [282],\nDeepSpeed-FastGen [279], Sarathi-Serve [283],\nLightLLM [278]\nMemory Management\nS3 [284], vLLM [51], LightLLM [278], FlashIn-\nfer [285]\nFig. 17. Taxonomy of the optimization for LLM serving system.\nof the head size dimension is structured as a 16-byte con-\ntiguous vector for K cache, while FlashInfer [285] orches-\ntrates diverse data layouts for the KV cache, accompanied\nby an appropriately designed memory access scheme. The\noptimization of the attention operator in conjunction with\npaged KV cache storage remains a forefront challenge in the\nadvancement of serving systems.\n6.2.2\nContinuous Batching\nThe request lengths in a batch can be different, leading\nto low utilization when shorter requests are finished and\nlonger requests are still running. Due to the asynchronous\nnature of requests in serving scenarios, there exists an\nopportunity that such periods of low utilization could be\nmitigated. The continuous batching technique is proposed\nto leverage the opportunity by batching new requests once\nsome old requests are finished. ORCA [277] is the first to\nutilize the continuous batching technique in LLM serving.\nThe computation of each request encompasses multiple\niterations, with each iteration representing either a pre-\nfilling step or a decoding step. The author suggests that\ndifferent requests can be batched at the iteration level. The\nwork implements iteration-level batching in linear oper-\nators, concatenating different requests together in the se-\nquence dimension. Hence, the spare storage and computa-\ntional resources corresponding to the completed requests\nare promptly released. Following ORCA, vLLM [51] ex-\ntends the technique to the attention computation, enabling\nrequests with different KV cache lengths to be batched to-\ngether. Sarathi [282], DeepSpeed-FastGen [279] and Sarathi-\nServe [283] further introduce a split-and-fuse method to\nbatch together prefilling requests and decoding requests.\nSpecifically, this method first splits the long prefilling re-\nquest in the sequence dimension, and then batches it to-\ngether with multiple short decoding requests. The split-and-\nfuse method balances the workloads among different itera-\ntions, and significantly reduces the tail latency via removing\nthe stalls from new requests. LightLLM [278] also adopts the\nsplit-and-fuse method.\nThe split-and-fuse technology operates on the premise\nthat requests during the prefilling stage can be partitioned\ninto discrete chunks. Chunked-prefill methodology involves\nsegmenting prefilling requests along the sequence dimen-\nsion, thereby preventing the potential bottlenecks for other\nrequests. This strategy capitalizes on the auto-regressive\ncharacteristics inherent in LLMs, where attention compu-\ntation only relies on prior tokens. Consequently, the math-\nematical equivalence of chunked-prefill technology is guar-\nanteed, positioning it as a leading approach for reducing\nrequest latency in LLM serving.\n6.2.3\nScheduling Strategy\nIn LLM serving, the job length of each request exhibits\nvariability, and hence the order of executing requests sig-\nnificantly impacts the throughput of the serving system.\nThe head-of-line blocking [280] happens when long requests\nare accorded priority. Specifically, memory usage grows\nrapidly in response to long requests, impeding subsequent\nrequests when the system exhausts its memory capacity. The\npioneering work ORCA [277] and open-source systems, in-\ncluding vLLM [51] and LightLLM [278], employ the simple\nfirst-come-first-serve (FCFS) principle to schedule requests.\nDeepSpeed-FastGen [279] gives priority to the decoding re-\nquests to enhance the performance. FastServe [280] proposes\na preemptive scheduling strategy to optimize the head-of-\nline blocking problem, achieving low job completion time\n(JCT) in LLM serving. FastServe employs a multi-level\nfeedback queue (MLFQ) to prioritize the requests with the\nshortest remaining time. Since the auto-regressive decod-\ning approach poses unknown request lengths, FastServe\npredicts the length first and utilizes a skip-join fashion to\nfind the proper priority for each request. Unlike previous\nwork, VTC [281] discusses the fairness in LLM serving.\nVTC introduces a cost function based on token numbers to\nmeasure fairness among clients, and further proposes a fair\nscheduler to ensure fairness.\n6.2.4\nDistributed Systems\nIn order to achieve high throughput, LLM services are\ncommonly deployed on distributed platforms. Recent works\nhave additionally focused on optimizing the performance\nof such inference services by exploiting distributed char-\nacteristics. Notably, observing that the computations of\nprefilling and decoding have interference with each other,\nsplitwise [272], TetriInfer [273] and DistServe [274] demon-\nstrate the efficiency of disaggregating the prefilling and the\ndecoding steps of a request. In this way, the two distinct\nsteps are processed independently based on their char-\nacteristics. ExeGPT [287] also adopts such disaggregated\narchitecture, and proposes different strategies with con-\ntrollable variables to maximize system throughput under\n\n27\nTABLE 6\nComparison of multiple open-source inference engines and serving systems. \u201d-\u201d denotes no serving support. Note that the scheduling method of\nTensorRT-LLM is not open-sourced.\nModel\nInference Optimization\nInference\n(token/s)\nServing Optimization\nServing\n(req/s)\nAttention\nLinear\nGraph\nSpeculative Decoding\nMemory\nBatching\nScheduling\nHuggingFace [260]\n\u2713\n38.963\n-\n-\n-\n-\nDeepSpeed [258]\n\u2713\n\u2713\n80.947\nblocked\nsplit-and-fuse\ndecode prioritized\n6.78\nvLLM [51]\n\u2713\n\u2713\n90.052\npaged\ncontinuous batching\nprefill prioritized\n7.11\nOpenPPL [263]\n\u2713\n\u2713\n81.169\n-\n-\n-\n-\nFlashDecoding++ [253]\n\u2713\n\u2713\n\u2713\n106.636\n-\n-\n-\n-\nLightLLM [278]\n\u2713\n73.599\ntoken-wise\nsplit-and-fuse\nprefill prioritized\n10.29\nTensorRT-LLM [222]\n\u2713\n\u2713\n\u2713\n\u2713\n92.512\npaged\ncontinuous batching\n-\n5.87\ncertain latency constraints. Llumnix [288] reschedules the\nrequests at runtime for different serving objectives including\nload balancing, de-fragmentation, and prioritization. Spot-\nServe [275] is designed to provide LLM service on clouds\nwith preemptible GPU instances. SpotServe efficiently han-\ndles challenges including dynamic parallel control and in-\nstance migration, and also utilizes the auto-regressive nature\nof LLMs to achieve token-level state recovery. Moreover,\nInfinite-LLM [276] parallels different parts of the sequence\nin the attention operator across the data center, to ad-\ndress the challenges when serving extremely long contexts.\nLoongServe [289] proposes the elastic sequence parallelism\nto manage the elastic resource demand at the iteration level,\nreducing the data movement of KV cache via elaborately\ndesigned scheduling.\n6.3\nHardware Accelerator Design\nPrevious research efforts [290], [291], [292] have focused on\noptimizing Transformer architectures, particularly enhanc-\ning the attention operator, often employing sparse methods\nto facilitate FPGA deployment. The FACT [293] accelera-\ntor achieves superior energy efficiency compared to the\nNVIDIA V100 GPU through mixed-precision quantization\nfor linear operators and algorithm-hardware co-design, yet\nthese approaches are not tailored for generative LLMs.\nRecent work like ALLO [294] highlights FPGA advan-\ntages in managing the memory-intensive decoding stage\nand emphasizes the importance of model compression tech-\nniques for LLMs\u2019 efficient FPGA deployment. Conversely,\nDFX [295] focuses on decoding stage optimizations but lacks\nmodel compression methods, limiting scalability to larger\nmodels and longer inputs (up to 1.5B model and 256 tokens).\nALLO builds on these insights, further offering a library\nof High-level Synthesis (HLS) kernels that are composable\nand reusable. ALLO\u2019s implementation demonstrates supe-\nrior generation speed-up compared to DFX in the prefilling\nstage, achieving enhanced energy efficiency and speedup\nover the NVIDIA A100 GPU during decoding.\nFlightLLM [296] also leverages these insights, introduc-\ning a configurable sparse digital signal processor (DSP)\nchain for various sparsity patterns with high computa-\ntional efficiency. It proposes an always-on-chip decode\nscheme with mixed-precision support to enhance mem-\nory bandwidth utilization. FlightLLM achieves 6.0\u00d7 higher\nenergy efficiency and 1.8\u00d7 better cost efficiency than the\nNVIDIA V100S GPU for Llama2-7B models, with 1.2\u00d7\nhigher throughput than the NVIDIA A100 GPU during\ndecoding.\n6.4\nComparison of LLM Frameworks\nWe compare the performance of multiple LLM frame-\nworks in Table 6. The inference throughput is measured\nwith Llama2-7B (batch size=1, input length=1k, output\nlength=128). The serving performance is the maximum\nthroughput measured on the ShareGPT [297] dataset. Both\nare derived on a single NVIDIA A100 80GB GPU. Among\nthe mentioned frameworks, DeepSpeed [258], vLLM [51],\nLightLLM [278] and TensorRT-LLM [222] integrate the serv-\ning function to serve asynchronous requests from multiple\nusers. We also list the optimizations for each framework in\nthe table. All the frameworks except HuggingFace imple-\nment operator-level or graph-level optimizations to enhance\nperformance, and some of them also support the speculative\ndecoding technique. Note that the speculative decoding\ntechnique is off when we measure the inference perfor-\nmance for all frameworks. The results of inference through-\nput show that FlashDecoding++ and TensorRT-LLM out-\nperform others with optimizations covering predominant\noperators and the computational graph. From the aspect of\nserving, all the frameworks use fine-grained and discontigu-\nous storage for KV cache, and apply the continuous batching\ntechniques to improve the system utilization. Unlike vLLM\nand LightLLM, DeepSpeed prioritizes the decoding requests\nin scheduling, which means no new request is merged if\nthere are enough existing decoding requests in the batch.\n6.5\nKnowledge, Suggestions and Future Direction\nThe system-level optimization improves efficiency while\nbringing no accuracy degradation, hence becoming preva-\nlent in the LLM inference practice. The optimization for\ninference is also applicable to serving. Recently, the oper-\nator optimization has been closely combined with practi-\ncal serving scenarios, e.g.,, RadixAttention [52] designed\nspecifically for prefix caching, and tree attention\n[243] to\naccelerate speculative decoding verification. The iterating of\napplications and scenarios will continue to put forward new\nrequirements for operator development.\nGiven the multifaceted objectives inherent in real-world\nserving systems, such as JCT, system throughput, and fair-\nness, the design of scheduling strategies becomes corre-\nspondingly intricate. Within the domain of LLM serving,\nwhere the length of requests is indeterminate, extant litera-\nture commonly relies on predictive mechanisms to facilitate\n\n28\nthe design of scheduling strategies. However, the efficacy\nof current predictors [273] falls short of ideal standards,\nindicating the potential for refinement and optimization in\nserving scheduling strategy development.\n7\nDISCUSSIONS OF KEY APPLICATION SCENAR-\nIOS\nCurrent research endeavors have made significant strides in\nexploring the boundaries of efficient LLM inference across\nvarious optimization levels. However, further studies are\nwarranted to enhance LLM efficiency in practical scenarios.\nWe have provided promising future directions for opti-\nmization techniques at the data-level (Sec. 4.3), model-level\n(Sec. 5.3), and system-level (Sec. 6.5). In this section, we\nsummarize four critical scenarios: agent and multi-model\nframework, long-context LLMs, edge scenario deployment,\nand security-efficiency synergy, and provide a broader dis-\ncussion on them.\nAgent and Multi-Model Framework. As discussed in\nSec. 4.3, recent advancements in agent and multi-model\nframeworks [55], [56], [57] have significantly improved\nagents\u2019 capabilities to handle complex tasks and human\nrequests by harnessing the powerful abilities of LLMs. These\nframeworks, while increasing the computational demands\nof LLMs, introduce more parallelism into the structure of\nLLMs\u2019 output content, thereby creating opportunities for\ndata-level and system-level optimizations such as output or-\nganization techniques [52]. Furthermore, these frameworks\nnaturally introduce a new optimization level, i.e., pipeline-\nlevel, which holds potential for efficiency enhancements at\nthis level [58].\nIn addition, there is a growing research trend [298] fo-\ncused on extending AI agents into the multimodal domain,\nwhich often utilize Large Multimodal Models (LMMs) as\nthe core of these agent systems. To enhance the efficiency of\nthese emerging LMM-based agents, designing optimization\ntechniques for LMMs is a promising research direction.\nLong-Context LLMs. Currently, LLMs face the challenge\nof handling increasingly longer input contexts. However,\nthe self-attention operation, the fundamental component\nof Transformer-style LLMs, exhibits quadratic complexity\nin relation to the context length, imposing constraints on\nmaximum context length during both training and infer-\nence phases. Various strategies have been explored to ad-\ndress this limitation, including input compression (Sec. 4.1),\nsparse attention (Sec. 5.2.2), design of low-complexity struc-\ntures (Sec. 5.1.3), and optimization of attention opera-\ntors (Sec. 6.1.1). Notably, non-Transformer architectures\n(Sec. 5.1.3) with sub-quadratic or linear complexity have\nrecently garnered significant interest from researchers.\nDespite their efficiency, the competitiveness of these\nnovel architectures compared to the Transformer archi-\ntecture across various abilities, such as in-context learn-\ning ability and long-range modeling ability, is still under\nscrutiny [76], [299]. Therefore, exploring the capabilities of\nthese new architectures from multiple angles and address-\ning their limitations remains a valuable pursuit. Moreover,\nit is crucial to determine the necessary context lengths for\nvarious scenarios and tasks, as well as identify the next-\ngeneration architecture that will serve as the foundational\nbackbone for LLMs in the future.\nEdge Scenario Deployment. While considerable efforts\nhave been directed towards enhancing the efficiency of\nLLM inference, deploying LLMs onto extremely resource-\nconstrained edge devices like mobile phones presents ongo-\ning challenges. Recently, numerous researchers [300], [301],\n[302], [303], [304], [305], [306], [307], [308], [309], [310] have\nshown interest in pre-training smaller language models\nwith 1B to 3B parameters. Models of this scale offer re-\nduced resource costs during inference and hold potential for\nachieving generalization abilities and competitive perfor-\nmance compared to larger models. However, the methods\nto develop such efficient and powerful smaller language\nmodels remain under-explored.\nSeveral studies have initiated this promising direction.\nFor instance, MiniCPM [309] conducts sandbox experi-\nments to determine optimal pre-training hyper-parameters.\nPanGu-\u03c0-Pro [302] suggests initializing model weights from\npre-trained LLMs using metrics and techniques from model\npruning. MobileLLM [310] adopts a\u201cdeep and thin\u201d archi-\ntecture for small model design and proposes weight sharing\nacross different layers to increase the number of layers\nwithout additional memory costs. Nevertheless, a perfor-\nmance gap still exists between small and large models,\nnecessitating future studies to narrow this gap. In the future,\nthere is a crucial need for research aimed at identifying the\nmodel scale limited in the edge scenarios, and exploring the\nboundaries of various optimization methods on designing\nsmaller models.\nBeyond designing smaller models, system-level opti-\nmization offers a promising direction in LLM deployment. A\nnotable recent project, MLC-LLM [311], successfully deploys\nthe LLaMA-7B model on mobile phones. MLC-LLM pri-\nmarily employs compilation techniques like fusion, memory\nplanning, and loop optimization to enhance latency and re-\nduce memory cost during inference. Additionally, adopting\nthe cloud-edge collaboration techniques, or designing more\nsophisticated hardware accelerators can also help deploy\nLLMs onto edge devices.\nSecurity-Efficiency Synergy. In addition to task perfor-\nmance and efficiency, security is also a crucial factor that\nmust be considered in LLM applications [312], [313]. Cur-\nrent research primarily focuses on efficiency optimiza-\ntion without adequately addressing security considerations.\nTherefore, it is critical to investigate the interplay between\nefficiency and security and determine whether the current\noptimization techniques compromise the security of LLMs.\nIf these techniques negatively impacts LLMs\u2019 security, a\npromising direction would involve developing new opti-\nmization methods or refining the existing ones to achieve\na better trade-off between LLMs\u2019 efficiency and security.\n8\nCONCLUSION\nEfficient LLM inference focuses on reducing the compu-\ntational, memory access, and memory costs during LLM\ninference processes, aiming to optimize efficiency metrics\nsuch as latency, throughput, storage, power, and energy.\nThis survey offers a comprehensive review of efficient LLM\ninference research, presenting insights, recommendations,\n\n29\nand future directions for key techniques. Initially, we intro-\nduce a hierarchical taxonomy encompassing data-, model-\n, and system-level optimizations. Subsequently, guided by\nthis taxonomy, we meticulously examine and summarize\nstudies at each level and sub-field. For well-established\ntechniques like model quantization and efficient serving\nsystems, we conduct experiments to evaluate and analyze\ntheir performance. Based on these analyses, we offer practi-\ncal suggestions and identify promising research avenues for\npractitioners and researchers in the field.\nACKNOWLEDGEMENTS\nThis work was supported by National Natural Science\nFoundation of China (No. 62325405, 62104128, U19B2019,\nU21B2031, 61832007, 62204164), Tsinghua EE Xilinx AI Re-\nsearch Fund, and Beijing National Research Center for In-\nformation Science and Technology (BNRist). We thank for\nall the support from Infinigence-AI. We thank Xiangsheng\nShi, Zinan Lin, Xinhao Yang, Hongyi Wang, Linfeng Zhang,\nYulin Wang, Xuemin Sun, Saiqian Zhang for their valuable\nsuggestions on the paper. We thank Shengxiang Wang, Qiuli\nMao for providing the efficiency profiling data of quantized\noperators.\nREFERENCES\n[1]\nA. Radford, K. Narasimhan, T. Salimans, I. Sutskever et al.,\n\u201cImproving language understanding by generative pre-training,\u201d\n2018.\n[2]\nA. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever\net al., \u201cLanguage models are unsupervised multitask learners,\u201d\nOpenAI blog, vol. 1, no. 8, p. 9, 2019.\n[3]\nT. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhari-\nwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al.,\n\u201cLanguage models are few-shot learners,\u201d Advances in neural\ninformation processing systems, vol. 33, pp. 1877\u20131901, 2020.\n[4]\nS. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen,\nC. Dewan, M. Diab, X. Li, X. V. Lin et al., \u201cOpt: Open pre-trained\ntransformer language models,\u201d arXiv preprint arXiv:2205.01068,\n2022.\n[5]\nH. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,\nT. Lacroix, B. Rozi`ere, N. Goyal, E. Hambro, F. Azhar et al.,\n\u201cLlama: Open and efficient foundation language models,\u201d arXiv\npreprint arXiv:2302.13971, 2023.\n[6]\nA. Yang, B. Xiao, B. Wang, B. Zhang, C. Bian, C. Yin, C. Lv, D. Pan,\nD. Wang, D. Yan et al., \u201cBaichuan 2: Open large-scale language\nmodels,\u201d arXiv preprint arXiv:2309.10305, 2023.\n[7]\nW.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang, L. Zheng,\nS. Zhuang, Y. Zhuang, J. E. Gonzalez et al., \u201cVicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt quality,\u201d See\nhttps://vicuna. lmsys. org (accessed 14 April 2023), 2023.\n[8]\nD. Li, R. Shao, A. Xie, Y. Sheng, L. Zheng, J. Gonzalez, I. Stoica,\nX. Ma, and H. Zhang, \u201cHow long can context length of open-\nsource llms truly promise?\u201d in NeurIPS 2023 Workshop on Instruc-\ntion Tuning and Instruction Following, 2023.\n[9]\nB. Workshop, T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili\u00b4c,\nD. Hesslow, R. Castagn\u00b4e, A. S. Luccioni, F. Yvon et al., \u201cBloom: A\n176b-parameter open-access multilingual language model,\u201d arXiv\npreprint arXiv:2211.05100, 2022.\n[10]\nE. Almazrouei, H. Alobeidli, A. Alshamsi, A. Cappelli, R. Cojo-\ncaru, M. Debbah, \u00b4E. Goffinet, D. Hesslow, J. Launay, Q. Malartic\net al., \u201cThe falcon series of open language models,\u201d arXiv preprint\narXiv:2311.16867, 2023.\n[11]\nZ. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, and J. Tang,\n\u201cGlm: General language model pretraining with autoregressive\nblank infilling,\u201d arXiv preprint arXiv:2103.10360, 2021.\n[12]\nA. Q. Jiang, A. Sablayrolles, A. Roux, A. Mensch, B. Savary,\nC. Bamford, D. S. Chaplot, D. d. l. Casas, E. B. Hanna, F. Bressand\net al., \u201cMixtral of experts,\u201d arXiv preprint arXiv:2401.04088, 2024.\n[13]\nJ. Yang, H. Jin, R. Tang, X. Han, Q. Feng, H. Jiang, S. Zhong,\nB. Yin, and X. Hu, \u201cHarnessing the power of llms in practice: A\nsurvey on chatgpt and beyond,\u201d ACM Transactions on Knowledge\nDiscovery from Data, 2023.\n[14]\nJ. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le,\nD. Zhou et al., \u201cChain-of-thought prompting elicits reasoning in\nlarge language models,\u201d Advances in Neural Information Processing\nSystems, vol. 35, pp. 24 824\u201324 837, 2022.\n[15]\nM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan,\nH. Edwards, Y. Burda, N. Joseph, G. Brockman et al., \u201cEval-\nuating large language models trained on code,\u201d arXiv preprint\narXiv:2107.03374, 2021.\n[16]\nS. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz,\nE. Kamar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg et al., \u201cSparks\nof artificial general intelligence: Early experiments with gpt-4,\u201d\narXiv preprint arXiv:2303.12712, 2023.\n[17]\nX. Zhu, J. Li, Y. Liu, C. Ma, and W. Wang, \u201cA survey on\nmodel compression for large language models,\u201d arXiv preprint\narXiv:2308.07633, 2023.\n[18]\nS. Park, J. Choi, S. Lee, and U. Kang, \u201cA comprehensive survey\nof compression algorithms for language models,\u201d arXiv preprint\narXiv:2401.15347, 2024.\n[19]\nW. Wang, W. Chen, Y. Luo, Y. Long, Z. Lin, L. Zhang, B. Lin,\nD. Cai, and X. He, \u201cModel compression and efficient infer-\nence for large language models: A survey,\u201d arXiv preprint\narXiv:2402.09748, 2024.\n[20]\nY. Tang, Y. Wang, J. Guo, Z. Tu, K. Han, H. Hu, and\nD. Tao, \u201cA survey on transformer compression,\u201d arXiv preprint\narXiv:2402.05964, 2024.\n[21]\nT. Ding, T. Chen, H. Zhu, J. Jiang, Y. Zhong, J. Zhou, G. Wang,\nZ. Zhu, I. Zharkov, and L. Liang, \u201cThe efficiency spectrum of\nlarge language models: An algorithmic survey,\u201d arXiv preprint\narXiv:2312.00678, 2023.\n[22]\nX. Miao, G. Oliaro, Z. Zhang, X. Cheng, H. Jin, T. Chen,\nand Z. Jia, \u201cTowards efficient generative large language model\nserving: A survey from algorithms to systems,\u201d arXiv preprint\narXiv:2312.15234, 2023.\n[23]\nZ. Wan, X. Wang, C. Liu, S. Alam, Y. Zheng, Z. Qu, S. Yan, Y. Zhu,\nQ. Zhang, M. Chowdhury et al., \u201cEfficient large language models:\nA survey,\u201d arXiv preprint arXiv:2312.03863, vol. 1, 2023.\n[24]\nM. Xu, W. Yin, D. Cai, R. Yi, D. Xu, Q. Wang, B. Wu, Y. Zhao,\nC. Yang, S. Wang et al., \u201cA survey of resource-efficient llm and\nmultimodal foundation models,\u201d arXiv preprint arXiv:2401.08092,\n2024.\n[25]\nW. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min,\nB. Zhang, J. Zhang, Z. Dong et al., \u201cA survey of large language\nmodels,\u201d arXiv preprint arXiv:2303.18223, 2023.\n[26]\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, \u0141. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d\nAdvances in neural information processing systems, vol. 30, 2017.\n[27]\nZ. Yuan, Y. Shang, Y. Zhou, Z. Dong, C. Xue, B. Wu, Z. Li,\nQ. Gu, Y. J. Lee, Y. Yan et al., \u201cLlm inference unveiled: Survey and\nroofline model insights,\u201d arXiv preprint arXiv:2402.16363, 2024.\n[28]\nA. Golden, S. Hsia, F. Sun, B. Acun, B. Hosmer, Y. Lee, Z. DeVito,\nJ. Johnson, G.-Y. Wei, D. Brooks et al., \u201cIs flash attention stable?\u201d\narXiv preprint arXiv:2405.02803, 2024.\n[29]\nP. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal,\nH. K\u00a8uttler, M. Lewis, W.-t. Yih, T. Rockt\u00a8aschel et al., \u201cRetrieval-\naugmented generation for knowledge-intensive nlp tasks,\u201d Ad-\nvances in Neural Information Processing Systems, vol. 33, pp. 9459\u2013\n9474, 2020.\n[30]\nA. Chevalier, A. Wettig, A. Ajith, and D. Chen, \u201cAdapt-\ning language models to compress contexts,\u201d arXiv preprint\narXiv:2305.14788, 2023.\n[31]\nW. Shi, S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis,\nL. Zettlemoyer, and W. tau Yih, \u201cReplug: Retrieval-augmented\nblack-box language models,\u201d 2023.\n[32]\nA. Asai, Z. Wu, Y. Wang, A. Sil, and H. Hajishirzi, \u201cSelf-\nrag: Learning to retrieve, generate, and critique through self-\nreflection,\u201d 2023.\n[33]\nD. Wingate, M. Shoeybi, and T. Sorensen, \u201cPrompt compres-\nsion and contrastive conditioning for controllability and toxicity\nreduction in language models,\u201d arXiv preprint arXiv:2210.03162,\n2022.\n[34]\nJ. Mu, X. L. Li, and N. Goodman, \u201cLearning to compress prompts\nwith gist tokens,\u201d arXiv preprint arXiv:2304.08467, 2023.\n\n30\n[35]\nT. Ge, J. Hu, X. Wang, S.-Q. Chen, and F. Wei, \u201cIn-context\nautoencoder for context compression in a large language model,\u201d\narXiv preprint arXiv:2307.06945, 2023.\n[36]\nF. Xu, W. Shi, and E. Choi, \u201cRecomp: Improving retrieval-\naugmented lms with compression and selective augmentation,\u201d\narXiv preprint arXiv:2310.04408, 2023.\n[37]\nW. Fei, X. Niu, P. Zhou, L. Hou, B. Bai, L. Deng, and W. Han, \u201cEx-\ntending context window of large language models via semantic\ncompression,\u201d arXiv preprint arXiv:2312.09571, 2023.\n[38]\nW. Zhou, Y. E. Jiang, R. Cotterell, and M. Sachan, \u201cEfficient\nprompting via dynamic in-context learning,\u201d arXiv preprint\narXiv:2305.11170, 2023.\n[39]\nY. Li, B. Dong, F. Guerin, and C. Lin, \u201cCompressing context\nto enhance inference efficiency of large language models,\u201d in\nProceedings of the 2023 Conference on Empirical Methods in Natural\nLanguage Processing, 2023, pp. 6342\u20136353.\n[40]\nF. Yin, J. Vig, P. Laban, S. Joty, C. Xiong, and C.-S. J. Wu, \u201cDid you\nread the instructions? rethinking the effectiveness of task defi-\nnitions in instruction learning,\u201d arXiv preprint arXiv:2306.01150,\n2023.\n[41]\nH. Jung and K.-J. Kim, \u201cDiscrete prompt compression with\nreinforcement learning,\u201d arXiv preprint arXiv:2308.08758, 2023.\n[42]\nH. Jiang, Q. Wu, C.-Y. Lin, Y. Yang, and L. Qiu, \u201cLlmlingua:\nCompressing prompts for accelerated inference of large language\nmodels,\u201d in The 2023 Conference on Empirical Methods in Natural\nLanguage Processing, 2023.\n[43]\nH. Jiang, Q. Wu, X. Luo, D. Li, C.-Y. Lin, Y. Yang, and\nL. Qiu, \u201cLongllmlingua: Accelerating and enhancing llms in\nlong context scenarios via prompt compression,\u201d arXiv preprint\narXiv:2310.06839, 2023.\n[44]\nX. Huang, L. L. Zhang, K.-T. Cheng, and M. Yang, \u201cBoosting llm\nreasoning: Push the limits of few-shot learning with reinforced\nin-context pruning,\u201d arXiv preprint arXiv:2312.08901, 2023.\n[45]\nQ. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, J. Xu,\nand Z. Sui, \u201cA survey for in-context learning,\u201d arXiv preprint\narXiv:2301.00234, 2022.\n[46]\nX. L. Li and P. Liang, \u201cPrefix-tuning: Optimizing continuous\nprompts for generation,\u201d in Proceedings of the 59th Annual Meeting\nof the Association for Computational Linguistics and the 11th Inter-\nnational Joint Conference on Natural Language Processing (Volume 1:\nLong Papers), 2021, pp. 4582\u20134597.\n[47]\nX. Ning, Z. Lin, Z. Zhou, H. Yang, and Y. Wang, \u201cSkeleton-of-\nthought: Large language models can do parallel decoding,\u201d arXiv\npreprint arXiv:2307.15337, 2023.\n[48]\nS. Jin, Y. Wu, H. Zheng, Q. Zhang, M. Lentz, Z. M. Mao,\nA. Prakash, F. Qian, and D. Zhuo, \u201cAdaptive skeleton graph\ndecoding,\u201d arXiv preprint arXiv:2402.12280, 2024.\n[49]\nM. Liu, A. Zeng, B. Wang, P. Zhang, J. Tang, and Y. Dong,\n\u201cApar: Llms can do auto-parallel auto-regressive decoding,\u201d\narXiv preprint arXiv:2401.06761, 2024.\n[50]\nT. Cai, Y. Li, Z. Geng, H. Peng, J. D. Lee, D. Chen, and T. Dao,\n\u201cMedusa: Simple llm inference acceleration framework with mul-\ntiple decoding heads,\u201d 2024.\n[51]\nW. Kwon, Z. Li, S. Zhuang, Y. Sheng, L. Zheng, C. H. Yu,\nJ. Gonzalez, H. Zhang, and I. Stoica, \u201cEfficient memory manage-\nment for large language model serving with pagedattention,\u201d in\nProceedings of the 29th Symposium on Operating Systems Principles,\n2023, pp. 611\u2013626.\n[52]\nL. Zheng, L. Yin, Z. Xie, J. Huang, C. Sun, C. H. Yu, S. Cao,\nC. Kozyrakis, I. Stoica, J. E. Gonzalez et al., \u201cEfficiently pro-\ngramming large language models using sglang,\u201d arXiv preprint\narXiv:2312.07104, 2023.\n[53]\nS. Yao, D. Yu, J. Zhao, I. Shafran, T. Griffiths, Y. Cao, and\nK. Narasimhan, \u201cTree of thoughts: Deliberate problem solving\nwith large language models,\u201d Advances in Neural Information\nProcessing Systems, vol. 36, 2024.\n[54]\nM. Besta, N. Blach, A. Kubicek, R. Gerstenberger, M. Podstawski,\nL. Gianinazzi, J. Gajda, T. Lehmann, H. Niewiadomski, P. Nyczyk\net al., \u201cGraph of thoughts: Solving elaborate problems with\nlarge language models,\u201d in Proceedings of the AAAI Conference on\nArtificial Intelligence, vol. 38, no. 16, 2024, pp. 17 682\u201317 690.\n[55]\nZ. Xi, W. Chen, X. Guo, W. He, Y. Ding, B. Hong, M. Zhang,\nJ. Wang, S. Jin, E. Zhou et al., \u201cThe rise and potential of\nlarge language model based agents: A survey,\u201d arXiv preprint\narXiv:2309.07864, 2023.\n[56]\nQ. Sun, Z. Yin, X. Li, Z. Wu, X. Qiu, and L. Kong, \u201cCorex:\nPushing the boundaries of complex reasoning through multi-\nmodel collaboration,\u201d arXiv preprint arXiv:2310.00280, 2023.\n[57]\nT. Guo, X. Chen, Y. Wang, R. Chang, S. Pei, N. V. Chawla,\nO. Wiest, and X. Zhang, \u201cLarge language model based multi-\nagents: A survey of progress and challenges,\u201d arXiv preprint\narXiv:2402.01680, 2024.\n[58]\nL. Chen, M. Zaharia, and J. Zou, \u201cFrugalgpt: How to use large\nlanguage models while reducing cost and improving perfor-\nmance,\u201d arXiv preprint arXiv:2305.05176, 2023.\n[59]\nY. Li, T. Cai, Y. Zhang, D. Chen, and D. Dey, \u201cWhat makes\nconvolutional models great on long sequence modeling?\u201d arXiv\npreprint arXiv:2210.09298, 2022.\n[60]\nD. W. Romero, A. Kuzina, E. J. Bekkers, J. M. Tomczak, and\nM. Hoogendoorn, \u201cCkconv: Continuous kernel convolution for\nsequential data,\u201d arXiv preprint arXiv:2102.02611, 2021.\n[61]\nM. Poli, S. Massaroli, E. Nguyen, D. Y. Fu, T. Dao, S. Baccus,\nY. Bengio, S. Ermon, and C. R\u00b4e, \u201cHyena hierarchy: Towards larger\nconvolutional language models,\u201d in International Conference on\nMachine Learning.\nPMLR, 2023, pp. 28 043\u201328 078.\n[62]\nB. Peng, E. Alcaide, Q. Anthony, A. Albalak, S. Arcadinho,\nH. Cao, X. Cheng, M. Chung, M. Grella, K. K. GV et al.,\n\u201cRwkv: Reinventing rnns for the transformer era,\u201d arXiv preprint\narXiv:2305.13048, 2023.\n[63]\nY. Sun, L. Dong, S. Huang, S. Ma, Y. Xia, J. Xue, J. Wang, and\nF. Wei, \u201cRetentive network: A successor to transformer for large\nlanguage models,\u201d arXiv preprint arXiv:2307.08621, 2023.\n[64]\nA. Gu, T. Dao, S. Ermon, A. Rudra, and C. R\u00b4e, \u201cHippo: Recurrent\nmemory with optimal polynomial projections,\u201d Advances in neural\ninformation processing systems, vol. 33, pp. 1474\u20131487, 2020.\n[65]\nA. Gu, I. Johnson, K. Goel, K. Saab, T. Dao, A. Rudra, and\nC. R\u00b4e, \u201cCombining recurrent, convolutional, and continuous-\ntime models with linear state space layers,\u201d Advances in neural\ninformation processing systems, vol. 34, pp. 572\u2013585, 2021.\n[66]\nA. Gu, K. Goel, and C. R\u00b4e, \u201cEfficiently modeling long sequences\nwith structured state spaces,\u201d arXiv preprint arXiv:2111.00396,\n2021.\n[67]\nA. Gupta, A. Gu, and J. Berant, \u201cDiagonal state spaces are as ef-\nfective as structured state spaces,\u201d Advances in Neural Information\nProcessing Systems, vol. 35, pp. 22 982\u201322 994, 2022.\n[68]\nA. Gu, K. Goel, A. Gupta, and C. R\u00b4e, \u201cOn the parameterization\nand initialization of diagonal state space models,\u201d Advances in\nNeural Information Processing Systems, vol. 35, pp. 35 971\u201335 983,\n2022.\n[69]\nH. Mehta, A. Gupta, A. Cutkosky, and B. Neyshabur, \u201cLong\nrange language modeling via gated state spaces,\u201d in International\nConference on Learning Representations, 2023.\n[70]\nD. Y. Fu, T. Dao, K. K. Saab, A. W. Thomas, A. Rudra, and C. R\u00b4e,\n\u201cHungry hungry hippos: Towards language modeling with state\nspace models,\u201d arXiv preprint arXiv:2212.14052, 2022.\n[71]\nR. Hasani, M. Lechner, T.-H. Wang, M. Chahine, A. Amini, and\nD. Rus, \u201cLiquid structural state-space models,\u201d arXiv preprint\narXiv:2209.12951, 2022.\n[72]\nJ. T. Smith, A. Warrington, and S. W. Linderman, \u201cSimpli-\nfied state space layers for sequence modeling,\u201d arXiv preprint\narXiv:2208.04933, 2022.\n[73]\nJ. Pilault, M. Fathi, O. Firat, C. Pal, P.-L. Bacon, and R. Goroshin,\n\u201cBlock-state transformers,\u201d Advances in Neural Information Pro-\ncessing Systems, vol. 36, 2024.\n[74]\nJ. Wang, J. N. Yan, A. Gu, and A. M. Rush, \u201cPretraining without\nattention,\u201d arXiv preprint arXiv:2212.10544, 2022.\n[75]\nA. Gu and T. Dao, \u201cMamba: Linear-time sequence modeling with\nselective state spaces,\u201d arXiv preprint arXiv:2312.00752, 2023.\n[76]\nJ. Park, J. Park, Z. Xiong, N. Lee, J. Cho, S. Oymak, K. Lee,\nand D. Papailiopoulos, \u201cCan mamba learn how to learn? a\ncomparative study on in-context learning tasks,\u201d arXiv preprint\narXiv:2402.04248, 2024.\n[77]\nN. Shazeer, \u201cFast transformer decoding: One write-head is all\nyou need,\u201d arXiv preprint arXiv:1911.02150, 2019.\n[78]\nJ. Ainslie, J. Lee-Thorp, M. de Jong, Y. Zemlyanskiy, F. Lebr\u00b4on,\nand S. Sanghai, \u201cGqa: Training generalized multi-query trans-\nformer models from multi-head checkpoints,\u201d arXiv preprint\narXiv:2305.13245, 2023.\n[79]\nS. Wang, B. Z. Li, M. Khabsa, H. Fang, and H. Ma, \u201cLin-\nformer: Self-attention with linear complexity,\u201d arXiv preprint\narXiv:2006.04768, 2020.\n\n31\n[80]\nG. I. Winata, S. Cahyawijaya, Z. Lin, Z. Liu, and P. Fung,\n\u201cLightweight and efficient end-to-end speech recognition using\nlow-rank transformer,\u201d in ICASSP 2020-2020 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2020, pp. 6144\u20136148.\n[81]\nA. Gupta, Y. Yuan, Y. Zhou, and C. Mendis, \u201cFlurka: Fast fused\nlow-rank & kernel attention,\u201d arXiv preprint arXiv:2306.15799,\n2023.\n[82]\nX. Ma, X. Kong, S. Wang, C. Zhou, J. May, H. Ma, and L. Zettle-\nmoyer, \u201cLuna: Linear unified nested attention,\u201d Advances in Neu-\nral Information Processing Systems, vol. 34, pp. 2441\u20132453, 2021.\n[83]\nJ. Lee, Y. Lee, J. Kim, A. Kosiorek, S. Choi, and Y. W. Teh,\n\u201cSet transformer: A framework for attention-based permutation-\ninvariant neural networks,\u201d in International conference on machine\nlearning.\nPMLR, 2019, pp. 3744\u20133753.\n[84]\nA. Katharopoulos, A. Vyas, N. Pappas, and F. Fleuret, \u201cTrans-\nformers are rnns: Fast autoregressive transformers with linear\nattention,\u201d in International conference on machine learning.\nPMLR,\n2020, pp. 5156\u20135165.\n[85]\nK. M. Choromanski, V. Likhosherstov, D. Dohan, X. Song,\nA. Gane, T. Sarlos, P. Hawkins, J. Q. Davis, A. Mohiuddin,\nL. Kaiser et al., \u201cRethinking attention with performers,\u201d in In-\nternational Conference on Learning Representations, 2020.\n[86]\nH. Peng, N. Pappas, D. Yogatama, R. Schwartz, N. Smith, and\nL. Kong, \u201cRandom feature attention,\u201d in International Conference\non Learning Representations, 2022.\n[87]\nP. Kacham, V. Mirrokni, and P. Zhong, \u201cPolysketchformer: Fast\ntransformers via sketches for polynomial kernels,\u201d arXiv preprint\narXiv:2310.01655, 2023.\n[88]\nW. Fedus, B. Zoph, and N. Shazeer, \u201cSwitch transformers: Scaling\nto trillion parameter models with simple and efficient sparsity,\u201d\nThe Journal of Machine Learning Research, vol. 23, no. 1, pp. 5232\u2013\n5270, 2022.\n[89]\nZ. Zhang, Y. Lin, Z. Liu, P. Li, M. Sun, and J. Zhou, \u201cMoefication:\nTransformer feed-forward layers are mixtures of experts,\u201d in\nFindings of the Association for Computational Linguistics: ACL 2022,\n2022, pp. 877\u2013890.\n[90]\nZ.-F. Gao, P. Liu, W. X. Zhao, Z.-Y. Lu, and J.-R. Wen, \u201cParameter-\nefficient mixture-of-experts architecture for pre-trained language\nmodels,\u201d in Proceedings of the 29th International Conference on\nComputational Linguistics, 2022, pp. 3263\u20133273.\n[91]\nA. Komatsuzaki, J. Puigcerver, J. Lee-Thorp, C. R. Ruiz,\nB. Mustafa, J. Ainslie, Y. Tay, M. Dehghani, and N. Houlsby,\n\u201cSparse\nupcycling:\nTraining\nmixture-of-experts\nfrom\ndense\ncheckpoints,\u201d arXiv preprint arXiv:2212.05055, 2022.\n[92]\nM. Lewis, S. Bhosale, T. Dettmers, N. Goyal, and L. Zettlemoyer,\n\u201cBase layers: Simplifying training of large, sparse models,\u201d in\nInternational Conference on Machine Learning.\nPMLR, 2021, pp.\n6265\u20136274.\n[93]\nY. Zhou, T. Lei, H. Liu, N. Du, Y. Huang, V. Zhao, A. M.\nDai, Q. V. Le, J. Laudon et al., \u201cMixture-of-experts with expert\nchoice routing,\u201d Advances in Neural Information Processing Systems,\nvol. 35, pp. 7103\u20137114, 2022.\n[94]\nB. Zoph, I. Bello, S. Kumar, N. Du, Y. Huang, J. Dean, N. Shazeer,\nand W. Fedus, \u201cSt-moe: Designing stable and transferable sparse\nexpert models,\u201d arXiv preprint arXiv:2202.08906, 2022.\n[95]\nD. Dai, L. Dong, S. Ma, B. Zheng, Z. Sui, B. Chang, and F. Wei,\n\u201cStablemoe: Stable routing strategy for mixture of experts,\u201d in\nProceedings of the 60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), 2022, pp. 7085\u20137095.\n[96]\nT. Chen, Z. Zhang, A. K. JAISWAL, S. Liu, and Z. Wang, \u201cSparse\nmoe as the new dropout: Scaling dense and self-slimmable\ntransformers,\u201d in The Eleventh International Conference on Learning\nRepresentations, 2022.\n[97]\nN. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu,\nM. Krikun, Y. Zhou, A. W. Yu, O. Firat et al., \u201cGlam: Efficient scal-\ning of language models with mixture-of-experts,\u201d in International\nConference on Machine Learning.\nPMLR, 2022, pp. 5547\u20135569.\n[98]\nN. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton,\nand J. Dean, \u201cOutrageously large neural networks: The sparsely-\ngated mixture-of-experts layer,\u201d in International Conference on\nLearning Representations, 2016.\n[99]\nD. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang,\nM. Krikun, N. Shazeer, and Z. Chen, \u201cGshard: Scaling giant\nmodels with conditional computation and automatic sharding,\u201d\narXiv preprint arXiv:2006.16668, 2020.\n[100] C. Hwang, W. Cui, Y. Xiong, Z. Yang, Z. Liu, H. Hu, Z. Wang,\nR. Salas, J. Jose, P. Ram et al., \u201cTutel: Adaptive mixture-of-experts\nat scale,\u201d Proceedings of Machine Learning and Systems, vol. 5, 2023.\n[101] D. P. Bertsekas, \u201cAuction algorithms for network flow problems:\nA tutorial introduction,\u201d Computational optimization and applica-\ntions, vol. 1, pp. 7\u201366, 1992.\n[102] Z. Dai, G. Lai, Y. Yang, and Q. Le, \u201cFunnel-transformer: Filtering\nout sequential redundancy for efficient language processing,\u201d\nAdvances in neural information processing systems, vol. 33, pp. 4271\u2013\n4282, 2020.\n[103] L. Zhu, B. Liao, Q. Zhang, X. Wang, W. Liu, and X. Wang,\n\u201cVision mamba: Efficient visual representation learning with\nbidirectional state space model,\u201d arXiv preprint arXiv:2401.09417,\n2024.\n[104] W. Hua, Z. Dai, H. Liu, and Q. Le, \u201cTransformer quality in linear\ntime,\u201d in International Conference on Machine Learning.\nPMLR,\n2022, pp. 9099\u20139117.\n[105] AI21, \u201cJamba: Ai21\u2019s groundbreaking ssm-transformer model,\u201d\nMarch 2024. [Online]. Available: https://www.ai21.com/blog/\nannouncing-jamba\n[106] W. He, K. Han, Y. Tang, C. Wang, Y. Yang, T. Guo, and\nY. Wang, \u201cDensemamba: State space models with dense hidden\nconnection for efficient large language models,\u201d arXiv preprint\narXiv:2403.00818, 2024.\n[107] Q. Anthony, Y. Tokpanov, P. Glorioso, and B. Millidge, \u201cBlack-\nmamba: Mixture of experts for state-space models,\u201d arXiv preprint\narXiv:2402.01771, 2024.\n[108] M. Pi\u00b4oro, K. Ciebiera, K. Kr\u00b4ol, J. Ludziejewski, and S. Jaszczur,\n\u201cMoe-mamba: Efficient selective state space models with mixture\nof experts,\u201d arXiv preprint arXiv:2401.04081, 2024.\n[109] S. Zhai, W. Talbott, N. Srivastava, C. Huang, H. Goh, R. Zhang,\nand J. Susskind, \u201cAn attention free transformer,\u201d arXiv preprint\narXiv:2105.14103, 2021.\n[110] T. Schuster, A. Fisch, J. Gupta, M. Dehghani, D. Bahri, V. Tran,\nY. Tay, and D. Metzler, \u201cConfident adaptive language modeling,\u201d\nAdvances in Neural Information Processing Systems, vol. 35, pp.\n17 456\u201317 472, 2022.\n[111] L. Del Corro, A. Del Giorno, S. Agarwal, B. Yu, A. Awadallah, and\nS. Mukherjee, \u201cSkipdecode: Autoregressive skip decoding with\nbatching and caching for efficient llm inference,\u201d arXiv preprint\narXiv:2307.02628, 2023.\n[112] W. Liu, P. Zhou, Z. Wang, Z. Zhao, H. Deng, and Q. Ju, \u201cFastbert:\na self-distilling bert with adaptive inference time,\u201d in Proceedings\nof the 58th Annual Meeting of the Association for Computational\nLinguistics, 2020, pp. 6035\u20136044.\n[113] J. Kong, J. Wang, L.-C. Yu, and X. Zhang, \u201cAccelerating inference\nfor pretrained language models by unified multi-perspective\nearly exiting,\u201d in Proceedings of the 29th International Conference\non Computational Linguistics, 2022, pp. 4677\u20134686.\n[114] K. Liao, Y. Zhang, X. Ren, Q. Su, X. Sun, and B. He, \u201cA global\npast-future early exit method for accelerating inference of pre-\ntrained language models,\u201d in Proceedings of the 2021 Conference\nof the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, 2021, pp. 2013\u20132023.\n[115] J. Xin, R. Tang, J. Lee, Y. Yu, and J. Lin, \u201cDeebert: Dynamic\nearly exiting for accelerating bert inference,\u201d in Proceedings of the\n58th Annual Meeting of the Association for Computational Linguistics,\n2020, pp. 2246\u20132251.\n[116] W. Zhou, C. Xu, T. Ge, J. McAuley, K. Xu, and F. Wei, \u201cBert loses\npatience: Fast and robust inference with early exit,\u201d Advances in\nNeural Information Processing Systems, vol. 33, pp. 18 330\u201318 341,\n2020.\n[117] T. Sun, X. Liu, W. Zhu, Z. Geng, L. Wu, Y. He, Y. Ni, G. Xie, X.-J.\nHuang, and X. Qiu, \u201cA simple hash-based early exiting approach\nfor language understanding and generation,\u201d in Findings of the\nAssociation for Computational Linguistics: ACL 2022, 2022, pp. 2409\u2013\n2421.\n[118] Y. Huang, Y. Chen, Z. Yu, and K. McKeown, \u201cIn-context learning\ndistillation: Transferring few-shot learning ability of pre-trained\nlanguage models,\u201d arXiv preprint arXiv:2212.10670, 2022.\n[119] J. Zhao, W. Zhao, A. Drozdov, B. Rozonoyer, M. A. Sultan,\nJ.-Y. Lee, M. Iyyer, and A. McCallum, \u201cMultistage collabora-\ntive knowledge distillation from large language models,\u201d arXiv\npreprint arXiv:2311.08640, 2023.\n[120] C.-Y. Hsieh, C.-L. Li, C.-K. Yeh, H. Nakhost, Y. Fujii, A. Ratner,\nR. Krishna, C.-Y. Lee, and T. Pfister, \u201cDistilling step-by-step!\n\n32\noutperforming larger language models with less training data\nand smaller model sizes,\u201d arXiv preprint arXiv:2305.02301, 2023.\n[121] L. H. Li, J. Hessel, Y. Yu, X. Ren, K.-W. Chang, and Y. Choi,\n\u201cSymbolic chain-of-thought distillation: Small models can also\u201d\nthink\u201d step-by-step,\u201d arXiv preprint arXiv:2306.14050, 2023.\n[122] L. C. Magister, J. Mallinson, J. Adamek, E. Malmi, and A. Sev-\neryn, \u201cTeaching small language models to reason,\u201d arXiv preprint\narXiv:2212.08410, 2022.\n[123] H. Chen, S. Wu, X. Quan, R. Wang, M. Yan, and J. Zhang, \u201cMcc-\nkd: Multi-cot consistent knowledge distillation,\u201d arXiv preprint\narXiv:2310.14747, 2023.\n[124] N. Ho, L. Schmid, and S.-Y. Yun, \u201cLarge language models are\nreasoning teachers,\u201d arXiv preprint arXiv:2212.10071, 2022.\n[125] K. Shridhar, A. Stolfo, and M. Sachan, \u201cDistilling reasoning\ncapabilities into smaller language models,\u201d in Findings of the\nAssociation for Computational Linguistics: ACL 2023, 2023, pp. 7059\u2013\n7073.\n[126] X. Zhu, B. Qi, K. Zhang, X. Long, and B. Zhou, \u201cPad: Program-\naided distillation specializes large models in reasoning,\u201d arXiv\npreprint arXiv:2305.13888, 2023.\n[127] P. Wang, Z. Wang, Z. Li, Y. Gao, B. Yin, and X. Ren, \u201cScott:\nSelf-consistent\nchain-of-thought\ndistillation,\u201d\narXiv\npreprint\narXiv:2305.01879, 2023.\n[128] Z. Chen, Q. Gao, A. Bosselut, A. Sabharwal, and K. Richardson,\n\u201cDisco: distilling counterfactuals with large language models,\u201d\nin Proceedings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), 2023, pp. 5514\u2013\n5528.\n[129] M. Wu, A. Waheed, C. Zhang, M. Abdul-Mageed, and A. F. Aji,\n\u201cLamini-lm: A diverse herd of distilled models from large-scale\ninstructions,\u201d arXiv preprint arXiv:2304.14402, 2023.\n[130] Y. Jiang, C. Chan, M. Chen, and W. Wang, \u201cLion: Adversarial\ndistillation of proprietary large language models,\u201d in Proceedings\nof the 2023 Conference on Empirical Methods in Natural Language\nProcessing, 2023, pp. 3134\u20133154.\n[131] Y. Gu, L. Dong, F. Wei, and M. Huang, \u201cKnowledge distillation\nof large language models,\u201d arXiv preprint arXiv:2306.08543, 2023.\n[132] R. Agarwal, N. Vieillard, P. Stanczyk, S. Ramos, M. Geist, and\nO. Bachem, \u201cGkd: Generalized knowledge distillation for auto-\nregressive sequence models,\u201d arXiv preprint arXiv:2306.13649,\n2023.\n[133] C. Liang, S. Zuo, Q. Zhang, P. He, W. Chen, and T. Zhao, \u201cLess\nis more: Task-aware layer-wise distillation for language model\ncompression,\u201d in International Conference on Machine Learning.\nPMLR, 2023, pp. 20 852\u201320 867.\n[134] I. Timiryasov and J.-L. Tastet, \u201cBaby llama: knowledge distillation\nfrom an ensemble of teachers trained on a small dataset with no\nperformance penalty,\u201d arXiv preprint arXiv:2308.02019, 2023.\n[135] C. Zhang, Y. Yang, J. Liu, J. Wang, Y. Xian, B. Wang, and D. Song,\n\u201cLifting the curse of capacity gap in distilling language models,\u201d\narXiv preprint arXiv:2305.12129, 2023.\n[136] L. Hou, Z. Huang, L. Shang, X. Jiang, X. Chen, and Q. Liu, \u201cDyn-\nabert: Dynamic bert with adaptive width and depth,\u201d Advances\nin Neural Information Processing Systems, vol. 33, pp. 9782\u20139793,\n2020.\n[137] S. Padmanabhan, Y. Onoe, M. J. Zhang, G. Durrett, and E. Choi,\n\u201cPropagating knowledge updates to lms through distillation,\u201d\narXiv preprint arXiv:2306.09306, 2023.\n[138] Y. Yin, C. Chen, L. Shang, X. Jiang, X. Chen, and Q. Liu, \u201cAu-\ntotinybert: Automatic hyper-parameter optimization for efficient\npre-trained language models,\u201d arXiv preprint arXiv:2107.13686,\n2021.\n[139] J. Xu, X. Tan, R. Luo, K. Song, J. Li, T. Qin, and T.-Y. Liu, \u201cNas-\nbert: task-agnostic and adaptive-size bert compression with neu-\nral architecture search,\u201d in Proceedings of the 27th ACM SIGKDD\nConference on Knowledge Discovery & Data Mining, 2021, pp. 1933\u2013\n1943.\n[140] A. Klein, J. Golebiowski, X. Ma, V. Perrone, and C. Archambeau,\n\u201cStructural pruning of large language models via neural archi-\ntecture search,\u201d 2023.\n[141] M. Javaheripi, G. de Rosa, S. Mukherjee, S. Shah, T. Religa,\nC. C. Teodoro Mendes, S. Bubeck, F. Koushanfar, and D. Dey,\n\u201cLitetransformersearch: Training-free neural architecture search\nfor efficient language models,\u201d Advances in Neural Information\nProcessing Systems, vol. 35, pp. 24 254\u201324 267, 2022.\n[142] D. D. Xu, S. Mukherjee, X. Liu, D. Dey, W. Wang, X. Zhang,\nA. Awadallah, and J. Gao, \u201cFew-shot task-agnostic neural archi-\ntecture search for distilling large language models,\u201d Advances in\nNeural Information Processing Systems, vol. 35, pp. 28 644\u201328 656,\n2022.\n[143] A. Kaushal, T. Vaidhya, and I. Rish, \u201cLord: Low rank decomposi-\ntion of monolingual code llms for one-shot compression,\u201d arXiv\npreprint arXiv:2309.14021, 2023.\n[144] M. Xu, Y. L. Xu, and D. P. Mandic, \u201cTensorgpt: Efficient com-\npression of the embedding layer in llms based on the tensor-train\ndecomposition,\u201d arXiv preprint arXiv:2307.00526, 2023.\n[145] Y. Li, Y. Yu, Q. Zhang, C. Liang, P. He, W. Chen, and T. Zhao,\n\u201cLosparse: Structured compression of large language models\nbased on low-rank and sparse approximation,\u201d arXiv preprint\narXiv:2306.11222, 2023.\n[146] R. Saha, V. Srivastava, and M. Pilanci, \u201cMatrix compression via\nrandomized low rank and low precision factorization,\u201d arXiv\npreprint arXiv:2310.11028, 2023.\n[147] Z. Yao, X. Wu, C. Li, S. Youn, and Y. He, \u201cZeroquant-v2: Exploring\npost-training quantization in llms from comprehensive study to\nlow rank compensation,\u201d arXiv preprint arXiv:2303.08302, 2023.\n[148] R. Chand, Y. Prabhu, and P. Kumar, \u201cDsformer: Effective com-\npression of text-transformers by dense-sparse weight factoriza-\ntion,\u201d arXiv preprint arXiv:2312.13211, 2023.\n[149] Z. Yuan, Y. Shang, Y. Song, Q. Wu, Y. Yan, and G. Sun, \u201cAsvd:\nActivation-aware singular value decomposition for compressing\nlarge language models,\u201d arXiv preprint arXiv:2312.05821, 2023.\n[150] R. Child, S. Gray, A. Radford, and I. Sutskever, \u201cGenerat-\ning long sequences with sparse transformers,\u201d arXiv preprint\narXiv:1904.10509, 2019.\n[151] G. Xiao, Y. Tian, B. Chen, S. Han, and M. Lewis, \u201cEfficient\nstreaming language models with attention sinks,\u201d arXiv preprint\narXiv:2309.17453, 2023.\n[152] I. Beltagy, M. E. Peters, and A. Cohan, \u201cLongformer: The long-\ndocument transformer,\u201d arXiv preprint arXiv:2004.05150, 2020.\n[153] M. Zaheer, G. Guruganesh, K. A. Dubey, J. Ainslie, C. Alberti,\nS. Ontanon, P. Pham, A. Ravula, Q. Wang, L. Yang et al., \u201cBig\nbird: Transformers for longer sequences,\u201d Advances in neural\ninformation processing systems, vol. 33, pp. 17 283\u201317 297, 2020.\n[154] S. Dai, H. Genc, R. Venkatesan, and B. Khailany, \u201cEfficient trans-\nformer inference with statically structured sparse attention,\u201d in\n2023 60th ACM/IEEE Design Automation Conference (DAC).\nIEEE,\n2023, pp. 1\u20136.\n[155] Anonymous, \u201cSemSA: Semantic sparse attention is hidden\nin large language models.\u201d 2023. [Online]. Available: https:\n//openreview.net/forum?id=eG9AkHtYYH\n[156] H. Wang, Z. Zhang, and S. Han, \u201cSpatten: Efficient sparse at-\ntention architecture with cascade token and head pruning,\u201d in\n2021 IEEE International Symposium on High-Performance Computer\nArchitecture (HPCA).\nIEEE, pp. 97\u2013110.\n[157] L. Ren, Y. Liu, S. Wang, Y. Xu, C. Zhu, and C. Zhai, \u201cSparse mod-\nular activation for efficient sequence modeling,\u201d arXiv preprint\narXiv:2306.11197, 2023.\n[158] S. Anagnostidis, D. Pavllo, L. Biggio, L. Noci, A. Lucchi,\nand T. Hoffmann, \u201cDynamic context pruning for efficient\nand interpretable autoregressive transformers,\u201d arXiv preprint\narXiv:2305.15805, 2023.\n[159] N. Kitaev, \u0141. Kaiser, and A. Levskaya, \u201cReformer: The efficient\ntransformer,\u201d arXiv preprint arXiv:2001.04451, 2020.\n[160] M. Pagliardini, D. Paliotta, M. Jaggi, and F. Fleuret, \u201cFaster causal\nattention over large sequences through sparse flash attention,\u201d\narXiv preprint arXiv:2306.01160, 2023.\n[161] A. Roy, M. Saffar, A. Vaswani, and D. Grangier, \u201cEfficient content-\nbased sparse attention with routing transformers,\u201d Transactions of\nthe Association for Computational Linguistics, vol. 9, pp. 53\u201368, 2021.\n[162] Y. Tay, D. Bahri, L. Yang, D. Metzler, and D.-C. Juan, \u201cSparse\nsinkhorn attention,\u201d in International Conference on Machine Learn-\ning.\nPMLR, 2020, pp. 9438\u20139447.\n[163] Z. Zhang, Y. Sheng, T. Zhou, T. Chen, L. Zheng, R. Cai, Z. Song,\nY. Tian, C. R\u00b4e, C. Barrett et al., \u201cH2o: Heavy-hitter oracle for\nefficient generative inference of large language models,\u201d Advances\nin Neural Information Processing Systems, vol. 36, 2024.\n[164] A. Feng, I. Li, Y. Jiang, and R. Ying, \u201cDiffuser: efficient transform-\ners with multi-hop attention diffusion for long sequences,\u201d in\nProceedings of the AAAI Conference on Artificial Intelligence, vol. 37,\nno. 11, 2023, pp. 12 772\u201312 780.\n[165] E. Frantar and D. Alistarh, \u201cSparsegpt: Massive language models\ncan be accurately pruned in one-shot,\u201d 2023.\n\n33\n[166] M. Sun, Z. Liu, A. Bair, and J. Z. Kolter, \u201cA simple and effective\npruning approach for large language models,\u201d arXiv preprint\narXiv:2306.11695, 2023.\n[167] H. Shao, B. Liu, and Y. Qian, \u201cOne-shot sensitivity-aware mixed\nsparsity pruning for large language models,\u201d arXiv preprint\narXiv:2310.09499, 2023.\n[168] A. Syed, P. H. Guo, and V. Sundarapandiyan, \u201cPrune and tune:\nImproving efficient pruning techniques for massive language\nmodels,\u201d 2023.\n[169] X. Wei, Y. Zhang, Y. Li, X. Zhang, R. Gong, J. Guo, and X. Liu,\n\u201cOutlier suppression+: Accurate quantization of large language\nmodels by equivalent and optimal shifting and scaling,\u201d arXiv\npreprint arXiv:2304.09145, 2023.\n[170] P. Xu, W. Shao, M. Chen, S. Tang, K. Zhang, P. Gao, F. An,\nY. Qiao, and P. Luo, \u201cBesa: Pruning large language models with\nblockwise parameter-efficient sparsity allocation,\u201d in The Twelfth\nInternational Conference on Learning Representations, 2023.\n[171] E. Kurtic, D. Campos, T. Nguyen, E. Frantar, M. Kurtz, B. Fineran,\nM. Goin, and D. Alistarh, \u201cThe optimal bert surgeon: Scalable\nand accurate second-order pruning for large language models,\u201d\nin Proceedings of the 2022 Conference on Empirical Methods in Natural\nLanguage Processing, 2022, pp. 4163\u20134181.\n[172] W. Kwon, S. Kim, M. W. Mahoney, J. Hassoun, K. Keutzer,\nand A. Gholami, \u201cA fast post-training pruning framework for\ntransformers,\u201d Advances in Neural Information Processing Systems,\nvol. 35, pp. 24 101\u201324 116, 2022.\n[173] Y. Zhang, H. Bai, H. Lin, J. Zhao, L. Hou, and C. V. Cannistraci,\n\u201cAn efficient plug-and-play post-training pruning strategy in\nlarge language models,\u201d 2023.\n[174] X. Ma, G. Fang, and X. Wang, \u201cLlm-pruner: On the structural\npruning of large language models,\u201d Advances in neural information\nprocessing systems, vol. 36, 2024.\n[175] M. Xia, T. Gao, Z. Zeng, and D. Chen, \u201cSheared llama: Accelerat-\ning language model pre-training via structured pruning,\u201d arXiv\npreprint arXiv:2310.06694, 2023.\n[176] E. Kurti\u00b4c, E. Frantar, and D. Alistarh, \u201cZiplm: Inference-aware\nstructured pruning of language models,\u201d Advances in Neural\nInformation Processing Systems, vol. 36, 2024.\n[177] M. Zhang, H. Chen, C. Shen, Z. Yang, L. Ou, X. Yu, and\nB. Zhuang, \u201cLoraprune: Pruning meets low-rank parameter-\nefficient fine-tuning,\u201d 2023.\n[178] T. Chen, T. Ding, B. Yadav, I. Zharkov, and L. Liang, \u201cLorashear:\nEfficient large language model structured pruning and knowl-\nedge recovery,\u201d arXiv preprint arXiv:2310.18356, 2023.\n[179] S. Ashkboos, M. L. Croci, M. G. d. Nascimento, T. Hoefler,\nand J. Hensman, \u201cSlicegpt: Compress large language models\nby deleting rows and columns,\u201d arXiv preprint arXiv:2401.15024,\n2024.\n[180] Q. Zhang, S. Zuo, C. Liang, A. Bukharin, P. He, W. Chen,\nand T. Zhao, \u201cPlaton: Pruning large transformer models with\nupper confidence bound of weight importance,\u201d in International\nConference on Machine Learning.\nPMLR, 2022, pp. 26 809\u201326 823.\n[181] M. Xia, Z. Zhong, and D. Chen, \u201cStructured pruning learns\ncompact and accurate models,\u201d in Proceedings of the 60th Annual\nMeeting of the Association for Computational Linguistics (Volume 1:\nLong Papers), 2022, pp. 1513\u20131528.\n[182] C. Tao, L. Hou, H. Bai, J. Wei, X. Jiang, Q. Liu, P. Luo, and\nN. Wong, \u201cStructured pruning for efficient generative pre-trained\nlanguage models,\u201d in Findings of the Association for Computational\nLinguistics: ACL 2023, 2023, pp. 10 880\u201310 895.\n[183] X. Lu, Q. Liu, Y. Xu, A. Zhou, S. Huang, B. Zhang, J. Yan, and\nH. Li, \u201cNot all experts are equal: Efficient expert pruning and\nskipping for mixture-of-experts large language models,\u201d arXiv\npreprint arXiv:2402.14800, 2024.\n[184] A. Muzio, A. Sun, and C. He, \u201cSeer-moe: Sparse expert efficiency\nthrough regularization for mixture-of-experts,\u201d arXiv preprint\narXiv:2404.05089, 2024.\n[185] P. Dong, L. Li, Z. Tang, X. Liu, X. Pan, Q. Wang, and X. Chu,\n\u201cPruner-zero: Evolving symbolic pruning metric from scratch for\nlarge language models,\u201d in International Conference on Machine\nLearning (ICML), 2024.\n[186] Y. Zhang, L. Zhao, M. Lin, S. Yunyun, Y. Yao, X. Han, J. Tanner,\nS. Liu, and R. Ji, \u201cDynamic sparse no training: Training-free fine-\ntuning for sparse llms,\u201d in International Conference on Learning\nRepresentations (ICLR), 2024.\n[187] S.-y. Liu, Z. Liu, X. Huang, P. Dong, and K.-T. Cheng, \u201cLlm-fp4: 4-\nbit floating-point quantized transformers,\u201d in The 2023 Conference\non Empirical Methods in Natural Language Processing, 2023.\n[188] L. Li, Q. Li, B. Zhang, and X. Chu, \u201cNorm tweaking: High-\nperformance low-bit quantization of large language models,\u201d\narXiv preprint arXiv:2309.02784, 2023.\n[189] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer,\n\u201cQlora: Efficient finetuning of quantized llms,\u201d Advances in Neu-\nral Information Processing Systems, vol. 36, 2024.\n[190] Y. Xu, L. Xie, X. Gu, X. Chen, H. Chang, H. Zhang, Z. Chen,\nX. Zhang, and Q. Tian, \u201cQa-lora: Quantization-aware low-\nrank adaptation of large language models,\u201d arXiv preprint\narXiv:2309.14717, 2023.\n[191] Y. Li, Y. Yu, C. Liang, P. He, N. Karampatziakis, W. Chen, and\nT. Zhao, \u201cLoftq: Lora-fine-tuning-aware quantization for large\nlanguage models,\u201d arXiv preprint arXiv:2310.08659, 2023.\n[192] E. Frantar, S. Ashkboos, T. Hoefler, and D. Alistarh, \u201cGptq:\nAccurate post-training quantization for generative pre-trained\ntransformers,\u201d arXiv preprint arXiv:2210.17323, 2022.\n[193] G. Park, M. Kim, S. Lee, J. Kim, B. Kwon, S. J. Kwon, B. Kim,\nY. Lee, D. Lee et al., \u201cLut-gemm: Quantized matrix multiplication\nbased on luts for efficient inference in large-scale generative lan-\nguage models,\u201d in The Twelfth International Conference on Learning\nRepresentations, 2023.\n[194] J. Lin, J. Tang, H. Tang, S. Yang, X. Dang, and S. Han, \u201cAwq:\nActivation-aware weight quantization for llm compression and\nacceleration,\u201d arXiv preprint arXiv:2306.00978, 2023.\n[195] C. Lee, J. Jin, T. Kim, H. Kim, and E. Park, \u201cOwq: Lessons learned\nfrom activation outliers for weight quantization in large language\nmodels,\u201d arXiv preprint arXiv:2306.02272, 2023.\n[196] T. Dettmers, R. Svirschevski, V. Egiazarian, D. Kuznedelev,\nE. Frantar, S. Ashkboos, A. Borzunov, T. Hoefler, and D. Alistarh,\n\u201cSpqr: A sparse-quantized representation for near-lossless llm\nweight compression,\u201d arXiv preprint arXiv:2306.03078, 2023.\n[197] S. Kim, C. Hooper, A. Gholami, Z. Dong, X. Li, S. Shen, M. W.\nMahoney, and K. Keutzer, \u201cSqueezellm: Dense-and-sparse quan-\ntization,\u201d arXiv preprint arXiv:2306.07629, 2023.\n[198] J. Chee, Y. Cai, V. Kuleshov, and C. De Sa, \u201cQuip: 2-bit quantiza-\ntion of large language models with guarantees,\u201d in Thirty-seventh\nConference on Neural Information Processing Systems, 2023.\n[199] Y. J. Kim, R. Henry, R. Fahim, and H. H. Awadalla, \u201cFinequant:\nUnlocking efficiency with fine-grained weight-only quantization\nfor llms,\u201d arXiv preprint arXiv:2308.09723, 2023.\n[200] K. Behdin, A. Acharya, A. Gupta, S. Keerthi, and R. Mazumder,\n\u201cQuantease:\nOptimization-based\nquantization\nfor\nlanguage\nmodels\u2013an efficient and intuitive algorithm,\u201d arXiv preprint\narXiv:2309.01885, 2023.\n[201] S. Li, X. Ning, K. Hong, T. Liu, L. Wang, X. Li, K. Zhong, G. Dai,\nH. Yang, and Y. Wang, \u201cLlm-mq: Mixed-precision quantization\nfor efficient llm deployment,\u201d 2023.\n[202] Z. Yao, R. Y. Aminabadi, M. Zhang, X. Wu, C. Li, and Y. He,\n\u201cZeroquant: Efficient and affordable post-training quantization\nfor large-scale transformers,\u201d in Advances in Neural Information\nProcessing Systems, 2022.\n[203] Y. Sheng, L. Zheng, B. Yuan, Z. Li, M. Ryabinin, B. Chen, P. Liang,\nC. Re, I. Stoica, and C. Zhang, \u201cFlexgen: High-throughput gen-\nerative inference of large language models with a single gpu,\u201d\n2023.\n[204] T. Dettmers, M. Lewis, Y. Belkada, and L. Zettlemoyer, \u201cLlm. int8\n(): 8-bit matrix multiplication for transformers at scale,\u201d arXiv\npreprint arXiv:2208.07339, 2022.\n[205] G. Xiao, J. Lin, M. Seznec, H. Wu, J. Demouth, and S. Han,\n\u201cSmoothquant: Accurate and efficient post-training quantization\nfor large language models,\u201d in International Conference on Machine\nLearning.\nPMLR, 2023, pp. 38 087\u201338 099.\n[206] Z. Yao, X. Wu, C. Li, S. Youn, and Y. He, \u201cZeroquant-v2: Exploring\npost-training quantization in llms from comprehensive study to\nlow rank compensation,\u201d arXiv preprint arXiv:2303.08302, 2023.\n[207] Z. Yuan, L. Niu, J. Liu, W. Liu, X. Wang, Y. Shang, G. Sun, Q. Wu,\nJ. Wu, and B. Wu, \u201cRptq: Reorder-based post-training quantiza-\ntion for large language models,\u201d arXiv preprint arXiv:2304.01089,\n2023.\n[208] C. Guo, J. Tang, W. Hu, J. Leng, C. Zhang, F. Yang, Y. Liu,\nM. Guo, and Y. Zhu, \u201cOlive: Accelerating large language models\nvia hardware-friendly outlier-victim pair quantization,\u201d in Pro-\nceedings of the 50th Annual International Symposium on Computer\nArchitecture, 2023, pp. 1\u201315.\n\n34\n[209] X. Wu, Z. Yao, and Y. He, \u201cZeroquant-fp: A leap forward in llms\npost-training w4a8 quantization using floating-point formats,\u201d\narXiv preprint arXiv:2307.09782, 2023.\n[210] W. Shao, M. Chen, Z. Zhang, P. Xu, L. Zhao, Z. Li, K. Zhang,\nP. Gao, Y. Qiao, and P. Luo, \u201cOmniquant: Omnidirectionally\ncalibrated quantization for large language models,\u201d in The Twelfth\nInternational Conference on Learning Representations, 2023.\n[211] J. Liu, R. Gong, X. Wei, Z. Dong, J. Cai, and B. Zhuang, \u201cQllm:\nAccurate and efficient low-bitwidth quantization for large lan-\nguage models,\u201d in The Twelfth International Conference on Learning\nRepresentations, 2023.\n[212] Y. Zhao, C.-Y. Lin, K. Zhu, Z. Ye, L. Chen, S. Zheng, L. Ceze,\nA. Krishnamurthy, T. Chen, and B. Kasikci, \u201cAtom: Low-bit\nquantization for efficient and accurate llm serving,\u201d arXiv preprint\narXiv:2310.19102, 2023.\n[213] W. Huang, Y. Liu, H. Qin, Y. Li, S. Zhang, X. Liu, M. Magno, and\nX. Qi, \u201cBillm: Pushing the limit of post-training quantization for\nllms,\u201d 2024.\n[214] S. Li, X. Ning, L. Wang, T. Liu, X. Shi, S. Yan, G. Dai, H. Yang, and\nY. Wang, \u201cEvaluating quantized large language models,\u201d arXiv\npreprint arXiv:2402.18158, 2024.\n[215] Y. Ma, H. Li, X. Zheng, F. Ling, X. Xiao, R. Wang, S. Wen, F. Chao,\nand R. Ji, \u201cAffinequant: Affine transformation quantization for\nlarge language models,\u201d in International Conference on Learning\nRepresentations (ICLR), 2024.\n[216] A. Tseng, J. Chee, Q. Sun, V. Kuleshov, and C. De Sa, \u201cQuip#:\nEven better llm quantization with hadamard incoherence and\nlattice codebooks,\u201d arXiv preprint arXiv:2402.04396, 2024.\n[217] S. Ashkboos, A. Mohtashami, M. L. Croci, B. Li, M. Jaggi, D. Al-\nistarh, T. Hoefler, and J. Hensman, \u201cQuarot: Outlier-free 4-bit\ninference in rotated llms,\u201d arXiv preprint arXiv:2404.00456, 2024.\n[218] Z.\nLiu,\nC.\nZhao,\nI.\nFedorov,\nB.\nSoran,\nD.\nChoudhary,\nR. Krishnamoorthi, V. Chandra, Y. Tian, and T. Blankevoort,\n\u201cSpinquant\u2013llm quantization with learned rotations,\u201d arXiv\npreprint arXiv:2405.16406, 2024.\n[219] C. Hooper, S. Kim, H. Mohammadzadeh, M. W. Mahoney, Y. S.\nShao, K. Keutzer, and A. Gholami, \u201cKvquant: Towards 10 million\ncontext length llm inference with kv cache quantization,\u201d arXiv\npreprint arXiv:2401.18079, 2024.\n[220] Z. Liu, J. Yuan, H. Jin, S. Zhong, Z. Xu, V. Braverman, B. Chen,\nand X. Hu, \u201cKivi: A tuning-free asymmetric 2bit quantization for\nkv cache,\u201d arXiv preprint arXiv:2402.02750, 2024.\n[221] E. Frantar and D. Alistarh, \u201cOptimal brain compression: A frame-\nwork for accurate post-training quantization and pruning,\u201d in\nAdvances in Neural Information Processing Systems, 2022.\n[222] N. Vaidya, F. Oh, and N. Comly, \u201cOptimizing inference on\nlarge language models with nvidia tensorrt-llm, now pub-\nlicly available,\u201d [Online], 2023, https://github.com/NVIDIA/\nTensorRT-LLM.\n[223] InternLM,\n\u201cLmdeploy,\u201d\n2024.\n[Online].\nAvailable:\nhttps://\ngithub.com/InternLM/lmdeploy\n[224] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang,\nand W. Chen, \u201cLora: Low-rank adaptation of large language\nmodels,\u201d arXiv preprint arXiv:2106.09685, 2021.\n[225] B. Hassibi, D. G. Stork, and G. J. Wolff, \u201cOptimal brain surgeon\nand general network pruning,\u201d in IEEE international conference on\nneural networks.\nIEEE, 1993, pp. 293\u2013299.\n[226] Y. LeCun, J. Denker, and S. Solla, \u201cOptimal brain damage,\u201d\nAdvances in neural information processing systems, vol. 2, 1989.\n[227] D. C. Mocanu, E. Mocanu, P. Stone, P. H. Nguyen, M. Gibescu,\nand A. Liotta, \u201cScalable training of artificial neural networks\nwith adaptive sparse connectivity inspired by network science,\u201d\nNature communications, vol. 9, no. 1, p. 2383, 2018.\n[228] B. Zoph and Q. Le, \u201cNeural architecture search with reinforce-\nment learning,\u201d in International Conference on Learning Representa-\ntions, 2016.\n[229] X. Wang, Y. Zheng, Z. Wan, and M. Zhang, \u201cSvd-llm: Truncation-\naware singular value decomposition for large language model\ncompression,\u201d arXiv preprint arXiv:2403.07378, 2024.\n[230] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright,\nP. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray et al., \u201cTrain-\ning language models to follow instructions with human feedback,\n2022,\u201d URL https://arxiv. org/abs/2203.02155, vol. 13, 2022.\n[231] Y. Han, G. Huang, S. Song, L. Yang, H. Wang, and Y. Wang, \u201cDy-\nnamic neural networks: A survey,\u201d IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, vol. 44, no. 11, pp. 7436\u20137456,\n2021.\n[232] C. Xu and J. McAuley, \u201cA survey on dynamic neural networks\nfor natural language processing,\u201d in Findings of the Association for\nComputational Linguistics: EACL 2023, 2023, pp. 2370\u20132381.\n[233] X. He, I. Keivanloo, Y. Xu, X. He, B. Zeng, S. Rajagopalan, and\nT. Chilimbi, \u201cMagic pyramid: Accelerating inference with early\nexiting and token pruning,\u201d Image, 2023.\n[234] TogetherAI,\n\u201cPaving\nthe\nway\nto\nefficient\narchitectures:\nStripedhyena-7b,\nopen\nsource\nmodels\noffering\na\nglimpse\ninto a world beyond transformers,\u201d December 2023. [Online].\nAvailable: https://www.together.ai/blog/stripedhyena-7b\n[235] A. Jaiswal, Z. Gan, X. Du, B. Zhang, Z. Wang, and Y. Yang,\n\u201cCompressing llms: The truth is rarely pure and never simple,\u201d\narXiv preprint arXiv:2310.01382, 2023.\n[236] Y. Leviathan, M. Kalman, and Y. Matias, \u201cFast inference from\ntransformers via speculative decoding,\u201d in International Confer-\nence on Machine Learning.\nPMLR, 2023, pp. 19 274\u201319 286.\n[237] C. Chen, S. Borgeaud, G. Irving, J.-B. Lespiau, L. Sifre, and\nJ. Jumper, \u201cAccelerating large language model decoding with\nspeculative sampling,\u201d arXiv preprint arXiv:2302.01318, 2023.\n[238] Y. Zhou, K. Lyu, A. S. Rawat, A. K. Menon, A. Rostamizadeh,\nS. Kumar, J.-F. Kagy, and R. Agarwal, \u201cDistillspec: Improving\nspeculative decoding via knowledge distillation,\u201d arXiv preprint\narXiv:2310.08461, 2023.\n[239] J. Zhang, J. Wang, H. Li, L. Shou, K. Chen, G. Chen, and S. Mehro-\ntra, \u201cDraft & verify: Lossless large language model acceleration\nvia self-speculative decoding,\u201d arXiv preprint arXiv:2309.08168,\n2023.\n[240] X. Liu, L. Hu, P. Bailis, I. Stoica, Z. Deng, A. Cheung,\nand H. Zhang, \u201cOnline speculative decoding,\u201d arXiv preprint\narXiv:2310.07177, 2023.\n[241] G. Monea, A. Joulin, and E. Grave, \u201cPass: Parallel speculative\nsampling,\u201d arXiv preprint arXiv:2311.13581, 2023.\n[242] Z. He, Z. Zhong, T. Cai, J. D. Lee, and D. He, \u201cRest: Retrieval-\nbased speculative decoding,\u201d arXiv preprint arXiv:2311.08252,\n2023.\n[243] X. Miao, G. Oliaro, Z. Zhang, X. Cheng, Z. Wang, R. Y. Y.\nWong, Z. Chen, D. Arfeen, R. Abhyankar, and Z. Jia, \u201cSpecinfer:\nAccelerating generative llm serving with speculative inference\nand token tree verification,\u201d arXiv preprint arXiv:2305.09781, 2023.\n[244] B. Spector and C. Re, \u201cAccelerating llm inference with staged\nspeculative decoding,\u201d arXiv preprint arXiv:2308.04623, 2023.\n[245] Z. Chen, X. Yang, J. Lin, C. Sun, J. Huang, and K. C.-C. Chang,\n\u201cCascade speculative drafting for even faster llm inference,\u201d\narXiv preprint arXiv:2312.11462, 2023.\n[246] Y. Fu, P. Bailis, I. Stoica, and H. Zhang, \u201cBreaking the sequential\ndependency\nof\nllm\ninference\nusing\nlookahead\ndecoding,\u201d\nNovember 2023. [Online]. Available: https://lmsys.org/blog/\n2023-11-21-lookahead-decoding/\n[247] Y. Li, C. Zhang, and H. Zhang, \u201cEagle: Lossless acceleration\nof llm decoding by feature extrapolation,\u201d December 2023.\n[Online]. Available: https://sites.google.com/view/eagle-llm\n[248] Z. Sun, A. T. Suresh, J. H. Ro, A. Beirami, H. Jain, and F. Yu,\n\u201cSpectr: Fast speculative decoding via optimal transport,\u201d arXiv\npreprint arXiv:2310.15141, 2023.\n[249] F. Liu, Y. Tang, Z. Liu, Y. Ni, K. Han, and Y. Wang, \u201cKanga-\nroo: Lossless self-speculative decoding via double early exiting,\u201d\narXiv preprint arXiv:2404.18911, 2024.\n[250] ggerganov, \u201cInference of meta\u2019s llama model (and others) in\npure c/c++,\u201d 2024. [Online]. Available: https://github.com/\nggerganov/llama.cpp\n[251] Y. Song, Z. Mi, H. Xie, and H. Chen, \u201cPowerinfer: Fast large\nlanguage model serving with a consumer-grade gpu,\u201d arXiv\npreprint arXiv:2312.12456, 2023.\n[252] J. He and J. Zhai, \u201cFastdecode: High-throughput gpu-efficient\nllm serving using heterogeneous pipelines,\u201d arXiv preprint\narXiv:2403.11421, 2024.\n[253] K. Hong, G. Dai, J. Xu, Q. Mao, X. Li, J. Liu, K. Chen, Y. Dong,\nand Y. Wang, \u201cFlashdecoding++: Faster large language model\ninference on gpus,\u201d 2024.\n[254] T. Gale, D. Narayanan, C. Young, and M. Zaharia, \u201cMegablocks:\nEfficient sparse training with mixture-of-experts,\u201d in Proceedings\nof Machine Learning and Systems (MLSys), 2023.\n[255] T. Dao, D. Fu, S. Ermon, A. Rudra, and C. R\u00b4e, \u201cFlashattention:\nFast and memory-efficient exact attention with io-awareness,\u201d\nAdvances in Neural Information Processing Systems, vol. 35, pp.\n16 344\u201316 359, 2022.\n\n35\n[256] T. Dao, \u201cFlashattention-2: Faster attention with better parallelism\nand work partitioning,\u201d arXiv preprint arXiv:2307.08691, 2023.\n[257] Y. Zhai, C. Jiang, L. Wang, X. Jia, S. Zhang, Z. Chen, X. Liu,\nand Y. Zhu, \u201cBytetransformer: A high-performance transformer\nboosted for variable-length inputs,\u201d in 2023 IEEE International\nParallel and Distributed Processing Symposium (IPDPS).\nIEEE,\n2023, pp. 344\u2013355.\n[258] R. Y. Aminabadi, S. Rajbhandari, A. A. Awan, C. Li, D. Li,\nE. Zheng, O. Ruwase, S. Smith, M. Zhang, J. Rasley et al.,\n\u201cDeepspeed-inference: enabling efficient inference of transformer\nmodels at unprecedented scale,\u201d in SC22: International Conference\nfor High Performance Computing, Networking, Storage and Analysis.\nIEEE, 2022, pp. 1\u201315.\n[259] T. Dao, D. Haziza, F. Massa, and G. Sizov, \u201cFlash-decoding for\nlong-context inference,\u201d [Online], 2023, https://crfm.stanford.\nedu/2023/10/12/flashdecoding.html.\n[260] HuggingFace, \u201cTransformers: State-of-the-art machine learning\nfor pytorch, tensorflow, and jax.\u201d [Online], 2024, https://github.\ncom/huggingface/transformers.\n[261] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,\nT. Lacroix, B. Rozi`ere, N. Goyal, E. Hambro, F. Azhar et al.,\n\u201cLlama: Open and efficient foundation language models,\u201d arXiv\npreprint arXiv:2302.13971, 2023.\n[262] Z. Du, Y. Qian, X. Liu, M. Ding, J. Qiu, Z. Yang, and J. Tang,\n\u201cGlm: General language model pretraining with autoregressive\nblank infilling,\u201d in Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers),\n2022, pp. 320\u2013335.\n[263] Sensetime, \u201cOpenppl: A high-performance deep learning infer-\nence platform,\u201d [Online], 2023, https://openppl.ai/home.\n[264] NVIDIA, \u201ccublas: Basic linear algebra on nvidia gpus,\u201d [Online],\n2017, https://developer.nvidia.com/cublas.\n[265] \u2014\u2014, \u201cCutlass: Cuda templates for linear algebra subroutines,\u201d\n[Online], 2017, https://github.com/NVIDIA/cutlass.\n[266] S. Wang, \u201cFastgemv: High-speed gemv kernels,\u201d [Online], 2023,\nhttps://github.com/wangsiping97/FastGEMV.\n[267] P. Tillet, H. T. Kung, and D. Cox, \u201cTriton: an intermediate lan-\nguage and compiler for tiled neural network computations,\u201d in\nProceedings of the 3rd ACM SIGPLAN International Workshop on\nMachine Learning and Programming Languages, 2019, pp. 10\u201319.\n[268] C. Zhang et al., \u201cBeyond the speculative game: A survey of\nspeculative execution in large language models,\u201d arXiv preprint\narXiv:2404.14897, 2024.\n[269] H. Xia et al., \u201cUnlocking efficiency in large language model\ninference: A comprehensive survey of speculative decoding,\u201d\narXiv preprint arXiv:2401.07851, 2024.\n[270] M. Stern, N. Shazeer, and J. Uszkoreit, \u201cBlockwise parallel\ndecoding for deep autoregressive models,\u201d Advances in Neural\nInformation Processing Systems, vol. 31, 2018.\n[271] V. Nair and G. E. Hinton, \u201cRectified linear units improve re-\nstricted boltzmann machines,\u201d in Proceedings of the 27th interna-\ntional conference on machine learning (ICML-10), 2010, pp. 807\u2013814.\n[272] P. Patel, E. Choukse, C. Zhang, \u00b4I\u02dcnigo Goiri, A. Shah, S. Maleki,\nand R. Bianchini, \u201cSplitwise: Efficient generative llm inference\nusing phase splitting,\u201d arXiv preprint arXiv:2311.18677, 2023.\n[273] C. Hu, H. Huang, L. Xu, X. Chen, J. Xu, S. Chen, H. Feng,\nC. Wang, S. Wang, Y. Bao, N. Sun, and Y. Shan, \u201cInference without\ninterference: Disaggregate llm inference for mixed downstream\nworkloads,\u201d arXiv preprint arXiv:2401.11181, 2024.\n[274] Y. Zhong, S. Liu, J. Chen, J. Hu, Y. Zhu, X. Liu, X. Jin, and\nH. Zhang, \u201cDistserve: Disaggregating prefill and decoding for\ngoodput-optimized large language model serving,\u201d arXiv preprint\narXiv:2401.09670, 2024.\n[275] X. Miao, C. Shi, J. Duan, X. Xi, D. Lin, B. Cui, and Z. Jia, \u201cSpot-\nserve: Serving generative large language models on preemptible\ninstances,\u201d arXiv preprint arXiv:2311.15566, 2023.\n[276] B. Lin, T. Peng, C. Zhang, M. Sun, L. Li, H. Zhao, W. Xiao,\nQ. Xu, X. Qiu, S. Li, Z. Ji, Y. Li, and W. Lin, \u201cInfinite-llm: Efficient\nllm service for long context with distattention and distributed\nkvcache,\u201d arXiv preprint arXiv:2401.02669, 2024.\n[277] G.-I. Yu, J. S. Jeong, G.-W. Kim, S. Kim, and B.-G. Chun, \u201cOrca:\nA distributed serving system for transformer-based generative\nmodels,\u201d in Proceedings of the 16th USENIX Symposium on Operat-\ning Systems Design and Implementation, 2022, pp. 521\u2013538.\n[278] ModelTC,\n\u201cLightllm,\u201d\nFebruary\n2024.\n[Online].\nAvailable:\nhttps://github.com/ModelTC/lightllm/\n[279] C. Holmes, M. Tanaka, M. Wyatt, A. A. Awan, J. Rasley, S. Ra-\njbhandari, R. Y. Aminabadi, H. Qin, A. Bakhtiari, L. Kurilenko,\nand Y. He, \u201cDeepspeed-fastgen: High-throughput text genera-\ntion for llms via mii and deepspeed-inference,\u201d arXiv preprint\narXiv:2401.08671, 2024.\n[280] B. Wu, Y. Zhong, Z. Zhang, G. Huang, X. Liu, and X. Jin, \u201cFast\ndistributed inference serving for large language models,\u201d arXiv\npreprint arXiv:2305.05920, 2023.\n[281] Y. Sheng, S. Cao, D. Li, B. Zhu, Z. Li, and D. Zhuo, \u201cFairness in\nserving large language models,\u201d arXiv preprint arXiv:2401.00588,\n2024.\n[282] A. Agrawal, A. Panwar, J. Mohan, N. Kwatra, B. Gulavani,\nand R. Ramjee, \u201cSarathi: Efficient llm inference by piggybacking\ndecodes with chunked prefills,\u201d arXiv preprint arXiv:2308.16369,\n2023.\n[283] A. Agrawal, N. Kedia, A. Panwar, J. Mohan, N. Kwatra, B. S.\nGulavani, A. Tumanov, , and R. Ramjee, \u201cTaming throughput-\nlatency tradeoff in llm inference with sarathi-serve,\u201d arXiv\npreprint arXiv:2403.02310, 2024.\n[284] Y. Jin, C.-F. Wu, D. Brooks, and G.-Y. Wei, \u201cS3: Increasing gpu\nutilization during generative inference for higher throughput,\u201d\narXiv preprint arXiv:2306.06000, 2023.\n[285] Z. Ye, \u201cflashinfer,\u201d March 2024. [Online]. Available: https:\n//github.com/flashinfer-ai/flashinfer\n[286] NVIDIA, \u201cFastertransformer: About transformer related opti-\nmization, including bert, gpt,\u201d [Online], 2017, https://github.\ncom/NVIDIA/FasterTransformer.\n[287] H. Oh, K. Kim, J. Kim, S. Kim, J. Lee, D.-s. Chang, and J. Seo,\n\u201cExegpt: Constraint-aware resource scheduling for llm infer-\nence,\u201d in Proceedings of the 29th ACM International Conference on\nArchitectural Support for Programming Languages and Operating\nSystems, Volume 2, 2024, pp. 369\u2013384.\n[288] B. Sun, Z. Huang, H. Zhao, W. Xiao, X. Zhang, Y. Li, and\nW. Lin, \u201cLlumnix: Dynamic scheduling for large language model\nserving,\u201d arXiv preprint arXiv:2406.03243, 2024.\n[289] B. Wu, S. Liu, Y. Zhong, P. Sun, X. Liu, and X. Jin, \u201cLoongserve:\nEfficiently serving long-context large language models with elas-\ntic sequence parallelism,\u201d arXiv preprint arXiv:2404.09526, 2024.\n[290] B. Li, S. Pandey, H. Fang, Y. Lyv, J. Li, J. Chen, M. Xie, L. Wan,\nH. Liu, and C. Ding, \u201cFtrans: Energy-efficient acceleration of\ntransformers using fpga,\u201d arXiv preprint arXiv:2007.08563, 2020.\n[291] T. J. Ham, Y. Lee, S. H. Seo, S. Kim, H. Choi, S. J. Jun, and J. W. Lee,\n\u201cElsa: Hardware-software co-design for efficient, lightweight\nself-attention mechanism in neural networks,\u201d in ACM/IEEE 48th\nAnnual International Symposium on Computer Architecture, 2021,\npp. 692\u2013705.\n[292] H. Fan, T. Chau, S. I. Venieris, R. Lee, A. Kouris, W. Luk, N. D.\nLane, and M. S. Abdelfattah, \u201cAdaptable butterfly accelerator for\nattention-based nns via hardware and algorithm co-design,\u201d in\nIEEE/ACM International Symposium on Microarchitecture, 2022, pp.\n599\u2013615.\n[293] Y. Qin, Y. Wang, D. Deng, Z. Zhao, X. Yang, L. Liu, S. Wei,\nY. Hu, and S. Yin, \u201cFact: Ffn-attention co-optimized transformer\narchitecture with eager correlation prediction,\u201d in Proceedings of\nthe 50th Annual International Symposium on Computer Architecture,\n2023, pp. 1\u201314.\n[294] H. Chen, J. Zhang, Y. Du, S. Xiang, Z. Yue, N. Zhang, Y. Cai,\nand Z. Zhang, \u201cUnderstanding the potential of fpga-based spatial\nacceleration for large language model inference,\u201d arXiv preprint\narXiv:2312.15159, 2023.\n[295] S. Hong, S. Moon, J. Kim, S. Lee, M. Kim, D. Lee, and J.-Y.\nKim, \u201cDfx: A low-latency multi-fpga appliance for accelerating\ntransformer-based text generation,\u201d in IEEE Hot Chips 34 Sympo-\nsium, 2022.\n[296] S. Zeng, J. Liu, G. Dai, X. Yang, T. Fu, H. Wang, W. Ma, H. Sun,\nS. Li, Z. Huang et al., \u201cFlightllm: Efficient large language model\ninference with a complete mapping flow on fpga,\u201d arXiv preprint\narXiv:2401.03868, 2024.\n[297] S. teams, \u201cSharegpt,\u201d 2023. [Online]. Available: https://sharegpt.\ncom/\n[298] J. Xie, Z. Chen, R. Zhang, X. Wan, and G. Li, \u201cLarge multimodal\nagents: A survey,\u201d arXiv preprint arXiv:2402.15116, 2024.\n[299] I. Lee, N. Jiang, and T. Berg-Kirkpatrick, \u201cExploring the relation-\nship between model architecture and in-context learning ability,\u201d\narXiv preprint arXiv:2310.08049, 2023.\n[300] S. Biderman, H. Schoelkopf, Q. G. Anthony, H. Bradley,\nK. O\u2019Brien, E. Hallahan, M. A. Khan, S. Purohit, U. S. Prashanth,\n\n36\nE. Raff et al., \u201cPythia: A suite for analyzing large language models\nacross training and scaling,\u201d in International Conference on Machine\nLearning.\nPMLR, 2023, pp. 2397\u20132430.\n[301] J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge,\nY. Han, F. Huang et al., \u201cQwen technical report,\u201d arXiv preprint\narXiv:2309.16609, 2023.\n[302] Y. Tang, F. Liu, Y. Ni, Y. Tian, Z. Bai, Y.-Q. Hu, S. Liu, S. Jui,\nK. Han, and Y. Wang, \u201cRethinking optimization and architecture\nfor tiny language models,\u201d arXiv preprint arXiv:2402.02791, 2024.\n[303] Y. Li, S. Bubeck, R. Eldan, A. Del Giorno, S. Gunasekar, and Y. T.\nLee, \u201cTextbooks are all you need ii: phi-1.5 technical report,\u201d\narXiv preprint arXiv:2309.05463, 2023.\n[304] S.\nGunasekar,\nY.\nZhang,\nJ.\nAneja,\nC.\nC.\nT.\nMendes,\nA. Del Giorno, S. Gopi, M. Javaheripi, P. Kauffmann, G. de Rosa,\nO. Saarikivi et al., \u201cTextbooks are all you need,\u201d arXiv preprint\narXiv:2306.11644, 2023.\n[305] P. Zhang, G. Zeng, T. Wang, and W. Lu, \u201cTinyllama: An open-\nsource small language model,\u201d arXiv preprint arXiv:2401.02385,\n2024.\n[306] C. Zhang, D. Song, Z. Ye, and Y. Gao, \u201cTowards the law\nof capacity gap in distilling language models,\u201d arXiv preprint\narXiv:2311.07052, 2023.\n[307] X. Geng and H. Liu, \u201cOpenllama: An open reproduction of\nllama,\u201d May 2023. [Online]. Available: https://github.com/\nopenlm-research/open llama\n[308] M. Bellagente, J. Tow, D. Mahan, D. Phung, M. Zhuravinskyi,\nR. Adithyan, J. Baicoianu, B. Brooks, N. Cooper, A. Datta\net al., \u201cStable lm 2 1.6 b technical report,\u201d arXiv preprint\narXiv:2402.17834, 2024.\n[309] \u201cMinicpm: Unveiling the potential of end-side large language\nmodels,\u201d 2024.\n[310] Z. Liu, C. Zhao, F. Iandola, C. Lai, Y. Tian, I. Fedorov, Y. Xiong,\nE. Chang, Y. Shi, R. Krishnamoorthi et al., \u201cMobilellm: Optimiz-\ning sub-billion parameter language models for on-device use\ncases,\u201d arXiv preprint arXiv:2402.14905, 2024.\n[311] M.\nteam,\n\u201cMLC-LLM,\u201d\n2023.\n[Online].\nAvailable:\nhttps:\n//github.com/mlc-ai/mlc-llm\n[312] Y. Yao, J. Duan, K. Xu, Y. Cai, Z. Sun, and Y. Zhang, \u201cA survey on\nlarge language model (llm) security and privacy: The good, the\nbad, and the ugly,\u201d High-Confidence Computing, p. 100211, 2024.\n[313] Y. Li, H. Wen, W. Wang, X. Li, Y. Yuan, G. Liu, J. Liu, W. Xu,\nX. Wang, Y. Sun et al., \u201cPersonal llm agents: Insights and sur-\nvey about the capability, efficiency and security,\u201d arXiv preprint\narXiv:2401.05459, 2024.\n",
    "IEEE Network \u2022 November/December 2024\n219\n0890-8044/24\u00a92024IEEE\nAbstract\nCombined with artificial intelligence (AI) tech-\nnology, Space-Air-Ground Integrated Networks \n(SAGINs) play a crucial role in realizing the 6G \nvision of self-awareness, ubiquitous intelligence, \nand Internet of Everything (IoE). Compared with \n5G, the 6G vision demands higher performance in \nkey performance indexes (KPIs) such as peak data \nrate, user experience data rate, delay, coverage \npercentage, reliability, etc. And, the independent \nconfiguration and deployment of network func-\ntions through network deployment automation is \nessential for meeting these 6G KPIs. However, tra-\nditional deployment strategies lack flexibility and \napplicability, relying on manual intervention. To \naddress this, we analyze the characteristics of var-\nious AI algorithms in 6G SAGINs and propose a \nfederated learning (FL)-assisted deep reinforcement \nlearning (DRL) framework, which jointly optimizes \ndeployment strategies through local and global col-\nlaboration. Case studies verify the effectiveness of \nthis approach in improving network deployment \nautomation and ensuring related KPIs in data man-\nagement, resource allocation, and other tasks. \nFinally, we discuss the significant challenges that AI \nwill face in deploying 6G SAGIN settings.\nIntroduction\nThe sixth-generation mobile communication stan-\ndard (6G) is anticipated to be a comprehensive \nimprovement of 10 to 100 times compared with \n5G. Furthermore, 6G vision has higher require-\nments in key performance indexes (KPIs) such as \npeak data rate (>100 Gb/s), user experience data \nrate (>10 Gb/s), delay (<1 ms), coverage percent \n(>99%), reliability (>99.999%), and other aspects \n[1]. The Space-Air-Ground Integrated Networks \n(SAGINs) is the key trend for enabling 6G deploy-\nment and realizing Internet of Everything (IoE) \n[2]. As shown in Fig. 1, 6G SAGIN is built upon \nterrestrial networks and expanded through sat-\nellite networks, encompassing space, air, land, \nocean, etc., providing information guarantees for \ndiverse user activities. In particular, the non-ter-\nrestrial networks (NTNs) [3], a 3rd Generation \nPartnership Project (3GPP) initiative, aim to inte-\ngrate satellite and air communications with the \nterrestrial network, enabling broader coverage \nto meet user access requirements. Therefore, \nthe future wireless communications technology \nwill make transformative improvements in qual-\nity of service (QoS) and quality of experience \n(QoE). At that time, emerging technologies such \nas Connected and Automated Vehicles (CAVs), \nInternet of Things (IoT), Mobile Edge Computing \n(MEC), Social Networks, Intelligent Transporta-\ntion Systems (ITS), Holographic Projection, Virtual \nReality (VR), Extended Reality (XR), and so on will \nachieve breakthrough developments [4].\nTo meet the richer service and performance \nrequirements in 6G SAGIN settings, it is neces-\nsary to achieve necessary breakthroughs in critical \ntechnologies based on existing wireless network \narchitectures. Network deployment automation \nplays a crucial role in meeting the 6G KPIs [1]. \nSpecifically, it is the automatic deployment of net-\nwork functions and decisions according to the \nnetwork environment without manual intervention. \nThrough it, some network informatization strate-\ngies can be specified and implemented, thereby \nimproving the overall efficiency and level of net-\nwork management. For instance, 6G SAGINs will \ninvolve a significant amount of resources, including \nspectrum, energy, and computing resources. By \nintelligent resource allocation and management, \nnetwork deployment automation can enhance \nresource utilization efficiency and reduce operating \nexpenses (OPEX). Moreover, the deployment and \nswitching of large-scale infrastructure are essential \nfor 6G SAGINs. Through automated management, \nnetwork deployment automation enables efficient, \naccurate, flexible, and reliable task decision-making \nand deployment. Furthermore, network deploy-\nment automation processes like data management, \nresource allocation, and task offloading will be \nwidely utilized across all aspects of 6G SAGINs. \nCurrently, many related research activities [2], [4], \n[5], [6], [7] are using various emerging technolo-\ngies to support the development of 6G SAGINs, \nwhich also significantly promotes automation of \nnetwork deployment. For instance, to support key \nAI-Enabled Deployment Automation for 6G Space-Air-Ground Integrated Networks: \nChallenges, Design, and Outlook\nSheng Wu\n, Ning Chen\n, Ailing Xiao\n, Haoge Jia\n, Chunxiao Jiang\n, and Peiying Zhang\nOPEN CALL ARTICLE\nDigital Object Identifier:\n10.1109/MNET.2024.3368753\nDate of Current Version:\n18 November 2024\nDate of Publication:\n22 February 2024\nSheng Wu, Ning Chen, Ailing Xiao, and Haoge Jia are with the School of Information and Communication Engineering, Beijing University \nof Posts and Telecommunications, Beijing 100876, China; Chunxiao Jiang is with the Tsinghua Space Center and the Beijing National \nResearch Center for Information Science and Technology, Tsinghua University, Beijing 100084, China; Peiying Zhang (corresponding \nauthor) is with the College of Computer Science and Technology, Qingdao Institute of Software, China University of Petroleum (East \nChina), Qingdao 266580, China, and also with the Key Laboratory of Computing Power Network and Information Security, Ministry of \nEducation, Shandong Computer Science Center (National Supercomputer Center in Jinan), Qilu University of Technology (Shandong \nAcademy of Sciences), Jinan 250013, China.\nhorized licensed use limited to: NUST School of Electrical Engineering and Computer Science (SEECS). Downloaded on March 11,2025 at 17:06:11 UTC from IEEE Xplore.  Restrictions app\n\nIEEE Network \u2022 November/December 2024\n220\nservices, Hou et al. [2] utilize diverse resources at \nthe edge of 6G SAGINs to build low-latency and \nultrareliable edge intelligence.\nAI Boosts Network Automation Deployment\nNotably, traditional network deployment automa-\ntion faces the following challenges in 6G SAGIN \nsettings: (1) It relies on predefined rules and lacks \nthe ability to make intelligent decisions based on \nreal-time network state and demand, resulting in \ninflexible and less adaptable deployments. (2) It \nis unable to self-tune and optimize performance \nbased on historical data and feedback. Instead, it \nrelies on manual intervention. (3) It is prevalent in \nmanual configuration and management, including \ndevice configuration and network topology plan-\nning. This approach has low fault tolerance and \nstruggles to handle large-scale network deploy-\nment requirements. (4) It is designed for specific \nscenarios and lacks universality and adaptability.\nIn recent years, artificial intelligence (AI) has \nbeen widely employed at all levels of network \narchitecture and has significantly improved the \nlevel of network automation, which aligns with \nthe trend of self-awareness and ubiquitous intelli-\ngence for the 6G vision [4]. Specifically, relative \nto traditional deployment strategies, AI-based \nnetwork automation deployment methods have \nthe following advantages. Firstly, they can rapidly \nanalyze and process large amounts of network \ndata, automatically completing deployment tasks \nbased on preset rules, and improving deployment \nspeed and efficiency. Secondly, these methods \ncan automatically adjust and optimize network \ndeployment schemes in response to real-time net-\nwork environment changes and demand. Thirdly, \nthey accurately identify and analyze network prob-\nlems such as faults and security vulnerabilities, \nproviding corresponding solutions to enhance the \naccuracy and reliability of deployment. Fourthly, \nthey automate the management and configuration \nof network devices, offering scalable and large-\nscale deployment solutions. Lastly, these methods \ncontinuously learn and optimize the deployment \nprocess using historical data, providing intelligent \ndecision-making to improve network performance \nand effectiveness. Therefore, AI algorithms \ndemonstrate high efficiency, adaptability, accu-\nracy, reliability, scalability, and intelligence in the \nnetwork deployment automation process.\nMotivation\nExcitingly, DRL shows more promising prospects \nin various network decisions. Specifically, it uti-\nlizes deep neural networks (DNNs) to replace \nQ-tables in reinforcement learning (RL). Through \nDNNs, the agent can obtain a higherdimen-\nsional abstract feature representation, thereby \nimproving the perception ability of the model. \nSecond, the value function of each action \nis evaluated by the expected reward, and the \ncurrent state is mapped to the corresponding \naction through a certain strategy. Based on this \naction, the environment generates a correspond-\ning reward, which will guide the next positive \naction. The above process is repeated until the \noptimal strategy is reached. Therefore, DRL has \nmore satisfactory perception and decision-mak-\ning capabilities simultaneously, which will show \npromising potential in the automation process of \n6G SAGIN deployment [8].\nIn addition, ubiquitous computing will lead to \na surge of data, which will gradually expose issues \nFIGURE 1. 6G space-air-ground integrated network architecture.\nTo meet the richer service and performance  requirements in 6G SAGIN settings, it is necessary \nto achieve necessary breakthroughs in critical technologies based on existing wireless network \narchitectures.\nhorized licensed use limited to: NUST School of Electrical Engineering and Computer Science (SEECS). Downloaded on March 11,2025 at 17:06:11 UTC from IEEE Xplore.  Restrictions app\n\nIEEE Network \u2022 November/December 2024\n221\nsuch as data privacy and real-time performance. \nSpecifically, traditional AI-enabled applications \nneed to upload all training data to a cloud server \nfor unified training and send it back to update \neach local server [9]. This will lead to privacy \nbreaches and inefficiencies in learning. It is worth \nmentioning that federated learning (FL) is widely \nemployed to solve the phenomenon of data silos. \nOn the premise that the data does not leave the \nlocal, parameters are shared for joint modeling, \nwhich not only ensures data privacy but also \neffectively alleviates training delay.\nTherefore, an AI paradigm that can not only \nensure security and privacy but also ensure task \ndecision-making performance is necessary for 6G \nSAGINs. Therefore, the contributions of this work \nare summarized as follows,\n1.\t We articulate the challenges of deployment \nautomation in 6G SAGIN settings and sum-\nmarize and analyze the pros and cons of the \ndifferent AI techniques that are available.\n2.\t For the \u201c4V+S\u201d feature of 6G SAGIN sce-\nnarios, we propose an FL-assisted DRL \nframework to support various automation \nprocesses. Through case studies the effec-\ntiveness of this approach in improving net-\nwork deployment automation and ensuring \nrelated KPIs has been verified.\nIn what follows, we first present the fun-\ndamentals of AI algorithms and discuss their \ncharacteristics and deployments. Second, for \nthe deployment automation of 6G SAGINs, we \npropose an FL-assisted DRL model and analyze \nand discuss its principles and related applications. \nFinally, we summarize this work and look forward \nto the future.\nFundamentals of AI Deployment\nAI is a significantly broad science that consists \nof different fields such as machine learning (ML), \ndeep learning (DL), RL, DRL, FL, etc [5]. In gen-\neral, one of the main goals of AI research is to \nmake machines capable of complex tasks, that \nis, to increase the automation of tasks. A sche-\nmatic diagram of the AI algorithms deployed in \n6G SAGINs is illustrated in Fig. 2.\nData Characterization in 6G SAGIN Settings\nAccording to Statista, the number of connected \nIoT devices worldwide is expected to surge to 25.4 \nbillion in 2030. In 6G SAGIN settings, edge com-\nputing, fog computing, and other mechanisms \nwill deploy ubiquitous computing in booming IoT \ndevices [10]. This will generate massive big data \nthat exhibit \u201c4V+S\u201d characteristics, specifically,\n1) Volume: With the advancement of the \nglobal seamless coverage process and the \nwidespread deployment of AI in computing equip-\nment, large volumes of data will be generated.\n2) Variety: It is mainly reflected in data sources, \ntypes, and correlations. Specifically, data may orig-\ninate from sensors, ITS, social networks, IoT, etc., \nwhich leads to a variety of big data forms, such \nas structured data, semi-structured data, etc. In \naddition, data types are also a form of representa-\ntion, such as image, audio, video, etc. Also, these \ndata tend to show strong correlations, especially \nin social networks.\n3) Velocity: Emerging technologies impose \nstricter requirements on the response speed of \n6G SAGINs. In particular, related technologies \nsuch as VR, XR, and navigation have higher QoS \nrequirements for latency. Specifically, it must be \nrequired to collect valuable information from mas-\nsive data in a short time.\n4) Value: Only a relatively small part of \nthe massive data is of value. Therefore, mining \nvaluable data for forecasting future trends and \npatterns, analyzing it through AI-related algo-\nrithms, and applying it to various fields can create \nmore important value behind the data.\n5) Security: With the popularization of com-\nputing and the evolution and development of AI \ntechnology, much more attention has been paid \nto data security in the process of data sharing. \nHow to balance data sharing and data privacy will \nalso be an essential challenge in 6G SAGINs.\nComparison of Different AI Algorithms in 6G SAGIN \nDeployment\nAs mentioned earlier, in 6G SAGIN settings, \ndue to the ubiquity of computing, the amount \nof data will expand rapidly. Therefore, how \nto effectively utilize this information to pro-\nvide support for network adjustment and \ndecision-making is difficult to be solved. In \nturn, it leads to a series of network deployment \nautomation issues, such as resource allocation, \ntask offloading, path planning, etc. Moreover, \nFIGURE 2. Deployment of AI algorithms in 6G space-air-ground integrated networks.\nhorized licensed use limited to: NUST School of Electrical Engineering and Computer Science (SEECS). Downloaded on March 11,2025 at 17:06:11 UTC from IEEE Xplore.  Restrictions app\n\nIEEE Network \u2022 November/December 2024\n222\nrelevant AI algorithms will serve as fundamen-\ntal means in the deployment automation of 6G \nSAGINs. Therefore, how to scientifically, rea-\nsonably, and effectively manage, analyze, and \nmine the data of \u201c4V+S\u201d characteristics in 6G \nSAGINs to meet the needs of various intelligent \napplications will be important challenges for AI \ndeployment [11].\nAt present, there is already some related \nresearch combined with AI that has been con-\ntinuously trying to solve the challenges. For \nexample, our previous work [12] leveraged DRL \nto deploy relevant models for resource alloca-\ntion strategies of SAGINs, driving automation of \nnetwork deployment. To solve the deployment \nproblem of AI in mobile applications of future \n6G networks, Letaief et al. [5] combine FL to \nprovide a training paradigm of parameter shar-\ning to deal with security and complexity issues \nsatisfactorily.\nIn AI algorithms, ML is a technique that \nenables computer programs to acquire knowl-\nedge and experience automatically through data. \nDL is a form of ML that employs DNNs to simu-\nlate and solve intricate problems. RL is a method \nfor agents to learn by interacting with their envi-\nronment, aiming to maximize cumulative rewards \nthrough trial-and-error learning. Moreover, DRL \ncombines the strengths of DL and RL, utilizing \nDNNs to enhance the agent\u2019s learning efficiency. \nFL, on the other hand, is a distributed learning \napproach that employs joint learning across \nmultiple devices to train a global model while \nsafeguarding the privacy of local data on each \ndevice. In such scenarios, different AI algorithms \nwill present different advantages and disadvan-\ntages. According to experience and surveys, Table \n1 presents the pros and cons of different AI tech-\nniques that are available.\nFramework of AI-Enabled 6G SAGIN \nDeployment Automation\nBased on the above analysis, different AI algo-\nrithms play different roles in the automation \nprocess of 6G SAGIN deployment. In particu-\nlar, DRL will play a more dominant role in the \ndynamic network environment due to its good \ninteraction performance and perception of the \nenvironment. In addition, the training mechanism \nof FL will better protect the data privacy of differ-\nent sub-networks, thereby improving the security \nof the data. Therefore, based on the above rea-\nsoning and inspiration, we propose an FL-assisted \nDRL framework as shown in Fig. 3, which \nensures the performance of task decision-making \nand data security, thereby further improving the \ndeployment automation of network tasks.\nIn this section, we first introduce the principle \nanalysis of the proposed framework for FL-assisted \nDRL. Second, its application on various network \ntasks to facilitate network deployment automation \nis introduced.\nFL-Assisted DRL Framework\nAs mentioned above, in 6G SAGINs, there are \ndifferent technical sub-networks associated with \ndifferent user requirements. Each sub-network has \nspecific local characteristics and service require-\nments. For example, the requirements for the \ndelay in the intelligent transportation system are \nrelatively strict; the aeronautical ad-hoc network \nhas high mobility; the social network has high \nrequirements for data security [13]. In addition, \nthe security of information between different \nsub-networks is particularly important. When the \ncentralized training strategy collects the global \nnetwork deployment information, it collects all \nsub-networks information for unified training, \nwhich is not friendly to various intelligent applica-\ntions. Since a huge security hole will be created, \nit also makes the problem more complicated and \nincreases the learning time.\nTherefore, considering the differentiated QoS \nrequirements of different sub-networks and the \nsecurity issues in the network deployment opti-\nmization process, an FL-based training paradigm \nis necessary. To better focus on the dynamics \nand heterogeneity of each sub-network, local \nservers and local DRL models are deployed in \neach sub-network to effectively interact with \nthe sub-network environment. It is important to \ndeploy a global server and a global DRL model \non the global network. Specifically, the global \nmodel has the following functions:\n\u2022\t Collect federated actions and federated \nstates, which are the combination of local \nactions and states of each subnet respec-\ntively, and will be used for training the glob-\nal DRL model. In this way, it can effectively \nfocus on the global characteristics of the \nnetwork (inter-subnet).\n\u2022\t Parameter Aggregation. In the training para-\ndigm of FL-assisted DRL, all distributed local \nmodels upload parameters to the global \nmodel. Without sharing local data, the glob-\nal model aggregates all local parameters and \nshares the training parameters for collabora-\ntive optimization.\nAI algorithms\nAdvantages\nDisadvantages\nDeep Learning\n(1) Fit arbitrary complex functions and complex nonlinear \nmappings; (2) Powerful representation ability in high-\ndimensional space; (3) Strong robustness and fault \ntolerance to noisy data.\n(1) Long training time; (2) Rely on computing \nand storage resources, sufficient training \ndata, and high hardware; (2) Occur gradient \ndisappearance and slow convergence in \ndeep layers; (3) Complex model design.\nReinforcement \nLearning\n(1) Weak training data dependence, unsupervised \nlearning; (2) Relatively fewer constraints, fewer \nparameters, and lower hardware requirements; (3) \nAdopt a trial-and-error learning strategy, and tend to the \noverall revenue.\n(1) Weak generalization and portability; (2) \nIn high-dimensional spaces rely on larger \naction and state spaces; (3) Complex specific \nreward function design; (4) Weak perception \nand representation.\nDeep \nReinforcement \nLearning\nCombining the advantages of DL and RL at the same \ntime: (1) Good perception and decision-making; (2) \nBetter performance for problems in continuous or high-\ndimensional discrete action space; (3) Easier to converge.\n(1) Complex specific reward function design; \n(2) Weak data privacy and protection.\nFederated \nLearning\n(1) Strong data privacy and protection; (2) All parties \ninvolved are equal; (3) Multi-party collaboration to build \na shared model; (4) Higher learning efficiency by the \ndistributed architecture.\n(1) The global network is susceptible to the \ninstability of local networks; (2) Lack of \ninteraction with environments.\nTABLE 1. A comparison of advantages and disadvantages of different AI \ndeployments in 6G SAGINs.\nMoreover, relevant AI algorithms will serve as fundamental means in the deployment automation of 6G \nSAGINs.\nhorized licensed use limited to: NUST School of Electrical Engineering and Computer Science (SEECS). Downloaded on March 11,2025 at 17:06:11 UTC from IEEE Xplore.  Restrictions app\n\nIEEE Network \u2022 November/December 2024\n223\n\u2022\t The global server distributes aggregated \nweights, federated parameters, etc. to each \nlocal model after optimizing training with \nshared weights.\nUnlike the traditional learning architecture, the \nFL paradigm adopts a distributed architecture and \nonly shares local training parameters. Therefore, \nit effectively protects the privacy of data, accel-\nerates the deployment of distributed ML, and \nreduces the demand for resources such as com-\nputing and communication.\nIn the era of big data produced by 6G SAGINs, \ntraditional ML and DL algorithms require a large \namount of manually processed data as input. In \nparticular, supervised learning often requires man-\nual labeling of the data. Despite the emergence \nof a series of automated processing tools, the loss \nof human time cost has not decreased. As an \nobject-oriented technique, the DRL algorithm alle-\nviates the above problems while efficiently fitting \ncomplex objects. Specifically, it learns through the \nfollowing modes:\n\u2022\t Extract States. The DRL algorithm automat-\nically extracts environmental information to \nconstruct a feature matrix as a state input. It \ndoes not require specific manual processing \nand marking, and only needs to formulate \nextraction rules.\n\u2022\t Combine the powerful feature representa-\ntion ability and nonlinear fitting ability of \nDNN, carry out forward feature processing \nand calculation, and then obtain the guide \nof current actions.\n\u2022\t Combine the reward mechanism and gradi-\nent descent and other strategies to optimize \nand update the parameters of the model. \nSpecifically, the model will be optimized \ntowards a gradient with a high reward value \nand a global optimum.\n\u2022\t Repeat the above process according to \ntime slices and other methods to better pay \nattention to the dynamics of networks.\nIn summary, in the sub-network, the local model \ncollects local data for learning and analysis to gain \na grasp of the local environment and needs. The \nglobal model aggregates all uploaded local param-\neters, federation states, etc., and performs iterative \nupdates to better focus on the global characteristics \nof the network. Finally, the aggregation parameters \nand rewards are distributed to each local model. \nTherefore, the proposed framework not only \nensures the security and privacy of local data but \nalso pays better attention to the local characteristics \nof the local network and the global characteristics \nof the global network, which will effectively improve \nthe performance of task decision-making.\nCase Study: Taking the Industrial Internet of \nThings (IIoT)1 as the background, the data man-\nagement task is taken as an example. In [9], we \nadapt the FL-assisted DRL framework as shown in \nFig. 3 to solve the problem of data management \nand learning with \u201c4V+S\u201d characteristics in IIoT. \n1 An important architecture \nin the era of Industry 4.0, \nwhere extensive IIoT devices \nwill be continuously and \nfrequently connected to the \nInternet of Things, and will \ngenerate data with the char-\nacteristics of \u201c4V+S.\u201d\nFIGURE 3. The proposed FL-assisted DRL framework for 6G SAGIN deployment automation.\nhorized licensed use limited to: NUST School of Electrical Engineering and Computer Science (SEECS). Downloaded on March 11,2025 at 17:06:11 UTC from IEEE Xplore.  Restrictions app\n\nIEEE Network \u2022 November/December 2024\n224\nFrom all angles, all aspects of experiments are \ncarried out. Through experimental analysis, it is \ndetermined that the framework is efficient in data \nmanagement tasks on the premise of ensuring \ndata security. Therefore, the proposed FL-assisted \nDRL framework effectively enhances the deploy-\nment automation process of data management for \n6G SAGINs.\nTaking SAGINs as the background, the \nresource allocation task is taken as an example. In \n[12], we adopt and deploy the architecture of dis-\ntributed DRL for resource management decisions. \nThe experimental results show that the DRL-based \nmethod can better interact with the environ-\nment, and the distributed architecture can better \nfocus on local and global characteristics, thereby \nachieving better task performance. Inspired by \nFL, as discussed in [14], we propose an FL-based \nmulti-domain virtual network embedding algo-\nrithm, which adopts the architecture of Fig. 3 to \nmake resource allocation decisions for multi-do-\nmain physical networks. Specifically, the work \nutilizes the distributed paradigm, deploying a \nlocal server in each physical domain and a global \nserver. In addition, GT-ITM was used to gener-\nate a simulation environment, and the simulation \nexperiment was implemented based on Python. \nHeuristic, RL, and deep neural network (DNN) \n+ RL strategies were selected as benchmarks for \ncomparative experiments. Also, service requests \narrive in time order based on Poisson distribu-\ntion, with a start time of 22s and an end time of \n20, 000s. The time unit is 1000s, and the change \nof indicators value for 20-time units is recorded. \nThe pairwise comparison of the results can clarify \nthe effectiveness of different strategies. As illus-\ntrated in Fig. 4, the final experimental results show \nthat it can efficiently provide low-cost, high-reve-\nnue, and fast-response decision-making, and it is \nsignificantly better than other ML strategies. This \nwork significantly reduces resource fragmenta-\ntion and enhances learning efficiency and data \nprivacy. Therefore, the proposed FL-assisted DRL \nframework effectively enhances the deployment \nautomation process of resource allocation for 6G \nSAGINs.\nAI-Enabled Network Deployment Automation\nIn addition, as illustrated in Fig. 5, tasks like \nresource allocation that can be modeled as Mar-\nkov Decision Processes (MDPs) in 6G SAGINs \ncan be effectively modeled by this framework. For \nexample, data management, task offloading, etc. \nThat is, the FL-assisted DEL framework has good \ngeneralization and generality.\n1) Resource Allocation: Due to frequent \nservice requests and task scheduling, limited \nresources (computing, bandwidth, etc.) pose chal-\nlenges to the normal operation of the network. \nAs the basic conditions for network operation, \nthese requirements should be met with the least \nresource cost as much as possible to maintain the \noperation of different services and network func-\ntions. The FL-assisted DRL algorithm can observe \nthe complex resource state in the network and \ncalculate the optimal resource allocation strategy \nfor indicators such as long-term benefits in the \ncurrent view [12].\n2) Data Management: In 6G SAGINs, the fre-\nquent access of massive IoT devices will generate \na large amount of dynamic data. In particular, the \nprivacy of data in the IIoT environment presents \nchallenges. To effectively manage these data to \nmine the enormous value, and avoid the data \nFIGURE 5. Network deployment automation facilitated by the proposed FL-assist-\ned DRL model.\nFIGURE 4. In [14], the comparison results of the proposed FL-assisted DRL model \nand existing works on different indicators. Three benchmarks are adopted \nseparately: \u201cDeep Neural Network (DNN) + RL\u201d, \u201cRL\u201d and \u201cHeuristic\u201d \nstrategies. Evaluation indicators for long-term average revenue, long-term \naverage revenue-cost ratio, and allocation success acceptance rate imply \nresource consumption, and the higher their values, the higher the resource \nutilization rate. Additionally, evaluation indicator time comparison of each \ncommunication round (s) implies completion time. For details, see [14].\nhorized licensed use limited to: NUST School of Electrical Engineering and Computer Science (SEECS). Downloaded on March 11,2025 at 17:06:11 UTC from IEEE Xplore.  Restrictions app\n\nIEEE Network \u2022 November/December 2024\n225\nbeing directly presented to a third party, the FL-as-\nsisted DRL framework will provide a reasonable \nsolution [9].\n3) Task Offloading: With the deep integration \nof the computing network and IoT, edge com-\nputing (EC) and MEC will flourish. The traditional \nmethod uploads the collected data to the com-\nputing center and returns the result to the device \nto guide its behavior after calculation and anal-\nysis. However, it is difficult to meet the demand \nfor the delay only through the enhancement of \ncommunication capability. Using ubiquitous com-\nputing power, EC offloads computing-intensive or \nlatency-sensitive tasks to edge devices to realize \nthe entire process of data collection, processing, \nanalysis, and decision-making. It avoids the task \ncongestion of the core network while greatly \nreducing the delay. For limited edge resources, \nhow to reasonably arrange the offloading strat-\negy to achieve the goal of the shortest delay is \nthe problem to be solved. In addition, the high-\nspeed mobility of edge devices in MEC and the \nheterogeneity of computing tasks further pose \nchallenges. Notably, it can likewise be modeled as \nan MDP process [15].\nBesides, it is easy to reason that the FL-assisted \nDRL framework can be employed in various \naspects of 6G SAGINs. Specifically, it can provide \nexcellent decision support for various applications \nof the network under the premise of ensuring \nthe data security requirements of each sub-net-\nwork. Moreover, it illustrates that AI-enabled \nalgorithms will drive the automation of 6G SAGIN \ndeployments.\nConclusion and Future Outlook\nIn this work, we analyze the architecture and \nchallenges of 6G SAGINs in detail. On this \nbasis, we present the fundamentals of related AI \nalgorithms and their different characteristics in \nnetwork deployment automation. Furthermore, \nwe propose an FL-assisted DRL model and dis-\ncuss its principles and applications in network \ndeployment.\nAlthough the advantages of AI in 6G SAGIN \nsettings are already obvious, in future work, there \nare still the following challenges to be solved:\nSupport of wireless communication tech-\nnology and hardware equipment: It is expected \nthat 6G will expand to the terahertz band, and \nhardware equipment that supports this signal will \ndirectly affect the deployment of AI algorithms. \nIn addition, limited device resources (especially \ncomputing) will also limit the deployment and \ndecision-making of AI algorithms. Therefore, an \nefficient hardware device to support future wire-\nless communication scenarios is essential.\nIntegration of 6G new technologies: For 6G \nscenarios, a large number of emerging technolo-\ngies will be produced. How to deploy generalized \nAI algorithms for specific characteristics is a key \nchallenge to be solved.\nConstruction of simulation environment: \nThe development of emerging communication \ntechnologies have contributed to the increasing \ncomplexity of network environments, including \ncharacteristics such as heterogeneity, dynamics, \nand self-organization. To promote the practica-\nbility of AI algorithms, researchers rely heavily \non simulation experiment environments during \nthe design and deployment phases of AI algo-\nrithms. In addition, AI algorithms are also prone \nto overfitting to a customized environment. \nTherefore, a simulation environment platform \nthat integrates various algorithmic benchmarks \nand simulates realistic network environments is \nurgently desired.\nComplexity of AI algorithms: Communica-\ntion technologies represented by B5G and 6G \nwill bring higher device access density, larger net-\nwork cluster scale, and more ad-hoc coexistence. \nThe deployment of complex AI algorithms may \ninvolve extensive physical node coverage, which \nwill make the service response delay more serious \nand reduce the users\u2019 QoS and QoE. Therefore, a \nmore lightweight AI algorithm deployment is an \nimportant issue.\nDeployment of Explainable AI: Explainable \nML, represented by Deep Fuzzy Neural Systems \n(DNFS), breaks the black-box model of tradi-\ntional AI algorithms fitting data laws. Explainable \nAI algorithms will make the decision-making laws \nof the model more transparent to researchers, \nwhich is conducive to the diagnosis, analysis, and \nupgrading of products, and thus the effective col-\nlaboration between humans and machines.\nTrade-off between redundancy and cost: \nA key feature of 6G SAGIN is its capacity to \nsupport Native AI, that is, 6G carries native AI \nand AI is spread throughout SAGIN. Through \nenvironment awareness and data support, \ndeployment automation for SAGIN is enabled, \nand communication-sensing-computing integra-\ntion will become an important technology for \n6G SAGIN. In addition, in the future 6G SAGIN \nintegrated with communication-sensing-comput-\ning, duplicate or redundant functions, processes, \nand resources can be deployed to enhance sys-\ntem reliability, fault tolerance, and resilience [6]. \nFor instance, (1) establishing multiple backup \ncommunication routes to ensure resilient links \nin dynamic environments; (2) deploying redun-\ndant backup equipment resources to prevent \nservice interruptions caused by node failures. \nHowever, these aspects of communication-sens-\ning-computing bring additional resource costs \nin terms of AI load. For example, (1) Commu-\nnication: redundant communication routes or \nfunctions result in increased data transmission \noverhead, requiring effective management and \ncoordination of data flow by AI algorithms to \navoid unnecessary redundancy; (2) Sensing: AI \nalgorithms need to perceive changes in the sur-\nrounding environment in real-time to cope with \npotential node and link failures; (3) Comput-\ning: redundant functions and processes require \nadditional computing resources, making AI algo-\nrithms responsible for managing and optimizing \nthe network need to deal with increasing com-\nplexity; (4) Other aspects: AI models need to be \ncontinuously trained, maintained, and updated \nto adapt to redundant configurations in dynamic \nenvironments, affecting processing time and \nenergy consumption. In summary, a good trade-\noff between redundancy and cost needs to be \nmaintained in the future 6G SAGIN system.\nMoreover, it illustrates that AI-enabled algorithms will drive the automation of 6G SAGIN deployments.\nhorized licensed use limited to: NUST School of Electrical Engineering and Computer Science (SEECS). Downloaded on March 11,2025 at 17:06:11 UTC from IEEE Xplore.  Restrictions app\n\nIEEE Network \u2022 November/December 2024\n226\nAcknowledgment\nThis work was supported in part by the National \nNatural Science Foundation of China under \nGrant 62341104, Grant 62325108, and Grant \n62341131; and in part by the Natural Science \nFoundation of Shandong Province under Grant \nZR2023LZH017 and Grant ZR2022LZH015.\nReferences\n[1] S. Chen et al., \u201cVision, requirements, and technology trend \nof 6G: How to tackle the challenges of system coverage, \ncapacity, user data-rate and movement speed,\u201d IEEE Wireless \nCommun., vol. 27, no. 2, pp. 218\u2013228, Apr. 2020.\n[2] X. Hou et al., \u201cEdge intelligence for mission-critical 6G ser-\nvices in space-air-ground integrated networks,\u201d IEEE Netw., \nvol. 36, no. 2, pp. 181\u2013189, Mar. 2022.\n[3] Study on New Radio (NR) to Support Non-Terrestrial Net-\nworks, document TR 38.811, Version 15.4.0, 3rd Gener. \nPartnership Project (3GPP), 2020.\n[4] Z. Wang et al., \u201cAI-based cloud-edge-device collaboration \nin 6G space-air-ground integrated power IoT,\u201d IEEE Wireless \nCommun., vol. 29, no. 1, pp. 16\u201323, Feb. 2022.\n[5] K. B. Letaief et al., \u201cThe roadmap to 6G: AI empowered \nwireless networks,\u201d IEEE Commun. Mag., vol. 57, no. 8, pp. \n84\u201390, Aug. 2019.\n[6] P. Yang, L. Kong, and G. Chen, \u201cSpectrum sharing for 5G/6G \nURLLC: Research frontiers and standards,\u201d IEEE Commun. \nStand. Mag., vol. 5, no. 2, pp. 120\u2013125, Jun. 2021.\n[7] H. Cui et al., \u201cSpace-air-ground integrated network (SAGIN) \nfor 6G: Requirements, architecture and challenges,\u201d China \nCommun., vol. 19, no. 2, pp. 90\u2013108, Feb. 2022.\n[8] W. Chen et al., \u201cDeep reinforcement learning for Internet \nof Things: A comprehensive survey,\u201d IEEE Commun. Surveys \nTuts., vol. 23, no. 3, pp. 1659\u20131692, 3rd Quart., 2021.\n[9] P. Zhang et al., \u201cDeep reinforcement learning assisted feder-\nated learning algorithm for data management of IIoT,\u201d IEEE \nTrans. Ind. Informat., vol. 17, no. 12, pp. 8475\u20138484, Dec. \n2021.\n[10] S. Baek et al., \u201c3GPP new radio release 16: Evolution of 5G \nfor industrial Internet of Things,\u201d IEEE Commun. Mag., vol. \n59, no. 1, pp. 41\u201347, Jan. 2021.\n[11] F. Tang et al., \u201cFederated learning for intelligent transmission \nwith space-air-ground integrated network (SAGIN) toward \n6G,\u201d IEEE Netw., vol. 37, no. 2, pp. 198\u2013204, Mar./Apr. \n2023.\n[12] C. Wang et al., \u201cIncorporating distributed DRL into storage \nresource optimization of space-air-ground integrated wire-\nless communication network,\u201d IEEE J. Select. Topics Signal \nProcess., vol. 16, no. 3, pp. 434\u2013446, Apr. 2022.\n[13] J. Liu et al., \u201cTask-oriented intelligent networking architec-\nture for the space\u2013air\u2013ground\u2013aqua integrated network,\u201d \nIEEE Internet Things J., vol. 7, no. 6, pp. 5345\u20135358, Jun. \n2020.\n[14] P. Zhang et al., \u201cMulti-domain virtual network embedding \nalgorithm based on horizontal federated learning,\u201d IEEE \nTrans. Inf. Forensics Security, vol. 18, pp. 3363\u20133375, 2023.\n[15] H. Shen, Y. Tian, T. Wang, and G. Bai, \u201cSlicing-based task \noffloading in space-air-ground integrated vehicular net-\nworks,\u201d IEEE Trans. Mobile Comput., early access, Jun. 7, \n2023, doi: 10.1109/TMC.2023.3283852.\nBiographies\nSheng Wu (Member, IEEE) (thuraya@bupt.edu.cn) received the \nPh.D. degree in electronic engineering from Tsinghua University, \nBeijing, China, in 2014. He was a Post-Doctoral Researcher with \nthe Tsinghua Space Center, Tsinghua University. He is currently \na Professor with the Beijing University of Posts and Telecommu-\nnications. He has published more than 80 top journal and con-\nference papers. His research interests include iterative detection \nand decoding, channel estimation, massive MIMO, and satellite \ncommunications.\nNing Chen (Student Member, IEEE) (nchen@bupt.edu.cn) is \ncurrently pursuing the Ph.D. degree with the School of Infor-\nmation and Communication Engineering, Beijing University of \nPosts and Telecommunications, Beijing, China. He has published \nmore than ten high-level articles. His research interests include \nsatellite communications, network virtualization, and artificial \nintelligence for networking.\nAiling Xiao (Member, IEEE) (xiao_ailing@bupt.edu.cn) received \nthe Ph.D. degree in computer science and technology from the \nBeijing University of Posts and Telecommunications, Beijing, \nChina, in 2015. She was a Post-Doctoral Researcher with the \nDepartment of Electronic Engineering, Tsinghua University, Bei-\njing, from 2015 to 2018. She is currently an Associate Professor \nwith the Beijing University of Posts and Telecommunications. \nShe has published over 20 top journal and conference papers. \nHer research interests include maritime communications and \nQoE-oriented network management.\nHaoge Jia (Member, IEEE) (jhg@bupt.edu.cn) received the B.S. \nand Ph.D. degrees in electronic engineering from Tsinghua Uni-\nversity, Beijing, China, in 2016 and 2022, respectively. She is \ncurrently a Post-Doctoral Researcher with the School of Informa-\ntion and Communication Engineering, Beijing University of Posts \nand Telecommunications, Beijing. Her major research interests \ninclude random access, interference analysis, and signal process-\ning in satellite communication.\nChunxiao Jiang (Fellow, IEEE) (jchx@tsinghua.edu.cn) received \nthe Ph.D. degree in electronic engineering from Tsinghua \nUniversity, Beijing, China, in 2013. He visited the University \nof Maryland College Park and the University of Southampton. \nHe was a Post-Doctoral Researcher with the Department of \nElectronic Engineering, Tsinghua University. He is currently an \nAssociate Professor with the Tsinghua Space Center, Tsinghua \nUniversity. His research interests include application of game \ntheory, optimization, and statistical theories to communication, \nnetworking, and resource allocation problems, in particular \nspace networks and heterogeneous networks.\nPeiying Zhang (Member, IEEE) (zhangpeiying@upc.edu.cn) \nreceived the Ph.D. degree from the School of Information \nand Communication Engineering, Beijing University of Posts \nand Telecommunications, China, in 2019. He is currently an \nAssociate Professor with the College of Computer Science \nand Technology, China University of Petroleum (East China). \nHe has published multiple technical research papers in top \njournals and conferences. He is the leading guest editor and \nan editorial board member of multiple journals. His research \ninterests include semantic computing, future internet archi-\ntecture, network virtualization, and artificial intelligence for \nnetworking.\nhorized licensed use limited to: NUST School of Electrical Engineering and Computer Science (SEECS). Downloaded on March 11,2025 at 17:06:11 UTC from IEEE Xplore.  Restrictions app\n",
    "Published as a conference paper at ICLR 2021\nAN IMAGE IS WORTH 16X16 WORDS:\nTRANSFORMERS FOR IMAGE RECOGNITION AT SCALE\nAlexey Dosovitskiy\u2217,\u2020, Lucas Beyer\u2217, Alexander Kolesnikov\u2217, Dirk Weissenborn\u2217,\nXiaohua Zhai\u2217, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer,\nGeorg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby\u2217,\u2020\n\u2217equal technical contribution, \u2020equal advising\nGoogle Research, Brain Team\n{adosovitskiy, neilhoulsby}@google.com\nABSTRACT\nWhile the Transformer architecture has become the de-facto standard for natural\nlanguage processing tasks, its applications to computer vision remain limited. In\nvision, attention is either applied in conjunction with convolutional networks, or\nused to replace certain components of convolutional networks while keeping their\noverall structure in place. We show that this reliance on CNNs is not necessary\nand a pure transformer applied directly to sequences of image patches can perform\nvery well on image classi\ufb01cation tasks. When pre-trained on large amounts of\ndata and transferred to multiple mid-sized or small image recognition benchmarks\n(ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent\nresults compared to state-of-the-art convolutional networks while requiring sub-\nstantially fewer computational resources to train.1\n1\nINTRODUCTION\nSelf-attention-based architectures, in particular Transformers (Vaswani et al., 2017), have become\nthe model of choice in natural language processing (NLP). The dominant approach is to pre-train on\na large text corpus and then \ufb01ne-tune on a smaller task-speci\ufb01c dataset (Devlin et al., 2019). Thanks\nto Transformers\u2019 computational ef\ufb01ciency and scalability, it has become possible to train models of\nunprecedented size, with over 100B parameters (Brown et al., 2020; Lepikhin et al., 2020). With the\nmodels and datasets growing, there is still no sign of saturating performance.\nIn computer vision, however, convolutional architectures remain dominant (LeCun et al., 1989;\nKrizhevsky et al., 2012; He et al., 2016). Inspired by NLP successes, multiple works try combining\nCNN-like architectures with self-attention (Wang et al., 2018; Carion et al., 2020), some replacing\nthe convolutions entirely (Ramachandran et al., 2019; Wang et al., 2020a). The latter models, while\ntheoretically ef\ufb01cient, have not yet been scaled effectively on modern hardware accelerators due to\nthe use of specialized attention patterns. Therefore, in large-scale image recognition, classic ResNet-\nlike architectures are still state of the art (Mahajan et al., 2018; Xie et al., 2020; Kolesnikov et al.,\n2020).\nInspired by the Transformer scaling successes in NLP, we experiment with applying a standard\nTransformer directly to images, with the fewest possible modi\ufb01cations. To do so, we split an image\ninto patches and provide the sequence of linear embeddings of these patches as an input to a Trans-\nformer. Image patches are treated the same way as tokens (words) in an NLP application. We train\nthe model on image classi\ufb01cation in supervised fashion.\nWhen trained on mid-sized datasets such as ImageNet without strong regularization, these mod-\nels yield modest accuracies of a few percentage points below ResNets of comparable size. This\nseemingly discouraging outcome may be expected: Transformers lack some of the inductive biases\n1Fine-tuning\ncode\nand\npre-trained\nmodels\nare\navailable\nat\nhttps://github.com/\ngoogle-research/vision_transformer\n1\narXiv:2010.11929v2  [cs.CV]  3 Jun 2021\n\nPublished as a conference paper at ICLR 2021\ninherent to CNNs, such as translation equivariance and locality, and therefore do not generalize well\nwhen trained on insuf\ufb01cient amounts of data.\nHowever, the picture changes if the models are trained on larger datasets (14M-300M images). We\n\ufb01nd that large scale training trumps inductive bias. Our Vision Transformer (ViT) attains excellent\nresults when pre-trained at suf\ufb01cient scale and transferred to tasks with fewer datapoints. When\npre-trained on the public ImageNet-21k dataset or the in-house JFT-300M dataset, ViT approaches\nor beats state of the art on multiple image recognition benchmarks. In particular, the best model\nreaches the accuracy of 88.55% on ImageNet, 90.72% on ImageNet-ReaL, 94.55% on CIFAR-100,\nand 77.63% on the VTAB suite of 19 tasks.\n2\nRELATED WORK\nTransformers were proposed by Vaswani et al. (2017) for machine translation, and have since be-\ncome the state of the art method in many NLP tasks. Large Transformer-based models are often\npre-trained on large corpora and then \ufb01ne-tuned for the task at hand: BERT (Devlin et al., 2019)\nuses a denoising self-supervised pre-training task, while the GPT line of work uses language mod-\neling as its pre-training task (Radford et al., 2018; 2019; Brown et al., 2020).\nNaive application of self-attention to images would require that each pixel attends to every other\npixel. With quadratic cost in the number of pixels, this does not scale to realistic input sizes. Thus,\nto apply Transformers in the context of image processing, several approximations have been tried in\nthe past. Parmar et al. (2018) applied the self-attention only in local neighborhoods for each query\npixel instead of globally. Such local multi-head dot-product self attention blocks can completely\nreplace convolutions (Hu et al., 2019; Ramachandran et al., 2019; Zhao et al., 2020). In a different\nline of work, Sparse Transformers (Child et al., 2019) employ scalable approximations to global self-\nattention in order to be applicable to images. An alternative way to scale attention is to apply it in\nblocks of varying sizes (Weissenborn et al., 2019), in the extreme case only along individual axes (Ho\net al., 2019; Wang et al., 2020a). Many of these specialized attention architectures demonstrate\npromising results on computer vision tasks, but require complex engineering to be implemented\nef\ufb01ciently on hardware accelerators.\nMost related to ours is the model of Cordonnier et al. (2020), which extracts patches of size 2 \u00d7 2\nfrom the input image and applies full self-attention on top. This model is very similar to ViT,\nbut our work goes further to demonstrate that large scale pre-training makes vanilla transformers\ncompetitive with (or even better than) state-of-the-art CNNs. Moreover, Cordonnier et al. (2020)\nuse a small patch size of 2 \u00d7 2 pixels, which makes the model applicable only to small-resolution\nimages, while we handle medium-resolution images as well.\nThere has also been a lot of interest in combining convolutional neural networks (CNNs) with forms\nof self-attention, e.g. by augmenting feature maps for image classi\ufb01cation (Bello et al., 2019) or by\nfurther processing the output of a CNN using self-attention, e.g. for object detection (Hu et al., 2018;\nCarion et al., 2020), video processing (Wang et al., 2018; Sun et al., 2019), image classi\ufb01cation (Wu\net al., 2020), unsupervised object discovery (Locatello et al., 2020), or uni\ufb01ed text-vision tasks (Chen\net al., 2020c; Lu et al., 2019; Li et al., 2019).\nAnother recent related model is image GPT (iGPT) (Chen et al., 2020a), which applies Transformers\nto image pixels after reducing image resolution and color space. The model is trained in an unsu-\npervised fashion as a generative model, and the resulting representation can then be \ufb01ne-tuned or\nprobed linearly for classi\ufb01cation performance, achieving a maximal accuracy of 72% on ImageNet.\nOur work adds to the increasing collection of papers that explore image recognition at larger scales\nthan the standard ImageNet dataset. The use of additional data sources allows to achieve state-of-\nthe-art results on standard benchmarks (Mahajan et al., 2018; Touvron et al., 2019; Xie et al., 2020).\nMoreover, Sun et al. (2017) study how CNN performance scales with dataset size, and Kolesnikov\net al. (2020); Djolonga et al. (2020) perform an empirical exploration of CNN transfer learning from\nlarge scale datasets such as ImageNet-21k and JFT-300M. We focus on these two latter datasets as\nwell, but train Transformers instead of ResNet-based models used in prior works.\n2\n\nPublished as a conference paper at ICLR 2021\nTransformer Encoder\nMLP \nHead\nVision Transformer (ViT)\n*\nLinear Projection of Flattened Patches\n* Extra learnable\n     [ cl ass]  embedding\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0\nPatch + Position \nEmbedding\nClass\nBird\nBall\nCar\n...\nEmbedded \nPatches\nMulti-Head \nAttention\nNorm\nMLP\nNorm\n+\nL x\n+\nTransformer Encoder\nFigure 1: Model overview. We split an image into \ufb01xed-size patches, linearly embed each of them,\nadd position embeddings, and feed the resulting sequence of vectors to a standard Transformer\nencoder. In order to perform classi\ufb01cation, we use the standard approach of adding an extra learnable\n\u201cclassi\ufb01cation token\u201d to the sequence. The illustration of the Transformer encoder was inspired by\nVaswani et al. (2017).\n3\nMETHOD\nIn model design we follow the original Transformer (Vaswani et al., 2017) as closely as possible.\nAn advantage of this intentionally simple setup is that scalable NLP Transformer architectures \u2013 and\ntheir ef\ufb01cient implementations \u2013 can be used almost out of the box.\n3.1\nVISION TRANSFORMER (VIT)\nAn overview of the model is depicted in Figure 1. The standard Transformer receives as input a 1D\nsequence of token embeddings. To handle 2D images, we reshape the image x \u2208RH\u00d7W \u00d7C into a\nsequence of \ufb02attened 2D patches xp \u2208RN\u00d7(P 2\u00b7C), where (H, W) is the resolution of the original\nimage, C is the number of channels, (P, P) is the resolution of each image patch, and N = HW/P 2\nis the resulting number of patches, which also serves as the effective input sequence length for the\nTransformer. The Transformer uses constant latent vector size D through all of its layers, so we\n\ufb02atten the patches and map to D dimensions with a trainable linear projection (Eq. 1). We refer to\nthe output of this projection as the patch embeddings.\nSimilar to BERT\u2019s [class] token, we prepend a learnable embedding to the sequence of embed-\nded patches (z0\n0 = xclass), whose state at the output of the Transformer encoder (z0\nL) serves as the\nimage representation y (Eq. 4). Both during pre-training and \ufb01ne-tuning, a classi\ufb01cation head is at-\ntached to z0\nL. The classi\ufb01cation head is implemented by a MLP with one hidden layer at pre-training\ntime and by a single linear layer at \ufb01ne-tuning time.\nPosition embeddings are added to the patch embeddings to retain positional information. We use\nstandard learnable 1D position embeddings, since we have not observed signi\ufb01cant performance\ngains from using more advanced 2D-aware position embeddings (Appendix D.4). The resulting\nsequence of embedding vectors serves as input to the encoder.\nThe Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded self-\nattention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3). Layernorm (LN) is applied before\nevery block, and residual connections after every block (Wang et al., 2019; Baevski & Auli, 2019).\n3\n\nPublished as a conference paper at ICLR 2021\nThe MLP contains two layers with a GELU non-linearity.\nz0 = [xclass; x1\npE; x2\npE; \u00b7 \u00b7 \u00b7 ; xN\np E] + Epos,\nE \u2208R(P 2\u00b7C)\u00d7D, Epos \u2208R(N+1)\u00d7D\n(1)\nz\u2032\n\u2113= MSA(LN(z\u2113\u22121)) + z\u2113\u22121,\n\u2113= 1 . . . L\n(2)\nz\u2113= MLP(LN(z\u2032\n\u2113)) + z\u2032\n\u2113,\n\u2113= 1 . . . L\n(3)\ny = LN(z0\nL)\n(4)\nInductive bias.\nWe note that Vision Transformer has much less image-speci\ufb01c inductive bias than\nCNNs. In CNNs, locality, two-dimensional neighborhood structure, and translation equivariance are\nbaked into each layer throughout the whole model. In ViT, only MLP layers are local and transla-\ntionally equivariant, while the self-attention layers are global. The two-dimensional neighborhood\nstructure is used very sparingly: in the beginning of the model by cutting the image into patches and\nat \ufb01ne-tuning time for adjusting the position embeddings for images of different resolution (as de-\nscribed below). Other than that, the position embeddings at initialization time carry no information\nabout the 2D positions of the patches and all spatial relations between the patches have to be learned\nfrom scratch.\nHybrid Architecture.\nAs an alternative to raw image patches, the input sequence can be formed\nfrom feature maps of a CNN (LeCun et al., 1989). In this hybrid model, the patch embedding\nprojection E (Eq. 1) is applied to patches extracted from a CNN feature map. As a special case,\nthe patches can have spatial size 1x1, which means that the input sequence is obtained by simply\n\ufb02attening the spatial dimensions of the feature map and projecting to the Transformer dimension.\nThe classi\ufb01cation input embedding and position embeddings are added as described above.\n3.2\nFINE-TUNING AND HIGHER RESOLUTION\nTypically, we pre-train ViT on large datasets, and \ufb01ne-tune to (smaller) downstream tasks. For\nthis, we remove the pre-trained prediction head and attach a zero-initialized D \u00d7 K feedforward\nlayer, where K is the number of downstream classes. It is often bene\ufb01cial to \ufb01ne-tune at higher\nresolution than pre-training (Touvron et al., 2019; Kolesnikov et al., 2020). When feeding images\nof higher resolution, we keep the patch size the same, which results in a larger effective sequence\nlength. The Vision Transformer can handle arbitrary sequence lengths (up to memory constraints),\nhowever, the pre-trained position embeddings may no longer be meaningful. We therefore perform\n2D interpolation of the pre-trained position embeddings, according to their location in the original\nimage. Note that this resolution adjustment and patch extraction are the only points at which an\ninductive bias about the 2D structure of the images is manually injected into the Vision Transformer.\n4\nEXPERIMENTS\nWe evaluate the representation learning capabilities of ResNet, Vision Transformer (ViT), and the\nhybrid. To understand the data requirements of each model, we pre-train on datasets of varying size\nand evaluate many benchmark tasks. When considering the computational cost of pre-training the\nmodel, ViT performs very favourably, attaining state of the art on most recognition benchmarks at\na lower pre-training cost. Lastly, we perform a small experiment using self-supervision, and show\nthat self-supervised ViT holds promise for the future.\n4.1\nSETUP\nDatasets. To explore model scalability, we use the ILSVRC-2012 ImageNet dataset with 1k classes\nand 1.3M images (we refer to it as ImageNet in what follows), its superset ImageNet-21k with\n21k classes and 14M images (Deng et al., 2009), and JFT (Sun et al., 2017) with 18k classes and\n303M high-resolution images. We de-duplicate the pre-training datasets w.r.t. the test sets of the\ndownstream tasks following Kolesnikov et al. (2020). We transfer the models trained on these\ndataset to several benchmark tasks: ImageNet on the original validation labels and the cleaned-up\nReaL labels (Beyer et al., 2020), CIFAR-10/100 (Krizhevsky, 2009), Oxford-IIIT Pets (Parkhi et al.,\n2012), and Oxford Flowers-102 (Nilsback & Zisserman, 2008). For these datasets, pre-processing\nfollows Kolesnikov et al. (2020).\n4\n\nPublished as a conference paper at ICLR 2021\nModel\nLayers\nHidden size D\nMLP size\nHeads\nParams\nViT-Base\n12\n768\n3072\n12\n86M\nViT-Large\n24\n1024\n4096\n16\n307M\nViT-Huge\n32\n1280\n5120\n16\n632M\nTable 1: Details of Vision Transformer model variants.\nWe also evaluate on the 19-task VTAB classi\ufb01cation suite (Zhai et al., 2019b). VTAB evaluates\nlow-data transfer to diverse tasks, using 1 000 training examples per task. The tasks are divided into\nthree groups: Natural \u2013 tasks like the above, Pets, CIFAR, etc. Specialized \u2013 medical and satellite\nimagery, and Structured \u2013 tasks that require geometric understanding like localization.\nModel Variants. We base ViT con\ufb01gurations on those used for BERT (Devlin et al., 2019), as\nsummarized in Table 1. The \u201cBase\u201d and \u201cLarge\u201d models are directly adopted from BERT and we\nadd the larger \u201cHuge\u201d model. In what follows we use brief notation to indicate the model size and\nthe input patch size: for instance, ViT-L/16 means the \u201cLarge\u201d variant with 16\u00d716 input patch size.\nNote that the Transformer\u2019s sequence length is inversely proportional to the square of the patch size,\nthus models with smaller patch size are computationally more expensive.\nFor the baseline CNNs, we use ResNet (He et al., 2016), but replace the Batch Normalization lay-\ners (Ioffe & Szegedy, 2015) with Group Normalization (Wu & He, 2018), and used standardized\nconvolutions (Qiao et al., 2019). These modi\ufb01cations improve transfer (Kolesnikov et al., 2020),\nand we denote the modi\ufb01ed model \u201cResNet (BiT)\u201d. For the hybrids, we feed the intermediate fea-\nture maps into ViT with patch size of one \u201cpixel\u201d. To experiment with different sequence lengths,\nwe either (i) take the output of stage 4 of a regular ResNet50 or (ii) remove stage 4, place the same\nnumber of layers in stage 3 (keeping the total number of layers), and take the output of this extended\nstage 3. Option (ii) results in a 4x longer sequence length, and a more expensive ViT model.\nTraining & Fine-tuning. We train all models, including ResNets, using Adam (Kingma & Ba,\n2015) with \u03b21 = 0.9, \u03b22 = 0.999, a batch size of 4096 and apply a high weight decay of 0.1, which\nwe found to be useful for transfer of all models (Appendix D.1 shows that, in contrast to common\npractices, Adam works slightly better than SGD for ResNets in our setting). We use a linear learning\nrate warmup and decay, see Appendix B.1 for details. For \ufb01ne-tuning we use SGD with momentum,\nbatch size 512, for all models, see Appendix B.1.1. For ImageNet results in Table 2, we \ufb01ne-tuned at\nhigher resolution: 512 for ViT-L/16 and 518 for ViT-H/14, and also used Polyak & Juditsky (1992)\naveraging with a factor of 0.9999 (Ramachandran et al., 2019; Wang et al., 2020b).\nMetrics. We report results on downstream datasets either through few-shot or \ufb01ne-tuning accuracy.\nFine-tuning accuracies capture the performance of each model after \ufb01ne-tuning it on the respective\ndataset. Few-shot accuracies are obtained by solving a regularized least-squares regression problem\nthat maps the (frozen) representation of a subset of training images to {\u22121, 1}K target vectors. This\nformulation allows us to recover the exact solution in closed form. Though we mainly focus on\n\ufb01ne-tuning performance, we sometimes use linear few-shot accuracies for fast on-the-\ufb02y evaluation\nwhere \ufb01ne-tuning would be too costly.\n4.2\nCOMPARISON TO STATE OF THE ART\nWe \ufb01rst compare our largest models \u2013 ViT-H/14 and ViT-L/16 \u2013 to state-of-the-art CNNs from\nthe literature. The \ufb01rst comparison point is Big Transfer (BiT) (Kolesnikov et al., 2020), which\nperforms supervised transfer learning with large ResNets. The second is Noisy Student (Xie et al.,\n2020), which is a large Ef\ufb01cientNet trained using semi-supervised learning on ImageNet and JFT-\n300M with the labels removed. Currently, Noisy Student is the state of the art on ImageNet and\nBiT-L on the other datasets reported here. All models were trained on TPUv3 hardware, and we\nreport the number of TPUv3-core-days taken to pre-train each of them, that is, the number of TPU\nv3 cores (2 per chip) used for training multiplied by the training time in days.\nTable 2 shows the results. The smaller ViT-L/16 model pre-trained on JFT-300M outperforms BiT-L\n(which is pre-trained on the same dataset) on all tasks, while requiring substantially less computa-\ntional resources to train. The larger model, ViT-H/14, further improves the performance, especially\non the more challenging datasets \u2013 ImageNet, CIFAR-100, and the VTAB suite. Interestingly, this\n5\n\nPublished as a conference paper at ICLR 2021\nOurs-JFT\nOurs-JFT\nOurs-I21k\nBiT-L\nNoisy Student\n(ViT-H/14)\n(ViT-L/16)\n(ViT-L/16)\n(ResNet152x4)\n(Ef\ufb01cientNet-L2)\nImageNet\n88.55 \u00b1 0.04\n87.76 \u00b1 0.03\n85.30 \u00b1 0.02\n87.54 \u00b1 0.02\n88.4/88.5\u2217\nImageNet ReaL\n90.72 \u00b1 0.05\n90.54 \u00b1 0.03\n88.62 \u00b1 0.05\n90.54\n90.55\nCIFAR-10\n99.50 \u00b1 0.06\n99.42 \u00b1 0.03\n99.15 \u00b1 0.03\n99.37 \u00b1 0.06\n\u2212\nCIFAR-100\n94.55 \u00b1 0.04\n93.90 \u00b1 0.05\n93.25 \u00b1 0.05\n93.51 \u00b1 0.08\n\u2212\nOxford-IIIT Pets\n97.56 \u00b1 0.03\n97.32 \u00b1 0.11\n94.67 \u00b1 0.15\n96.62 \u00b1 0.23\n\u2212\nOxford Flowers-102\n99.68 \u00b1 0.02\n99.74 \u00b1 0.00\n99.61 \u00b1 0.02\n99.63 \u00b1 0.03\n\u2212\nVTAB (19 tasks)\n77.63 \u00b1 0.23\n76.28 \u00b1 0.46\n72.72 \u00b1 0.21\n76.29 \u00b1 1.70\n\u2212\nTPUv3-core-days\n2.5k\n0.68k\n0.23k\n9.9k\n12.3k\nTable 2:\nComparison with state of the art on popular image classi\ufb01cation benchmarks. We re-\nport mean and standard deviation of the accuracies, averaged over three \ufb01ne-tuning runs. Vision\nTransformer models pre-trained on the JFT-300M dataset outperform ResNet-based baselines on all\ndatasets, while taking substantially less computational resources to pre-train. ViT pre-trained on the\nsmaller public ImageNet-21k dataset performs well too. \u2217Slightly improved 88.5% result reported\nin Touvron et al. (2020).\nVTAB (19 tasks)\n65\n70\n75\n80\nAccuracy [%]\nNatural (7 tasks)\n70\n80\n90\nSpecialized (4 tasks)\n80\n82\n85\n88\n90\nStructured (8 tasks)\n50\n60\n70\nViT-H/14\nBiT-L (R152x4)\nVIVI-Ex-100% (R50x3)\nS4L (R50x1)\nFigure 2: Breakdown of VTAB performance in Natural, Specialized, and Structured task groups.\nmodel still took substantially less compute to pre-train than prior state of the art. However, we note\nthat pre-training ef\ufb01ciency may be affected not only by the architecture choice, but also other pa-\nrameters, such as training schedule, optimizer, weight decay, etc. We provide a controlled study of\nperformance vs. compute for different architectures in Section 4.4. Finally, the ViT-L/16 model\npre-trained on the public ImageNet-21k dataset performs well on most datasets too, while taking\nfewer resources to pre-train: it could be trained using a standard cloud TPUv3 with 8 cores in ap-\nproximately 30 days.\nFigure 2 decomposes the VTAB tasks into their respective groups, and compares to previous SOTA\nmethods on this benchmark: BiT, VIVI \u2013 a ResNet co-trained on ImageNet and Youtube (Tschannen\net al., 2020), and S4L \u2013 supervised plus semi-supervised learning on ImageNet (Zhai et al., 2019a).\nViT-H/14 outperforms BiT-R152x4, and other methods, on the Natural and Structured tasks. On the\nSpecialized the performance of the top two models is similar.\n4.3\nPRE-TRAINING DATA REQUIREMENTS\nThe Vision Transformer performs well when pre-trained on a large JFT-300M dataset. With fewer\ninductive biases for vision than ResNets, how crucial is the dataset size? We perform two series of\nexperiments.\nFirst, we pre-train ViT models on datasets of increasing size: ImageNet, ImageNet-21k, and JFT-\n300M. To boost the performance on the smaller datasets, we optimize three basic regularization\nparameters \u2013 weight decay, dropout, and label smoothing. Figure 3 shows the results after \ufb01ne-\ntuning to ImageNet (results on other datasets are shown in Table 5)2. When pre-trained on the\nsmallest dataset, ImageNet, ViT-Large models underperform compared to ViT-Base models, despite\n(moderate) regularization. With ImageNet-21k pre-training, their performances are similar. Only\nwith JFT-300M, do we see the full bene\ufb01t of larger models. Figure 3 also shows the performance\n2Note that the ImageNet pre-trained models are also \ufb01ne-tuned, but again on ImageNet. This is because the\nresolution increase during \ufb01ne-tuning improves the performance.\n6\n\nPublished as a conference paper at ICLR 2021\nImageNet\nImageNet-21k\nJFT-300M\nPre-training dataset\n70\n75\n80\n85\n90\nImageNet Top1 Accuracy [%]\nBiT\nViT-B/32\nViT-B/16\nViT-L/32\nViT-L/16\nViT-H/14\nFigure 3:\nTransfer to ImageNet.\nWhile\nlarge ViT models perform worse than BiT\nResNets (shaded area) when pre-trained on\nsmall datasets, they shine when pre-trained on\nlarger datasets. Similarly, larger ViT variants\novertake smaller ones as the dataset grows.\n10 M\n30 M\n100 M\n300 M\nNumber of JFT pre-training samples\n30\n40\n50\n60\n70\nLinear 5-shot ImageNet Top1 [%]\nViT-L/16\nViT-L/32\nViT-B/32\nViT-b/32\nResNet50x1 (BiT)\nResNet152x2 (BiT)\nFigure 4: Linear few-shot evaluation on Ima-\ngeNet versus pre-training size. ResNets per-\nform better with smaller pre-training datasets\nbut plateau sooner than ViT, which performs\nbetter with larger pre-training. ViT-b is ViT-B\nwith all hidden dimensions halved.\n102\n103\n90\n95\nTransfer accuracy [%]\nAverage-5\nTransformer (ViT)\nResNet (BiT)\nHybrid\n102\n103\n75\n80\n85\n90\nImageNet\nTransformer (ViT)\nResNet (BiT)\nHybrid\nTotal pre-training compute [exaFLOPs]\nFigure 5: Performance versus pre-training compute for different architectures: Vision Transformers,\nResNets, and hybrids. Vision Transformers generally outperform ResNets with the same compu-\ntational budget. Hybrids improve upon pure Transformers for smaller model sizes, but the gap\nvanishes for larger models.\nregion spanned by BiT models of different sizes. The BiT CNNs outperform ViT on ImageNet, but\nwith the larger datasets, ViT overtakes.\nSecond, we train our models on random subsets of 9M, 30M, and 90M as well as the full JFT-\n300M dataset. We do not perform additional regularization on the smaller subsets and use the same\nhyper-parameters for all settings. This way, we assess the intrinsic model properties, and not the\neffect of regularization. We do, however, use early-stopping, and report the best validation accuracy\nachieved during training. To save compute, we report few-shot linear accuracy instead of full \ufb01ne-\ntuning accuracy. Figure 4 contains the results. Vision Transformers over\ufb01t more than ResNets with\ncomparable computational cost on smaller datasets. For example, ViT-B/32 is slightly faster than\nResNet50; it performs much worse on the 9M subset, but better on 90M+ subsets. The same is true\nfor ResNet152x2 and ViT-L/16. This result reinforces the intuition that the convolutional inductive\nbias is useful for smaller datasets, but for larger ones, learning the relevant patterns directly from\ndata is suf\ufb01cient, even bene\ufb01cial.\nOverall, the few-shot results on ImageNet (Figure 4), as well as the low-data results on VTAB\n(Table 2) seem promising for very low-data transfer. Further analysis of few-shot properties of ViT\nis an exciting direction of future work.\n7\n\nPublished as a conference paper at ICLR 2021\n4.4\nSCALING STUDY\nWe perform a controlled scaling study of different models by evaluating transfer performance from\nJFT-300M. In this setting data size does not bottleneck the models\u2019 performances, and we assess\nperformance versus pre-training cost of each model. The model set includes: 7 ResNets, R50x1,\nR50x2 R101x1, R152x1, R152x2, pre-trained for 7 epochs, plus R152x2 and R200x3 pre-trained\nfor 14 epochs; 6 Vision Transformers, ViT-B/32, B/16, L/32, L/16, pre-trained for 7 epochs, plus\nL/16 and H/14 pre-trained for 14 epochs; and 5 hybrids, R50+ViT-B/32, B/16, L/32, L/16 pre-\ntrained for 7 epochs, plus R50+ViT-L/16 pre-trained for 14 epochs (for hybrids, the number at the\nend of the model name stands not for the patch size, but for the total dowsampling ratio in the ResNet\nbackbone).\nFigure 5 contains the transfer performance versus total pre-training compute (see Appendix D.5\nfor details on computational costs). Detailed results per model are provided in Table 6 in the Ap-\npendix. A few patterns can be observed. First, Vision Transformers dominate ResNets on the\nperformance/compute trade-off. ViT uses approximately 2 \u22124\u00d7 less compute to attain the same\nperformance (average over 5 datasets). Second, hybrids slightly outperform ViT at small compu-\ntational budgets, but the difference vanishes for larger models. This result is somewhat surprising,\nsince one might expect convolutional local feature processing to assist ViT at any size. Third, Vision\nTransformers appear not to saturate within the range tried, motivating future scaling efforts.\n4.5\nINSPECTING VISION TRANSFORMER\nInput\nAttention\nFigure 6: Representative ex-\namples of attention from the\noutput token to the input\nspace. See Appendix D.7 for\ndetails.\nTo begin to understand how the Vision Transformer processes im-\nage data, we analyze its internal representations. The \ufb01rst layer of\nthe Vision Transformer linearly projects the \ufb02attened patches into a\nlower-dimensional space (Eq. 1). Figure 7 (left) shows the top prin-\ncipal components of the the learned embedding \ufb01lters. The com-\nponents resemble plausible basis functions for a low-dimensional\nrepresentation of the \ufb01ne structure within each patch.\nAfter the projection, a learned position embedding is added to the\npatch representations. Figure 7 (center) shows that the model learns\nto encode distance within the image in the similarity of position em-\nbeddings, i.e. closer patches tend to have more similar position em-\nbeddings. Further, the row-column structure appears; patches in the\nsame row/column have similar embeddings. Finally, a sinusoidal\nstructure is sometimes apparent for larger grids (Appendix D). That\nthe position embeddings learn to represent 2D image topology ex-\nplains why hand-crafted 2D-aware embedding variants do not yield\nimprovements (Appendix D.4).\nSelf-attention allows ViT to integrate information across the entire\nimage even in the lowest layers. We investigate to what degree\nthe network makes use of this capability. Speci\ufb01cally, we compute\nthe average distance in image space across which information is\nintegrated, based on the attention weights (Figure 7, right). This\n\u201cattention distance\u201d is analogous to receptive \ufb01eld size in CNNs.\nWe \ufb01nd that some heads attend to most of the image already in the lowest layers, showing that\nthe ability to integrate information globally is indeed used by the model. Other attention heads\nhave consistently small attention distances in the low layers. This highly localized attention is\nless pronounced in hybrid models that apply a ResNet before the Transformer (Figure 7, right),\nsuggesting that it may serve a similar function as early convolutional layers in CNNs. Further, the\nattention distance increases with network depth. Globally, we \ufb01nd that the model attends to image\nregions that are semantically relevant for classi\ufb01cation (Figure 6).\n4.6\nSELF-SUPERVISION\nTransformers show impressive performance on NLP tasks. However, much of their success stems\nnot only from their excellent scalability but also from large scale self-supervised pre-training (Devlin\n8\n\nPublished as a conference paper at ICLR 2021\nRGB embedding filters\n(first 28 principal components)\n1\n2\n3\n4\n5\n6\n7\nInput patch column\n1\n2\n3\n4\n5\n6\n7\nInput patch row\nPosition embedding similarity\n1\n1\nCosine similarity\n0\n5\n10\n15\n20\nNetwork depth (layer)\n0\n20\n40\n60\n80\n100\n120\nMean attention distance (pixels)\nViT-L/16\nHead 1\nHead 2\nHead 3\n...\nFigure 7: Left: Filters of the initial linear embedding of RGB values of ViT-L/32. Center: Sim-\nilarity of position embeddings of ViT-L/32. Tiles show the cosine similarity between the position\nembedding of the patch with the indicated row and column and the position embeddings of all other\npatches. Right: Size of attended area by head and network depth. Each dot shows the mean attention\ndistance across images for one of 16 heads at one layer. See Appendix D.7 for details.\net al., 2019; Radford et al., 2018). We also perform a preliminary exploration on masked patch\nprediction for self-supervision, mimicking the masked language modeling task used in BERT. With\nself-supervised pre-training, our smaller ViT-B/16 model achieves 79.9% accuracy on ImageNet, a\nsigni\ufb01cant improvement of 2% to training from scratch, but still 4% behind supervised pre-training.\nAppendix B.1.2 contains further details. We leave exploration of contrastive pre-training (Chen\net al., 2020b; He et al., 2020; Bachman et al., 2019; H\u00b4enaff et al., 2020) to future work.\n5\nCONCLUSION\nWe have explored the direct application of Transformers to image recognition. Unlike prior works\nusing self-attention in computer vision, we do not introduce image-speci\ufb01c inductive biases into\nthe architecture apart from the initial patch extraction step. Instead, we interpret an image as a\nsequence of patches and process it by a standard Transformer encoder as used in NLP. This simple,\nyet scalable, strategy works surprisingly well when coupled with pre-training on large datasets.\nThus, Vision Transformer matches or exceeds the state of the art on many image classi\ufb01cation\ndatasets, whilst being relatively cheap to pre-train.\nWhile these initial results are encouraging, many challenges remain. One is to apply ViT to other\ncomputer vision tasks, such as detection and segmentation. Our results, coupled with those in Carion\net al. (2020), indicate the promise of this approach. Another challenge is to continue exploring self-\nsupervised pre-training methods. Our initial experiments show improvement from self-supervised\npre-training, but there is still large gap between self-supervised and large-scale supervised pre-\ntraining. Finally, further scaling of ViT would likely lead to improved performance.\nACKNOWLEDGEMENTS\nThe work was performed in Berlin, Z\u00a8urich, and Amsterdam. We thank many colleagues at Google\nfor their help, in particular Andreas Steiner for crucial help with the infrastructure and the open-\nsource release of the code; Joan Puigcerver and Maxim Neumann for help with the large-scale\ntraining infrastructure; Dmitry Lepikhin, Aravindh Mahendran, Daniel Keysers, Mario Lu\u02c7ci\u00b4c, Noam\nShazeer, Ashish Vaswani, and Colin Raffel for useful discussions.\nREFERENCES\nSamira Abnar and Willem Zuidema. Quantifying attention \ufb02ow in transformers. In ACL, 2020.\nPhilip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing\nmutual information across views. In NeurIPS, 2019.\n9\n\nPublished as a conference paper at ICLR 2021\nAlexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. In\nICLR, 2019.\nI. Bello, B. Zoph, Q. Le, A. Vaswani, and J. Shlens. Attention augmented convolutional networks.\nIn ICCV, 2019.\nLucas Beyer, Olivier J. H\u00b4enaff, Alexander Kolesnikov, Xiaohua Zhai, and A\u00a8aron van den Oord. Are\nwe done with imagenet? arXiv, 2020.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. arXiv, 2020.\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\nSergey Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020.\nMark Chen, Alec Radford, Rewon Child, Jeff Wu, and Heewoo Jun. Generative pretraining from\npixels. In ICML, 2020a.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework\nfor contrastive learning of visual representations. In ICML, 2020b.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. UNITER: UNiversal Image-TExt Representation Learning. In ECCV, 2020c.\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\ntransformers. arXiv, 2019.\nJean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi. On the relationship between self-\nattention and convolutional layers. In ICLR, 2020.\nJ. Deng, W. Dong, R. Socher, L. Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\nimage database. In CVPR, 2009.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. In NAACL, 2019.\nJosip Djolonga, Jessica Yung, Michael Tschannen, Rob Romijnders, Lucas Beyer, Alexander\nKolesnikov, Joan Puigcerver, Matthias Minderer, Alexander D\u2019Amour, Dan Moldovan, Sylvan\nGelly, Neil Houlsby, Xiaohua Zhai, and Mario Lucic. On robustness and transferability of convo-\nlutional neural networks. arXiv, 2020.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In CVPR, 2016.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.\nMomentum contrast for\nunsupervised visual representation learning. In CVPR, 2020.\nJonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in multidi-\nmensional transformers. arXiv, 2019.\nHan Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen Wei. Relation networks for object\ndetection. In CVPR, 2018.\nHan Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local relation networks for image recognition.\nIn ICCV, 2019.\nZilong Huang, Xinggang Wang, Yunchao Wei, Lichao Huang, Humphrey Shi, Wenyu Liu, and\nThomas S. Huang. Ccnet: Criss-cross attention for semantic segmentation. In ICCV, 2020.\nOlivier J. H\u00b4enaff, Aravind Srinivas, Jeffrey De Fauw, Ali Razavi, Carl Doersch, S. M. Ali Eslami,\nand Aaron van den Oord. Data-ef\ufb01cient image recognition with contrastive predictive coding. In\nICML, 2020.\n10\n\nPublished as a conference paper at ICLR 2021\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. 2015.\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\nAlexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly,\nand Neil Houlsby. Big transfer (BiT): General visual representation learning. In ECCV, 2020.\nAlex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.\nAlex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classi\ufb01cation with deep convo-\nlutional neural networks. In NIPS, 2012.\nY. LeCun, B. Boser, J. Denker, D. Henderson, R. Howard, W. Hubbard, and L. Jackel. Backpropa-\ngation applied to handwritten zip code recognition. Neural Computation, 1:541\u2013551, 1989.\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,\nMaxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional\ncomputation and automatic sharding. arXiv, 2020.\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. VisualBERT: A\nSimple and Performant Baseline for Vision and Language. In Arxiv, 2019.\nFrancesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold,\nJakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot atten-\ntion. arXiv, 2020.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. ViLBERT: Pretraining Task-Agnostic Visi-\nolinguistic Representations for Vision-and-Language Tasks. In NeurIPS. 2019.\nDhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li,\nAshwin Bharambe, and Laurens van der Maaten.\nExploring the limits of weakly supervised\npretraining. In ECCV, 2018.\nM. Nilsback and A. Zisserman. Automated \ufb02ower classi\ufb01cation over a large number of classes. In\nICVGIP, 2008.\nOmkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In CVPR,\n2012.\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and\nDustin Tran. Image transformer. In ICML, 2018.\nB. T. Polyak and A. B. Juditsky. Acceleration of stochastic approximation by averaging. SIAM\nJournal on Control and Optimization, 30(4):838\u2013855, 1992.\ndoi: 10.1137/0330046.\nURL\nhttps://doi.org/10.1137/0330046.\nSiyuan Qiao, Huiyu Wang, Chenxi Liu, Wei Shen, and Alan Yuille. Weight standardization. arXiv\npreprint arXiv:1903.10520, 2019.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-\nstanding with unsupervised learning. Technical Report, 2018.\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. Technical Report, 2019.\nPrajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jon Shlens.\nStand-alone self-attention in vision models. In NeurIPS, 2019.\nChen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable ef-\nfectiveness of data in deep learning era. In ICCV, 2017.\nChen Sun, Austin Myers, Carl Vondrick, Kevin Murphy, and Cordelia Schmid. Videobert: A joint\nmodel for video and language representation learning. In ICCV, 2019.\n11\n\nPublished as a conference paper at ICLR 2021\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve Jegou. Fixing the train-test resolution\ndiscrepancy. In NeurIPS. 2019.\nHugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve Jegou. Fixing the train-test resolution\ndiscrepancy: Fixef\ufb01cientnet. arXiv preprint arXiv:2003.08237, 2020.\nMichael Tschannen, Josip Djolonga, Marvin Ritter, Aravindh Mahendran, Neil Houlsby, Sylvain\nGelly, and Mario Lucic. Self-supervised learning of video-induced visual invariances. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June\n2020.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017.\nHuiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen.\nAxial-deeplab: Stand-alone axial-attention for panoptic segmentation. In ECCV, 2020a.\nHuiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh\nChen.\nAxial-deeplab: Stand-alone axial-attention for panoptic segmentation.\narXiv preprint\narXiv:2003.07853, 2020b.\nQiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, and Lidia S. Chao.\nLearning deep transformer models for machine translation. In ACL, 2019.\nXiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In\nCVPR, 2018.\nDirk Weissenborn, Oscar T\u00a8ackstr\u00a8om, and Jakob Uszkoreit. Scaling autoregressive video models. In\nICLR, 2019.\nBichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Masayoshi Tomizuka, Kurt\nKeutzer, and Peter Vajda. Visual transformers: Token-based image representation and processing\nfor computer vision. arxiv, 2020.\nYuxin Wu and Kaiming He. Group normalization. In ECCV, 2018.\nQizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V. Le. Self-training with noisy student\nimproves imagenet classi\ufb01cation. In CVPR, 2020.\nXiaohua Zhai, Avital Oliver, Alexander Kolesnikov, and Lucas Beyer. S4L: Self-Supervised Semi-\nSupervised Learning. In ICCV, 2019a.\nXiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario\nLucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. A\nlarge-scale study of representation learning with the visual task adaptation benchmark. arXiv\npreprint arXiv:1910.04867, 2019b.\nHengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring self-attention for image recognition. In\nCVPR, 2020.\n12\n\nPublished as a conference paper at ICLR 2021\nModels\nDataset\nEpochs\nBase LR\nLR decay\nWeight decay\nDropout\nViT-B/{16,32}\nJFT-300M\n7\n8 \u00b7 10\u22124\nlinear\n0.1\n0.0\nViT-L/32\nJFT-300M\n7\n6 \u00b7 10\u22124\nlinear\n0.1\n0.0\nViT-L/16\nJFT-300M\n7/14\n4 \u00b7 10\u22124\nlinear\n0.1\n0.0\nViT-H/14\nJFT-300M\n14\n3 \u00b7 10\u22124\nlinear\n0.1\n0.0\nR50x{1,2}\nJFT-300M\n7\n10\u22123\nlinear\n0.1\n0.0\nR101x1\nJFT-300M\n7\n8 \u00b7 10\u22124\nlinear\n0.1\n0.0\nR152x{1,2}\nJFT-300M\n7\n6 \u00b7 10\u22124\nlinear\n0.1\n0.0\nR50+ViT-B/{16,32}\nJFT-300M\n7\n8 \u00b7 10\u22124\nlinear\n0.1\n0.0\nR50+ViT-L/32\nJFT-300M\n7\n2 \u00b7 10\u22124\nlinear\n0.1\n0.0\nR50+ViT-L/16\nJFT-300M\n7/14\n4 \u00b7 10\u22124\nlinear\n0.1\n0.0\nViT-B/{16,32}\nImageNet-21k\n90\n10\u22123\nlinear\n0.03\n0.1\nViT-L/{16,32}\nImageNet-21k\n30/90\n10\u22123\nlinear\n0.03\n0.1\nViT-\u2217\nImageNet\n300\n3 \u00b7 10\u22123\ncosine\n0.3\n0.1\nTable 3: Hyperparameters for training. All models are trained with a batch size of 4096 and learn-\ning rate warmup of 10k steps. For ImageNet we found it bene\ufb01cial to additionally apply gradient\nclipping at global norm 1. Training resolution is 224.\nAPPENDIX\nA\nMULTIHEAD SELF-ATTENTION\nStandard qkv self-attention (SA, Vaswani et al. (2017)) is a popular building block for neural archi-\ntectures. For each element in an input sequence z \u2208RN\u00d7D, we compute a weighted sum over all\nvalues v in the sequence. The attention weights Aij are based on the pairwise similarity between\ntwo elements of the sequence and their respective query qi and key kj representations.\n[q, k, v] = zUqkv\nUqkv \u2208RD\u00d73Dh,\n(5)\nA = softmax\n\u0010\nqk\u22a4/\np\nDh\n\u0011\nA \u2208RN\u00d7N,\n(6)\nSA(z) = Av .\n(7)\nMultihead self-attention (MSA) is an extension of SA in which we run k self-attention operations,\ncalled \u201cheads\u201d, in parallel, and project their concatenated outputs. To keep compute and number of\nparameters constant when changing k, Dh (Eq. 5) is typically set to D/k.\nMSA(z) = [SA1(z); SA2(z); \u00b7 \u00b7 \u00b7 ; SAk(z)] Umsa\nUmsa \u2208Rk\u00b7Dh\u00d7D\n(8)\nB\nEXPERIMENT DETAILS\nB.1\nTRAINING\nTable 3 summarizes our training setups for our different models. We found strong regularization\nto be key when training models from scratch on ImageNet. Dropout, when used, is applied after\nevery dense layer except for the the qkv-projections and directly after adding positional- to patch\nembeddings. Hybrid models are trained with the exact setup as their ViT counterparts. Finally, all\ntraining is done on resolution 224.\nB.1.1\nFINE-TUNING\nWe \ufb01ne-tune all ViT models using SGD with a momentum of 0.9. We run a small grid search over\nlearning rates, see learning rate ranges in Table 4. To do so, we use small sub-splits from the training\nset (10% for Pets and Flowers, 2% for CIFAR, 1% ImageNet) as development set and train on the\nremaining data. For \ufb01nal results we train on the entire training set and evaluate on the respective\ntest data. For \ufb01ne-tuning ResNets and hybrid models we use the exact same setup, with the only\nexception of ImageNet where we add another value 0.06 to the learning rate sweep. Additionally,\n13\n\nPublished as a conference paper at ICLR 2021\nDataset\nSteps\nBase LR\nImageNet\n20 000\n{0.003, 0.01, 0.03, 0.06}\nCIFAR100\n10 000\n{0.001, 0.003, 0.01, 0.03}\nCIFAR10\n10 000\n{0.001, 0.003, 0.01, 0.03}\nOxford-IIIT Pets\n500\n{0.001, 0.003, 0.01, 0.03}\nOxford Flowers-102\n500\n{0.001, 0.003, 0.01, 0.03}\nVTAB (19 tasks)\n2 500\n0.01\nTable 4: Hyperparameters for \ufb01ne-tuning. All models are \ufb01ne-tuned with cosine learning rate decay,\na batch size of 512, no weight decay, and grad clipping at global norm 1. If not mentioned otherwise,\n\ufb01ne-tuning resolution is 384.\nfor ResNets we also run the setup of Kolesnikov et al. (2020) and select the best results across\nthis run and our sweep. Finally, if not mentioned otherwise, all \ufb01ne-tuning experiments run at 384\nresolution (running \ufb01ne-tuning at different resolution than training is common practice (Kolesnikov\net al., 2020)).\nWhen transferring ViT models to another dataset, we remove the whole head (two linear layers) and\nreplace it by a single, zero-initialized linear layer outputting the number of classes required by the\ntarget dataset. We found this to be a little more robust than simply re-initializing the very last layer.\nFor VTAB we follow the protocol in Kolesnikov et al. (2020), and use the same hyperparameter\nsetting for all tasks. We use a learning rate of 0.01 and train for 2500 steps (Tab. 4). We chose this\nsetting by running a small sweep over two learning rates and two schedules, and selecting the setting\nwith the highest VTAB score on the 200-example validation sets. We follow the pre-processing used\nin Kolesnikov et al. (2020), except that we do not use task-speci\ufb01c input resolutions. Instead we \ufb01nd\nthat Vision Transformer bene\ufb01ts most from a high resolution (384 \u00d7 384) for all tasks.\nB.1.2\nSELF-SUPERVISION\nWe employ the masked patch prediction objective for preliminary self-supervision experiments. To\ndo so we corrupt 50% of patch embeddings by either replacing their embeddings with a learnable\n[mask] embedding (80%), a random other patch embedding (10%) or just keeping them as is\n(10%). This setup is very similar to the one used for language by Devlin et al. (2019). Finally, we\npredict the 3-bit, mean color (i.e., 512 colors in total) of every corrupted patch using their respective\npatch representations.\nWe trained our self-supervised model for 1M steps (ca. 14 epochs) with batch size 4096 on JFT. We\nuse Adam, with a base learning rate of 2\u00b710\u22124, warmup of 10k steps and cosine learning rate decay.\nAs prediction targets for pretraining we tried the following settings: 1) predicting only the mean,\n3bit color (i.e., 1 prediction of 512 colors), 2) predicting a 4 \u00d7 4 downsized version of the 16 \u00d7 16\npatch with 3bit colors in parallel (i.e., 16 predictions of 512 colors), 3) regression on the full patch\nusing L2 (i.e., 256 regressions on the 3 RGB channels). Surprisingly, we found that all worked quite\nwell, though L2 was slightly worse. We report \ufb01nal results only for option 1) because it has shown\nbest few-shot performance. We also experimented with 15% corruption rate as used by Devlin et al.\n(2019) but results were also slightly worse on our few-shot metrics.\nLastly, we would like to remark that our instantiation of masked patch prediction doesn\u2019t require\nsuch an enormous amount of pretraining nor a large dataset such as JFT in order to lead to sim-\nilar performance gains on ImageNet classi\ufb01cation. That is, we observed diminishing returns on\ndownstream performance after 100k pretraining steps, and see similar gains when pretraining on\nImageNet.\nC\nADDITIONAL RESULTS\nWe report detailed results corresponding to the \ufb01gures presented in the paper. Table 5 corresponds\nto Figure 3 from the paper and shows transfer performance of different ViT models pre-trained\non datasets of increasing size: ImageNet, ImageNet-21k, and JFT-300M. Table 6 corresponds to\n14\n\nPublished as a conference paper at ICLR 2021\nViT-B/16\nViT-B/32\nViT-L/16\nViT-L/32\nViT-H/14\nImageNet\nCIFAR-10\n98.13\n97.77\n97.86\n97.94\n-\nCIFAR-100\n87.13\n86.31\n86.35\n87.07\n-\nImageNet\n77.91\n73.38\n76.53\n71.16\n-\nImageNet ReaL\n83.57\n79.56\n82.19\n77.83\n-\nOxford Flowers-102\n89.49\n85.43\n89.66\n86.36\n-\nOxford-IIIT-Pets\n93.81\n92.04\n93.64\n91.35\n-\nImageNet-21k\nCIFAR-10\n98.95\n98.79\n99.16\n99.13\n99.27\nCIFAR-100\n91.67\n91.97\n93.44\n93.04\n93.82\nImageNet\n83.97\n81.28\n85.15\n80.99\n85.13\nImageNet ReaL\n88.35\n86.63\n88.40\n85.65\n88.70\nOxford Flowers-102\n99.38\n99.11\n99.61\n99.19\n99.51\nOxford-IIIT-Pets\n94.43\n93.02\n94.73\n93.09\n94.82\nJFT-300M\nCIFAR-10\n99.00\n98.61\n99.38\n99.19\n99.50\nCIFAR-100\n91.87\n90.49\n94.04\n92.52\n94.55\nImageNet\n84.15\n80.73\n87.12\n84.37\n88.04\nImageNet ReaL\n88.85\n86.27\n89.99\n88.28\n90.33\nOxford Flowers-102\n99.56\n99.27\n99.56\n99.45\n99.68\nOxford-IIIT-Pets\n95.80\n93.40\n97.11\n95.83\n97.56\nTable 5: Top1 accuracy (in %) of Vision Transformer on various datasets when pre-trained on Im-\nageNet, ImageNet-21k or JFT300M. These values correspond to Figure 3 in the main text. Models\nare \ufb01ne-tuned at 384 resolution. Note that the ImageNet results are computed without additional\ntechniques (Polyak averaging and 512 resolution images) used to achieve results in Table 2.\nEpochs\nImageNet\nImageNet ReaL\nCIFAR-10\nCIFAR-100\nPets\nFlowers\nexaFLOPs\nname\nViT-B/32\n7\n80.73\n86.27\n98.61\n90.49\n93.40\n99.27\n55\nViT-B/16\n7\n84.15\n88.85\n99.00\n91.87\n95.80\n99.56\n224\nViT-L/32\n7\n84.37\n88.28\n99.19\n92.52\n95.83\n99.45\n196\nViT-L/16\n7\n86.30\n89.43\n99.38\n93.46\n96.81\n99.66\n783\nViT-L/16\n14\n87.12\n89.99\n99.38\n94.04\n97.11\n99.56\n1567\nViT-H/14\n14\n88.08\n90.36\n99.50\n94.71\n97.11\n99.71\n4262\nResNet50x1\n7\n77.54\n84.56\n97.67\n86.07\n91.11\n94.26\n50\nResNet50x2\n7\n82.12\n87.94\n98.29\n89.20\n93.43\n97.02\n199\nResNet101x1\n7\n80.67\n87.07\n98.48\n89.17\n94.08\n95.95\n96\nResNet152x1\n7\n81.88\n87.96\n98.82\n90.22\n94.17\n96.94\n141\nResNet152x2\n7\n84.97\n89.69\n99.06\n92.05\n95.37\n98.62\n563\nResNet152x2\n14\n85.56\n89.89\n99.24\n91.92\n95.75\n98.75\n1126\nResNet200x3\n14\n87.22\n90.15\n99.34\n93.53\n96.32\n99.04\n3306\nR50x1+ViT-B/32\n7\n84.90\n89.15\n99.01\n92.24\n95.75\n99.46\n106\nR50x1+ViT-B/16\n7\n85.58\n89.65\n99.14\n92.63\n96.65\n99.40\n274\nR50x1+ViT-L/32\n7\n85.68\n89.04\n99.24\n92.93\n96.97\n99.43\n246\nR50x1+ViT-L/16\n7\n86.60\n89.72\n99.18\n93.64\n97.03\n99.40\n859\nR50x1+ViT-L/16\n14\n87.12\n89.76\n99.31\n93.89\n97.36\n99.11\n1668\nTable 6: Detailed results of model scaling experiments. These correspond to Figure 5 in the main\npaper. We show transfer accuracy on several datasets, as well as the pre-training compute (in ex-\naFLOPs).\nFigure 5 from the paper and shows the transfer performance of ViT, ResNet, and hybrid models of\nvarying size, as well as the estimated computational cost of their pre-training.\nD\nADDITIONAL ANALYSES\nD.1\nSGD VS. ADAM FOR RESNETS\nResNets are typically trained with SGD and our use of Adam as optimizer is quite unconventional.\nHere we show the experiments that motivated this choice. Namely, we compare the \ufb01ne-tuning\n15\n\nPublished as a conference paper at ICLR 2021\nResNet50\nResNet152x2\nDataset\nAdam\nSGD\nAdam\nSGD\nImageNet\n77.54\n78.24\n84.97\n84.37\nCIFAR10\n97.67\n97.46\n99.06\n99.07\nCIFAR100\n86.07\n85.17\n92.05\n91.06\nOxford-IIIT Pets\n91.11\n91.00\n95.37\n94.79\nOxford Flowers-102\n94.26\n92.06\n98.62\n99.32\nAverage\n89.33\n88.79\n94.01\n93.72\nTable 7: Fine-tuning ResNet models pre-trained with Adam and SGD.\n100\n101\nRelative Compute\n0.2\n0.3\n0.4\n0.5\n0.6\nImageNet 5shot\nModels\nAll\nDepth\nPatch size\nWidth MLP\nWidth\n100\n101\nRelative Compute\n0.4\n0.5\n0.6\n0.7\n0.8\nAverage 5shot\nModels\nAll\nDepth\nPatch size\nWidth MLP\nWidth\nFigure 8: Scaling different model dimensions of the Vision Transformer.\nperformance of two ResNets \u2013 50x1 and 152x2 \u2013 pre-trained on JFT with SGD and Adam. For\nSGD, we use the hyperparameters recommended by Kolesnikov et al. (2020). Results are presented\nin Table 7. Adam pre-training outperforms SGD pre-training on most datasets and on average.\nThis justi\ufb01es the choice of Adam as the optimizer used to pre-train ResNets on JFT. Note that the\nabsolute numbers are lower than those reported by Kolesnikov et al. (2020), since we pre-train only\nfor 7 epochs, not 30.\nD.2\nTRANSFORMER SHAPE\nWe ran ablations on scaling different dimensions of the Transformer architecture to \ufb01nd out which\nare best suited for scaling to very large models. Figure 8 shows 5-shot performance on ImageNet\nfor different con\ufb01gurations. All con\ufb01gurations are based on a ViT model with 8 layers, D = 1024,\nDMLP = 2048 and a patch size of 32, the intersection of all lines. We can see that scaling the\ndepth results in the biggest improvements which are clearly visible up until 64 layers. However,\ndiminishing returns are already visible after 16 layers. Interestingly, scaling the width of the net-\nwork seems to result in the smallest changes. Decreasing the patch size and thus increasing the\neffective sequence length shows surprisingly robust improvements without introducing parameters.\nThese \ufb01ndings suggest that compute might be a better predictor of performance than the number of\nparameters, and that scaling should emphasize depth over width if any. Overall, we \ufb01nd that scaling\nall dimensions proportionally results in robust improvements.\nD.3\nHEAD TYPE AND CLASS TOKEN\nIn order to stay as close as possible to the original Transformer model, we made use of an additional\n[class] token, which is taken as image representation. The output of this token is then trans-\nformed into a class prediction via a small multi-layer perceptron (MLP) with tanh as non-linearity\nin the single hidden layer.\nThis design is inherited from the Transformer model for text, and we use it throughout the main\npaper. An initial attempt at using only image-patch embeddings, globally average-pooling (GAP)\nthem, followed by a linear classi\ufb01er\u2014just like ResNet\u2019s \ufb01nal feature map\u2014performed very poorly.\nHowever, we found that this is neither due to the extra token, nor to the GAP operation. Instead,\n16\n\nPublished as a conference paper at ICLR 2021\n0\n1\n2\n3\n4\n5\n6\n7\nEpochs of training\n25\n30\n35\n40\n45\n50\n55\n60\nImageNet linear 5-shot accuracy [%]\nCLS-Token, lr=8e-4\nGAP, lr=8e-4\nGAP, lr=3e-4\nFigure 9: Comparison of class-token and global average pooling classi\ufb01ers. Both work similarly\nwell, but require different learning-rates.\nPos. Emb.\nDefault/Stem\nEvery Layer\nEvery Layer-Shared\nNo Pos. Emb.\n0.61382\nN/A\nN/A\n1-D Pos. Emb.\n0.64206\n0.63964\n0.64292\n2-D Pos. Emb.\n0.64001\n0.64046\n0.64022\nRel. Pos. Emb.\n0.64032\nN/A\nN/A\nTable 8: Results of the ablation study on positional embeddings with ViT-B/16 model evaluated on\nImageNet 5-shot linear.\nthe difference in performance is fully explained by the requirement for a different learning-rate, see\nFigure 9.\nD.4\nPOSITIONAL EMBEDDING\nWe ran ablations on different ways of encoding spatial information using positional embedding. We\ntried the following cases:\n\u2022 Providing no positional information: Considering the inputs as a bag of patches.\n\u2022 1-dimensional positional embedding: Considering the inputs as a sequence of patches in\nthe raster order (default across all other experiments in this paper).\n\u2022 2-dimensional positional embedding: Considering the inputs as a grid of patches in two\ndimensions. In this case, two sets of embeddings are learned, each for one of the axes,\nX-embedding, and Y -embedding, each with size D/2. Then, based on the coordinate on\nthe path in the input, we concatenate the X and Y embedding to get the \ufb01nal positional\nembedding for that patch.\n\u2022 Relative positional embeddings: Considering the relative distance between patches to en-\ncode the spatial information as instead of their absolute position. To do so, we use 1-\ndimensional Relative Attention, in which we de\ufb01ne the relative distance all possible pairs\nof patches. Thus, for every given pair (one as query, and the other as key/value in the at-\ntention mechanism), we have an offset pq \u2212pk, where each offset is associated with an\nembedding. Then, we simply run extra attention, where we use the original query (the\ncontent of query), but use relative positional embeddings as keys. We then use the log-\nits from the relative attention as a bias term and add it to the logits of the main attention\n(content-based attention) before applying the softmax.\nIn addition to different ways of encoding spatial information, we also tried different ways of in-\ncorporating this information in our model. For the 1-dimensional and 2-dimensional positional\nembeddings, we tried three different cases: (1) add positional embeddings to the inputs right after\n17\n\nPublished as a conference paper at ICLR 2021\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10 11 12 13 14\nInput patch column\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\nInput patch row\nViT-L16\n7 epochs, LR=0.0002, WD=0.01\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10 11 12 13 14\nInput patch column\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\nInput patch row\nViT-L16\n7 epochs, LR=0.0004, WD=0.1\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10 11 12 13 14\nInput patch column\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\nInput patch row\nViT-L16\n14 epochs, LR=0.0004, WD=0.1\n1\n1\nCosine similarity\nFigure 10: Position embeddings of models trained with different hyperparameters.\nthe stem of them model and before feeding the inputs to the Transformer encoder (default across\nall other experiments in this paper); (2) learn and add positional embeddings to the inputs at the\nbeginning of each layer; (3) add a learned positional embeddings to the inputs at the beginning of\neach layer (shared between layers).\nTable 8 summarizes the results from this ablation study on a ViT-B/16 model. As we can see, while\nthere is a large gap between the performances of the model with no positional embedding and mod-\nels with positional embedding, there is little to no difference between different ways of encoding\npositional information. We speculate that since our Transformer encoder operates on patch-level\ninputs, as opposed to pixel-level, the differences in how to encode spatial information is less impor-\ntant. More precisely, in patch-level inputs, the spatial dimensions are much smaller than the original\npixel-level inputs, e.g., 14 \u00d7 14 as opposed to 224 \u00d7 224, and learning to represent the spatial re-\nlations in this resolution is equally easy for these different positional encoding strategies. Even so,\nthe speci\ufb01c pattern of position embedding similarity learned by the network depends on the training\nhyperparameters (Figure 10).\n0\n5\n10\n15\n20\nNetwork depth (layer)\n0\n20\n40\n60\n80\n100\n120\nMean attention distance (pixels)\nViT-L/16\nHead 1\nHead 2\nHead 3\n...\n0\n5\n10\n15\n20\nNetwork depth (layer)\n0\n20\n40\n60\n80\n100\n120\nR50x1 + ViT-L/16\nHead 1\nHead 2\nHead 3\n...\nFigure 11: Size of attended area by head and network depth. Attention distance was computed for\n128 example images by averaging the distance between the query pixel and all other pixels, weighted\nby the attention weight. Each dot shows the mean attention distance across images for one of 16\nheads at one layer. Image width is 224 pixels.\nD.5\nEMPIRICAL COMPUTATIONAL COSTS\nWe are also interested in real-world speed of the architectures on our hardware, which is not always\nwell predicted by theoretical FLOPs due to details like lane widths and cache sizes. For this purpose,\n18\n\nPublished as a conference paper at ICLR 2021\nwe perform timing of inference speed for the main models of interest, on a TPUv3 accelerator; the\ndifference between inference and backprop speed is a constant model-independent factor.\nFigure 12 (left) shows how many images one core can handle per second, across various input sizes.\nEvery single point refers to the peak performance measured across a wide range of batch-sizes. As\ncan be seen, the theoretical bi-quadratic scaling of ViT with image size only barely starts happening\nfor the largest models at the largest resolutions.\nAnother quantity of interest is the largest batch-size each model can \ufb01t onto a core, larger being\nbetter for scaling to large datasets. Figure 12 (right) shows this quantity for the same set of models.\nThis shows that large ViT models have a clear advantage in terms of memory-ef\ufb01ciency over ResNet\nmodels.\n64\n128\n224\n384\n512\nInput size [px]\n102\n103\n104\nPeak inference speed [img/sec/core]\n64\n128\n224\n384\n512\nInput size [px]\n102\n103\nLargest per-core batch-size\nR50x1\nR50x2\nViT-B/32\nViT-L/32\nViT-B/16\nViT-L/16\nViT-H/14\nR152x4\nFigure 12: Left: Real wall-clock timings of various architectures across input sizes. ViT models\nhave speed comparable to similar ResNets. Right: Largest per-core batch-size \ufb01tting on device with\nvarious architectures across input sizes. ViT models are clearly more memory-ef\ufb01cient.\nD.6\nAXIAL ATTENTION\nAxial Attention (Huang et al., 2020; Ho et al., 2019) is a simple, yet effective technique to run self-\nattention on large inputs that are organized as multidimensional tensors. The general idea of axial\nattention is to perform multiple attention operations, each along a single axis of the input tensor,\ninstead of applying 1-dimensional attention to the \ufb02attened version of the input. In axial attention,\neach attention mixes information along a particular axis, while keeping information along the other\naxes independent. Along this line, Wang et al. (2020b) proposed the AxialResNet model in which\nall the convolutions with kernel size 3 \u00d7 3 in a ResNet50 are replaced by axial self-attention, i.e.\na row and column attention, augmented by relative positional encoding. We have implemented\nAxialResNet as a baseline model.3.\nMoreover, we have modi\ufb01ed ViT to process inputs in the 2-dimensional shape, instead of a 1-\ndimensional sequence of patches, and incorporate Axial Transformer blocks, in which instead of\na self-attention followed by an MLP, we have a a row-self-attention plus an MLP followed by a\ncolumn-self-attention plus an MLP.\nFigure 13, present the performance of Axial ResNet, Axial-ViT-B/32 and Axial-ViT-B/16 on Ima-\ngeNet 5shot linear, when pretrained on JFT dataset, verses the pretraining compute, both in terms of\nnumber of FLOPs and inference time (example per seconds). As we can see, both Axial-ViT-B/32\nand Axial-ViT-B/16 do better than their ViT-B counterpart in terms of performance, but it comes at\n3Our implementation is based on the open-sourced PyTorch implementation in https://github.com/\ncsrhddlam/axial-deeplab. In our experiments, we reproduced the scores reported in (Wang et al.,\n2020b) in terms of accuracy, however, our implementation, similar to the open-source implementation, is very\nslow on TPUs. Therefore, we were not able to use it for extensive large-scale experiments. These may be\nunlocked by a carefully optimized implementation.\n19\n\nPublished as a conference paper at ICLR 2021\n102\nTotal compute [exaFLOPs]\n0.500\n0.525\n0.550\n0.575\n0.600\n0.625\n0.650\nImageNet 5-shot linear top-1 accuracy\nAxialViT-B/16\nAxialViT-B/32\nViT-B/16\nViT-B/32\nResNet50\nAxialResNet50\n102\n103\nPeak inference speed [img/sec/core]\n0.500\n0.525\n0.550\n0.575\n0.600\n0.625\n0.650\nImageNet 5-shot linear top-1 accuracy\nAxialViT-B/16\nAxialViT-B/32\nViT-B/16\nViT-B/32\nResNet50\nAxialResNet50\nFigure 13: Performance of Axial-Attention based models, in terms of top-1 accuracy on ImageNet\n5-shot linear, versus their speed in terms of number of FLOPs (left) and inference time (left).\nthe cost of more compute. This is because in Axial-ViT models, each Transformer block with global\nself-attention is replaced by two Axial Transformer blocks, one with row and one with column self-\nattention and although the sequence length that self-attention operates on is smaller in axial case,\nthere is a extra MLP per Axial-ViT block. For the AxialResNet, although it looks reasonable in\nterms of accuracy/compute trade-off (Figure 13, left), the naive implementation is extremely slow\non TPUs (Figure 13, right).\nD.7\nATTENTION DISTANCE\nTo understand how ViT uses self-attention to integrate information across the image, we analyzed\nthe average distance spanned by attention weights at different layers (Figure 11). This \u201cattention\ndistance\u201d is analogous to receptive \ufb01eld size in CNNs. Average attention distance is highly variable\nacross heads in lower layers, with some heads attending to much of the image, while others attend\nto small regions at or near the query location. As depth increases, attention distance increases for all\nheads. In the second half of the network, most heads attend widely across tokens.\nD.8\nATTENTION MAPS\nTo compute maps of the attention from the output token to the input space (Figures 6 and 14), we\nused Attention Rollout (Abnar & Zuidema, 2020). Brie\ufb02y, we averaged attention weights of ViT-\nL/16 across all heads and then recursively multiplied the weight matrices of all layers. This accounts\nfor the mixing of attention across tokens through all layers.\nD.9\nOBJECTNET RESULTS\nWe also evaluate our \ufb02agship ViT-H/14 model on the ObjectNet benchmark following the evaluation\nsetup in Kolesnikov et al. (2020), resulting in 82.1% top-5 accuracy and 61.7% top-1 accuracy.\nD.10\nVTAB BREAKDOWN\nTable 9 shows the scores attained on each of the VTAB-1k tasks.\n20\n\nPublished as a conference paper at ICLR 2021\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\n51\n52\n53\n54\n55\n56\n57\n58\n59\n60\n61\n62\n63\n64\n65\n66\n67\n68\n69\n70\n71\n72\n73\n74\n75\n76\n77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\nFigure 14: Further example attention maps as in Figure 6 (random selection).\n21\n\nPublished as a conference paper at ICLR 2021\nTable 9: Breakdown of VTAB-1k performance across tasks.\nCaltech101\nCIFAR-100\nDTD\nFlowers102\nPets\nSun397\nSVHN\nCamelyon\nEuroSAT\nResisc45\nRetinopathy\nClevr-Count\nClevr-Dist\nDMLab\ndSpr-Loc\ndSpr-Ori\nKITTI-Dist\nsNORB-Azim\nsNORB-Elev\nMean\nViT-H/14 (JFT) 95.3 85.5 75.2 99.7 97.2 65.0 88.9 83.3 96.7 91.4 76.6 91.7 63.8 53.1 79.4 63.3 84.5 33.2 51.2 77.6\nViT-L/16 (JFT) 95.4 81.9 74.3 99.7 96.7 63.5 87.4 83.6 96.5 89.7 77.1 86.4 63.1 49.7 74.5 60.5 82.2 36.2 51.1 76.3\nViT-L/16 (I21k) 90.8 84.1 74.1 99.3 92.7 61.0 80.9 82.5 95.6 85.2 75.3 70.3 56.1 41.9 74.7 64.9 79.9 30.5 41.7 72.7\n22\n",
    "Provided proper attribution is provided, Google hereby grants permission to\nreproduce the tables and figures in this paper solely for use in journalistic or\nscholarly works.\nAttention Is All You Need\nAshish Vaswani\u2217\nGoogle Brain\navaswani@google.com\nNoam Shazeer\u2217\nGoogle Brain\nnoam@google.com\nNiki Parmar\u2217\nGoogle Research\nnikip@google.com\nJakob Uszkoreit\u2217\nGoogle Research\nusz@google.com\nLlion Jones\u2217\nGoogle Research\nllion@google.com\nAidan N. Gomez\u2217\u2020\nUniversity of Toronto\naidan@cs.toronto.edu\n\u0141ukasz Kaiser\u2217\nGoogle Brain\nlukaszkaiser@google.com\nIllia Polosukhin\u2217\u2021\nillia.polosukhin@gmail.com\nAbstract\nThe dominant sequence transduction models are based on complex recurrent or\nconvolutional neural networks that include an encoder and a decoder. The best\nperforming models also connect the encoder and decoder through an attention\nmechanism. We propose a new simple network architecture, the Transformer,\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\nentirely. Experiments on two machine translation tasks show these models to\nbe superior in quality while being more parallelizable and requiring significantly\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\nto-German translation task, improving over the existing best results, including\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\nbest models from the literature. We show that the Transformer generalizes well to\nother tasks by applying it successfully to English constituency parsing both with\nlarge and limited training data.\n\u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\nattention and the parameter-free position representation and became the other person involved in nearly every\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\nour research.\n\u2020Work performed while at Google Brain.\n\u2021Work performed while at Google Research.\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023\n\n1\nIntroduction\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\nin particular, have been firmly established as state of the art approaches in sequence modeling and\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\narchitectures [38, 24, 15].\nRecurrent models typically factor computation along the symbol positions of the input and output\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\nstates ht, as a function of the previous hidden state ht\u22121 and the input for position t. This inherently\nsequential nature precludes parallelization within training examples, which becomes critical at longer\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\ncomputation [32], while also improving model performance in case of the latter. The fundamental\nconstraint of sequential computation, however, remains.\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\nare used in conjunction with a recurrent network.\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\n2\nBackground\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\nblock, computing hidden representations in parallel for all input and output positions. In these models,\nthe number of operations required to relate signals from two arbitrary input or output positions grows\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\ndescribed in section 3.2.\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\naligned recurrence and have been shown to perform well on simple-language question answering and\nlanguage modeling tasks [34].\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\nentirely on self-attention to compute representations of its input and output without using sequence-\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\nself-attention and discuss its advantages over models such as [17, 18] and [9].\n3\nModel Architecture\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\n[10], consuming the previously generated symbols as additional input when generating the next.\n2\n\nFigure 1: The Transformer - model architecture.\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\nrespectively.\n3.1\nEncoder and Decoder Stacks\nEncoder:\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\nlayers, produce outputs of dimension dmodel = 512.\nDecoder:\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\npredictions for position i can depend only on the known outputs at positions less than i.\n3.2\nAttention\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\n3\n\nScaled Dot-Product Attention\nMulti-Head Attention\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\nattention layers running in parallel.\nof the values, where the weight assigned to each value is computed by a compatibility function of the\nquery with the corresponding key.\n3.2.1\nScaled Dot-Product Attention\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\nquery with all keys, divide each by \u221adk, and apply a softmax function to obtain the weights on the\nvalues.\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\nthe matrix of outputs as:\nAttention(Q, K, V ) = softmax(QKT\n\u221adk\n)V\n(1)\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\nof\n1\n\u221adk . Additive attention computes the compatibility function using a feed-forward network with\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\nmatrix multiplication code.\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\nextremely small gradients 4. To counteract this effect, we scale the dot products by\n1\n\u221adk .\n3.2.2\nMulti-Head Attention\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\nvariables with mean 0 and variance 1. Then their dot product, q \u00b7 k = Pdk\ni=1 qiki, has mean 0 and variance dk.\n4\n\noutput values. These are concatenated and once again projected, resulting in the final values, as\ndepicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation\nsubspaces at different positions. With a single attention head, averaging inhibits this.\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\nwhere headi = Attention(QW Q\ni , KW K\ni , V W V\ni )\nWhere the projections are parameter matrices W Q\ni\n\u2208Rdmodel\u00d7dk, W K\ni\n\u2208Rdmodel\u00d7dk, W V\ni\n\u2208Rdmodel\u00d7dv\nand W O \u2208Rhdv\u00d7dmodel.\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\nis similar to that of single-head attention with full dimensionality.\n3.2.3\nApplications of Attention in our Model\nThe Transformer uses multi-head attention in three different ways:\n\u2022 In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\nand the memory keys and values come from the output of the encoder. This allows every\nposition in the decoder to attend over all positions in the input sequence. This mimics the\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\n[38, 2, 9].\n\u2022 The encoder contains self-attention layers. In a self-attention layer all of the keys, values\nand queries come from the same place, in this case, the output of the previous layer in the\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\nencoder.\n\u2022 Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\nall positions in the decoder up to and including that position. We need to prevent leftward\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\ninside of scaled dot-product attention by masking out (setting to \u2212\u221e) all values in the input\nof the softmax which correspond to illegal connections. See Figure 2.\n3.3\nPosition-wise Feed-Forward Networks\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\nconnected feed-forward network, which is applied to each position separately and identically. This\nconsists of two linear transformations with a ReLU activation in between.\nFFN(x) = max(0, xW1 + b1)W2 + b2\n(2)\nWhile the linear transformations are the same across different positions, they use different parameters\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\ndff = 2048.\n3.4\nEmbeddings and Softmax\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by \u221admodel.\n5\n\nTable 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\nLayer Type\nComplexity per Layer\nSequential\nMaximum Path Length\nOperations\nSelf-Attention\nO(n2 \u00b7 d)\nO(1)\nO(1)\nRecurrent\nO(n \u00b7 d2)\nO(n)\nO(n)\nConvolutional\nO(k \u00b7 n \u00b7 d2)\nO(1)\nO(logk(n))\nSelf-Attention (restricted)\nO(r \u00b7 n \u00b7 d)\nO(1)\nO(n/r)\n3.5\nPositional Encoding\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\norder of the sequence, we must inject some information about the relative or absolute position of the\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\nlearned and fixed [9].\nIn this work, we use sine and cosine functions of different frequencies:\nPE(pos,2i) = sin(pos/100002i/dmodel)\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2\u03c0 to 10000 \u00b7 2\u03c0. We\nchose this function because we hypothesized it would allow the model to easily learn to attend by\nrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\nPEpos.\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\nduring training.\n4\nWhy Self-Attention\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\ntional layers commonly used for mapping one variable-length sequence of symbol representations\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi \u2208Rd, such as a hidden\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\nconsider three desiderata.\nOne is the total computational complexity per layer. Another is the amount of computation that can\nbe parallelized, as measured by the minimum number of sequential operations required.\nThe third is the path length between long-range dependencies in the network. Learning long-range\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\nability to learn such dependencies is the length of the paths forward and backward signals have to\ntraverse in the network. The shorter these paths between any combination of positions in the input\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\nthe maximum path length between any two input and output positions in networks composed of the\ndifferent layer types.\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\n6\n\nlength n is smaller than the representation dimensionality d, which is most often the case with\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\nthe input sequence centered around the respective output position. This would increase the maximum\npath length to O(n/r). We plan to investigate this approach further in future work.\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\nor O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\nbetween any two positions in the network. Convolutional layers are generally more expensive than\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\nconsiderably, to O(k \u00b7 n \u00b7 d + n \u00b7 d2). Even with k = n, however, the complexity of a separable\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\nthe approach we take in our model.\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\nand semantic structure of the sentences.\n5\nTraining\nThis section describes the training regime for our models.\n5.1\nTraining Data and Batching\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\ntarget tokens.\n5.2\nHardware and Schedule\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\n(3.5 days).\n5.3\nOptimizer\nWe used the Adam optimizer [20] with \u03b21 = 0.9, \u03b22 = 0.98 and \u03f5 = 10\u22129. We varied the learning\nrate over the course of training, according to the formula:\nlrate = d\u22120.5\nmodel \u00b7 min(step_num\u22120.5, step_num \u00b7 warmup_steps\u22121.5)\n(3)\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\nwarmup_steps = 4000.\n5.4\nRegularization\nWe employ three types of regularization during training:\n7\n\nTable 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\nModel\nBLEU\nTraining Cost (FLOPs)\nEN-DE\nEN-FR\nEN-DE\nEN-FR\nByteNet [18]\n23.75\nDeep-Att + PosUnk [39]\n39.2\n1.0 \u00b7 1020\nGNMT + RL [38]\n24.6\n39.92\n2.3 \u00b7 1019\n1.4 \u00b7 1020\nConvS2S [9]\n25.16\n40.46\n9.6 \u00b7 1018\n1.5 \u00b7 1020\nMoE [32]\n26.03\n40.56\n2.0 \u00b7 1019\n1.2 \u00b7 1020\nDeep-Att + PosUnk Ensemble [39]\n40.4\n8.0 \u00b7 1020\nGNMT + RL Ensemble [38]\n26.30\n41.16\n1.8 \u00b7 1020\n1.1 \u00b7 1021\nConvS2S Ensemble [9]\n26.36\n41.29\n7.7 \u00b7 1019\n1.2 \u00b7 1021\nTransformer (base model)\n27.3\n38.1\n3.3 \u00b7 1018\nTransformer (big)\n28.4\n41.8\n2.3 \u00b7 1019\nResidual Dropout\nWe apply dropout [33] to the output of each sub-layer, before it is added to the\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\nPdrop = 0.1.\nLabel Smoothing\nDuring training, we employed label smoothing of value \u03f5ls = 0.1 [36]. This\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\n6\nResults\n6.1\nMachine Translation\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\nthe competitive models.\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\ndropout rate Pdrop = 0.1, instead of 0.3.\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\nused beam search with a beam size of 4 and length penalty \u03b1 = 0.6 [38]. These hyperparameters\nwere chosen after experimentation on the development set. We set the maximum output length during\ninference to input length + 50, but terminate early when possible [38].\nTable 2 summarizes our results and compares our translation quality and training costs to other model\narchitectures from the literature. We estimate the number of floating point operations used to train a\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\nsingle-precision floating-point capacity of each GPU 5.\n6.2\nModel Variations\nTo evaluate the importance of different components of the Transformer, we varied our base model\nin different ways, measuring the change in performance on English-to-German translation on the\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n8\n\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\nper-word perplexities.\nN\ndmodel\ndff\nh\ndk\ndv\nPdrop\n\u03f5ls\ntrain\nPPL\nBLEU\nparams\nsteps\n(dev)\n(dev)\n\u00d7106\nbase\n6\n512\n2048\n8\n64\n64\n0.1\n0.1\n100K\n4.92\n25.8\n65\n(A)\n1\n512\n512\n5.29\n24.9\n4\n128\n128\n5.00\n25.5\n16\n32\n32\n4.91\n25.8\n32\n16\n16\n5.01\n25.4\n(B)\n16\n5.16\n25.1\n58\n32\n5.01\n25.4\n60\n(C)\n2\n6.11\n23.7\n36\n4\n5.19\n25.3\n50\n8\n4.88\n25.5\n80\n256\n32\n32\n5.75\n24.5\n28\n1024\n128\n128\n4.66\n26.0\n168\n1024\n5.12\n25.4\n53\n4096\n4.75\n26.2\n90\n(D)\n0.0\n5.77\n24.6\n0.2\n4.95\n25.5\n0.0\n4.67\n25.3\n0.2\n5.47\n25.7\n(E)\npositional embedding instead of sinusoids\n4.92\n25.7\nbig\n6\n1024\n4096\n16\n0.3\n300K\n4.33\n26.4\n213\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\ncheckpoint averaging. We present these results in Table 3.\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\nresults to the base model.\n6.3\nEnglish Constituency Parsing\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\nfor the semi-supervised setting.\nWe performed only a small number of experiments to select the dropout, both attention and residual\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\nremained unchanged from the English-to-German base translation model. During inference, we\n9\n\nTable 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\nof WSJ)\nParser\nTraining\nWSJ 23 F1\nVinyals & Kaiser el al. (2014) [37]\nWSJ only, discriminative\n88.3\nPetrov et al. (2006) [29]\nWSJ only, discriminative\n90.4\nZhu et al. (2013) [40]\nWSJ only, discriminative\n90.4\nDyer et al. (2016) [8]\nWSJ only, discriminative\n91.7\nTransformer (4 layers)\nWSJ only, discriminative\n91.3\nZhu et al. (2013) [40]\nsemi-supervised\n91.3\nHuang & Harper (2009) [14]\nsemi-supervised\n91.3\nMcClosky et al. (2006) [26]\nsemi-supervised\n92.1\nVinyals & Kaiser el al. (2014) [37]\nsemi-supervised\n92.1\nTransformer (4 layers)\nsemi-supervised\n92.7\nLuong et al. (2015) [23]\nmulti-task\n93.0\nDyer et al. (2016) [8]\ngenerative\n93.3\nincreased the maximum output length to input length + 300. We used a beam size of 21 and \u03b1 = 0.3\nfor both WSJ only and the semi-supervised setting.\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\nprisingly well, yielding better results than all previously reported models with the exception of the\nRecurrent Neural Network Grammar [8].\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\nParser [29] even when training only on the WSJ training set of 40K sentences.\n7\nConclusion\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\nmulti-headed self-attention.\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\nmodel outperforms even all previously reported ensembles.\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\nplan to extend the Transformer to problems involving input and output modalities other than text and\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\nThe code we used to train and evaluate our models is available at https://github.com/\ntensorflow/tensor2tensor.\nAcknowledgements\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\ncomments, corrections and inspiration.\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733, 2016.\n10\n\n[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\nmachine translation. CoRR, abs/1406.1078, 2014.\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\npreprint arXiv:1610.02357, 2016.\n[7] Junyoung Chung, \u00c7aglar G\u00fcl\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\nnetwork grammars. In Proc. of NAACL, 2016.\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\n[10] Alex Graves.\nGenerating sequences with recurrent neural networks.\narXiv preprint\narXiv:1308.0850, 2013.\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 770\u2013778, 2016.\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\u00fcrgen Schmidhuber. Gradient flow in\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\n[13] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation,\n9(8):1735\u20131780, 1997.\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\nLanguage Processing, pages 832\u2013841. ACL, August 2009.\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\n[16] \u0141ukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\nInformation Processing Systems, (NIPS), 2016.\n[17] \u0141ukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\non Learning Representations (ICLR), 2016.\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\n2017.\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\nIn International Conference on Learning Representations, 2017.\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\narXiv:1703.10722, 2017.\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\narXiv:1703.03130, 2017.\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\n11\n\n[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313\u2013330, 1993.\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\npages 152\u2013159. ACL, June 2006.\n[27] Ankur Parikh, Oscar T\u00e4ckstr\u00f6m, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\nmodel. In Empirical Methods in Natural Language Processing, 2016.\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433\u2013440. ACL, July\n2006.\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\npreprint arXiv:1608.05859, 2016.\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\nlayer. arXiv preprint arXiv:1701.06538, 2017.\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\nLearning Research, 15(1):1929\u20131958, 2014.\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems 28, pages 2440\u20132448. Curran Associates,\nInc., 2015.\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\nnetworks. In Advances in Neural Information Processing Systems, pages 3104\u20133112, 2014.\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\nAdvances in Neural Information Processing Systems, 2015.\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google\u2019s neural machine\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\narXiv:1609.08144, 2016.\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\n1: Long Papers), pages 434\u2013443. ACL, August 2013.\n12\n\nAttention Visualizations\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nIt\nis\nin\nthis\nspirit\nthat\na\nmajority\nof\nAmerican\ngovernments\nhave\npassed\nnew\nlaws\nsince\n2009\nmaking\nthe\nregistration\nor\nvoting\nprocess\nmore\ndifficult\n.\n<EOS>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\n<pad>\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\nthe verb \u2018making\u2019, completing the phrase \u2018making...more difficult\u2019. Attentions here shown only for\nthe word \u2018making\u2019. Different colors represent different heads. Best viewed in color.\n13\n\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\nFull attentions for head 5. Bottom: Isolated attentions from just the word \u2018its\u2019 for attention heads 5\nand 6. Note that the attentions are very sharp for this word.\n14\n\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nThe\nLaw\nwill\nnever\nbe\nperfect\n,\nbut\nits\napplication\nshould\nbe\njust\n-\nthis\nis\nwhat\nwe\nare\nmissing\n,\nin\nmy\nopinion\n.\n<EOS>\n<pad>\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\nsentence. We give two such examples above, from two different heads from the encoder self-attention\nat layer 5 of 6. The heads clearly learned to perform different tasks.\n15\n",
    "Deep Reinforcement Learning\nfrom Human Preferences\nPaul F Christiano\nOpenAI\npaul@openai.com\nJan Leike\nDeepMind\nleike@google.com\nTom B Brown\nnottombrown@gmail.com\nMiljan Martic\nDeepMind\nmiljanm@google.com\nShane Legg\nDeepMind\nlegg@google.com\nDario Amodei\nOpenAI\ndamodei@openai.com\nAbstract\nFor sophisticated reinforcement learning (RL) systems to interact usefully with\nreal-world environments, we need to communicate complex goals to these systems.\nIn this work, we explore goals de\ufb01ned in terms of (non-expert) human preferences\nbetween pairs of trajectory segments. We show that this approach can effectively\nsolve complex RL tasks without access to the reward function, including Atari\ngames and simulated robot locomotion, while providing feedback on less than\n1% of our agent\u2019s interactions with the environment. This reduces the cost of\nhuman oversight far enough that it can be practically applied to state-of-the-art\nRL systems. To demonstrate the \ufb02exibility of our approach, we show that we can\nsuccessfully train complex novel behaviors with about an hour of human time.\nThese behaviors and environments are considerably more complex than any which\nhave been previously learned from human feedback.\n1\nIntroduction\nRecent success in scaling reinforcement learning (RL) to large problems has been driven in domains\nthat have a well-speci\ufb01ed reward function (Mnih et al., 2015, 2016; Silver et al., 2016). Unfortunately,\nmany tasks involve goals that are complex, poorly-de\ufb01ned, or hard to specify. Overcoming this\nlimitation would greatly expand the possible impact of deep RL and could increase the reach of\nmachine learning more broadly.\nFor example, suppose that we wanted to use reinforcement learning to train a robot to clean a table or\nscramble an egg. It\u2019s not clear how to construct a suitable reward function, which will need to be a\nfunction of the robot\u2019s sensors. We could try to design a simple reward function that approximately\ncaptures the intended behavior, but this will often result in behavior that optimizes our reward\nfunction without actually satisfying our preferences. This dif\ufb01culty underlies recent concerns about\nmisalignment between our values and the objectives of our RL systems (Bostrom, 2014; Russell,\n2016; Amodei et al., 2016). If we could successfully communicate our actual objectives to our agents,\nit would be a signi\ufb01cant step towards addressing these concerns.\nIf we have demonstrations of the desired task, we can extract a reward function using inverse\nreinforcement learning (Ng and Russell, 2000). This reward function can then be used to train\nan agent with reinforcement learning. More directly, we can use imitation learning to clone the\ndemonstrated behavior. However, these approaches are not directly applicable to behaviors that are\ndif\ufb01cult for humans to demonstrate (such as controlling a robot with many degrees of freedom but\nvery non-human morphology).\narXiv:1706.03741v4  [stat.ML]  17 Feb 2023\n\nAn alternative approach is to allow a human to provide feedback on our system\u2019s current behavior\nand to use this feedback to de\ufb01ne the task. In principle this \ufb01ts within the paradigm of reinforcement\nlearning, but using human feedback directly as a reward function is prohibitively expensive for RL\nsystems that require hundreds or thousands of hours of experience. In order to practically train deep\nRL systems with human feedback, we need to decrease the amount of feedback required by several\norders of magnitude.\nOur approach is to learn a reward function from human feedback and then to optimize that reward\nfunction. This basic approach has been considered previously, but we confront the challenges involved\nin scaling it up to modern deep RL and demonstrate by far the most complex behaviors yet learned\nfrom human feedback.\nIn summary, we desire a solution to sequential decision problems without a well-speci\ufb01ed reward\nfunction that\n1. enables us to solve tasks for which we can only recognize the desired behavior, but not\nnecessarily demonstrate it,\n2. allows agents to be taught by non-expert users,\n3. scales to large problems, and\n4. is economical with user feedback.\nRL algorithm\nenvironment\nobservation\naction\nhuman \nfeedback\nreward predictor\npredicted\nreward\nFigure 1: Schematic illustration of our approach:\nthe reward predictor is trained asynchronously\nfrom comparisons of trajectory segments, and the\nagent maximizes predicted reward.\nOur algorithm \ufb01ts a reward function to the hu-\nman\u2019s preferences while simultaneously training\na policy to optimize the current predicted reward\nfunction (see Figure 1). We ask the human to\ncompare short video clips of the agent\u2019s behav-\nior, rather than to supply an absolute numerical\nscore. We found comparisons to be easier for hu-\nmans to provide in some domains, while being\nequally useful for learning human preferences.\nComparing short video clips is nearly as fast as\ncomparing individual states, but we show that\nthe resulting comparisons are signi\ufb01cantly more\nhelpful. Moreover, we show that collecting feed-\nback online improves the system\u2019s performance\nand prevents it from exploiting weaknesses of\nthe learned reward function.\nOur experiments take place in two domains: Atari games in the Arcade Learning Environment (Belle-\nmare et al., 2013), and robotics tasks in the physics simulator MuJoCo (Todorov et al., 2012). We\nshow that a small amount of feedback from a non-expert human, ranging from \ufb01fteen minutes to \ufb01ve\nhours, suf\ufb01ces to learn most of the original RL tasks even when the reward function is not observable.\nWe then consider some novel behaviors in each domain, such as performing a back\ufb02ip or driving\nwith the \ufb02ow of traf\ufb01c. We show that our algorithm can learn these behaviors from about an hour of\nfeedback\u2014even though it is unclear how to hand-engineer a reward function that would incentivize\nthem.\n1.1\nRelated Work\nA long line of work studies reinforcement learning from human ratings or rankings, including Akrour\net al. (2011), Pilarski et al. (2011), Akrour et al. (2012), Wilson et al. (2012), Sugiyama et al. (2012),\nWirth and F\u00fcrnkranz (2013), Daniel et al. (2015), El Asri et al. (2016), Wang et al. (2016), and\nWirth et al. (2016). Other lines of research considers the general problem of reinforcement learning\nfrom preferences rather than absolute reward values (F\u00fcrnkranz et al., 2012; Akrour et al., 2014),\nand optimizing using human preferences in settings other than reinforcement learning (Machwe and\nParmee, 2006; Secretan et al., 2008; Brochu et al., 2010; S\u00f8rensen et al., 2016).\nOur algorithm follows the same basic approach as Akrour et al. (2012) and Akrour et al. (2014). They\nconsider continuous domains with four degrees of freedom and small discrete domains, where they\ncan assume that the reward is linear in the expectations of hand-coded features. We instead consider\n2\n\nphysics tasks with dozens of degrees of freedom and Atari tasks with no hand-engineered features;\nthe complexity of our environments force us to use different RL algorithms and reward models, and\nto cope with different algorithmic tradeoffs. One notable difference is that Akrour et al. (2012) and\nAkrour et al. (2014) elicit preferences over whole trajectories rather than short clips. So although we\ngather about two orders of magnitude more comparisons, our experiments require less than one order\nof magnitude more human time. Other differences focus on changing our training procedure to cope\nwith the nonlinear reward models and modern deep RL, for example using asynchronous training and\nensembling.\nOur approach to feedback elicitation closely follows Wilson et al. (2012). However, Wilson et al.\n(2012) assumes that the reward function is the distance to some unknown \u201ctarget\u201d policy (which is\nitself a linear function of hand-coded features). They \ufb01t this reward function using Bayesian inference,\nand rather than performing RL they produce trajectories using the MAP estimate of the target policy.\nTheir experiments involve \u201csynthetic\u201d human feedback which is drawn from their Bayesian model,\nwhile we perform experiments with feedback gathered from non-expert users. It is not clear if the\nmethods in Wilson et al. (2012) can be extended to complex tasks or if they can work with real human\nfeedback.\nMacGlashan et al. (2017), Pilarski et al. (2011), Knox and Stone (2009), and Knox (2012) perform\nexperiments involving reinforcement learning from actual human feedback, although their algorithmic\napproach is less similar. In MacGlashan et al. (2017) and Pilarski et al. (2011), learning only occurs\nduring episodes where the human trainer provides feedback. This appears to be infeasible in domains\nlike Atari games where thousands of hours of experience are required to learn a high-quality policy,\nand would be prohibitively expensive even for the simplest tasks we consider. TAMER (Knox, 2012;\nKnox and Stone, 2013) also learn a reward function, however they consider much simpler settings\nwhere the desired policy can be learned relatively quickly.\nOur work could also be seen of a speci\ufb01c instance of the cooperative inverse reinforcement learning\nframework (Had\ufb01eld-Menell et al., 2016). This framework considers a two-player game between\na human and a robot interacting with an environment with the purpose of maximizing the human\u2019s\nreward function. In our setting the human is only allowed to interact with this game by stating their\npreferences.\nCompared to all prior work, our key contribution is to scale human feedback up to deep reinforcement\nlearning and to learn much more complex behaviors. This \ufb01ts into a recent trend of scaling reward\nlearning methods to large deep learning systems, for example inverse RL (Finn et al., 2016), imitation\nlearning (Ho and Ermon, 2016; Stadie et al., 2017), semi-supervised skill generalization (Finn et al.,\n2017), and bootstrapping RL from demonstrations (Silver et al., 2016; Hester et al., 2017).\n2\nPreliminaries and Method\n2.1\nSetting and Goal\nWe consider an agent interacting with an environment over a sequence of steps; at each time t the\nagent receives an observation ot \u2208O from the environment and then sends an action at \u2208A to the\nenvironment.\nIn traditional reinforcement learning, the environment would also supply a reward rt \u2208R and the\nagent\u2019s goal would be to maximize the discounted sum of rewards. Instead of assuming that the\nenvironment produces a reward signal, we assume that there is a human overseer who can express\npreferences between trajectory segments. A trajectory segment is a sequence of observations and\nactions, \u03c3 = ((o0, a0), (o1, a1), . . . , (ok\u22121, ak\u22121)) \u2208(O \u00d7 A)k. Write \u03c31 \u227b\u03c32 to indicate that the\nhuman preferred trajectory segment \u03c31 to trajectory segment \u03c32. Informally, the goal of the agent is\nto produce trajectories which are preferred by the human, while making as few queries as possible to\nthe human.\nMore precisely, we will evaluate our algorithms\u2019 behavior in two ways:\n3\n\nQuantitative: We say that preferences \u227bare generated by a reward function1 r : O \u00d7 A \u2192R if\n\u0000\u0000o1\n0, a1\n0\n\u0001\n, . . . ,\n\u0000o1\nk\u22121, a1\nk\u22121\n\u0001\u0001\n\u227b\n\u0000\u0000o2\n0, a2\n0\n\u0001\n, . . . ,\n\u0000o2\nk\u22121, a2\nk\u22121\n\u0001\u0001\nwhenever\nr\n\u0000o1\n0, a1\n0\n\u0001\n+ \u00b7 \u00b7 \u00b7 + r\n\u0000o1\nk\u22121, a1\nk\u22121\n\u0001\n> r\n\u0000o2\n0, a2\n0\n\u0001\n+ \u00b7 \u00b7 \u00b7 + r\n\u0000o2\nk\u22121, a2\nk\u22121\n\u0001\n.\nIf the human\u2019s preferences are generated by a reward function r, then our agent ought to\nreceive a high total reward according to r. So if we know the reward function r, we can\nevaluate the agent quantitatively. Ideally the agent will achieve reward nearly as high as if it\nhad been using RL to optimize r.\nQualitative: Sometimes we have no reward function by which we can quantitatively evaluate\nbehavior (this is the situation where our approach would be practically useful). In these\ncases, all we can do is qualitatively evaluate how well the agent satis\ufb01es to the human\u2019s\npreferences. In this paper, we will start from a goal expressed in natural language, ask a\nhuman to evaluate the agent\u2019s behavior based on how well it ful\ufb01lls that goal, and then\npresent videos of agents attempting to ful\ufb01ll that goal.\nOur model based on trajectory segment comparisons is very similar to the trajectory preference\nqueries used in Wilson et al. (2012), except that we don\u2019t assume that we can reset the system to\nan arbitrary state2 and so our segments generally begin from different states. This complicates the\ninterpretation of human comparisons, but we show that our algorithm overcomes this dif\ufb01culty even\nwhen the human raters have no understanding of our algorithm.\n2.2\nOur Method\nAt each point in time our method maintains a policy \u03c0 : O \u2192A and a reward function estimate\n\u02c6r : O \u00d7 A \u2192R, each parametrized by deep neural networks.\nThese networks are updated by three processes:\n1. The policy \u03c0 interacts with the environment to produce a set of trajectories {\u03c4 1, . . . , \u03c4 i}.\nThe parameters of \u03c0 are updated by a traditional reinforcement learning algorithm, in order\nto maximize the sum of the predicted rewards rt = \u02c6r(ot, at).\n2. We select pairs of segments\n\u0000\u03c31, \u03c32\u0001\nfrom the trajectories {\u03c4 1, . . . , \u03c4 i} produced in step 1,\nand send them to a human for comparison.\n3. The parameters of the mapping \u02c6r are optimized via supervised learning to \ufb01t the comparisons\ncollected from the human so far.\nThese processes run asynchronously, with trajectories \ufb02owing from process (1) to process (2), human\ncomparisons \ufb02owing from process (2) to process (3), and parameters for \u02c6r \ufb02owing from process (3)\nto process (1). The following subsections provide details on each of these processes.\n2.2.1\nOptimizing the Policy\nAfter using \u02c6r to compute rewards, we are left with a traditional reinforcement learning problem. We\ncan solve this problem using any RL algorithm that is appropriate for the domain. One subtlety is\nthat the reward function \u02c6r may be non-stationary, which leads us to prefer methods which are robust\nto changes in the reward function. This led us to focus on policy gradient methods, which have been\napplied successfully for such problems (Ho and Ermon, 2016).\nIn this paper, we use advantage actor-critic (A2C; Mnih et al., 2016) to play Atari games, and trust\nregion policy optimization (TRPO; Schulman et al., 2015) to perform simulated robotics tasks. In\n1Here we assume here that the reward is a function of the observation and action. In our experiments in\nAtari environments, we instead assume the reward is a function of the preceding 4 observations. In a general\npartially observable environment, we could instead consider reward functions that depend on the whole sequence\nof observations, and model this reward function with a recurrent neural network.\n2Wilson et al. (2012) also assumes the ability to sample reasonable initial states. But we work with high\ndimensional state spaces for which random states will not be reachable and the intended policy inhabits a\nlow-dimensional manifold.\n4\n\neach case, we used parameter settings which have been found to work well for traditional RL tasks.\nThe only hyperparameter which we adjusted was the entropy bonus for TRPO. This is because TRPO\nrelies on the trust region to ensure adequate exploration, which can lead to inadequate exploration if\nthe reward function is changing.\nWe normalized the rewards produced by \u02c6r to have zero mean and constant standard deviation. This is\na typical preprocessing step which is particularly appropriate here since the position of the rewards is\nunderdetermined by our learning problem.\n2.2.2\nPreference Elicitation\nThe human overseer is given a visualization of two trajectory segments, in the form of short movie\nclips. In all of our experiments, these clips are between 1 and 2 seconds long.\nThe human then indicates which segment they prefer, that the two segments are equally good, or that\nthey are unable to compare the two segments.\nThe human judgments are recorded in a database D of triples\n\u0000\u03c31, \u03c32, \u00b5\n\u0001\n, where \u03c31 and \u03c32 are the\ntwo segments and \u00b5 is a distribution over {1, 2} indicating which segment the user preferred. If the\nhuman selects one segment as preferable, then \u00b5 puts all of its mass on that choice. If the human\nmarks the segments as equally preferable, then \u00b5 is uniform. Finally, if the human marks the segments\nas incomparable, then the comparison is not included in the database.\n2.2.3\nFitting the Reward Function\nWe can interpret a reward function estimate \u02c6r as a preference-predictor if we view \u02c6r as a latent factor\nexplaining the human\u2019s judgments and assume that the human\u2019s probability of preferring a segment\n\u03c3i depends exponentially on the value of the latent reward summed over the length of the clip:3\n\u02c6P\n\u0002\n\u03c31 \u227b\u03c32\u0003\n=\nexp P \u02c6r\n\u0000o1\nt, a1\nt\n\u0001\nexp P \u02c6r(o1\nt, a1\nt) + exp P \u02c6r(o2\nt, a2\nt).\n(1)\nWe choose \u02c6r to minimize the cross-entropy loss between these predictions and the actual human\nlabels:\nloss(\u02c6r) = \u2212\nX\n(\u03c31,\u03c32,\u00b5)\u2208D\n\u00b5(1) log \u02c6P\n\u0002\n\u03c31 \u227b\u03c32\u0003\n+ \u00b5(2) log \u02c6P\n\u0002\n\u03c32 \u227b\u03c31\u0003\n.\nThis follows the Bradley-Terry model (Bradley and Terry, 1952) for estimating score functions\nfrom pairwise preferences, and is the specialization of the Luce-Shephard choice rule (Luce, 2005;\nShepard, 1957) to preferences over trajectory segments. It can be understood as equating rewards\nwith a preference ranking scale analogous to the famous Elo ranking system developed for chess (Elo,\n1978). Just as the difference in Elo points of two chess players estimates the probability of one player\ndefeating the other in a game of chess, the difference in predicted reward of two trajectory segments\nestimates the probability that one is chosen over the other by the human.\nOur actual algorithm incorporates a number of modi\ufb01cations to this basic approach, which early\nexperiments discovered to be helpful and which are analyzed in Section 3.3:\n\u2022 We \ufb01t an ensemble of predictors, each trained on |D| triples sampled from D with replace-\nment. The estimate \u02c6r is de\ufb01ned by independently normalizing each of these predictors and\nthen averaging the results.\n\u2022 A fraction of 1/e of the data is held out to be used as a validation set for each predictor.\nWe use \u21132 regularization and adjust the regularization coef\ufb01cient to keep the validation loss\nbetween 1.1 and 1.5 times the training loss. In some domains we also apply dropout for\nregularization.\n\u2022 Rather than applying a softmax directly as described in Equation 1, we assume there is a\n10% chance that the human responds uniformly at random. Conceptually this adjustment is\nneeded because human raters have a constant probability of making an error, which doesn\u2019t\ndecay to 0 as the difference in reward difference becomes extreme.\n3Equation 1 does not use discounting, which could be interpreted as modeling the human to be indifferent\nabout when things happen in the trajectory segment. Using explicit discounting or inferring the human\u2019s discount\nfunction would also be reasonable choices.\n5\n\n2.2.4\nSelecting Queries\nWe decide how to query preferences based on an approximation to the uncertainty in the reward\nfunction estimator, similar to Daniel et al. (2014): we sample a large number of pairs of trajectory\nsegments of length k, use each reward predictor in our ensemble to predict which segment will be\npreferred from each pair, and then select those trajectories for which the predictions have the highest\nvariance across ensemble members. This is a crude approximation and the ablation experiments in\nSection 3 show that in some tasks it actually impairs performance. Ideally, we would want to query\nbased on the expected value of information of the query (Akrour et al., 2012; Krueger et al., 2016),\nbut we leave it to future work to explore this direction further.\n3\nExperimental Results\nWe implemented our algorithm in TensorFlow (Abadi et al., 2016).\nWe interface with Mu-\nJoCo (Todorov et al., 2012) and the Arcade Learning Environment (Bellemare et al., 2013) through\nthe OpenAI Gym (Brockman et al., 2016).\n3.1\nReinforcement Learning Tasks with Unobserved Rewards\nIn our \ufb01rst set of experiments, we attempt to solve a range of benchmark tasks for deep RL without\nobserving the true reward. Instead, the agent learns about the goal of the task only by asking a human\nwhich of two trajectory segments is better. Our goal is to solve the task in a reasonable amount of\ntime using as few queries as possible.\nIn our experiments, feedback is provided by contractors who are given a 1-2 sentence description\nof each task before being asked to compare several hundred to several thousand pairs of trajectory\nsegments for that task (see Appendix B for the exact instructions given to contractors). Each trajectory\nsegment is between 1 and 2 seconds long. Contractors responded to the average query in 3-5 seconds,\nand so the experiments involving real human feedback required between 30 minutes and 5 hours of\nhuman time.\nFor comparison, we also run experiments using a synthetic oracle whose preferences over trajectories\nexactly re\ufb02ect reward in the underlying task. That is, when the agent queries for a comparison, instead\nof sending the query to a human, we immediately reply by indicating a preference for whichever\ntrajectory segment actually receives a higher reward in the underlying task4. We also compare to\nthe baseline of RL training using the real reward. Our aim here is not to outperform but rather to\ndo nearly as well as RL without access to reward information and instead relying on much scarcer\nfeedback. Nevertheless, note that feedback from real humans does have the potential to outperform\nRL (and as shown below it actually does so on some tasks), because the human feedback might\nprovide a better-shaped reward.\nWe describe the details of our experiments in Appendix A, including model architectures, modi\ufb01ca-\ntions to the environment, and the RL algorithms used to optimize the policy.\n3.1.1\nSimulated Robotics\nThe \ufb01rst tasks we consider are eight simulated robotics tasks, implemented in MuJoCo (Todorov\net al., 2012), and included in OpenAI Gym (Brockman et al., 2016). We made small modi\ufb01cations\nto these tasks in order to avoid encoding information about the task in the environment itself (the\nmodi\ufb01cations are described in detail in Appendix A). The reward functions in these tasks are linear\nfunctions of distances, positions and velocities, and all are a quadratic function of the features. We\nincluded a simple cartpole task (\u201cpendulum\u201d) for comparison, since this is representative of the\ncomplexity of tasks studied in prior work.\nFigure 2 shows the results of training our agent with 700 queries to a human rater, compared to\nlearning from 350, 700, or 1400 synthetic queries, as well as to RL learning from the real reward.\n4In the case of Atari games with sparse rewards, it is relatively common for two clips to both have zero\nreward in which case the oracle outputs indifference. Because we considered clips rather than individual states,\nsuch ties never made up a large majority of our data. Moreover, ties still provide signi\ufb01cant information to the\nreward predictor as long as they are not too common.\n6\n\nFigure 2: Results on MuJoCo simulated robotics as measured on the tasks\u2019 true reward. We compare\nour method using real human feedback (purple), our method using synthetic feedback provided by\nan oracle (shades of blue), and reinforcement learning using the true reward function (orange). All\ncurves are the average of 5 runs, except for the real human feedback, which is a single run, and\neach point is the average reward over \ufb01ve consecutive batches. For Reacher and Cheetah feedback\nwas provided by an author due to time constraints. For all other tasks, feedback was provided by\ncontractors unfamiliar with the environments and with our algorithm. The irregular progress on\nHopper is due to one contractor deviating from the typical labeling schedule.\nWith 700 labels we are able to nearly match reinforcement learning on all of these tasks. Training\nwith learned reward functions tends to be less stable and higher variance, while having a comparable\nmean performance.\nSurprisingly, by 1400 labels our algorithm performs slightly better than if it had simply been given\nthe true reward, perhaps because the learned reward function is slightly better shaped\u2014the reward\nlearning procedure assigns positive rewards to all behaviors that are typically followed by high\nreward.\nReal human feedback is typically only slightly less effective than the synthetic feedback; depending\non the task human feedback ranged from being half as ef\ufb01cient as ground truth feedback to being\nequally ef\ufb01cient. On the Ant task the human feedback signi\ufb01cantly outperformed the synthetic\nfeedback, apparently because we asked humans to prefer trajectories where the robot was \u201cstanding\nupright,\u201d which proved to be useful reward shaping. (There was a similar bonus in the RL reward\nfunction to encourage the robot to remain upright, but the simple hand-crafted bonus was not as\nuseful.)\n3.1.2\nAtari\nThe second set of tasks we consider is a set of seven Atari games in the Arcade Learning Environ-\nment (Bellemare et al., 2013), the same games presented in Mnih et al., 2013.\nFigure 3 shows the results of training our agent with 5,500 queries to a human rater, compared to\nlearning from 350, 700, or 1400 synthetic queries, as well as to RL learning from the real reward.\nOur method has more dif\ufb01culty matching RL in these challenging environments, but nevertheless it\ndisplays substantial learning on most of them and matches or even exceeds RL on some. Speci\ufb01cally,\non BeamRider and Pong, synthetic labels match or come close to RL even with only 3,300 such\nlabels. On Seaquest and Qbert synthetic feedback eventually performs near the level of RL but learns\nmore slowly. On SpaceInvaders and Breakout synthetic feedback never matches RL, but nevertheless\nthe agent improves substantially, often passing the \ufb01rst level in SpaceInvaders and reaching a score of\n20 on Breakout, or 50 with enough labels.\n7\n\nFigure 3: Results on Atari games as measured on the tasks\u2019 true reward. We compare our method using\nreal human feedback (purple), our method using synthetic feedback provided by an oracle (shades of\nblue), and reinforcement learning using the true reward function (orange). All curves are the average\nof 3 runs, except for the real human feedback which is a single run, and each point is the average\nreward over about 150,000 consecutive frames.\nFigure 4: Four frames from a single back\ufb02ip. The agent is trained to perform a sequence of back\ufb02ips,\nlanding upright each time. The video is available at this link.\nOn most of the games real human feedback performs similar to or slightly worse than synthetic\nfeedback with the same number of labels, and often comparably to synthetic feedback that has 40%\nfewer labels. This may be due to human error in labeling, inconsistency between different contractors\nlabeling the same run, or the uneven rate of labeling by contractors, which can cause labels to be\noverly concentrated in narrow parts of state space. The latter problems could potentially be addressed\nby future improvements to the pipeline for outsourcing labels. On Qbert, our method fails to learn\nto beat the \ufb01rst level with real human feedback; this may be because short clips in Qbert can be\nconfusing and dif\ufb01cult to evaluate. Finally, Enduro is dif\ufb01cult for A3C to learn due to the dif\ufb01culty\nof successfully passing other cars through random exploration, and is correspondingly dif\ufb01cult to\nlearn with synthetic labels, but human labelers tend to reward any progress towards passing cars,\nessentially shaping the reward and thus outperforming A3C in this game (the results are comparable\nto those achieved with DQN).\n3.2\nNovel behaviors\nExperiments with traditional RL tasks help us understand whether our method is effective, but the\nultimate purpose of human interaction is to solve tasks for which no reward function is available.\nUsing the same parameters as in the previous experiments, we show that our algorithm can learn\nnovel complex behaviors. We demonstrate:\n1. The Hopper robot performing a sequence of back\ufb02ips (see Figure 4). This behavior was\ntrained using 900 queries in less than an hour. The agent learns to consistently perform a\nback\ufb02ip, land upright, and repeat.\n8\n\nFigure 5: Performance of our algorithm on MuJoCo tasks after removing various components, as\ndescribed in Section Section 3.3. All graphs are averaged over 5 runs, using 700 synthetic labels\neach.\n2. The Half-Cheetah robot moving forward while standing on one leg. This behavior was\ntrained using 800 queries in under an hour.\n3. Keeping alongside other cars in Enduro. This was trained with roughly 1,300 queries\nand 4 million frames of interaction with the environment; the agent learns to stay almost\nexactly even with other moving cars for a substantial fraction of the episode, although it gets\nconfused by changes in background.\nVideos of these behaviors can be found at this link. These behaviors were trained using feedback\nfrom the authors.\n3.3\nAblation Studies\nIn order to better understand the performance of our algorithm, we consider a range of modi\ufb01cations:\n1. We pick queries uniformly at random rather than prioritizing queries for which there is\ndisagreement (random queries).\n2. We train only one predictor rather than an ensemble (no ensemble). In this setting, we also\nchoose queries at random, since there is no longer an ensemble that we could use to estimate\ndisagreement.\n3. We train on queries only gathered at the beginning of training, rather than gathered through-\nout training (no online queries).\n4. We remove the \u21132 regularization and use only dropout (no regularization).\n5. On the robotics tasks only, we use trajectory segments of length 1 (no segments).\n6. Rather than \ufb01tting \u02c6r using comparisons, we consider an oracle which provides the true\ntotal reward over a trajectory segment, and \ufb01t \u02c6r to these total rewards using mean squared\nerror (target).\nThe results are presented in Figure 5 for MuJoCo and Figure 6 for Atari.\nOf particular interest is the poor performance of of\ufb02ine reward predictor training; here we \ufb01nd\nthat due to the nonstationarity of the occupancy distribution, the predictor captures only part of the\ntrue reward, and maximizing this partial reward can lead to bizarre behavior that is undesirable as\nmeasured by the true reward (Amodei et al., 2016). For instance, on Pong of\ufb02ine training sometimes\nleads our agent to avoid losing points but not to score points; this can result in extremely long volleys\n9\n\nFigure 6: Performance of our algorithm on Atari tasks after removing various components, as\ndescribed in Section 3.3. All curves are an average of 3 runs using 5,500 synthetic labels (see minor\nexceptions in Section A.2).\nthat repeat the same sequence of events ad in\ufb01nitum (videos at this link). This type of behavior\ndemonstrates that in general human feedback needs to be intertwined with RL learning rather than\nprovided statically.\nOur main motivation for eliciting comparisons rather than absolute scores was that we found it much\neasier for humans to provide consistent comparisons than consistent absolute scores, especially on the\ncontinuous control tasks and on the qualitative tasks in Section 3.2; nevertheless it seems important\nto understand how using comparisons affects performance. For continuous control tasks we found\nthat predicting comparisons worked much better than predicting scores. This is likely because the\nscale of rewards varies substantially and this complicates the regression problem, which is smoothed\nsigni\ufb01cantly when we only need to predict comparisons. In the Atari tasks we clipped rewards and\neffectively only predicted the sign, avoiding these dif\ufb01culties (this is not a suitable solution for the\ncontinuous control tasks because the relative magnitude of the reward are important to learning). In\nthese tasks comparisons and targets had signi\ufb01cantly different performance, but neither consistently\noutperformed the other.\nWe also observed large performance differences when using single frames rather than clips5. In order\nto obtain the same results using single frames we would need to have collected signi\ufb01cantly more\ncomparisons. In general we discovered that asking humans to compare longer clips was signi\ufb01cantly\nmore helpful per clip, and signi\ufb01cantly less helpful per frame. We found that for short clips it took\nhuman raters a while just to understand the situation, while for longer clips the evaluation time was\na roughly linear function of the clip length. We tried to choose the shortest clip length for which\nthe evaluation time was linear. In the Atari environments we also found that it was often easier to\ncompare longer clips because they provide more context than single frames.\n4\nDiscussion and Conclusions\nAgent-environment interactions are often radically cheaper than human interaction. We show that by\nlearning a separate reward model using supervised learning, it is possible to reduce the interaction\ncomplexity by roughly 3 orders of magnitude. Not only does this show that we can meaningfully\ntrain deep RL agents from human preferences, but also that we are already hitting diminishing returns\n5We only ran these tests on continuous control tasks because our Atari reward model depends on a sequence\nof consecutive frames rather than a single frame, as described in Section A.2\n10\n\non further sample-complexity improvements because the cost of compute is already comparable to\nthe cost of non-expert feedback.6\nAlthough there is a large literature on preference elicitation and reinforcement learning from unknown\nreward functions, we provide the \ufb01rst evidence that these techniques can be economically scaled up to\nstate-of-the-art reinforcement learning systems. This represents a step towards practical applications\nof deep RL to complex real-world tasks.\nFuture work may be able to improve the ef\ufb01ciency of learning from human preferences, and expand\nthe range of tasks to which it can be applied.\nIn the long run it would be desirable to make learning a task from human preferences no more dif\ufb01cult\nthan learning it from a programmatic reward signal, ensuring that powerful RL systems can be applied\nin the service of complex human values rather than low-complexity goals.\nAcknowledgments\nWe thank Olivier Pietquin, Bilal Piot, Laurent Orseau, Pedro Ortega, Victoria Krakovna, Owain\nEvans, Andrej Karpathy, Igor Mordatch, and Jack Clark for reading drafts of the paper. We thank\nTyler Adkisson, Mandy Beri, Jessica Richards, Heather Tran, and other contractors for providing the\ndata used to train our agents. Finally, we thank OpenAI and DeepMind for providing a supportive\nresearch environment and for supporting and encouraging this collaboration.\nReferences\nMartin Abadi et al. Tensor\ufb02ow: Large-scale machine learning on heterogeneous distributed systems.\narXiv preprint arXiv:1603.04467, 2016.\nRiad Akrour, Marc Schoenauer, and Michele Sebag. Preference-based policy learning. Machine\nlearning and knowledge discovery in databases, pages 12\u201327, 2011.\nRiad Akrour, Marc Schoenauer, and Mich\u00e8le Sebag. April: Active preference learning-based\nreinforcement learning. In Joint European Conference on Machine Learning and Knowledge\nDiscovery in Databases, pages 116\u2013131, 2012.\nRiad Akrour, Marc Schoenauer, Mich\u00e8le Sebag, and Jean-Christophe Souplet. Programming by\nfeedback. In International Conference on Machine Learning, pages 1503\u20131511, 2014.\nDario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Man\u00e9.\nConcrete problems in AI safety. arXiv preprint arXiv:1606.06565, 2016.\nMarc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling.\nThe Arcade Learning\nEnvironment: An evaluation platform for general agents. Journal of Arti\ufb01cial Intelligence Research,\n47:253\u2013279, 2013.\nNick Bostrom. Superintelligence: Paths, Dangers, Strategies. Oxford University Press, 2014.\nRalph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. The method\nof paired comparisons. Biometrika, 39(3/4):324\u2013345, 1952.\nEric Brochu, Tyson Brochu, and Nando de Freitas. A bayesian interactive optimization approach\nto procedural animation design. In Proceedings of the 2010 ACM SIGGRAPH/Eurographics\nSymposium on Computer Animation, pages 103\u2013112. Eurographics Association, 2010.\nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and\nWojciech Zaremba. OpenAI Gym. arXiv preprint arXiv:1606.01540, 2016.\nChristian Daniel, Malte Viering, Jan Metz, Oliver Kroemer, and Jan Peters. Active reward learning.\nIn Robotics: Science and Systems, 2014.\n6For the Atari experiments we are using a virtual machine with 16 CPUs and one Nvidia K80 GPU which\ncosts ~$700/month on GCE. Training takes about a day, so the compute cost is ~$25. Training with 5k labels\ncorresponds roughly to 5 hours of human labour, at US minimum wage this totals ~$36.\n11\n\nChristian Daniel, Oliver Kroemer, Malte Viering, Jan Metz, and Jan Peters. Active reward learning\nwith a novel acquisition function. Autonomous Robots, 39(3):389\u2013405, 2015.\nLayla El Asri, Bilal Piot, Matthieu Geist, Romain Laroche, and Olivier Pietquin. Score-based\ninverse reinforcement learning. In International Conference on Autonomous Agents and Multiagent\nSystems, pages 457\u2013465, 2016.\nArpad Elo. The Rating of Chessplayers, Past and Present. Arco Pub., 1978.\nChelsea Finn, Sergey Levine, and Pieter Abbeel. Guided cost learning: Deep inverse optimal control\nvia policy optimization. In International Conference on Machine Learning, volume 48, 2016.\nChelsea Finn, Tianhe Yu, Justin Fu, Pieter Abbeel, and Sergey Levine. Generalizing skills with\nsemi-supervised reinforcement learning. In International Conference on Learning Representations,\n2017.\nJohannes F\u00fcrnkranz, Eyke H\u00fcllermeier, Weiwei Cheng, and Sang-Hyeun Park. Preference-based\nreinforcement learning: A formal framework and a policy iteration algorithm. Machine learning,\n89(1-2):123\u2013156, 2012.\nDylan Had\ufb01eld-Menell, Stuart Russell, Pieter Abbeel, and Anca Dragan. Cooperative inverse\nreinforcement learning. In Advances in Neural Information Processing Systems, pages 3909\u20133917,\n2016.\nTodd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal Piot, Andrew\nSendonaris, Gabriel Dulac-Arnold, Ian Osband, John Agapiou, Joel Z Leibo, and Audrunas\nGruslys. Learning from demonstrations for real world reinforcement learning. arXiv preprint\narXiv:1704.03732, 2017.\nJonathan Ho and Stefano Ermon. Generative adversarial imitation learning. In Advances in Neural\nInformation Processing Systems, pages 4565\u20134573, 2016.\nW Bradley Knox and Peter Stone. Interactively shaping agents via human reinforcement: The\nTAMER framework. In International Conference on Knowledge Capture, pages 9\u201316, 2009.\nW. Bradley Knox and Peter Stone. Learning non-myopically from human-generated reward. In Jihie\nKim, Jeffrey Nichols, and Pedro A. Szekely, editors, IUI, pages 191\u2013202. ACM, 2013. ISBN\n978-1-4503-1965-2. URL http://doi.acm.org/10.1145/2449396.\nWilliam Bradley Knox. Learning from human-generated reward. PhD thesis, University of Texas at\nAustin, 2012.\nDavid Krueger, Jan Leike, Owain Evans, and John Salvatier. Active reinforcement learning: Observ-\ning rewards at a cost. In Future of Interactive Learning Machines, NIPS Workshop, 2016.\nR Duncan Luce. Individual choice behavior: A theoretical analysis. Courier Corporation, 2005.\nJames MacGlashan, Mark K Ho, Robert Loftin, Bei Peng, David Roberts, Matthew E Taylor, and\nMichael L Littman. Interactive learning from policy-dependent human feedback. arXiv preprint\narXiv:1701.06049, 2017.\nAT Machwe and IC Parmee. Introducing machine learning within an interactive evolutionary design\nenvironment. In DS 36: Proceedings DESIGN 2006, the 9th International Design Conference,\nDubrovnik, Croatia, 2006.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan\nWierstra, and Martin Riedmiller. Playing Atari with deep reinforcement learning. arXiv preprint\narXiv:1312.5602, 2013.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare,\nAlex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, Stig Petersen, Charles\nBeattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane\nLegg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature,\n518(7540):529\u2013533, 2015.\n12\n\nVolodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim\nHarley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement\nlearning. In International Conference on Machine Learning, pages 1928\u20131937, 2016.\nAndrew Y Ng and Stuart Russell. Algorithms for inverse reinforcement learning. In International\nConference on Machine learning, pages 663\u2013670, 2000.\nPatrick M Pilarski, Michael R Dawson, Thomas Degris, Farbod Fahimi, Jason P Carey, and Richard\nSutton. Online human training of a myoelectric prosthesis controller via actor-critic reinforcement\nlearning. In International Conference on Rehabilitation Robotics, pages 1\u20137, 2011.\nStuart Russell. Should we fear supersmart robots? Scienti\ufb01c American, 314(6):58, 2016.\nJohn Schulman, Sergey Levine, Pieter Abbeel, Michael I Jordan, and Philipp Moritz. Trust region\npolicy optimization. In International Conference on Machine Learning, pages 1889\u20131897, 2015.\nJimmy Secretan, Nicholas Beato, David B D Ambrosio, Adelein Rodriguez, Adam Campbell, and\nKenneth O Stanley. Picbreeder: Evolving pictures collaboratively online. In Conference on Human\nFactors in Computing Systems, pages 1759\u20131768, 2008.\nRoger N Shepard. Stimulus and response generalization: A stochastic model relating generalization\nto distance in psychological space. Psychometrika, 22(4):325\u2013345, 1957.\nDavid Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche,\nJulian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman,\nDominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine\nLeach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of Go with\ndeep neural networks and tree search. Nature, 529(7587):484\u2013489, 2016.\nPatrikk D S\u00f8rensen, Jeppeh M Olsen, and Sebastian Risi. Breeding a diversity of super mario\nbehaviors through interactive evolution. In Computational Intelligence and Games (CIG), 2016\nIEEE Conference on, pages 1\u20137. IEEE, 2016.\nBradly C Stadie, Pieter Abbeel, and Ilya Sutskever. Third-person imitation learning. In International\nConference on Learning Representations, 2017.\nHiroaki Sugiyama, Toyomi Meguro, and Yasuhiro Minami. Preference-learning based inverse\nreinforcement learning for dialog control. In INTERSPEECH, pages 222\u2013225, 2012.\nEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.\nIn International Conference on Intelligent Robots and Systems, pages 5026\u20135033, 2012.\nSida I Wang, Percy Liang, and Christopher D Manning. Learning language games through interaction.\narXiv preprint arXiv:1606.02447, 2016.\nAaron Wilson, Alan Fern, and Prasad Tadepalli. A Bayesian approach for policy learning from\ntrajectory preference queries. In Advances in Neural Information Processing Systems, pages\n1133\u20131141, 2012.\nChristian Wirth and Johannes F\u00fcrnkranz. Preference-based reinforcement learning: A preliminary\nsurvey. In ECML/PKDD Workshop on Reinforcement Learning from Generalized Feedback:\nBeyond Numeric Rewards, 2013.\nChristian Wirth, J F\u00fcrnkranz, Gerhard Neumann, et al. Model-free preference-based reinforcement\nlearning. In AAAI, pages 2222\u20132228, 2016.\n13\n\nA\nExperimental Details\nMany RL environments have termination conditions that depend on the behavior of the agent, such\nas ending an episode when the agent dies or falls over. We found that such termination conditions\nencode information about the task even when the reward function is not observable. To avoid this\nsubtle source of supervision, which could potentially confound our attempts to learn from human\npreferences only, we removed all variable-length episodes:\n\u2022 In the Gym versions of our robotics tasks, the episode ends when certain parameters go\noutside of a prescribed range (for example when the robot falls over). We replaced these\ntermination conditions by a penalty which encourages the parameters to remain in the range\n(and which the agent must learn).\n\u2022 In Atari games, we do not send life loss or episode end signals to the agent (we do continue\nto actually reset the environment), effectively converting the environment into a single\ncontinuous episode. When providing synthetic oracle feedback we replace episode ends\nwith a penalty in all games except Pong; the agent must learn this penalty.\nRemoving variable length episodes leaves the agent with only the information encoded in the\nenvironment itself; human feedback provides its only guidance about what it ought to do.\nAt the beginning of training we compare a number of trajectory segments drawn from rollouts of an\nuntrained (randomly initialized) policy. In the Atari domain we also pretrain the reward predictor\nfor 200 epochs before beginning RL training, to reduce the likelihood of irreversibly learning a bad\npolicy based on an untrained predictor. For the rest of training, labels are fed in at a rate decaying\ninversely with the number of timesteps; after twice as many timesteps have elapsed, we answer about\nhalf as many queries per unit time. The details of this schedule are described in each section. This\n\u201clabel annealing\u201d allows us to balance the importance of having a good predictor from the start with\nthe need to adapt the predictor as the RL agent learns and encounters new states. When training\nwith real human feedback, we attempt to similarly anneal the label rate, although in practice this is\napproximate because contractors give feedback at uneven rates.\nExcept where otherwise stated we use an ensemble of 3 predictors, and draw a factor 10 more clip\npair candidates than we ultimately present to the human, with the presented clips being selected via\nmaximum variance between the different predictors as described in Section 2.2.4.\nA.1\nSimulated Robotics Tasks\nThe OpenAI Gym continuous control tasks penalize large torques. Because torques are not di-\nrectly visible to a human supervisor, these reward functions are not good representatives of human\npreferences over trajectories and so we removed them.\nFor the simulated robotics tasks, we optimize policies using trust region policy optimization (TRPO,\nSchulman et al., 2015) with discount rate \u03b3 = 0.995 and \u03bb = 0.97. The reward predictor is a two-\nlayer neural network with 64 hidden units each, using leaky ReLUs (\u03b1 = 0.01) as nonlinearities.7 We\ncompare trajectory segments that last 1.5 seconds, which varies from 15 to 60 timesteps depending\non the task.\nWe normalize the reward predictions to have standard deviation 1. When learning from the reward\npredictor, we add an entropy bonus of 0.01 on all tasks except swimmer, where we use an entropy\nbonus of 0.001. As noted in Section 2.2.1, this entropy bonus helps to incentivize the increased\nexploration needed to deal with a changing reward function.\nWe collect 25% of our comparisons from a randomly initialized policy network at the beginning of\ntraining, and our rate of labeling after T frames 2 \u2217106/(T + 2 \u2217106).\n7All of these reward functions are second degree polynomials of the input features, and so if we were\nconcerned only with these tasks we could take a simpler approach to learning the reward function. However,\nusing this more \ufb02exible architecture allows us to immediately generalize to tasks for which the reward function\nis not so simple, as described in Section 3.2.\n14\n\nA.2\nAtari\nOur Atari agents are trained using the standard set of environment wrappers used by Mnih et al.\n(2015): 0 to 30 no-ops in the beginning of an episode, max-pooling over adjacent frames, stacking\nof 4 frames, a frameskip of 4, life loss ending an episode (but not resetting the environment), and\nrewards clipped to [\u22121, 1].\nAtari games include a visual display of the score, which in theory could be used to trivially infer\nthe reward. Since we want to focus instead on inferring the reward from the complex dynamics\nhappening in the game, we replace the score area with a constant black background on all seven\ngames. On BeamRider we additionally blank out the enemy ship count, and on Enduro we blank out\nthe speedometer.\nFor the Atari tasks we optimize policies using the A3C algorithm (Mnih et al., 2016) in synchronous\nform (A2C), with policy architecture as described in Mnih et al. (2015). We use standard settings for\nthe hyperparameters: an entropy bonus of \u03b2 = 0.01, learning rate of 0.0007 decayed linearly to reach\nzero after 80 million timesteps (although runs were actually trained for only 50 million timesteps),\nn = 5 steps per update, N = 16 parallel workers, discount rate \u03b3 = 0.99, and policy gradient using\nAdam with \u03b1 = 0.99 and \u03f5 = 10\u22125.\nFor the reward predictor, we use 84x84 images as inputs (the same as the inputs to the policy), and\nstack 4 frames for a total 84x84x4 input tensor. This input is fed through 4 convolutional layers\nof size 7x7, 5x5, 3x3, and 3x3 with strides 3, 2, 1, 1, each having 16 \ufb01lters, with leaky ReLU\nnonlinearities (\u03b1 = 0.01). This is followed by a fully connected layer of size 64 and then a scalar\noutput. All convolutional layers use batch norm and dropout with \u03b1 = 0.5 to prevent predictor\nover\ufb01tting. In addition we use \u21132 regularization with the adapative scheme described in Section 2.2.3.\nSince the reward predictor is ultimately used to compare two sums over timesteps, its scale is arbitrary,\nand we normalize it to have a standard deviation of 0.05 (we could equivalently have adjusted our\nlearning rates and entropy bonus, but this choice allowed us to use the same parameters as for the real\nreward function).\nWe compare trajectory segments of 25 timesteps (1.7 seconds at 15 fps with frame skipping).\nWe collect 500 comparisons from a randomly initialized policy network at the beginning of training,\nand our rate of labeling after T frames of training is decreased every 5 \u2217106 frames, to be roughly\nproportional to 5 \u2217106/(T + 5 \u2217106).\nThe predictor is trained asynchronously from the RL agent, and on our hardware typically processes\n1 label per 10 RL timesteps. We maintain a buffer of only the last 3,000 labels and loop over this\nbuffer continuously; this is to ensure that the predictor gives enough weight to new labels (which can\nrepresent a shift in distribution) when the total number of labels becomes large.\nIn the ablation studies of Figure 5b, pretraining has 5,000 labels rather than 5,500, and the \u201ctarget\u201d\nbeamrider curve is averaged over 2 runs rather than 3.\nB\nInstructions Provided to Contractors\nB.1\nMuJoCo\nGiving feedback\nSign up for a slot in the spreadsheet. Then go to the appropriate URL\u2019s that we give you, and you\u2019ll\nbe repeatedly presented with two video clips of an AI controlling a virtual robot.\nLook at the clips and select the one in which better things happen. Only decide on events you\nactually witness in the clip.\nHere\u2019s a guide on what constitutes good and bad behavior in each speci\ufb01c domain:\n\u2022 Hopper: the \u201ccenter\u201d of the robot is the joint closest to the pointy end. The \ufb01rst priority is\nfor the center of the robot to move to the right (moving to the left is worse than not moving\nat all). If the two robots are roughly tied on this metric, then the tiebreaker is how high the\ncenter is.\n15\n\n\u2022 Walker: the \u201ccenter\u201d of the robot is the joint where the three limbs meet. The \ufb01rst priority\nis for the center of the robot to move to the right. If the two robots are roughly tied on this\nmetric, then the tiebreaker is how high the center is.\n\u2022 Swimmer: the \u201ccenter\u201d of the robot is the mark in the middle of its body. The center should\nmove to the right as fast as possible.\n\u2022 Cheetah: the robot should move to the right as fast as possible.\n\u2022 Ant: the \ufb01rst priority is for the robot to be standing upright, and failing that for the center of\nthe robot to be as high up as possible. If both robots are upright or neither is, the tie breaker\nis whichever one is moving faster to the right.\n\u2022 Reacher: the green dot on the robot arm should be as close as possible to the red dot. Being\nnear for a while and far for a while is worse than being at an intermediate distance for the\nentire clip.\n\u2022 Pendulum: the pendulum should be pointing approximately up. There will be a lot of ties\nwhere the pendulum has fallen and a lot of \u201ccan\u2019t tells\u201d where it is off the side of the screen.\nIf you can see one pendulum and it hasn\u2019t fallen down, that\u2019s better than being unable to see\nthe other pendulum.\n\u2022 Double-pendulum: both pendulums should be pointing approximately up (if they fall down,\nthe cart should try to swing them back up) and the cart should be near the center of the track.\nBeing high for a while and low for a while is worse than being at an intermediate distance\nthe entire time.\nIf both clips look about the same to you, then click \u201ctie\u201d. If you don\u2019t understand what\u2019s going on in\nthe clip or \ufb01nd it hard to evaluate, then click \u201ccan\u2019t tell\u201d.\nYou can speed up your feedback by using the arrow keys\nleft and right select clips, up is a tie, down is \u201ccan\u2019t tell\u201d.\nFAQ\nI got an error saying that we\u2019re out of clips. What\u2019s up? Occasionally the server may run out of\nclips to give you, and you\u2019ll see an error message. This is normal, just wait a minute and refresh the\npage. If you don\u2019t get clips for more than a couple minutes, please ping @tom on slack.\nDo I need to start right at the time listed in the spreadsheet? Starting 10 minutes before or after\nthe listed time is \ufb01ne.\nB.2\nAtari\nIn this task you\u2019ll be trying to teach an AI to play Atari games by giving it feedback\non how well it is playing.\nIMPORTANT. First play the game yourself for 5 minutes\nBefore providing feedback to the AI, play the game yourself for a \ufb01ve minutes to get a sense of how\nit works. It\u2019s often hard to tell what the game is about just by looking at short clips, especially if\nyou\u2019ve never played it before.\nPlay the game online for 5 minutes.8 You\u2019ll need to press F12 or click the GAME RESET button to\nstart the game. Then set a timer for 5 minutes and explore the game to see how it works.\nGiving feedback\nSign up for a slot in the spreadsheet. Then go to the appropriate URL\u2019s that we give you, and you\u2019ll\nbe repeatedly presented with two video clips of an AI playing the game.\nLook at the clips and select the one in which better things happen. For example, if the left clip\nshows the AI shooting an enemy ship while the right clip shows it being shot by an enemy ship, then\nbetter things happen in the left clip and thus the left clip is better. Only decide on actions you actually\nwitness in the clip.\n8e.g. http://www.free80sarcade.com/2600_Beamrider.php\n16\n\nHere\u2019s a guide on what constitutes good and bad play in each speci\ufb01c game:\n\u2022 BeamRider: shoot enemy ships (good), and don\u2019t get shot (very bad)\n\u2022 Breakout: hit the ball with the paddle, break the colored blocks, and don\u2019t let the ball fall\noff the bottom of the screen\n\u2022 Enduro: pass as many cars as you can, and don\u2019t get passed by cars\n\u2022 Pong: knock the ball past the opponent\u2019s orange paddle on the left (good), and don\u2019t let it\ngo past your green paddle on the right (bad)\n\u2022 Qbert: change the color of as many blocks as you can (good), but don\u2019t jump off the side or\nrun into enemies (very bad)\n\u2022 SpaceInvaders: shoot enemy ships (good), and don\u2019t let your ship (the one at the bottom of\nthe screen) get shot (very bad)\n\u2022 SeaQuest: Shoot the \ufb01sh and enemy submarines (good) and pick up the scuba divers. Don\u2019t\nlet your submarine run out of air or get hit by a \ufb01sh or torpedo (very bad)\n\u2022 Enduro (even mode): Avoid passing cars OR getting passed by them, you want to stay\neven with other cars (not having any around is OK too)\nDon\u2019t worry about how the agent got into the situation it is in (for instance, it doesn\u2019t matter if\none agent has more lives, or is now on a more advanced level); just focus on what happens in the clip\nitself.\nIf both clips look about the same to you, then click \u201ctie\u201d. If you don\u2019t understand what\u2019s going on\nin the clip or \ufb01nd it hard to evaluate, then click \u201ccan\u2019t tell\u201d. Try to minimize responding \u201ccan\u2019t tell\u201d\nunless you truly are confused.\nYou can speed up your feedback by using the arrow keys\nleft and right select clips, up is a tie, down is \u201ccan\u2019t tell\u201d.\nFAQ\nI got an error saying that we\u2019re out of clips. What\u2019s up? Occasionally the server may run out of\nclips to give you, and you\u2019ll see an error message. This is normal, just wait a minute and refresh the\npage. If you don\u2019t get clips for more than a couple minutes, please ping @tom on slack.\nIf the agent is already dead when the clip starts, how should I compare it? If the clip is after\ngetting killed (but not showing the dying), then its performance during the clip is neither good nor\nbad. You can treat it as purely average play. If you see it die, or it\u2019s possible that it contains a frame\nof it dying, then it\u2019s de\ufb01nitely bad.\nDo I need to start right at the time listed in the spreadsheet? Starting 30 minutes before or after\nthe listed time is \ufb01ne.\n17\n",
    "Denoising Diffusion Probabilistic Models\nJonathan Ho\nUC Berkeley\njonathanho@berkeley.edu\nAjay Jain\nUC Berkeley\najayj@berkeley.edu\nPieter Abbeel\nUC Berkeley\npabbeel@cs.berkeley.edu\nAbstract\nWe present high quality image synthesis results using diffusion probabilistic models,\na class of latent variable models inspired by considerations from nonequilibrium\nthermodynamics. Our best results are obtained by training on a weighted variational\nbound designed according to a novel connection between diffusion probabilistic\nmodels and denoising score matching with Langevin dynamics, and our models nat-\nurally admit a progressive lossy decompression scheme that can be interpreted as a\ngeneralization of autoregressive decoding. On the unconditional CIFAR10 dataset,\nwe obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On\n256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our imple-\nmentation is available at https://github.com/hojonathanho/diffusion.\n1\nIntroduction\nDeep generative models of all kinds have recently exhibited high quality samples in a wide variety\nof data modalities. Generative adversarial networks (GANs), autoregressive models, \ufb02ows, and\nvariational autoencoders (VAEs) have synthesized striking image and audio samples [14, 27, 3,\n58, 38, 25, 10, 32, 44, 57, 26, 33, 45], and there have been remarkable advances in energy-based\nmodeling and score matching that have produced images comparable to those of GANs [11, 55].\nFigure 1: Generated samples on CelebA-HQ 256 \u00d7 256 (left) and unconditional CIFAR10 (right)\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\narXiv:2006.11239v2  [cs.LG]  16 Dec 2020\n\n!\nxT\n\u2212! \u00b7 \u00b7 \u00b7 \u2212!\nxt\n\u2212\u2212\u2212\u2212\u2212! xt\u22121 \u2212! \u00b7 \u00b7 \u00b7 \u2212! x0\np\u2713(xt\u22121|xt)\nq(xt|xt\u22121)\nFigure 2: The directed graphical model considered in this work.\nThis paper presents progress in diffusion probabilistic models [53]. A diffusion probabilistic model\n(which we will call a \u201cdiffusion model\u201d for brevity) is a parameterized Markov chain trained using\nvariational inference to produce samples matching the data after \ufb01nite time. Transitions of this chain\nare learned to reverse a diffusion process, which is a Markov chain that gradually adds noise to the\ndata in the opposite direction of sampling until signal is destroyed. When the diffusion consists of\nsmall amounts of Gaussian noise, it is suf\ufb01cient to set the sampling chain transitions to conditional\nGaussians too, allowing for a particularly simple neural network parameterization.\nDiffusion models are straightforward to de\ufb01ne and ef\ufb01cient to train, but to the best of our knowledge,\nthere has been no demonstration that they are capable of generating high quality samples. We\nshow that diffusion models actually are capable of generating high quality samples, sometimes\nbetter than the published results on other types of generative models (Section 4). In addition, we\nshow that a certain parameterization of diffusion models reveals an equivalence with denoising\nscore matching over multiple noise levels during training and with annealed Langevin dynamics\nduring sampling (Section 3.2) [55, 61]. We obtained our best sample quality results using this\nparameterization (Section 4.2), so we consider this equivalence to be one of our primary contributions.\nDespite their sample quality, our models do not have competitive log likelihoods compared to other\nlikelihood-based models (our models do, however, have log likelihoods better than the large estimates\nannealed importance sampling has been reported to produce for energy based models and score\nmatching [11, 55]). We \ufb01nd that the majority of our models\u2019 lossless codelengths are consumed\nto describe imperceptible image details (Section 4.3). We present a more re\ufb01ned analysis of this\nphenomenon in the language of lossy compression, and we show that the sampling procedure of\ndiffusion models is a type of progressive decoding that resembles autoregressive decoding along a bit\nordering that vastly generalizes what is normally possible with autoregressive models.\n2\nBackground\nDiffusion models [53] are latent variable models of the form p\u03b8(x0) :=\nR\np\u03b8(x0:T ) dx1:T , where\nx1, . . . , xT are latents of the same dimensionality as the data x0 \u223cq(x0). The joint distribution\np\u03b8(x0:T ) is called the reverse process, and it is de\ufb01ned as a Markov chain with learned Gaussian\ntransitions starting at p(xT ) = N(xT ; 0, I):\np\u03b8(x0:T ) := p(xT )\nT\nY\nt=1\np\u03b8(xt\u22121|xt),\np\u03b8(xt\u22121|xt) := N(xt\u22121; \u00b5\u03b8(xt, t), \u03a3\u03b8(xt, t))\n(1)\nWhat distinguishes diffusion models from other types of latent variable models is that the approximate\nposterior q(x1:T |x0), called the forward process or diffusion process, is \ufb01xed to a Markov chain that\ngradually adds Gaussian noise to the data according to a variance schedule \u03b21, . . . , \u03b2T :\nq(x1:T |x0) :=\nT\nY\nt=1\nq(xt|xt\u22121),\nq(xt|xt\u22121) := N(xt;\np\n1 \u2212\u03b2txt\u22121, \u03b2tI)\n(2)\nTraining is performed by optimizing the usual variational bound on negative log likelihood:\nE [\u2212log p\u03b8(x0)] \u2264Eq\n\u0014\n\u2212log p\u03b8(x0:T )\nq(x1:T |x0)\n\u0015\n= Eq\n\u0014\n\u2212log p(xT ) \u2212\nX\nt\u22651\nlog p\u03b8(xt\u22121|xt)\nq(xt|xt\u22121)\n\u0015\n=: L (3)\nThe forward process variances \u03b2t can be learned by reparameterization [33] or held constant as\nhyperparameters, and expressiveness of the reverse process is ensured in part by the choice of\nGaussian conditionals in p\u03b8(xt\u22121|xt), because both processes have the same functional form when\n\u03b2t are small [53]. A notable property of the forward process is that it admits sampling xt at an\narbitrary timestep t in closed form: using the notation \u03b1t := 1 \u2212\u03b2t and \u00af\u03b1t := Qt\ns=1 \u03b1s, we have\nq(xt|x0) = N(xt; \u221a\u00af\u03b1tx0, (1 \u2212\u00af\u03b1t)I)\n(4)\n2\n\nEf\ufb01cient training is therefore possible by optimizing random terms of L with stochastic gradient\ndescent. Further improvements come from variance reduction by rewriting L (3) as:\nEq\n\u0014\nDKL(q(xT |x0) \u2225p(xT ))\n|\n{z\n}\nLT\n+\nX\nt>1\nDKL(q(xt\u22121|xt, x0) \u2225p\u03b8(xt\u22121|xt))\n|\n{z\n}\nLt\u22121\n\u2212log p\u03b8(x0|x1)\n|\n{z\n}\nL0\n\u0015\n(5)\n(See Appendix A for details. The labels on the terms are used in Section 3.) Equation (5) uses KL\ndivergence to directly compare p\u03b8(xt\u22121|xt) against forward process posteriors, which are tractable\nwhen conditioned on x0:\nq(xt\u22121|xt, x0) = N(xt\u22121; \u02dc\u00b5t(xt, x0), \u02dc\u03b2tI),\n(6)\nwhere\n\u02dc\u00b5t(xt, x0) :=\n\u221a\u00af\u03b1t\u22121\u03b2t\n1 \u2212\u00af\u03b1t\nx0 +\n\u221a\u03b1t(1 \u2212\u00af\u03b1t\u22121)\n1 \u2212\u00af\u03b1t\nxt\nand\n\u02dc\u03b2t := 1 \u2212\u00af\u03b1t\u22121\n1 \u2212\u00af\u03b1t\n\u03b2t\n(7)\nConsequently, all KL divergences in Eq. (5) are comparisons between Gaussians, so they can be\ncalculated in a Rao-Blackwellized fashion with closed form expressions instead of high variance\nMonte Carlo estimates.\n3\nDiffusion models and denoising autoencoders\nDiffusion models might appear to be a restricted class of latent variable models, but they allow a\nlarge number of degrees of freedom in implementation. One must choose the variances \u03b2t of the\nforward process and the model architecture and Gaussian distribution parameterization of the reverse\nprocess. To guide our choices, we establish a new explicit connection between diffusion models\nand denoising score matching (Section 3.2) that leads to a simpli\ufb01ed, weighted variational bound\nobjective for diffusion models (Section 3.4). Ultimately, our model design is justi\ufb01ed by simplicity\nand empirical results (Section 4). Our discussion is categorized by the terms of Eq. (5).\n3.1\nForward process and LT\nWe ignore the fact that the forward process variances \u03b2t are learnable by reparameterization and\ninstead \ufb01x them to constants (see Section 4 for details). Thus, in our implementation, the approximate\nposterior q has no learnable parameters, so LT is a constant during training and can be ignored.\n3.2\nReverse process and L1:T \u22121\nNow we discuss our choices in p\u03b8(xt\u22121|xt) = N(xt\u22121; \u00b5\u03b8(xt, t), \u03a3\u03b8(xt, t)) for 1 < t \u2264T. First,\nwe set \u03a3\u03b8(xt, t) = \u03c32\nt I to untrained time dependent constants. Experimentally, both \u03c32\nt = \u03b2t and\n\u03c32\nt = \u02dc\u03b2t = 1\u2212\u00af\u03b1t\u22121\n1\u2212\u00af\u03b1t \u03b2t had similar results. The \ufb01rst choice is optimal for x0 \u223cN(0, I), and the\nsecond is optimal for x0 deterministically set to one point. These are the two extreme choices\ncorresponding to upper and lower bounds on reverse process entropy for data with coordinatewise\nunit variance [53].\nSecond, to represent the mean \u00b5\u03b8(xt, t), we propose a speci\ufb01c parameterization motivated by the\nfollowing analysis of Lt. With p\u03b8(xt\u22121|xt) = N(xt\u22121; \u00b5\u03b8(xt, t), \u03c32\nt I), we can write:\nLt\u22121 = Eq\n\u0014 1\n2\u03c32\nt\n\u2225\u02dc\u00b5t(xt, x0) \u2212\u00b5\u03b8(xt, t)\u22252\n\u0015\n+ C\n(8)\nwhere C is a constant that does not depend on \u03b8. So, we see that the most straightforward parameteri-\nzation of \u00b5\u03b8 is a model that predicts \u02dc\u00b5t, the forward process posterior mean. However, we can expand\nEq. (8) further by reparameterizing Eq. (4) as xt(x0, \u03f5) = \u221a\u00af\u03b1tx0 + \u221a1 \u2212\u00af\u03b1t\u03f5 for \u03f5 \u223cN(0, I) and\napplying the forward process posterior formula (7):\nLt\u22121 \u2212C = Ex0,\u03f5\n\"\n1\n2\u03c32\nt\n\r\r\r\r\u02dc\u00b5t\n\u0012\nxt(x0, \u03f5),\n1\n\u221a\u00af\u03b1t\n(xt(x0, \u03f5) \u2212\n\u221a\n1 \u2212\u00af\u03b1t\u03f5)\n\u0013\n\u2212\u00b5\u03b8(xt(x0, \u03f5), t)\n\r\r\r\r\n2#\n(9)\n= Ex0,\u03f5\n\"\n1\n2\u03c32\nt\n\r\r\r\r\n1\n\u221a\u03b1t\n\u0012\nxt(x0, \u03f5) \u2212\n\u03b2t\n\u221a1 \u2212\u00af\u03b1t\n\u03f5\n\u0013\n\u2212\u00b5\u03b8(xt(x0, \u03f5), t)\n\r\r\r\r\n2#\n(10)\n3\n\nAlgorithm 1 Training\n1: repeat\n2:\nx0 \u223cq(x0)\n3:\nt \u223cUniform({1, . . . , T})\n4:\n\u03f5 \u223cN(0, I)\n5:\nTake gradient descent step on\n\u2207\u03b8\n\r\r\u03f5 \u2212\u03f5\u03b8(\u221a\u00af\u03b1tx0 + \u221a1 \u2212\u00af\u03b1t\u03f5, t)\n\r\r2\n6: until converged\nAlgorithm 2 Sampling\n1: xT \u223cN(0, I)\n2: for t = T, . . . , 1 do\n3:\nz \u223cN(0, I) if t > 1, else z = 0\n4:\nxt\u22121 =\n1\n\u221a\u03b1t\n\u0010\nxt \u2212\n1\u2212\u03b1t\n\u221a1\u2212\u00af\u03b1t \u03f5\u03b8(xt, t)\n\u0011\n+ \u03c3tz\n5: end for\n6: return x0\nEquation (10) reveals that \u00b5\u03b8 must predict\n1\n\u221a\u03b1t\n\u0010\nxt \u2212\n\u03b2t\n\u221a1\u2212\u00af\u03b1t \u03f5\n\u0011\ngiven xt. Since xt is available as\ninput to the model, we may choose the parameterization\n\u00b5\u03b8(xt, t) = \u02dc\u00b5t\n\u0012\nxt,\n1\n\u221a\u00af\u03b1t\n(xt \u2212\n\u221a\n1 \u2212\u00af\u03b1t\u03f5\u03b8(xt))\n\u0013\n=\n1\n\u221a\u03b1t\n\u0012\nxt \u2212\n\u03b2t\n\u221a1 \u2212\u00af\u03b1t\n\u03f5\u03b8(xt, t)\n\u0013\n(11)\nwhere \u03f5\u03b8 is a function approximator intended to predict \u03f5 from xt. To sample xt\u22121 \u223cp\u03b8(xt\u22121|xt) is\nto compute xt\u22121 =\n1\n\u221a\u03b1t\n\u0010\nxt \u2212\n\u03b2t\n\u221a1\u2212\u00af\u03b1t \u03f5\u03b8(xt, t)\n\u0011\n+\u03c3tz, where z \u223cN(0, I). The complete sampling\nprocedure, Algorithm 2, resembles Langevin dynamics with \u03f5\u03b8 as a learned gradient of the data\ndensity. Furthermore, with the parameterization (11), Eq. (10) simpli\ufb01es to:\nEx0,\u03f5\n\u0014\n\u03b22\nt\n2\u03c32\nt \u03b1t(1 \u2212\u00af\u03b1t)\n\r\r\u03f5 \u2212\u03f5\u03b8(\u221a\u00af\u03b1tx0 +\n\u221a\n1 \u2212\u00af\u03b1t\u03f5, t)\n\r\r2\u0015\n(12)\nwhich resembles denoising score matching over multiple noise scales indexed by t [55]. As Eq. (12)\nis equal to (one term of) the variational bound for the Langevin-like reverse process (11), we see\nthat optimizing an objective resembling denoising score matching is equivalent to using variational\ninference to \ufb01t the \ufb01nite-time marginal of a sampling chain resembling Langevin dynamics.\nTo summarize, we can train the reverse process mean function approximator \u00b5\u03b8 to predict \u02dc\u00b5t, or by\nmodifying its parameterization, we can train it to predict \u03f5. (There is also the possibility of predicting\nx0, but we found this to lead to worse sample quality early in our experiments.) We have shown that\nthe \u03f5-prediction parameterization both resembles Langevin dynamics and simpli\ufb01es the diffusion\nmodel\u2019s variational bound to an objective that resembles denoising score matching. Nonetheless,\nit is just another parameterization of p\u03b8(xt\u22121|xt), so we verify its effectiveness in Section 4 in an\nablation where we compare predicting \u03f5 against predicting \u02dc\u00b5t.\n3.3\nData scaling, reverse process decoder, and L0\nWe assume that image data consists of integers in {0, 1, . . . , 255} scaled linearly to [\u22121, 1]. This\nensures that the neural network reverse process operates on consistently scaled inputs starting from\nthe standard normal prior p(xT ). To obtain discrete log likelihoods, we set the last term of the reverse\nprocess to an independent discrete decoder derived from the Gaussian N(x0; \u00b5\u03b8(x1, 1), \u03c32\n1I):\np\u03b8(x0|x1) =\nD\nY\ni=1\nZ \u03b4+(xi\n0)\n\u03b4\u2212(xi\n0)\nN(x; \u00b5i\n\u03b8(x1, 1), \u03c32\n1) dx\n\u03b4+(x) =\n\u001a\u221e\nif x = 1\nx +\n1\n255\nif x < 1\n\u03b4\u2212(x) =\n\u001a\u2212\u221e\nif x = \u22121\nx \u2212\n1\n255\nif x > \u22121\n(13)\nwhere D is the data dimensionality and the i superscript indicates extraction of one coordinate.\n(It would be straightforward to instead incorporate a more powerful decoder like a conditional\nautoregressive model, but we leave that to future work.) Similar to the discretized continuous\ndistributions used in VAE decoders and autoregressive models [34, 52], our choice here ensures that\nthe variational bound is a lossless codelength of discrete data, without need of adding noise to the\ndata or incorporating the Jacobian of the scaling operation into the log likelihood. At the end of\nsampling, we display \u00b5\u03b8(x1, 1) noiselessly.\n3.4\nSimpli\ufb01ed training objective\nWith the reverse process and decoder de\ufb01ned above, the variational bound, consisting of terms derived\nfrom Eqs. (12) and (13), is clearly differentiable with respect to \u03b8 and is ready to be employed for\n4\n\nTable 1: CIFAR10 results. NLL measured in bits/dim.\nModel\nIS\nFID\nNLL Test (Train)\nConditional\nEBM [11]\n8.30\n37.9\nJEM [17]\n8.76\n38.4\nBigGAN [3]\n9.22\n14.73\nStyleGAN2 + ADA (v1) [29]\n10.06\n2.67\nUnconditional\nDiffusion (original) [53]\n\u22645.40\nGated PixelCNN [59]\n4.60\n65.93\n3.03 (2.90)\nSparse Transformer [7]\n2.80\nPixelIQN [43]\n5.29\n49.46\nEBM [11]\n6.78\n38.2\nNCSNv2 [56]\n31.75\nNCSN [55]\n8.87\u00b10.12\n25.32\nSNGAN [39]\n8.22\u00b10.05\n21.7\nSNGAN-DDLS [4]\n9.09\u00b10.10\n15.42\nStyleGAN2 + ADA (v1) [29]\n9.74 \u00b1 0.05\n3.26\nOurs (L, \ufb01xed isotropic \u03a3)\n7.67\u00b10.13\n13.51\n\u22643.70 (3.69)\nOurs (Lsimple)\n9.46\u00b10.11\n3.17\n\u22643.75 (3.72)\nTable 2: Unconditional CIFAR10 reverse\nprocess parameterization and training objec-\ntive ablation. Blank entries were unstable to\ntrain and generated poor samples with out-of-\nrange scores.\nObjective\nIS\nFID\n\u02dc\u00b5 prediction (baseline)\nL, learned diagonal \u03a3\n7.28\u00b10.10\n23.69\nL, \ufb01xed isotropic \u03a3\n8.06\u00b10.09\n13.22\n\u2225\u02dc\u00b5 \u2212\u02dc\u00b5\u03b8\u22252\n\u2013\n\u2013\n\u03f5 prediction (ours)\nL, learned diagonal \u03a3\n\u2013\n\u2013\nL, \ufb01xed isotropic \u03a3\n7.67\u00b10.13\n13.51\n\u2225\u02dc\u03f5 \u2212\u03f5\u03b8\u22252 (Lsimple)\n9.46\u00b10.11\n3.17\ntraining. However, we found it bene\ufb01cial to sample quality (and simpler to implement) to train on the\nfollowing variant of the variational bound:\nLsimple(\u03b8) := Et,x0,\u03f5\nh\r\r\u03f5 \u2212\u03f5\u03b8(\u221a\u00af\u03b1tx0 +\n\u221a\n1 \u2212\u00af\u03b1t\u03f5, t)\n\r\r2i\n(14)\nwhere t is uniform between 1 and T. The t = 1 case corresponds to L0 with the integral in the\ndiscrete decoder de\ufb01nition (13) approximated by the Gaussian probability density function times the\nbin width, ignoring \u03c32\n1 and edge effects. The t > 1 cases correspond to an unweighted version of\nEq. (12), analogous to the loss weighting used by the NCSN denoising score matching model [55].\n(LT does not appear because the forward process variances \u03b2t are \ufb01xed.) Algorithm 1 displays the\ncomplete training procedure with this simpli\ufb01ed objective.\nSince our simpli\ufb01ed objective (14) discards the weighting in Eq. (12), it is a weighted variational\nbound that emphasizes different aspects of reconstruction compared to the standard variational\nbound [18, 22]. In particular, our diffusion process setup in Section 4 causes the simpli\ufb01ed objective\nto down-weight loss terms corresponding to small t. These terms train the network to denoise data\nwith very small amounts of noise, so it is bene\ufb01cial to down-weight them so that the network can\nfocus on more dif\ufb01cult denoising tasks at larger t terms. We will see in our experiments that this\nreweighting leads to better sample quality.\n4\nExperiments\nWe set T = 1000 for all experiments so that the number of neural network evaluations needed\nduring sampling matches previous work [53, 55]. We set the forward process variances to constants\nincreasing linearly from \u03b21 = 10\u22124 to \u03b2T = 0.02. These constants were chosen to be small\nrelative to data scaled to [\u22121, 1], ensuring that reverse and forward processes have approximately\nthe same functional form while keeping the signal-to-noise ratio at xT as small as possible (LT =\nDKL(q(xT |x0) \u2225N(0, I)) \u224810\u22125 bits per dimension in our experiments).\nTo represent the reverse process, we use a U-Net backbone similar to an unmasked PixelCNN++ [52,\n48] with group normalization throughout [66]. Parameters are shared across time, which is speci\ufb01ed\nto the network using the Transformer sinusoidal position embedding [60]. We use self-attention at\nthe 16 \u00d7 16 feature map resolution [63, 60]. Details are in Appendix B.\n4.1\nSample quality\nTable 1 shows Inception scores, FID scores, and negative log likelihoods (lossless codelengths) on\nCIFAR10. With our FID score of 3.17, our unconditional model achieves better sample quality than\nmost models in the literature, including class conditional models. Our FID score is computed with\nrespect to the training set, as is standard practice; when we compute it with respect to the test set, the\nscore is 5.24, which is still better than many of the training set FID scores in the literature.\n5\n\nFigure 3: LSUN Church samples. FID=7.89\nFigure 4: LSUN Bedroom samples. FID=4.90\nAlgorithm 3 Sending x0\n1: Send xT \u223cq(xT |x0) using p(xT )\n2: for t = T \u22121, . . . , 2, 1 do\n3:\nSend xt \u223cq(xt|xt+1, x0) using p\u03b8(xt|xt+1)\n4: end for\n5: Send x0 using p\u03b8(x0|x1)\nAlgorithm 4 Receiving\n1: Receive xT using p(xT )\n2: for t = T \u22121, . . . , 1, 0 do\n3:\nReceive xt using p\u03b8(xt|xt+1)\n4: end for\n5: return x0\nWe \ufb01nd that training our models on the true variational bound yields better codelengths than training\non the simpli\ufb01ed objective, as expected, but the latter yields the best sample quality. See Fig. 1 for\nCIFAR10 and CelebA-HQ 256 \u00d7 256 samples, Fig. 3 and Fig. 4 for LSUN 256 \u00d7 256 samples [71],\nand Appendix D for more.\n4.2\nReverse process parameterization and training objective ablation\nIn Table 2, we show the sample quality effects of reverse process parameterizations and training\nobjectives (Section 3.2). We \ufb01nd that the baseline option of predicting \u02dc\u00b5 works well only when\ntrained on the true variational bound instead of unweighted mean squared error, a simpli\ufb01ed objective\nakin to Eq. (14). We also see that learning reverse process variances (by incorporating a parameterized\ndiagonal \u03a3\u03b8(xt) into the variational bound) leads to unstable training and poorer sample quality\ncompared to \ufb01xed variances. Predicting \u03f5, as we proposed, performs approximately as well as\npredicting \u02dc\u00b5 when trained on the variational bound with \ufb01xed variances, but much better when trained\nwith our simpli\ufb01ed objective.\n4.3\nProgressive coding\nTable 1 also shows the codelengths of our CIFAR10 models. The gap between train and test is at\nmost 0.03 bits per dimension, which is comparable to the gaps reported with other likelihood-based\nmodels and indicates that our diffusion model is not over\ufb01tting (see Appendix D for nearest neighbor\nvisualizations). Still, while our lossless codelengths are better than the large estimates reported for\nenergy based models and score matching using annealed importance sampling [11], they are not\ncompetitive with other types of likelihood-based generative models [7].\nSince our samples are nonetheless of high quality, we conclude that diffusion models have an inductive\nbias that makes them excellent lossy compressors. Treating the variational bound terms L1 +\u00b7 \u00b7 \u00b7+LT\nas rate and L0 as distortion, our CIFAR10 model with the highest quality samples has a rate of 1.78\nbits/dim and a distortion of 1.97 bits/dim, which amounts to a root mean squared error of 0.95 on a\nscale from 0 to 255. More than half of the lossless codelength describes imperceptible distortions.\nProgressive lossy compression\nWe can probe further into the rate-distortion behavior of our model\nby introducing a progressive lossy code that mirrors the form of Eq. (5): see Algorithms 3 and 4,\nwhich assume access to a procedure, such as minimal random coding [19, 20], that can transmit a\nsample x \u223cq(x) using approximately DKL(q(x) \u2225p(x)) bits on average for any distributions p and\nq, for which only p is available to the receiver beforehand. When applied to x0 \u223cq(x0), Algorithms 3\nand 4 transmit xT , . . . , x0 in sequence using a total expected codelength equal to Eq. (5). The receiver,\n6\n\nat any time t, has the partial information xt fully available and can progressively estimate:\nx0 \u2248\u02c6x0 =\n\u0000xt \u2212\n\u221a\n1 \u2212\u00af\u03b1t\u03f5\u03b8(xt)\n\u0001\n/\u221a\u00af\u03b1t\n(15)\ndue to Eq. (4). (A stochastic reconstruction x0 \u223cp\u03b8(x0|xt) is also valid, but we do not consider\nit here because it makes distortion more dif\ufb01cult to evaluate.) Figure 5 shows the resulting rate-\ndistortion plot on the CIFAR10 test set. At each time t, the distortion is calculated as the root mean\nsquared error\np\n\u2225x0 \u2212\u02c6x0\u22252/D, and the rate is calculated as the cumulative number of bits received\nso far at time t. The distortion decreases steeply in the low-rate region of the rate-distortion plot,\nindicating that the majority of the bits are indeed allocated to imperceptible distortions.\n0\n200\n400\n600\n800 1,000\n0\n20\n40\n60\n80\nReverse process steps (T \u2212t)\nDistortion (RMSE)\n0\n200\n400\n600\n800 1,000\n0\n0.5\n1\n1.5\nReverse process steps (T \u2212t)\nRate (bits/dim)\n0\n0.5\n1\n1.5\n0\n20\n40\n60\n80\nRate (bits/dim)\nDistortion (RMSE)\nFigure 5: Unconditional CIFAR10 test set rate-distortion vs. time. Distortion is measured in root mean squared\nerror on a [0, 255] scale. See Table 4 for details.\nProgressive generation\nWe also run a progressive unconditional generation process given by\nprogressive decompression from random bits. In other words, we predict the result of the reverse\nprocess, \u02c6x0, while sampling from the reverse process using Algorithm 2. Figures 6 and 10 show the\nresulting sample quality of \u02c6x0 over the course of the reverse process. Large scale image features\nappear \ufb01rst and details appear last. Figure 7 shows stochastic predictions x0 \u223cp\u03b8(x0|xt) with xt\nfrozen for various t. When t is small, all but \ufb01ne details are preserved, and when t is large, only large\nscale features are preserved. Perhaps these are hints of conceptual compression [18].\nFigure 6: Unconditional CIFAR10 progressive generation (\u02c6x0 over time, from left to right). Extended samples\nand sample quality metrics over time in the appendix (Figs. 10 and 14).\nFigure 7: When conditioned on the same latent, CelebA-HQ 256 \u00d7 256 samples share high-level attributes.\nBottom-right quadrants are xt, and other quadrants are samples from p\u03b8(x0|xt).\nConnection to autoregressive decoding\nNote that the variational bound (5) can be rewritten as:\nL = DKL(q(xT ) \u2225p(xT )) + Eq\n\" X\nt\u22651\nDKL(q(xt\u22121|xt) \u2225p\u03b8(xt\u22121|xt))\n#\n+ H(x0)\n(16)\n(See Appendix A for a derivation.) Now consider setting the diffusion process length T to the\ndimensionality of the data, de\ufb01ning the forward process so that q(xt|x0) places all probability mass\non x0 with the \ufb01rst t coordinates masked out (i.e. q(xt|xt\u22121) masks out the tth coordinate), setting\np(xT ) to place all mass on a blank image, and, for the sake of argument, taking p\u03b8(xt\u22121|xt) to\n7\n\nFigure 8: Interpolations of CelebA-HQ 256x256 images with 500 timesteps of diffusion.\nbe a fully expressive conditional distribution. With these choices, DKL(q(xT ) \u2225p(xT )) = 0, and\nminimizing DKL(q(xt\u22121|xt) \u2225p\u03b8(xt\u22121|xt)) trains p\u03b8 to copy coordinates t + 1, . . . , T unchanged\nand to predict the tth coordinate given t + 1, . . . , T. Thus, training p\u03b8 with this particular diffusion is\ntraining an autoregressive model.\nWe can therefore interpret the Gaussian diffusion model (2) as a kind of autoregressive model with\na generalized bit ordering that cannot be expressed by reordering data coordinates. Prior work has\nshown that such reorderings introduce inductive biases that have an impact on sample quality [38],\nso we speculate that the Gaussian diffusion serves a similar purpose, perhaps to greater effect since\nGaussian noise might be more natural to add to images compared to masking noise. Moreover, the\nGaussian diffusion length is not restricted to equal the data dimension; for instance, we use T = 1000,\nwhich is less than the dimension of the 32 \u00d7 32 \u00d7 3 or 256 \u00d7 256 \u00d7 3 images in our experiments.\nGaussian diffusions can be made shorter for fast sampling or longer for model expressiveness.\n4.4\nInterpolation\nWe can interpolate source images x0, x\u2032\n0 \u223cq(x0) in latent space using q as a stochastic encoder,\nxt, x\u2032\nt \u223cq(xt|x0), then decoding the linearly interpolated latent \u00afxt = (1 \u2212\u03bb)x0 + \u03bbx\u2032\n0 into image\nspace by the reverse process, \u00afx0 \u223cp(x0|\u00afxt). In effect, we use the reverse process to remove\nartifacts from linearly interpolating corrupted versions of the source images, as depicted in Fig. 8\n(left). We \ufb01xed the noise for different values of \u03bb so xt and x\u2032\nt remain the same. Fig. 8 (right)\nshows interpolations and reconstructions of original CelebA-HQ 256 \u00d7 256 images (t = 500). The\nreverse process produces high-quality reconstructions, and plausible interpolations that smoothly\nvary attributes such as pose, skin tone, hairstyle, expression and background, but not eyewear. Larger\nt results in coarser and more varied interpolations, with novel samples at t = 1000 (Appendix Fig. 9).\n5\nRelated Work\nWhile diffusion models might resemble \ufb02ows [9, 46, 10, 32, 5, 16, 23] and VAEs [33, 47, 37],\ndiffusion models are designed so that q has no parameters and the top-level latent xT has nearly zero\nmutual information with the data x0. Our \u03f5-prediction reverse process parameterization establishes a\nconnection between diffusion models and denoising score matching over multiple noise levels with\nannealed Langevin dynamics for sampling [55, 56]. Diffusion models, however, admit straightforward\nlog likelihood evaluation, and the training procedure explicitly trains the Langevin dynamics sampler\nusing variational inference (see Appendix C for details). The connection also has the reverse\nimplication that a certain weighted form of denoising score matching is the same as variational\ninference to train a Langevin-like sampler. Other methods for learning transition operators of Markov\nchains include infusion training [2], variational walkback [15], generative stochastic networks [1],\nand others [50, 54, 36, 42, 35, 65].\nBy the known connection between score matching and energy-based modeling, our work could have\nimplications for other recent work on energy-based models [67\u201369, 12, 70, 13, 11, 41, 17, 8]. Our\nrate-distortion curves are computed over time in one evaluation of the variational bound, reminiscent\nof how rate-distortion curves can be computed over distortion penalties in one run of annealed\nimportance sampling [24]. Our progressive decoding argument can be seen in convolutional DRAW\nand related models [18, 40] and may also lead to more general designs for subscale orderings or\nsampling strategies for autoregressive models [38, 64].\n8\n\n6\nConclusion\nWe have presented high quality image samples using diffusion models, and we have found connections\namong diffusion models and variational inference for training Markov chains, denoising score\nmatching and annealed Langevin dynamics (and energy-based models by extension), autoregressive\nmodels, and progressive lossy compression. Since diffusion models seem to have excellent inductive\nbiases for image data, we look forward to investigating their utility in other data modalities and as\ncomponents in other types of generative models and machine learning systems.\nBroader Impact\nOur work on diffusion models takes on a similar scope as existing work on other types of deep\ngenerative models, such as efforts to improve the sample quality of GANs, \ufb02ows, autoregressive\nmodels, and so forth. Our paper represents progress in making diffusion models a generally useful\ntool in this family of techniques, so it may serve to amplify any impacts that generative models have\nhad (and will have) on the broader world.\nUnfortunately, there are numerous well-known malicious uses of generative models. Sample gen-\neration techniques can be employed to produce fake images and videos of high pro\ufb01le \ufb01gures for\npolitical purposes. While fake images were manually created long before software tools were avail-\nable, generative models such as ours make the process easier. Fortunately, CNN-generated images\ncurrently have subtle \ufb02aws that allow detection [62], but improvements in generative models may\nmake this more dif\ufb01cult. Generative models also re\ufb02ect the biases in the datasets on which they\nare trained. As many large datasets are collected from the internet by automated systems, it can be\ndif\ufb01cult to remove these biases, especially when the images are unlabeled. If samples from generative\nmodels trained on these datasets proliferate throughout the internet, then these biases will only be\nreinforced further.\nOn the other hand, diffusion models may be useful for data compression, which, as data becomes\nhigher resolution and as global internet traf\ufb01c increases, might be crucial to ensure accessibility of\nthe internet to wide audiences. Our work might contribute to representation learning on unlabeled\nraw data for a large range of downstream tasks, from image classi\ufb01cation to reinforcement learning,\nand diffusion models might also become viable for creative uses in art, photography, and music.\nAcknowledgments and Disclosure of Funding\nThis work was supported by ONR PECASE and the NSF Graduate Research Fellowship under grant\nnumber DGE-1752814. Google\u2019s TensorFlow Research Cloud (TFRC) provided Cloud TPUs.\nReferences\n[1] Guillaume Alain, Yoshua Bengio, Li Yao, Jason Yosinski, Eric Thibodeau-Laufer, Saizheng Zhang, and\nPascal Vincent. GSNs: generative stochastic networks. Information and Inference: A Journal of the IMA,\n5(2):210\u2013249, 2016.\n[2] Florian Bordes, Sina Honari, and Pascal Vincent. Learning to generate samples from noise through infusion\ntraining. In International Conference on Learning Representations, 2017.\n[3] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high \ufb01delity natural\nimage synthesis. In International Conference on Learning Representations, 2019.\n[4] Tong Che, Ruixiang Zhang, Jascha Sohl-Dickstein, Hugo Larochelle, Liam Paull, Yuan Cao, and Yoshua\nBengio. Your GAN is secretly an energy-based model and you should use discriminator driven latent\nsampling. arXiv preprint arXiv:2003.06060, 2020.\n[5] Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential\nequations. In Advances in Neural Information Processing Systems, pages 6571\u20136583, 2018.\n[6] Xi Chen, Nikhil Mishra, Mostafa Rohaninejad, and Pieter Abbeel. PixelSNAIL: An improved autoregres-\nsive generative model. In International Conference on Machine Learning, pages 863\u2013871, 2018.\n[7] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\ntransformers. arXiv preprint arXiv:1904.10509, 2019.\n9\n\n[8] Yuntian Deng, Anton Bakhtin, Myle Ott, Arthur Szlam, and Marc\u2019Aurelio Ranzato. Residual energy-based\nmodels for text generation. arXiv preprint arXiv:2004.11714, 2020.\n[9] Laurent Dinh, David Krueger, and Yoshua Bengio. NICE: Non-linear independent components estimation.\narXiv preprint arXiv:1410.8516, 2014.\n[10] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using Real NVP. arXiv\npreprint arXiv:1605.08803, 2016.\n[11] Yilun Du and Igor Mordatch. Implicit generation and modeling with energy based models. In Advances in\nNeural Information Processing Systems, pages 3603\u20133613, 2019.\n[12] Ruiqi Gao, Yang Lu, Junpei Zhou, Song-Chun Zhu, and Ying Nian Wu. Learning generative ConvNets\nvia multi-grid modeling and sampling. In Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 9155\u20139164, 2018.\n[13] Ruiqi Gao, Erik Nijkamp, Diederik P Kingma, Zhen Xu, Andrew M Dai, and Ying Nian Wu. Flow\ncontrastive estimation of energy-based models. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 7518\u20137528, 2020.\n[14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing\nSystems, pages 2672\u20132680, 2014.\n[15] Anirudh Goyal, Nan Rosemary Ke, Surya Ganguli, and Yoshua Bengio. Variational walkback: Learning a\ntransition operator as a stochastic recurrent net. In Advances in Neural Information Processing Systems,\npages 4392\u20134402, 2017.\n[16] Will Grathwohl, Ricky T. Q. Chen, Jesse Bettencourt, and David Duvenaud.\nFFJORD: Free-form\ncontinuous dynamics for scalable reversible generative models. In International Conference on Learning\nRepresentations, 2019.\n[17] Will Grathwohl, Kuan-Chieh Wang, Joern-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi, and\nKevin Swersky. Your classi\ufb01er is secretly an energy based model and you should treat it like one. In\nInternational Conference on Learning Representations, 2020.\n[18] Karol Gregor, Frederic Besse, Danilo Jimenez Rezende, Ivo Danihelka, and Daan Wierstra. Towards\nconceptual compression. In Advances In Neural Information Processing Systems, pages 3549\u20133557, 2016.\n[19] Prahladh Harsha, Rahul Jain, David McAllester, and Jaikumar Radhakrishnan. The communication\ncomplexity of correlation. In Twenty-Second Annual IEEE Conference on Computational Complexity\n(CCC\u201907), pages 10\u201323. IEEE, 2007.\n[20] Marton Havasi, Robert Peharz, and Jos\u00e9 Miguel Hern\u00e1ndez-Lobato. Minimal random code learning:\nGetting bits back from compressed model parameters. In International Conference on Learning Represen-\ntations, 2019.\n[21] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs\ntrained by a two time-scale update rule converge to a local Nash equilibrium. In Advances in Neural\nInformation Processing Systems, pages 6626\u20136637, 2017.\n[22] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mo-\nhamed, and Alexander Lerchner. beta-VAE: Learning basic visual concepts with a constrained variational\nframework. In International Conference on Learning Representations, 2017.\n[23] Jonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel. Flow++: Improving \ufb02ow-based\ngenerative models with variational dequantization and architecture design. In International Conference on\nMachine Learning, 2019.\n[24] Sicong Huang, Alireza Makhzani, Yanshuai Cao, and Roger Grosse. Evaluating lossy compression rates of\ndeep generative models. In International Conference on Machine Learning, 2020.\n[25] Nal Kalchbrenner, Aaron van den Oord, Karen Simonyan, Ivo Danihelka, Oriol Vinyals, Alex Graves, and\nKoray Kavukcuoglu. Video pixel networks. In International Conference on Machine Learning, pages\n1771\u20131779, 2017.\n[26] Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman Casagrande, Edward Lockhart,\nFlorian Stimberg, Aaron van den Oord, Sander Dieleman, and Koray Kavukcuoglu. Ef\ufb01cient neural audio\nsynthesis. In International Conference on Machine Learning, pages 2410\u20132419, 2018.\n[27] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of GANs for improved\nquality, stability, and variation. In International Conference on Learning Representations, 2018.\n[28] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial\nnetworks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages\n10\n\n4401\u20134410, 2019.\n[29] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training\ngenerative adversarial networks with limited data. arXiv preprint arXiv:2006.06676v1, 2020.\n[30] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and\nimproving the image quality of StyleGAN. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 8110\u20138119, 2020.\n[31] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International\nConference on Learning Representations, 2015.\n[32] Diederik P Kingma and Prafulla Dhariwal. Glow: Generative \ufb02ow with invertible 1x1 convolutions. In\nAdvances in Neural Information Processing Systems, pages 10215\u201310224, 2018.\n[33] Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. arXiv preprint arXiv:1312.6114,\n2013.\n[34] Diederik P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improved\nvariational inference with inverse autoregressive \ufb02ow. In Advances in Neural Information Processing\nSystems, pages 4743\u20134751, 2016.\n[35] John Lawson, George Tucker, Bo Dai, and Rajesh Ranganath. Energy-inspired models: Learning with\nsampler-induced distributions. In Advances in Neural Information Processing Systems, pages 8501\u20138513,\n2019.\n[36] Daniel Levy, Matt D. Hoffman, and Jascha Sohl-Dickstein. Generalizing Hamiltonian Monte Carlo with\nneural networks. In International Conference on Learning Representations, 2018.\n[37] Lars Maal\u00f8e, Marco Fraccaro, Valentin Li\u00e9vin, and Ole Winther. BIVA: A very deep hierarchy of\nlatent variables for generative modeling. In Advances in Neural Information Processing Systems, pages\n6548\u20136558, 2019.\n[38] Jacob Menick and Nal Kalchbrenner. Generating high \ufb01delity images with subscale pixel networks and\nmultidimensional upscaling. In International Conference on Learning Representations, 2019.\n[39] Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for\ngenerative adversarial networks. In International Conference on Learning Representations, 2018.\n[40] Alex Nichol. VQ-DRAW: A sequential discrete VAE. arXiv preprint arXiv:2003.01599, 2020.\n[41] Erik Nijkamp, Mitch Hill, Tian Han, Song-Chun Zhu, and Ying Nian Wu. On the anatomy of MCMC-based\nmaximum likelihood learning of energy-based models. arXiv preprint arXiv:1903.12370, 2019.\n[42] Erik Nijkamp, Mitch Hill, Song-Chun Zhu, and Ying Nian Wu. Learning non-convergent non-persistent\nshort-run MCMC toward energy-based model. In Advances in Neural Information Processing Systems,\npages 5233\u20135243, 2019.\n[43] Georg Ostrovski, Will Dabney, and Remi Munos. Autoregressive quantile networks for generative modeling.\nIn International Conference on Machine Learning, pages 3936\u20133945, 2018.\n[44] Ryan Prenger, Rafael Valle, and Bryan Catanzaro. WaveGlow: A \ufb02ow-based generative network for\nspeech synthesis. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 3617\u20133621. IEEE, 2019.\n[45] Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-\ufb01delity images with VQ-\nVAE-2. In Advances in Neural Information Processing Systems, pages 14837\u201314847, 2019.\n[46] Danilo Rezende and Shakir Mohamed. Variational inference with normalizing \ufb02ows. In International\nConference on Machine Learning, pages 1530\u20131538, 2015.\n[47] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and approx-\nimate inference in deep generative models. In International Conference on Machine Learning, pages\n1278\u20131286, 2014.\n[48] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional networks for biomedical\nimage segmentation. In International Conference on Medical Image Computing and Computer-Assisted\nIntervention, pages 234\u2013241. Springer, 2015.\n[49] Tim Salimans and Durk P Kingma. Weight normalization: A simple reparameterization to accelerate\ntraining of deep neural networks. In Advances in Neural Information Processing Systems, pages 901\u2013909,\n2016.\n[50] Tim Salimans, Diederik Kingma, and Max Welling. Markov Chain Monte Carlo and variational inference:\nBridging the gap. In International Conference on Machine Learning, pages 1218\u20131226, 2015.\n11\n\n[51] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved\ntechniques for training gans. In Advances in Neural Information Processing Systems, pages 2234\u20132242,\n2016.\n[52] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. PixelCNN++: Improving the PixelCNN\nwith discretized logistic mixture likelihood and other modi\ufb01cations. In International Conference on\nLearning Representations, 2017.\n[53] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised\nlearning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pages\n2256\u20132265, 2015.\n[54] Jiaming Song, Shengjia Zhao, and Stefano Ermon. A-NICE-MC: Adversarial training for MCMC. In\nAdvances in Neural Information Processing Systems, pages 5140\u20135150, 2017.\n[55] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In\nAdvances in Neural Information Processing Systems, pages 11895\u201311907, 2019.\n[56] Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. arXiv\npreprint arXiv:2006.09011, 2020.\n[57] Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal\nKalchbrenner, Andrew Senior, and Koray Kavukcuoglu. WaveNet: A generative model for raw audio.\narXiv preprint arXiv:1609.03499, 2016.\n[58] Aaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks.\nInternational Conference on Machine Learning, 2016.\n[59] Aaron van den Oord, Nal Kalchbrenner, Oriol Vinyals, Lasse Espeholt, Alex Graves, and Koray\nKavukcuoglu. Conditional image generation with PixelCNN decoders. In Advances in Neural Information\nProcessing Systems, pages 4790\u20134798, 2016.\n[60] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing\nSystems, pages 5998\u20136008, 2017.\n[61] Pascal Vincent. A connection between score matching and denoising autoencoders. Neural Computation,\n23(7):1661\u20131674, 2011.\n[62] Sheng-Yu Wang, Oliver Wang, Richard Zhang, Andrew Owens, and Alexei A Efros. Cnn-generated images\nare surprisingly easy to spot...for now. In Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, 2020.\n[63] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 7794\u20137803,\n2018.\n[64] Auke J Wiggers and Emiel Hoogeboom. Predictive sampling with forecasting autoregressive models.\narXiv preprint arXiv:2002.09928, 2020.\n[65] Hao Wu, Jonas K\u00f6hler, and Frank No\u00e9. Stochastic normalizing \ufb02ows. arXiv preprint arXiv:2002.06707,\n2020.\n[66] Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European Conference on Computer\nVision (ECCV), pages 3\u201319, 2018.\n[67] Jianwen Xie, Yang Lu, Song-Chun Zhu, and Yingnian Wu. A theory of generative convnet. In International\nConference on Machine Learning, pages 2635\u20132644, 2016.\n[68] Jianwen Xie, Song-Chun Zhu, and Ying Nian Wu. Synthesizing dynamic patterns by spatial-temporal\ngenerative convnet. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\npages 7093\u20137101, 2017.\n[69] Jianwen Xie, Zilong Zheng, Ruiqi Gao, Wenguan Wang, Song-Chun Zhu, and Ying Nian Wu. Learning\ndescriptor networks for 3d shape synthesis and analysis. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, pages 8629\u20138638, 2018.\n[70] Jianwen Xie, Song-Chun Zhu, and Ying Nian Wu. Learning energy-based spatial-temporal generative\nconvnets for dynamic patterns. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2019.\n[71] Fisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao. LSUN: Construction of a large-scale\nimage dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015.\n[72] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146,\n2016.\n12\n\nExtra information\nLSUN\nFID scores for LSUN datasets are included in Table 3. Scores marked with \u2217are reported\nby StyleGAN2 as baselines, and other scores are reported by their respective authors.\nTable 3: FID scores for LSUN 256 \u00d7 256 datasets\nModel\nLSUN Bedroom\nLSUN Church\nLSUN Cat\nProgressiveGAN [27]\n8.34\n6.42\n37.52\nStyleGAN [28]\n2.65\n4.21\u2217\n8.53\u2217\nStyleGAN2 [30]\n-\n3.86\n6.93\nOurs (Lsimple)\n6.36\n7.89\n19.75\nOurs (Lsimple, large)\n4.90\n-\n-\nProgressive compression\nOur lossy compression argument in Section 4.3 is only a proof of concept,\nbecause Algorithms 3 and 4 depend on a procedure such as minimal random coding [20], which is\nnot tractable for high dimensional data. These algorithms serve as a compression interpretation of the\nvariational bound (5) of Sohl-Dickstein et al. [53], not yet as a practical compression system.\nTable 4: Unconditional CIFAR10 test set rate-distortion values (accompanies Fig. 5)\nReverse process time (T \u2212t + 1)\nRate (bits/dim)\nDistortion (RMSE [0, 255])\n1000\n1.77581\n0.95136\n900\n0.11994\n12.02277\n800\n0.05415\n18.47482\n700\n0.02866\n24.43656\n600\n0.01507\n30.80948\n500\n0.00716\n38.03236\n400\n0.00282\n46.12765\n300\n0.00081\n54.18826\n200\n0.00013\n60.97170\n100\n0.00000\n67.60125\nA\nExtended derivations\nBelow is a derivation of Eq. (5), the reduced variance variational bound for diffusion models. This\nmaterial is from Sohl-Dickstein et al. [53]; we include it here only for completeness.\nL = Eq\n\u0014\n\u2212log p\u03b8(x0:T )\nq(x1:T |x0)\n\u0015\n(17)\n= Eq\n\uf8ee\n\uf8f0\u2212log p(xT ) \u2212\nX\nt\u22651\nlog p\u03b8(xt\u22121|xt)\nq(xt|xt\u22121)\n\uf8f9\n\uf8fb\n(18)\n= Eq\n\"\n\u2212log p(xT ) \u2212\nX\nt>1\nlog p\u03b8(xt\u22121|xt)\nq(xt|xt\u22121) \u2212log p\u03b8(x0|x1)\nq(x1|x0)\n#\n(19)\n= Eq\n\"\n\u2212log p(xT ) \u2212\nX\nt>1\nlog\np\u03b8(xt\u22121|xt)\nq(xt\u22121|xt, x0) \u00b7 q(xt\u22121|x0)\nq(xt|x0)\n\u2212log p\u03b8(x0|x1)\nq(x1|x0)\n#\n(20)\n= Eq\n\"\n\u2212log\np(xT )\nq(xT |x0) \u2212\nX\nt>1\nlog\np\u03b8(xt\u22121|xt)\nq(xt\u22121|xt, x0) \u2212log p\u03b8(x0|x1)\n#\n(21)\n13\n\n= Eq\n\"\nDKL(q(xT |x0) \u2225p(xT )) +\nX\nt>1\nDKL(q(xt\u22121|xt, x0) \u2225p\u03b8(xt\u22121|xt)) \u2212log p\u03b8(x0|x1)\n#\n(22)\nThe following is an alternate version of L. It is not tractable to estimate, but it is useful for our\ndiscussion in Section 4.3.\nL = Eq\n\uf8ee\n\uf8f0\u2212log p(xT ) \u2212\nX\nt\u22651\nlog p\u03b8(xt\u22121|xt)\nq(xt|xt\u22121)\n\uf8f9\n\uf8fb\n(23)\n= Eq\n\uf8ee\n\uf8f0\u2212log p(xT ) \u2212\nX\nt\u22651\nlog p\u03b8(xt\u22121|xt)\nq(xt\u22121|xt) \u00b7 q(xt\u22121)\nq(xt)\n\uf8f9\n\uf8fb\n(24)\n= Eq\n\uf8ee\n\uf8f0\u2212log p(xT )\nq(xT ) \u2212\nX\nt\u22651\nlog p\u03b8(xt\u22121|xt)\nq(xt\u22121|xt) \u2212log q(x0)\n\uf8f9\n\uf8fb\n(25)\n= DKL(q(xT ) \u2225p(xT )) + Eq\n\uf8ee\n\uf8f0X\nt\u22651\nDKL(q(xt\u22121|xt) \u2225p\u03b8(xt\u22121|xt))\n\uf8f9\n\uf8fb+ H(x0)\n(26)\nB\nExperimental details\nOur neural network architecture follows the backbone of PixelCNN++ [52], which is a U-Net [48]\nbased on a Wide ResNet [72]. We replaced weight normalization [49] with group normalization [66]\nto make the implementation simpler. Our 32 \u00d7 32 models use four feature map resolutions (32 \u00d7 32\nto 4 \u00d7 4), and our 256 \u00d7 256 models use six. All models have two convolutional residual blocks\nper resolution level and self-attention blocks at the 16 \u00d7 16 resolution between the convolutional\nblocks [6]. Diffusion time t is speci\ufb01ed by adding the Transformer sinusoidal position embedding [60]\ninto each residual block. Our CIFAR10 model has 35.7 million parameters, and our LSUN and\nCelebA-HQ models have 114 million parameters. We also trained a larger variant of the LSUN\nBedroom model with approximately 256 million parameters by increasing \ufb01lter count.\nWe used TPU v3-8 (similar to 8 V100 GPUs) for all experiments. Our CIFAR model trains at 21\nsteps per second at batch size 128 (10.6 hours to train to completion at 800k steps), and sampling\na batch of 256 images takes 17 seconds. Our CelebA-HQ/LSUN (2562) models train at 2.2 steps\nper second at batch size 64, and sampling a batch of 128 images takes 300 seconds. We trained on\nCelebA-HQ for 0.5M steps, LSUN Bedroom for 2.4M steps, LSUN Cat for 1.8M steps, and LSUN\nChurch for 1.2M steps. The larger LSUN Bedroom model was trained for 1.15M steps.\nApart from an initial choice of hyperparameters early on to make network size \ufb01t within memory\nconstraints, we performed the majority of our hyperparameter search to optimize for CIFAR10 sample\nquality, then transferred the resulting settings over to the other datasets:\n\u2022 We chose the \u03b2t schedule from a set of constant, linear, and quadratic schedules, all\nconstrained so that LT \u22480. We set T = 1000 without a sweep, and we chose a linear\nschedule from \u03b21 = 10\u22124 to \u03b2T = 0.02.\n\u2022 We set the dropout rate on CIFAR10 to 0.1 by sweeping over the values {0.1, 0.2, 0.3, 0.4}.\nWithout dropout on CIFAR10, we obtained poorer samples reminiscent of the over\ufb01tting\nartifacts in an unregularized PixelCNN++ [52]. We set dropout rate on the other datasets to\nzero without sweeping.\n\u2022 We used random horizontal \ufb02ips during training for CIFAR10; we tried training both with\nand without \ufb02ips, and found \ufb02ips to improve sample quality slightly. We also used random\nhorizontal \ufb02ips for all other datasets except LSUN Bedroom.\n\u2022 We tried Adam [31] and RMSProp early on in our experimentation process and chose the\nformer. We left the hyperparameters to their standard values. We set the learning rate to\n2 \u00d7 10\u22124 without any sweeping, and we lowered it to 2 \u00d7 10\u22125 for the 256 \u00d7 256 images,\nwhich seemed unstable to train with the larger learning rate.\n14\n\n\u2022 We set the batch size to 128 for CIFAR10 and 64 for larger images. We did not sweep over\nthese values.\n\u2022 We used EMA on model parameters with a decay factor of 0.9999. We did not sweep over\nthis value.\nFinal experiments were trained once and evaluated throughout training for sample quality. Sample\nquality scores and log likelihood are reported on the minimum FID value over the course of training.\nOn CIFAR10, we calculated Inception and FID scores on 50000 samples using the original code\nfrom the OpenAI [51] and TTUR [21] repositories, respectively. On LSUN, we calculated FID\nscores on 50000 samples using code from the StyleGAN2 [30] repository. CIFAR10 and CelebA-HQ\nwere loaded as provided by TensorFlow Datasets (https://www.tensorflow.org/datasets),\nand LSUN was prepared using code from StyleGAN. Dataset splits (or lack thereof) are standard\nfrom the papers that introduced their usage in a generative modeling context. All details can be found\nin the source code release.\nC\nDiscussion on related work\nOur model architecture, forward process de\ufb01nition, and prior differ from NCSN [55, 56] in subtle but\nimportant ways that improve sample quality, and, notably, we directly train our sampler as a latent\nvariable model rather than adding it after training post-hoc. In greater detail:\n1. We use a U-Net with self-attention; NCSN uses a Re\ufb01neNet with dilated convolutions. We\ncondition all layers on t by adding in the Transformer sinusoidal position embedding, rather\nthan only in normalization layers (NCSNv1) or only at the output (v2).\n2. Diffusion models scale down the data with each forward process step (by a \u221a1 \u2212\u03b2t factor)\nso that variance does not grow when adding noise, thus providing consistently scaled inputs\nto the neural net reverse process. NCSN omits this scaling factor.\n3. Unlike NCSN, our forward process destroys signal (DKL(q(xT |x0) \u2225N(0, I)) \u22480), ensur-\ning a close match between the prior and aggregate posterior of xT . Also unlike NCSN, our\n\u03b2t are very small, which ensures that the forward process is reversible by a Markov chain\nwith conditional Gaussians. Both of these factors prevent distribution shift when sampling.\n4. Our Langevin-like sampler has coef\ufb01cients (learning rate, noise scale, etc.) derived rig-\norously from \u03b2t in the forward process. Thus, our training procedure directly trains our\nsampler to match the data distribution after T steps: it trains the sampler as a latent variable\nmodel using variational inference. In contrast, NCSN\u2019s sampler coef\ufb01cients are set by hand\npost-hoc, and their training procedure is not guaranteed to directly optimize a quality metric\nof their sampler.\nD\nSamples\nAdditional samples\nFigure 11, 13, 16, 17, 18, and 19 show uncurated samples from the diffusion\nmodels trained on CelebA-HQ, CIFAR10 and LSUN datasets.\nLatent structure and reverse process stochasticity\nDuring sampling, both the prior xT \u223c\nN(0, I) and Langevin dynamics are stochastic. To understand the signi\ufb01cance of the second source\nof noise, we sampled multiple images conditioned on the same intermediate latent for the CelebA\n256 \u00d7 256 dataset. Figure 7 shows multiple draws from the reverse process x0 \u223cp\u03b8(x0|xt) that\nshare the latent xt for t \u2208{1000, 750, 500, 250}. To accomplish this, we run a single reverse chain\nfrom an initial draw from the prior. At the intermediate timesteps, the chain is split to sample multiple\nimages. When the chain is split after the prior draw at xT =1000, the samples differ signi\ufb01cantly.\nHowever, when the chain is split after more steps, samples share high-level attributes like gender,\nhair color, eyewear, saturation, pose and facial expression. This indicates that intermediate latents\nlike x750 encode these attributes, despite their imperceptibility.\nCoarse-to-\ufb01ne interpolation\nFigure 9 shows interpolations between a pair of source CelebA\n256 \u00d7 256 images as we vary the number of diffusion steps prior to latent space interpolation.\nIncreasing the number of diffusion steps destroys more structure in the source images, which the\n15\n\nmodel completes during the reverse process. This allows us to interpolate at both \ufb01ne granularities\nand coarse granularities. In the limiting case of 0 diffusion steps, the interpolation mixes source\nimages in pixel space. On the other hand, after 1000 diffusion steps, source information is lost and\ninterpolations are novel samples.\nSource Rec. \u03bb=0.1 \u03bb=0.2 \u03bb=0.3 \u03bb=0.4 \u03bb=0.5 \u03bb=0.6 \u03bb=0.7 \u03bb=0.8 \u03bb=0.9\nRec. Source\n1000 steps\n875 steps\n750 steps\n625 steps\n500 steps\n375 steps\n250 steps\n125 steps\n0 steps\nFigure 9: Coarse-to-\ufb01ne interpolations that vary the number of diffusion steps prior to latent mixing.\n0\n200\n400\n600\n800 1,000\n2\n4\n6\n8\n10\nReverse process steps (T \u2212t)\nInception Score\n0\n200\n400\n600\n800 1,000\n0\n100\n200\n300\nReverse process steps (T \u2212t)\nFID\nFigure 10: Unconditional CIFAR10 progressive sampling quality over time\n16\n\nFigure 11: CelebA-HQ 256 \u00d7 256 generated samples\n17\n\n(a) Pixel space nearest neighbors\n(b) Inception feature space nearest neighbors\nFigure 12: CelebA-HQ 256 \u00d7 256 nearest neighbors, computed on a 100 \u00d7 100 crop surrounding the\nfaces. Generated samples are in the leftmost column, and training set nearest neighbors are in the\nremaining columns.\n18\n\nFigure 13: Unconditional CIFAR10 generated samples\n19\n\nFigure 14: Unconditional CIFAR10 progressive generation\n20\n\n(a) Pixel space nearest neighbors\n(b) Inception feature space nearest neighbors\nFigure 15: Unconditional CIFAR10 nearest neighbors. Generated samples are in the leftmost column,\nand training set nearest neighbors are in the remaining columns.\n21\n\nFigure 16: LSUN Church generated samples. FID=7.89\n22\n\nFigure 17: LSUN Bedroom generated samples, large model. FID=4.90\n23\n\nFigure 18: LSUN Bedroom generated samples, small model. FID=6.36\n24\n\nFigure 19: LSUN Cat generated samples. FID=19.75\n25\n",
    "fill\nGenerative AI at Work\u02da\nErik Brynjolfsson\nDanielle Li\nLindsey Raymond\nStanford & NBER\nMIT & NBER\nMIT\nNovember 7, 2024\nFirst Draft: 23 April 2023\nAbstract\nWe study the staggered introduction of a generative AI-based conversational assistant using\ndata from 5,172 customer support agents. Access to AI assistance increases worker productivity,\nas measured by issues resolved per hour, by 15% on average, with substantial heterogeneity\nacross workers. Less experienced and lower-skilled workers improve both the speed and quality of\ntheir output while the most experienced and highest-skilled workers see small gains in speed and\nsmall declines in quality. We also find evidence that AI assistance facilitates worker learning and\nimproves English fluency, particularly among international agents. While AI systems improve\nwith more training data, we find that the gains from AI adoption are largest for relatively\nrare problems, where human agents have less baseline training and experience.\nFinally, we\nprovide evidence that AI assistance improves the experience of work along two key dimensions:\ncustomers are more polite and less likely to ask to speak to a manager.\nJEL Classifications: D80, J24, M15, M51, O33\nKeywords: Generative AI, Large Language Models, Technology Adoption, Worker Productiv-\nity, Worker Learning, Experience of Work, Organizational Design.\n\u02daCorrespondence to erikb@stanford.edu, d_li@mit.edu, and lraymond@mit.edu. We are grateful to Daron Acemoglu, David\nAutor, Amittai Axelrod, Eleanor Dillon, Zayd Enam, Luis Garicano, Alex Frankel, Sam Manning, Sendhil Mullainathan, Emma\nPierson, Scott Stern, Ashesh Rambachan, John Van Reenen, Raffaella Sadun, Kathryn Shaw, Christopher Stanton, Sebastian\nThrun, and various seminar participants for helpful comments and suggestions. We thank Max Feng for providing excellent\nresearch assistance and the Stanford Digital Economy Lab for funding. The content is solely the responsibility of the authors\nand does not necessarily represent the official views of Stanford University, MIT, or the NBER.\narXiv:2304.11771v2  [econ.GN]  6 Nov 2024\n\nThe emergence of generative artificial intelligence (AI) has attracted significant attention, but\nfew studies have examined its economic impact. Although various generative AI tools have per-\nformed well in laboratory settings, excitement about their potential has been tempered by concerns\nthat these tools may be less effective in real-world settings, where they may encounter unfamil-\niar problems, face organizational resistance, or provide misleading information in a consequential\nenvironment (Peng et al., 2023a; Roose, 2023).\nIn this paper, we study the adoption of a generative AI tool that provides conversational guidance\nto customer support agents.1 This is, to our knowledge, the first study of the impact of generative AI\ndeployed at scale in the workplace. We find that access to AI assistance increases the productivity\nof agents by 15%, as measured by the number of customer issues they are able to resolve per hour.\nWe find that these gains accrue disproportionately to less-experienced and lower-skill customer\nsupport workers. This finding suggests that generative AI systems may be capable of capturing and\ndisseminating the behaviors of the most productive agents.\nComputers and software have transformed the economy with their ability to perform certain\ntasks with far more precision, speed, and consistency than humans. To be effective, these sys-\ntems typically require explicit and detailed instructions for how to transform inputs into outputs:\na software engineer must literally program the computer. Yet, despite significant advancements\nin traditional computing, many workplace activities\u2014such as writing emails, analyzing data, or\ncreating presentations\u2014are difficult to \u201ccodify\u201d\u2014and have therefore defied automation.\nMachine learning (ML) algorithms work differently from traditional computer programs: instead\nof requiring explicit instructions to function, these systems infer instructions from examples. Given\na training set of images, for instance, ML systems can learn to recognize specific individuals even\nthough one cannot fully explain what physical features characterize a given person\u2019s identity. This\nability highlights a key distinguishing aspect of ML systems: they can learn to perform tasks even\nwhen no instructions exist\u2014including tasks requiring tacit knowledge that could previously only be\ngained through lived experience (Polanyi, 1966; Autor, 2014; Brynjolfsson and Mitchell, 2017).2\n1A note on terminology. There are many definitions of artificial intelligence and of intelligence itself\u2014Legg et al.\n(2007) list over 70 of them. In this paper, we define \u201cartificial intelligence\u201d (AI) as an umbrella term that refers to\nsystems that exhibit intelligent behavior, such as learning reasoning and problem-solving. \u201cMachine learning\u201d (ML) is\na branch of AI that uses algorithms to learn from data, identify patterns, and make predictions or decisions without\nbeing explicitly programmed (Google, n.d.). Large language models (LLMs) and tools built around LLMs such as\nChatGPT are an increasingly important application of machine learning. LLMs generate new content, making them\na form of \u201cgenerative AI.\u201d \u201cGenerative AI\u201d is a type of artificial intelligence that can create new content, such as text,\nimages, or music, by learning patterns from existing data.\n2As Meijer (2018) puts it \u201cwhere the Software 1.0 Engineer formally specifies their problem, carefully designs algo-\nrithms, composes systems out of subsystems or decomposes complex systems into smaller components, the Software\n2.0 Engineer amasses training data and simply feeds it into an ML algorithm...\u201d\n1\n\nIn addition, ML systems are often trained on data from human workers, who naturally vary\nin their abilities.\nBy seeing many examples of tasks\u2014making sales pitches, driving a truck, or\ndiagnosing a patient, to name a few\u2014performed well and poorly, these models can implicitly learn\nwhat specific behaviors and characteristics set high-performing workers apart from their less effective\ncounterparts. That is, not only are generative AI models capable of performing complex tasks, they\nmight also be capable of capturing the skills that distinguish top workers. The use of ML tools may\ntherefore differentially expose lower-skill workers to new skills and techniques, leading to disparate\nchanges in productivity even among workers performing the same task.\nWe study the impact of generative AI on productivity and worker experience in the customer\nservice sector, an industry with one of the highest rates of AI adoption (Chui et al., 2021). We\nexamine the staggered deployment of a chat assistant using data from 5,000 agents working for a\nFortune 500 software firm that provides business process software. The tool we study is built on\na recent version of the Generative Pre-trained Transformer (GPT) family of large language models\ndeveloped by OpenAI (OpenAI, 2023). It monitors customer chats and provides agents with real-\ntime suggestions for how to respond. It is designed to augment agents, who remain responsible for\nthe conversation and are free to ignore or edit the AI\u2019s suggestions.\nWe have three sets of findings.\nFirst, AI assistance increases worker productivity, resulting in a 15% increase in the number of\nchats that an agent successfully resolves per hour. This increase reflects shifts in three components\nof productivity: a decline in the time it takes an agent to handle an individual chat, an increase in\nthe number of chats that an agent handles per hour (agents may handle multiple chats at once),\nand a small increase in the share of chats that are successfully resolved.\nWithin customer support workers, the productivity impacts of AI assistance are highly uneven.\nWe find that less-skilled and less-experienced workers improve significantly across all productivity\nmeasures we consider, including approximate increase of 30% in the number of issues they are\nable to resolve per hour. Access to the AI tool helps newer agents move more quickly down the\nexperience curve: treated agents with two months of tenure perform just as well as untreated agents\nwith more than six months of tenure. In contrast, we find minimal impacts on the productivity of\nmore-experienced or more-skilled workers. Indeed, we find evidence that AI assistance leads to a\nsmall decrease in the quality of conversations generated by the most skilled agents. These results\ncontrast, in spirit, with studies that find evidence of skill-biased technical change for earlier waves\nof computer technology and robotics (Bresnahan et al., 2002; Bartel et al., 2007; Dixon et al., 2020).\nOur second set of results investigates the mechanism underlying our main findings. We show that\nAI recommendations appear useful to workers: agents who follow recommendations more closely\n2\n\nsee larger gains in productivity, and adherence rates increase over time, particularly those who\nwere initially more skeptical. We also find that engagement with AI recommendations can generate\ndurable learning. Using data on software outages\u2014periods in which the AI software fails to provide\nany suggestions\u2014we show that workers see productivity gains relative to their pre-AI baseline\neven when recommendations are unavailable. These outage-period gains are more pronounced for\nworkers with more prior exposure to AI assistance, and especially those who had been following AI\nrecommendations more closely. In addition, we examine the heterogeneous impact of AI access by\nthe technical support conversation topics that agents encounter. While AI systems improve with\naccess to more training data, we find that the gains to AI adoption\u2014when used to complement\nhuman workers\u2014are largest for relatively rare problems, perhaps because agents are already capable\nof addressing the problems they encounter most frequently. We further analyze the text of agents\u2019\nchats and provide evidence that access to AI improves their English language fluency, especially\namong international agents.\nFinally, we compare the text of conversations before and after AI\nand provide suggestive evidence that AI adoption drives convergence in communication patterns:\nlow-skill agents begin communicating more like high-skill agents.\nOur third set of results focus on agents\u2019 experience of work. Work in contact centers3 is often\ndifficult. Agents are regularly exposed to hostile treatment from upset (and anonymous) customers,\nand because much work is outsourced, many agents work overnight shifts in order to service US\nbusiness hours. AI assistance may help agents communicate more effectively but could also increase\nthe likelihood that agents are perceived as mechanical or inauthentic. We show that access to AI\nassistance markedly improves how customers treat agents, as measured by the sentiment of their\nchat messages. We also find that customers are less likely to question the competence of agents by\nrequesting to speak to a supervisor. These changes come alongside a decrease in worker attrition,\nwhich is driven by the retention of newer workers.\nOur overall findings show that access to generative AI can increase the productivity of individual\nworkers, and improve their experience of work. We emphasize, however, that these findings capture\nmedium run impacts within a single firm: our paper is not designed to shed light on the aggregate\nemployment or wage effects of generative AI tools. In the longer run, firms may respond to increasing\nproductivity among novice workers by hiring more of them, or by seeking to develop more powerful\nAI systems that replace their use of labor altogether. This latter scenario highlights the possibility\nthat while the introduction of generative AI may increase demand for lower-skill labor within an\noccupation, the equilibrium response to AI assistance may lead to across occupation shifts in labor\n3The term \u201ccontact center\u201d updates the term \u201ccall center,\u201d to reflect the fact that a growing proportion of customer\nservice contacts no longer involve phone calls.\n3\n\ndemand that instead benefit higher skill workers (Autor et al., 2003; Acemoglu and Restrepo, 2018;\nAcemoglu, 2024).4 Unfortunately, our data do not allow us to observe changes in wages, overall\nlabor demand, or the skill composition of workers hired at the firm.\nOur results also highlight the longer-term incentive challenges raised by the adoption of AI\nsystems. In our data, top workers increase their adherence to AI recommendations, even though\nthose recommendations marginally decrease the quality of their conversations.\nYet, with fewer\noriginal contributions from the most skilled workers, future iterations of the AI model may be less\neffective in solving new problems.\nOur work therefore raises questions about whether and how\nworkers should be compensated for the data they provide for training AI systems.\nOur paper is related to a large literature on the impact of technological adoption on worker\nproductivity and the organization of work (e.g. Rosen, 1981; Autor et al., 1998; Athey and Stern,\n2002; Bresnahan et al., 2002; Bartel et al., 2007; Acemoglu et al., 2007; Hoffman et al., 2017; Bloom\net al., 2014; Michaels et al., 2014; Garicano and Rossi-Hansberg, 2015; Acemoglu and Restrepo, 2020;\nFelten et al., 2023). Many of these studies, particularly those focused on information technologies,\nfind evidence that IT complements higher-skill or more-educated workers (Akerman et al., 2015;\nTaniguchi and Yamada, 2022). For instance, Bartel et al. (2007) finds that firms that adopt IT tend\nto use more skilled labor and is associated with increased skill requirements for machine operators\nin valve manufacturing.\nOther research compares workers with different degrees of educational\nattainment between occupations; Acemoglu and Restrepo (2020) study the diffusion of robots and\nfinds that the negative effects of robots on employment are most pronounced for workers in blue-\ncollar occupations and those with fewer than a college education.\nThere have been substantially fewer studies involving AI-based technologies, generative or not.\nAcemoglu et al. (2022); Zolas et al. (2020); Calvino and Fontanelli (2023) examine economy-wide\ndata from the US and OECD and show that the adoption of AI tools is concentrated among larger\nand younger firms with relatively high productivity. So far, evidence on the productivity impacts\nof these technologies is mixed: for example, Acemoglu et al. (2022) finds no detectable relationship\nbetween investments in AI-specific tools, while Babina et al. (2022) finds evidence of a positive\nrelationship between firms\u2019 AI investments and their subsequent growth and valuations.5 These\nstudies all caution that the productivity effects of AI technologies may be challenging to identify at\nthe macro-level because AI-adopting firms differ substantially from non-adopters.\n4For example, transcripts from a fully automated customer service division could be sent to human analysts who\nmine this data for information on how to improve a product.\n5OECD (2023) reports that when surveyed firms are asked directly, 57% of employers in finance and 63% in\nmanufacturing reported that AI positively impacted productivity and 80% of surveyed workers who work with AI\nreport higher job performance.\n4\n\nEven at the decision level, the effects of AI on decision quality are mixed, often revealing\nunexpected challenges in human-AI collaboration. Some studies on AI decision support tools report\npositive impacts. For instance, Kanazawa et al. (2022) examine a non-generative AI tool for taxi\ndrivers that suggests routes with the highest likelihood of finding customers. This tool reduces\ndriver search time by 5%, with low-skill drivers seeing the largest reductions in search. On the\nother hand, several studies find that the combination of AI and human decisions performs worse\nthan either AI or humans alone (Hoffman et al., 2017; Angelova et al., 2023; Agarwal et al., 2023;\nPoursabzi-Sangdeh et al., 2021). In fact, a meta-analysis of more than 100 experimental studies\nconcludes that, on average, human-AI collaborations underperform both the AI alone and the best\nhuman decision-makers (Vaccaro et al., 2024). These results underscore the particular challenges\nintroduced when using AI-based tools designed to augment human decision making.\nIn this paper, we provide micro-level evidence on the adoption of a generative AI tool across\nthousands of workers employed by a given firm and its subcontractors. Our work is closely related to\nseveral other studies examining the impacts of generative AI in lab-like settings. Peng et al. (2023b)\nrecruit software engineers for a specific coding task (writing an HTTP server in JavaScript) and\nshow that those given access to GitHub Copilot complete this task twice as quickly. Similarly, Noy\nand Zhang (2023) conduct an online experiment showing that subjects given access to ChatGPT\ncomplete professional writing tasks more quickly. In the legal domain, Choi and Schwarcz (2023)\nprovided law students with AI assistance on a law school exam, while in management consulting,\nDell\u2019Acqua et al. (2023) find that access to GPT-4 suggestions improves the quality of responses\non management consulting tasks within its capabilities, but can negatively impact performance\non tasks outside its capabilities. Consistent with our findings, Noy and Zhang (2023), Choi and\nSchwarcz (2023), Peng et al. (2023a) and Dell\u2019Acqua et al. (2023) find that generative AI assistance\ncompresses the productivity distribution, with lower-skill workers driving the bulk of improvements.\nOur paper, however, is the first to examine longer-term effects in a real-world workplace where we\ncan also track patterns of learning, customer-side effects, and changes in the experience of work.\n1\nGenerative AI and Large Language Models\nIn recent years, the rapid pace of AI development and public release tools such as ChatGPT,\nGitHub Copilot, and DALL-E have attracted widespread attention, optimism, and alarm (The\nWhite House, 2022). These technologies are all examples of \u201cgenerative AI,\u201d a class of machine\nlearning technologies that can generate new content\u2014such as text, images, music, or video\u2014by\n5\n\nanalyzing patterns in existing data. In this section, we provide background on generative AI as a\ntechnology and discuss its potential economic implications.\n1.1\nTechnical Primer\nThis paper focuses on an important class of generative AI, large language models (LLMs). LLMs\nare neural network models designed to process sequential data (Bubeck et al., 2023). An LLM is\ntrained by learning to predict the next word in a sequence, given what has come before, using a large\ncorpus of text (such as Wikipedia, digitized books, or portions of the Internet). This knowledge of\nthe statistical co-occurrence of words allows it to generate new text that is grammatically correct\nand semantically meaningful. Although \u201clarge language model\u201d implies human language, the same\ntechniques can be used to produce other forms of sequential data (\u201ctext\u201d) such as protein sequences,\naudio, computer code or chess moves (Eloundou et al., 2023).\nRecent progress in generative AI has been driven by four factors: computing scale, earlier\ninnovations in model architecture, the ability to \u201cpre-train\u201d using large amounts of unlabeled data\nand refinements in training techniques.6\nFirst, the quality of LLMs is strongly dependent on scale: the amount of computing power used\nfor training, the number of model parameters, and dataset size (Kaplan et al., 2020). Firms are\nincreasingly devoting more resources to increasing this scale. The GPT-3 model included 175 billion\nparameters, was trained on 300 billion tokens, and generated approximately $5 million dollars in\ncomputing costs alone; the GPT-4 model, meanwhile, is estimated to include 1.8 trillion parameters,\ntrained on 13 trillion tokens, at a rumored computing-only cost of $65 million (Li, 2020; Brown et\nal., 2020; Patel and Wong, 2023)\nIn terms of model architecture, modern LLMs use two earlier key innovations: positional en-\ncoding and self-attention. Positional encodings keep track of the order in which a word occurs in\na given input.7 Meanwhile, self-attention assigns importance weights to each word in the context\nof the entire input text. Together, this approach enables models to capture long-range semantic\nrelationships within an input text, even when that text is broken up into smaller segments and\nprocessed in parallel (Vaswani et al., 2017; Bahdanau et al., 2015).\nNext, LLMs can be pre-trained on large amounts of unlabeled data from sources such as Reddit\nor Wikipedia. Because unlabeled data are far more prevalent than labeled data, LLMs can learn\nabout natural language on a much larger training corpus (Brown et al., 2020).\nBy seeing, for\nexample, that the word \u201cyellow\u201d is more likely to be observed with \u201cbanana\u201d or \u201csun\u201d or \u201crubber\n6For a more detailed technical review of progress, see Radford and Narasimhan (2018); Radford et al. (2019); Liu\net al. (2023); Ouyang et al. (2022).\n7For instance, a model would keep track of \u201cthe, 1\u201d instead of only \u201cthe\u201d (if \u201cthe\u201d was the first word in the sentence).\n6\n\nduckie,\u201d the model can learn about semantic and grammatical relationships even without explicit\nguidance (Radford and Narasimhan, 2018). The resulting model can be used in multiple applications\nbecause its training is not specific to a particular set of tasks.\nFinally, general-purpose LLMs can be further \u201cfine-tuned\u201d to generate output that matches the\npriorities of any specific setting (Ouyang et al., 2022; Liu et al., 2023). For example, a model trained\nto generate social media content would benefit from receiving labeled data that contain not just\nthe content of a post or tweet, but also information on the amount of user engagement it received.\nSimilarly, an LLM may generate several potential responses to a given query, but some of them\nmay be factually incorrect or contain toxic language. To discipline this model, human evaluators\ncan rank these outputs to train a reward function that prioritizes desirable responses. These types\nof refinements can significantly improve model quality by making a general-purpose model better\nsuited to its specific application (Ouyang et al., 2022).\nTogether, these innovations have generated meaningful improvements in model performance.\nThe Generative Pre-trained Transformer (GPT) family of models, in particular, has attracted con-\nsiderable media attention for their rapidly expanding capabilities.8\n1.2\nThe Economic Impacts of Generative AI\nComputers have historically excelled at executing pre-programmed instructions, making them par-\nticularly effective at tasks that can be reduced to explicit rules (Autor, 2014). Consequently, com-\nputerization has disproportionately reduced the demand for workers performing \u201croutine\u201d tasks such\nas data entry, bookkeeping, and assembly line work, reducing wages in these jobs (Acemoglu and\nAutor, 2011). At the same time, computerization has also increased the demand for workers who\npossess complementary skills such as programming, data analysis, and research. Along with changes\nin supply of skilled workers, these changes have contributed to the increase in wage inequality in\nthe United States and have been linked to a variety of organizational changes (Katz and Murphy,\n1992; Autor et al., 2003; Michaels et al., 2014; Bresnahan et al., 2002; Baker and Hubbard, 2003;\nOECD, 2023).\nIn contrast, generative AI tools do not require explicit instructions to perform tasks. If asked to\nwrite an email denying an employee a raise, generative AI tools will likely respond with a professional\nand conciliatory note. This occurs because the model will have seen many examples of workplace\ncommunication in which requests are declined in this manner. Importantly, the model produces\nsuch an output even though no programmer has explicitly specified what tone would be appropriate\n8For instance, GPT-4 has recently been shown to outperform humans in taking the US legal bar exam (Liu et al.,\n2023; Bubeck et al., 2023; OpenAI, 2023).\n7\n\nfor what context, nor even defined what a tone like \u201cprofessional\u201d or \u201cconciliatory\u201d means. Indeed,\nthe ability to behave \u201cappropriately\u201d is one that cannot be fully articulated even by those who\npossess it. This type of \u201ctacit knowledge\u201d underlies most tasks humans perform, both on and off\nthe job (Polanyi, 1966; Autor, 2014).\nThe fact that generative AI models display such skills suggests that they can acquire tacit\nknowledge that is embedded in the training examples they encounter. This ability expands the\ntypes of tasks that computers may be capable of performing to include non-routine tasks that rely on\njudgment and experience. For example, Github Copilot, an AI tool that generates code suggestions\nfor programmers, has achieved impressive performance on technical coding questions and, if asked,\ncan provide natural language explanations of how the code it produces works (Nguyen and Nadi,\n2022; Zhao, 2023). These capabilities extend to tasks traditionally dominated by highly skilled\nprofessionals, including complex mathematics, scientific analysis, coding, and financial modeling.\nAs these tasks have typically been performed by workers who were previously insulated from or\nbenefited from earlier waves of technological adoption, the rise of generative AI has the potential to\nsignificantly alter the established relationships between technology, labor productivity, and economic\ninequality (The White House, 2022).\nGenerative AI tools can not only expand the types of tasks that machines can perform, they may\nalso reveal valuable information about how the most productive human workers differ from others.\nThis is because the ML models underlying generative AI systems are commonly trained on data\ngenerated by human workers and, consequently, encounter many examples of people performing\ntasks both well and poorly. In learning to predict good outcomes from human-generated data, ML\nmodels can implicitly identify characteristics or patterns of behavior that distinguish high and low\nperformers, including subtleties rooted in tacit knowledge. Generative AI systems then take this\nknowledge and use it to produce new behaviors that embody what top performers might do. This\nability could be used in different ways: firms may choose to replace lower-skill workers with AI-based\ntools, such tools could be used to demonstrate best practices to help lower-skill workers improve or\nhelp less experienced workers get up to speed more quickly. In either case, generative AI tools may\nhave differential impacts by worker ability, even amongst workers performing the same tasks.\nDespite their potential, generative AI tools face significant challenges in real-world applications.\nAt a technical level, popular LLM-based tools, such as ChatGPT, have been shown to produce\nfalse or misleading information unpredictably, raising concerns about their reliability in high-stakes\nsituations. Second, while LLM models often perform well on specific tasks in the lab (OpenAI,\n2023; Peng et al., 2023b; Noy and Zhang, 2023), the types of problem that workers encounter in\nreal-world settings are likely to be broader and less predictable. Furthermore, generative AI tools\n8\n\noften require prompts from human operators, yet finding ways to effectively combine human and AI\nexpertise may be challenging: for instance, earlier research indicates that decision-support systems\nintegrating AI with human judgment often perform worse than those that rely on the human or\nthe AI alone (Vaccaro et al., 2024). These challenges raise concerns about the ability of AI systems\nto provide accurate assistance in every circumstance and\u2014perhaps more importantly\u2014workers\u2019\ncapacity to distinguish cases where AI suggestions are effective from those where they are not.\nFinally, the efficacy of new technologies is likely to depend on how they interact with existing\nworkplace structures. Promising technologies may have more limited effects in practice due to the\nneed for complementary organizational investments, skill development, or business process redesign.\nBecause generative AI technologies are only beginning to be used in the workplace, little is currently\nknown about their impacts.\n2\nOur Setting: LLMs for Customer Support\n2.1\nCustomer Support and Generative AI\nWe study the impact of generative AI in the customer service industry, an area with one of the\nhighest surveyed rates of AI adoption.9 Customer support interactions play a crucial role in building\nstrong customer relationships and company reputation; however, as in many industries, there is\nsubstantial variation in worker productivity (Berg et al., 2018; Syverson, 2011).\nNewer workers require significant training and take time to become more productive. At the\nsame time, turnover is high: industry estimates suggest that 60% of agents in contact centers\nleave each year, costing firms $10,000 to $20,000 dollars per agent (Buesing et al., 2020; Gretz and\nJacobson, 2018). To address these workforce challenges, the average supervisor spends at least 20\nhours per week coaching agents with lower performance (Berg et al., 2018). Faced with variable\nproductivity, high turnover, and high training costs, firms are increasingly turning to AI tools (Chui\net al., 2021).\nAt a technical level, customer support is well-suited for current generative AI tools. From an\nAI\u2019s perspective, customer-agent conversations can be thought of as a series of pattern-matching\nproblems in which one is looking for an optimal sequence of actions. When confronted with an issue\nsuch as \u201cI can\u2019t login,\u201d an AI/agent must identify which types of underlying problems are most likely\nto lead a customer to be unable to log in and think about which solutions typically resolve these\nproblems (\u201cCan you check that caps lock is not on?\u201d). At the same time, they must be attuned to\n9For instance, of the businesses that report using AI, 22% use AI in their customer service centers (Chui et al.,\n2021).\n9\n\na customer\u2019s emotional response, making sure to use language that increases the likelihood that a\ncustomer will respond positively (\u201cthat wasn\u2019t stupid of you at all! I always forget to check that\ntoo!\u201d). Because customer service conversations are widely recorded and digitized, pre-trained LLMs\ncan be fine-tuned for customer service using many examples of both successfully and unsuccessfully\nresolved conversations.\nCustomer service is also a setting where there is high variability in the abilities of individual\nagents. For example, in top-performing customer support agents are often more effective at diagnos-\ning the underlying technical issue given a customer\u2019s problem description. These workers often ask\nmore questions before settling on a diagnosis of the problem; this initially takes longer but reduces\nthe likelihood that agents waste time trying to solve the wrong problem. Such differences in agent\nbehavior can often be inferred from the large amounts of training data to which customer-service-\nspecific AI models have access. As a result, customer service is also a setting in which generative\nAI models can potentially encode some of the \u201cbest practices\u201d that top-performing agents use.\nIn the remainder of this section, we provide details about the firm we study and the AI tool\nthey adopt.\n2.2\nData Firm Background\nWe work with a company that provides AI-based customer service support software (hereafter, the\n\u201cAI firm\u201d) to study the deployment of their tool in one of their client firms (hereafter, the \u201cdata\nfirm\u201d).\nOur data firm is a Fortune 500 enterprise software company that specializes in business process\nsoftware for small and medium-sized businesses in the United States. It employs a variety of chat-\nbased technical support agents, both directly and by contracting with third-party outsourcing firms.\nThe majority of agents in our sample work from offices located in the Philippines, with a smaller\ngroup working in the United States and in other countries. Across locations, agents are engaged in\na fairly uniform job: answering technical support questions from US-based small business owners.\nWithin this group of agents, customer chats were assigned on the basis of agent availability,\nwith no additional pre-screening.10 Support sessions are relatively long, averaging 40 minutes, with\nmuch of the conversation spent trying to diagnose the underlying technical problem.\nThe job requires a combination of detailed product knowledge, problem solving skills, and the\nability to deal with frustrated customers.\n10Our data firm employed other groups of agents to provide chat-based support for different customer segments,\nsuch as self-employed individuals or larger businesses. Within chats from US-based small businesses, there was no\nadditional sorting.\n10\n\nOur firm measures productivity using three metrics that are standard in the customer service\nindustry: \u201caverage handle time,\u201d the average time an agent takes to finish a chat; \u201cresolution rate,\u201d\nthe share of conversations that the agent successfully resolves; and \u201cnet promoter score,\u201d (customer\nsatisfaction), which is calculated by randomly surveying customers after a chat and calculating the\npercentage of customers who would recommend an agent minus the percentage who would not. A\nproductive agent is able to field customer chats quickly while maintaining a high resolution rate\nand net promoter score.\nAcross locations, agents are organized into teams with a manager who provides feedback and\ntraining to agents. Once a week, managers hold one-on-one feedback sessions with each agent. For\nexample, a manager might share the solution to a new software bug, explain the implication of a\ntax change, or suggest how to better manage customer frustration with technical issues. Agents\nwork individually and the quality of their output does not directly affect others.\nAgents in our data are typically paid a baseline hourly wage, and receive bonuses for hitting\nspecific performance targets (usually as they relate to chats per hour and chat resolution rate).\nWhile we lack data on individual pay, the managers we interviewed reported that performance\nbonuses were moderate, accounting for an estimated 20 to 40 percent of total take home pay for the\ntypical customer service worker.\n2.3\nAI System Design\nThe AI system we study is designed to identify conversational patterns that predict efficient call\nresolution. It builds on a recent version of GPT and is fine-tuned on a large dataset of customer-\nagent conversations labeled with various outcomes, such as call resolution success and handling\ntime. Our AI firm also flags whether the agent in charge of the conversation was considered a \u201ctop\u201d\nperformer by the data firm for AI system training. Many aspects of successful agent behavior are\ndifficult to quantify, including asking clarifying questions, being attentive to customer concerns,\ndeescalating tense situations, adapting communication styles, and explaining complex topics in\nsimple terms. By explicitly training the AI model on text from top performers, the system places\nvalue on these subtle behaviors and tone. The AI firm further trains its model using a process\nsimilar in spirit to Ouyang et al. (2022) to prioritize agent responses that express empathy, provide\nappropriate technical documentation, and limit unprofessional language. This additional training\nmitigates some of the concerns associated with relying on LLMs to generate suggestions.\nOnce deployed, the AI system generates two main types of output: 1) real-time suggestions\nfor how agents should respond to customers and 2) links to the data firm\u2019s internal documenta-\n11\n\ntion for relevant technical issues. In both cases, recommendations are based on a history of the\nconversation.11\nAppendix Figure A.1 illustrates an example of AI assistance. In the chat window (Panel A),\nAlex, the customer, describes their problem to the agent. Here, the AI assistant generates two\nsuggested responses (Panel B). In this example, it has learned that phrases like \u201cI can definitely\nassist you with this!\u201d\nand \u201cHappy to help you get this fixed asap\u201d are associated with positive\noutcomes.\nPanel C of Appendix Figure A.1 shows an example of a technical recommendation\nfrom the AI system, which occurs when it recommends a link to the data firm\u2019s internal technical\ndocumentation.\nImportantly, the AI system we study is designed to augment, rather than replace, human agents.\nThe output is shown only to the agent, who has full discretion over whether to incorporate (fully\nor partially) the AI suggestions. Giving the agent final decision rights over messages to customers\nreduces the likelihood that off-topic or incorrect outputs make their way into customer conversations.\nFurthermore, the system does not provide suggestions when it has insufficient training data for that\nsituation. In these situations, the agent must respond on their own.\n3\nDeployment, Data, and Empirical Strategy\n3.1\nAI Rollout\nAI assistance was introduced after a small randomized pilot involving a small number of agents.12\nAppendix Figure A.2 illustrates the timing of the rollout, which primarily took place during the\nFall of 2020 and Winter of 2021. The variation in access and timing was influenced by two key\nfactors: bottlenecks in the onboarding process due to limited training resources, and the firm\u2019s\noverall budget for AI assistance.\nAgents gained access to the AI tool only after completing a three-hour online onboarding session\nconducted by the AI firm. To maintain quality and consistency, these training sessions were kept\nsmall and were exclusively led by one of two employees from our AI firm, both of whom had prior\ncontact center experience and deep knowledge of the AI system. Since these individuals had other\n11For example, the correct response when a customer says \u201cI can\u2019t track my employee\u2019s hours during business trips\u201d\ndepends on what version of the data firm\u2019s software the customer uses. Suppose that the customer has previously\nmentioned that they are using the premium version. In that case, they should have access to remote mobile device\ntimekeeping, meaning that the support agents need to diagnose and resolve a technical issue that prevents the software\nfrom working. If, however, the customer stated that they are using the standard version, then the correct solution\nis for the customer to upgrade to the premium version in order to access this feature. For more on context tracking,\nsee, for instance, Dunn et al. (2021).\n12The RCT is discussed in Section 4.1.1.\n12\n\nfull-time responsibilities, the AI firm could only offer limited training sessions each week, with\nsession timings adjusted to accommodate the time zones of the global workforce of the data firm.\nAdditionally, because generative AI was costly and relatively untested at that time, the data\nfirm allocated a limited budget for its deployment. Once the total number of onboarded agents\nreached the predefined contractual limit, onboarding ceased. However, when AI-enabled agents left,\nnew agents were added to these existing licenses. These constraints on licenses and training sessions\ncreated the variation in AI adoption that we analyze in our study.\nManagers at each office oversaw the selection and allocation of agents to training sessions. In\ninterviews, employees of our AI firm reported that managers made these decisions with the goal of\nminimizing customer service disruptions. If all agents on a team were assigned to the same training\nsession, customers would experience a large increase in wait time. To avoid this, managers tried to\nassign workers on the same team to different training sessions in order to maintain coverage. After\ntheir initial onboarding session, workers were not given any additional training on how to use the\nAI software. This is because the AI firm only employed a small product management team and\nwere not able to provide ongoing support or guidance to the thousands of agents using the tool.\nThese considerations meant that our AI rollout effectively occurred at the individual level: within\nthe same team, same office, individuals would be onboarded to AI assistance at different times. In\nOctober 2020, within a team, the average share of active workers with access to AI assistance was\nonly 5%, growing to about 70% in January of 2021. While our analysis primarily focuses on the\nindividual adoption dates, we also provide results in Appendix Table A.2 that instrument individual\nadoption dates with team-level adoption patterns.\n3.2\nSummary Statistics\nTable 1 provides details on the sample characteristics, divided into four groups: all agents (\u2018all\u201d),\nagents who never have access to the AI tool during our sample period (\u201cnever treated\u201d), pre-AI\nobservations for those who eventually get access (\u201ctreated, pre\u201d) and post-AI observations (\u201ctreated,\npost\u201d). In total, we observe the conversation text and outcomes associated with 3 million chats\nby 5,172 agents. Within this, we observe 1.2 million chats by 1,636 agents in the post-AI period.\nMost of the agents in our sample, 89%, are located outside of the United States, mainly in the\nPhilippines. For each agent, we observe their assigned manager, tenure, geographic location, and\nwhich firm employs the agent (the data firm or a subcontractor).\nTo assess the impacts of this deployment, we rely on several key outcome variables, all aggregated\nat the agent-month level, the most granular level with complete data. Our primary productivity\nmeasure is resolutions per hour (RPH), which reflects the number of chats a worker successfully\n13\n\nresolves per hour and serves as our summary of a worker\u2019s productivity. RPH is influenced by several\nfactors, which we also measure individually: the average time it takes to handle an individual chat\n(AHT), the number of chats an agent is able to handle per hour, which accounts for multitasking\n(CPH), and resolution rate (RR), the share of chats that are successfully resolved. In addition, we\nmeasure customer satisfaction using the net promoter score (NPS) from post-call surveys. While our\nmain outcome measures are at the agent-month level, some data, like chat duration, are available\nat a more granular chat level. We also construct additional measures of sentiment, topics, and\nlanguage fluency from chat text.\nOur dataset includes average handle time (AHT) and chats per hour (CPH) for all agents in our\nsample. However, subcontractors fail to consistently collect call quality metrics for all agents. As a\nresult, we only observe our omnibus productivity measure\u2014resolutions per hour\u2014for this smaller\nsubset of agents with call quality outcomes. Workers may work only for a portion of the year or part-\ntime, so we calculate year-month observations based solely on the periods that an agent is assigned\nto chats. Appendix Section A.2 includes a more extensive discussion of our sample construction\nand key variables.\nFigure 1 plots the raw distributions of our outcomes for each of the never, pre-, and post-\ntreatment subgroups. Several of our main results are readily visible in these raw data. In Panels\nA through D, we see that post-treatment agents do better along a range of outcomes, relative to\nboth never-treated agents and pre-treatment agents. In Panel E, we see no discernible differences\nin surveyed customer satisfaction among treated and non-treated groups.\nFocusing on our main productivity measure, Panel A of Figure 1 and Table 1 show that never-\ntreated agents resolve an average of 1.7 chats per hour, whereas post-treatment agents resolve 2.5\nchats per hour. Some of this difference may be due to initial selection: treated agents have higher\nresolutions per hour prior to AI model deployment (2.0 chats) relative to never treated agents (1.7).\nThis same pattern appears for chats per hour (Panel C) and resolution rates (Panel D): while ever-\ntreated agents appear to be stronger performers at the outset than agents who are never treated,\npost-treatment agents perform substantially better. Looking instead at average handle times (Panel\nB), we see a starker pattern: pre-treatment and never-treated agents have similar distributions of\naverage handle times, centered at 40 minutes, but post-treatment agents have a lower average handle\ntime of 35 minutes. These figures, of course, reflect raw differences that do not account for potential\nconfounding factors such as differences in agent experience or differences in selection into treatment.\nIn the next section, we will more precisely attribute these raw differences to the impact of AI model\ndeployment.\n14\n\n3.3\nEmpirical Strategy\nWe isolate the causal impact of access to AI recommendations using a standard difference-in-\ndifferences regression:\nyit \u201c \u03b4t ` \u03b1i ` \u03b2AIit ` \u03b3Xit ` \u03f5it\n(1)\nOur outcome variables, yit, is performance measures for agent i in year-month t, with resolutions\nper hour as our main measure of productivity. We measure these outcomes in levels, and report\npercentage changes off the baseline pre-treatment means. Our main variable of interest is AIit, an\nindicator that equals one if AI assistance is activated for agent i at time t. All regressions include\nyear-month fixed effects, \u03b4t, to control for common, time-varying factors such as tax season or the\nend of the business quarter. In our preferred specifications, we also include agent fixed effects \u03b1i to\ncontrol for time-invariant differences in productivity across agents and time-varying tenure controls\nXit (specifically, fixed effects for agent tenure in months). In our main specifications, we weight\neach agent-month equally and cluster standard errors at the agent level to reflect the fact that AI\naccess is rolled out individually, but Appendix Tables A.3 and A.4 show that our results are robust\nto alternative weightings and clustering.\nA rapidly growing literature has shown that two-way fixed effects regressions deliver consistent\nestimates only with strong assumptions about the homogeneity of treatment effects and may be\nbiased when treatment effects vary over time or by adoption cohort (Cengiz et al., 2019; de Chaise-\nmartin and D\u2019Haultf\u0153uille, 2020; Sun and Abraham, 2021; Goodman-Bacon, 2021; Callaway and\nSant\u2019Anna, 2021; Borusyak et al., 2022). For example, workers may take time to adjust to using the\nAI system, in which case its impact in the first month may be smaller. Alternatively, the onboarding\nof later cohorts of agents may be smoother, so that their treatment effects may be larger.\nWe study the dynamics of treatment effects using the interaction weighted (IW) estimator\nproposed in Sun and Abraham (2021).\nSun and Abraham (2021) show that this estimator is\nconsistent assuming parallel trends, no anticipatory behavior, and cohort-specific treatment effects\nthat follow the same dynamic profile.13 Both our main differences-in-differences and event study\nestimates are similar using robust estimators introduced in de Chaisemartin and D\u2019Haultf\u0153uille\n(2020), Borusyak et al. (2022), Callaway and Sant\u2019Anna (2021), and Sun and Abraham (2021), as\nwell as using traditional two-way fixed effects OLS. In fact, Appendix Figure A.10 shows similar\ntreatment effects across adoption cohorts (e.g. those that received AI access earlier or later, and\nwere thus subject to potentially different versions of the model). We also show our results our\n13This last assumption means that treatment effects are allowed to vary over event-time and that average treatment\neffects can vary across adoption-cohorts (because even if they follow the same event-time profile, we observe different\ncohorts for different periods of event-time).\n15\n\nsimilar clustering at different levels of granularity (Appendix Table A.3) and instrumenting agent\nadoption with the date the first worker within the agent\u2019s team received AI access (Appendix Table\nA.2).\n4\nMain Results\n4.1\nOverall Impacts\nTable 2 examines the impact of the deployment of the AI model on our primary measure of pro-\nductivity, resolutions per hour, using a standard two-way fixed effects model. In Column 1, we\nshow that, controlling for time and location fixed effects, access to AI recommendations increases\nresolutions per hour by 0.47 chats, up 23.9% from their pre-treatment mean of 1.97. In Column\n2, we include fixed effects for individual agents to account for potential differences between treated\nand untreated agents. In Column 3, we include additional fixed effects for agent tenure in months\nto account for time-varying experience levels. As we add controls, our effects fall slightly, so that,\nwith agent and tenure fixed effects, we find that the deployment of AI increases RPH by 0.30 chats\nor 15.2%.\nFigure 2 shows the accompanying IW event study estimates of Sun and Abraham (2021) for the\nimpact of AI assistance on RPH. We find a substantial and immediate increase in productivity in\nthe first month of deployment. This effect grows slightly in the second month and remains stable\nand persistent up to the end of our sample.\nIn Table 3, we report additional results using our preferred specification with fixed effects for\nyear-month, agent, location and agent tenure. Column 1 documents a 3.7 minute decrease in the\naverage duration of customer chats, an 8.5% decline from the baseline mean of 43 minutes (shorter\nhandle times are considered better). Next, Column 2 indicates a 0.37 unit increase in the number\nof chats that an agent can handle per hour. Relative to a baseline mean of 2.4, this represents an\nincrease of roughly 15%. Unlike average handle time, chats per hour account for the possibility that\nagents may handle multiple chats simultaneously. The fact that we find a stronger effect on this\noutcome suggests that AI enables agents to both speed up chats and multitask more effectively.\nColumn 3 of Table 3 indicates a small 1.3 percentage point increase in chat resolution rates.\nThis effect is economically modest and insignificant, given a high baseline resolution rate of 82%;\nwe interpret this as evidence that improvements in chat handling do not come at the expense of\nproblem solving on average. Finally, Column 4 finds no economically significant change in customer\nsatisfaction, as measured by net promoter scores: the coefficient is -0.12 percentage points and the\nmean is 80%.\n16\n\nAppendix Figure A.3 presents the accompanying event studies for additional outcomes. We see\nimmediate impacts on average handle time (Panel A) and chats per hour (Panel B), and relatively\nflat patterns for resolution rate (Panel C) and customer satisfaction (Panel D). We therefore interpret\nthese findings as saying that, on average, AI assistance increases productivity without negatively\nimpacting resolution rates and surveyed customer satisfaction.\n4.1.1\nRCT Analysis\nIn August 2020, our data firm conducted a pilot analysis involving approximately 50 workers, about\nhalf of whom were randomized into treatment. Unfortunately, we do not have information on the\ncontrol group workers that were part of the experiment, but we are able to identify the treatment\ngroup because these are the only workers who were given access to the AI tool in August of 2020.\nIn this section, we compare the productivity of these treated workers with the productivity of their\nthen-untreated colleagues.\nAppendix Table A.1 presents the overall difference-in-difference estimates.\nFocusing on the\ntreatment group of 22 workers, we still see a significant increase in resolutions per hour, as well as\na significant decrease in average handle time. The magnitudes of these effects are similar to those\nin our main sample. As with our main results, we see no average impacts on resolution rate and\ncustomer satisfaction. Appendix Figure A.5 reports the accompanying event studies for our various\noutcomes.\n4.1.2\nInstrumenting Individual AI adoption\nDuring the rollout process, managers were in charge of deciding which agents to onboard on to\nthe AI system, and scheduling when their training would occur. One may therefore be concerned\nthat the timing of an individual agent\u2019s AI adoption may be related to their performance if, for\ninstance, managers prioritized onboarding particularly weak or strong workers. This concern is less\nlikely to apply to the timing of an entire team\u2019s onboarding. In Appendix Table A.2, we present\nresults in which we instrument an individual agent\u2019s AI adoption date with the first adoption date\nof the worker\u2019s company (e.g. the main data firm or one of its subcontractors), office location, and\nteam.14 Although AI-access was in theory introduced across the entire firm, variation in training\nschedules and time zones made it so that some teams began the process sooner than others. Using\nthis approach, we estimate that individual AI adoption increases resolutions per hour by 0.55 chats\nper hour, compared to 0.30 under our main specification. Our effects for average handle time and\n14Information on team is missing for some workers. Workers without this information are grouped into a single\n\u201cmissing\u201d team.\n17\n\nchats per hour are essentially identical to our main effects: the larger magnitude for resolutions\nper hour comes from the fact that this IV approach estimates a significant and larger impact on\nresolution rates.\n4.1.3\nFurther Robustness\nWe report a variety of robustness tests for these findings.\nAppendix Table A.9 finds similar results using alternative difference-in-difference estimators in-\ntroduced in Callaway and Sant\u2019Anna (2021), Borusyak et al. (2022), de Chaisemartin and D\u2019Haultf\u0153uille\n(2020), and Sun and Abraham (2021). Unlike traditional OLS, these estimators avoid comparing\nbetween newly treated and already treated units. In most cases, we find larger effects of AI assis-\ntance using these alternatives. Similarly, Appendix Figure A.4 reports that our results are similar\nunder alternative event study estimators: Callaway and Sant\u2019Anna (2021), Borusyak et al. (2022),\nde Chaisemartin and D\u2019Haultf\u0153uille (2020), and traditional two-way fixed effects.\nIn Appendix Table A.3, we show that our standard errors are similar whether clustering at the\nindividual level, team level, or geographic location level. We choose to cluster at the individual\nlevel to reflect the individual variation in the AI roll out. While our standard errors are somewhat\nsmaller at this level, our results are still highly significant when clustering at the most conservative\nlocation level.15\nFinally, we explore robustness to alternative weighting. Our main results treat each worker-\nmonth observation equally in order to focus on the impact of generative AI on worker-level pro-\nductivity. In Appendix Table A.4, we reweight these estimates by the number of customer chats a\nworker conducts and show that our results are similar.\n4.2\nHeterogeneity by Agent Skill and Tenure\nThere is substantial interest in the distributional consequences of AI-based technologies. An ex-\ntensive literature suggests that earlier waves of information technology (e.g., the Internet and com-\nputers) complemented high-skilled workers, increasing their productivity and labor demand and\nwidening wage differentials. Together with important changes in relative supply and demand for\nskilled labor, these changes shaped patterns of wage inequality in the labor market (Goldin and\nKatz, 1998, 2008).\nUnlike earlier waves of IT, however, generative AI does not simply execute\nroutine tasks. Instead, as outlined in Section 1.2, AI models identify patterns in data that repli-\ncate the behaviors of many types of workers, including those engaged in non-routine, creative, or\n15Geographic location is usually available at the city level (e.g. Reno, Nevada) and one geographic location may\ncontain firms employed by different subcontractors, although a small share workers work remotely.\n18\n\nknowledge-based tasks. These fundamental technical differences suggest that generative AI may\nimpact workers in different ways.\nIn this section, we explore two components that are important for understanding the distribu-\ntional consequences of AI adoption: its impact on the productivity of less skilled and less experienced\ncontact center workers.\n4.2.1\nPre-treatment Worker Skill\nWe measure an agent\u2019s \u201cskill\u201d using an index incorporating three key performance indicators: call\nhandling speed, issue resolution rates, and customer satisfaction.\nTo construct this index, we\ncompute an agent\u2019s ranking within its employer company-month for each component productivity\nmeasure and then average these rankings into a single index. Next, we calculate the average index\nvalue over the three months for each agent, to smooth out month-to-month shocks in agent perfor-\nmance. An agent in the top quintile of this productivity index demonstrates excellence across all\nthree metrics: efficient call handling, high issue resolution rates, and superior customer satisfaction\nscores.\nPanel A of Figure 3 shows how our productivity effects vary based across workers in each quintiles\nour skill index, measured in the month prior to AI-access. To isolate the impact of worker skill,\nour regression specification includes a set of fixed effects for months of worker tenure. Appendix\nSection A.3 includes the exact specification. We find that the productivity impact of AI assistance\nis most pronounced for workers in the lowest skill quintile (leftmost side), who see a 0.5 increase or\n36% increase in resolutions per hour. In contrast, AI assistance does not lead to any productivity\nincrease for the most skilled workers (rightmost side).\nIn Figure A.6 we show that less-skilled agents consistently see the largest gains across our other\noutcomes. For the highest-skilled workers, we find mixed results: a zero effect on average handle\ntime (Panel A), a small, but positive effect for chats per hour (Panel B), and, interestingly, small\nbut statistically significant decreases in resolution rates and customer satisfaction (Panels C and\nD). These results are consistent with the idea that generative AI tools may function by exposing\nlower-skill workers to the best practices of higher-skill workers. Lower-skill workers benefit because\nAI assistance provides them with new solutions, whereas the best performers may see little benefit\nfrom being exposed to their own best practices. Indeed, the fact that we find negative effects along\nmeasures of chat quality\u2014resolution rate and customer satisfaction\u2014suggests that AI recommen-\ndations may distract top performers, or lead them to choose the faster or less cognitively taxing\noption (following suggestions) rather than taking the time to come up with their own responses.\nThis is potentially important to address because the conversations of top agents are also used for\n19\n\nongoing training: failing to incentivize these agents to continue to produce responses could reduce\nthe quality of the AI model in the future.\nA potential concern is that our results could be driven by mean reversion: agents that performed\nwell just prior to AI-adoption may see a natural decline in their productivity afterward, while lower-\nperforming agents may rebound. To address this concern, we plot raw resolutions per hour in event-\ntime, graphed by skill tercile at AI treatment in Appendix Figure A.9. If mean reversion were driving\nour observed effects, we would expect to see a convergence of productivity levels after treatment,\nwith top-tercile agents showing decreased performance and the least-skilled agents demonstrating\nimproved output. However, our analysis reveals a consistent linear increase in productivity across\nall skill levels after AI implementation, with no strong evidence of mean reversion, suggesting that\nproductivity gains are attributable to AI assistance.\n4.2.2\nPre-treatment Worker Experience\nNext, we repeat our previous analysis for agent tenure to understand how the treatment effects of\nAI access vary by worker experience. To do so, we divide agents into five groups based on their\nmonths of tenure at the time the AI model is introduced. Some agents have less than a month of\ntenure when they receive AI access, while others have more than a year of experience. To isolate\nthe impact of worker tenure, this analysis controls for the worker skill quintile at AI adoption, with\nthe regression specification located in Appendix Section A.3.\nIn Panel B of Figure 3, we see a clear monotonic pattern in which the least experienced agents\nsee the greatest gains in resolutions per hour. Agents with less than 1 month of tenure improve their\nresolutions per hour by 0.7 resolutions per hour, with larger effects for less-experienced workers. In\ncontrast, we see no effect for agents with more than a year of tenure.\nIn Figure A.7, we report results for other outcomes. In Panels A and B, we see that AI assistance\ngenerates large gains in call handling efficiency, measured by average handle times and chats per\nhour, respectively, among the newest workers. In Panels C and D, we find positive impacts of AI\nassistance on chat quality, as measured by resolution rates and customer satisfaction, respectively.\nFor the most experienced workers, we see modest positive effects for average handle time (Panel A),\npositive but statistically insignificant effects on chats per hour (Panel B), and small but statistically\nsignificant negative effects for measures of call quality and customer satisfaction (Panels C and D).\nOverall, these patterns are very similar to our findings for agent skill, even though our regressions\nare designed to isolate the distinct roles of skill and experience (e.g. our skill regressions control\nfor experience and vice versa). This suggests that even within the same task, access to AI systems\ndisproportionately improves the performance of both novice and less skilled workers.\n20\n\n4.2.3\nMoving Down the Experience Curve\nTo further explore how AI assistance impacts newer workers, we examine how worker productivity\nevolves on the job.16 In Figure 4, we plot productivity variables by agent tenure for three distinct\ngroups: agents who never receive access to the AI model (\u201cnever treated\u201d), those who have access\nfrom the time they join the firm (\u201calways treated\u201d), and those who receive access in their fifth month\nwith the firm (\u201ctreated 5 mo.\u201d).\nWe see that all agents begin around 1.8 resolutions per hour. Never-treated workers (long dashed\nblue line) slowly improve their productivity with experience, reaching approximately 2.5 resolutions\nper hour 8 to 10 months later. In contrast, workers who always have access to AI assistance (short\ndashed red line) increase their productivity to 2.5 resolutions per hour after only two months and\ncontinue to improve until they are resolving more than 3 chats per hour after five months of tenure.17\nComparing just these two groups suggests that access to AI recommendations helps workers move\nmore quickly down the experience curve and reduces ramp-up time.\nThe final group in Panel A tracks workers who begin their tenure without access to AI assistance\nbut who receive access after five months on the job (solid green line). These workers initially improve\nat the same rate as never-treated workers, but after gaining AI access in month 5, their productivity\nbegins to more rapidly increase, following the same trajectory as the always-treated agents. These\nfindings demonstrate that AI assistance not only accelerates ramp-up for new workers, but also\nimproves the rate at which even experienced workers improve in their roles.\nIn Appendix Figure A.8, we plot these curves for other outcomes. We see clear evidence that the\nexperience curve for always-treated agents is steeper for handle time, chats per hour, and resolution\nrates (Panels A through C). Panel D follows a similar but noisier pattern for customer satisfaction.\nAcross many of the outcomes we examine, agents with two months of tenure and access to AI\nassistance perform as well as or better than agents with more than six months of tenure who do not\nhave access. AI assistance alters the relationship between on-the-job productivity and time, with\npotential implications for how firms might value prior experience, or approach training and worker\ndevelopment.\n5\nAdherence, Learning, Topic Handling, and Conversational Change\nIn this section, we conduct a variety of analyses to better understand the mechanisms driving our\nmain results.\n16We avoid the term \u201clearning curve\u201d because we cannot distinguish if workers are learning or merely following\nrecommendations.\n17Our sample ends here because we have very few observations more than five months after treatment.\n21\n\nFirst, we examine how workers engage with AI recommendations. We show that workers are\nselective about the recommendations they adopt, following the recommendations 35% on average.\nWe find that the returns on AI assistance are highest for workers who choose to follow the recom-\nmendations. Consistent with a story in which workers find AI recommendations helpful, we show\nthat adherence rates increase over time for all workers, especially among more experienced workers:\nby the end of our sample, we see similar adherence rates across worker tenure and skill.\nSecond, we explore whether AI-assistance helps workers learn. Using information on software\noutages in which AI assistance is temporarily unavailable, we provide evidence that exposure to AI\nleads to durable changes in worker skills. We find that workers exposed to AI recommendations\ncontinue to perform better during outages, and this effect is greater after more exposure and for\nagents who follow AI recommendations more closely when the software is working.\nNext, we provide evidence on two ways in which AI assistance may improve workers\u2019 conver-\nsations: providing them with the information to address specific customer problems and refining\ntheir overall conversational fluency. Our findings indicate that AI access is particularly beneficial\nwhen workers encounter less routine issues, where they might otherwise struggle to find solutions.\nIn addition, we also show that AI assistance improves workers\u2019 English language skills, with the\nmost substantial gains seen among those with the lowest initial proficiency.\nLastly, we analyze the text of conversations to assess how AI assistance influences the commu-\nnication patterns of higher- and lower-skill workers. In particular, we track how an agent\u2019s words\nchange before and after AI adoption and find evidence of larger textual shifts for lower-skill workers.\nWe also show that the similarity of conversations conducted by high- and low-skill agents increases\nfollowing AI-adoption. These results suggest that AI assistance may help lower-skill agents converge\ntowards their higher-skill peers.\nTaken together, our results suggest that examining and following AI recommendations helps\nworkers\u2014particularly lower-skilled workers\u2014learn to adopt best practices gathered from higher-\nskill and more experienced agents.\n5.1\nAdherence to AI recommendations\nThe AI tool we study makes suggestions, but agents are ultimately responsible for what they say\nto the customer. Thus far, our analysis evaluates the effect of AI assistance, irrespective of the\nfrequency with which users adhere to its suggestions. Here, we examine how closely agents adhere\nto AI recommendations, and document the association between adherence and returns to adoption.\nWe define \u201cadherence\u201d as the proportion of AI suggestions an agent typically adopts.\nThe AI\ncompany considers an agent to have adhered when they either directly copy the AI\u2019s proposed text\n22\n\nor manually enter highly similar content. To gauge initial adherence, we classify each treated agent\ninto a quintile based on their level of adherence during their first month using the AI tool.\nPanel A of Figure 5 shows the distribution of average agent-month-level adherence for our post-\nAI sample, weighted by the log number of AI recommendations provided to that agent in that month.\nThe average adherence rate is 38%, with an interquartile range of 23% to 50%: agents frequently\nignore recommendations. In fact, the share of recommendations followed is similar to the share of\nother publicly reported numbers for generative AI tools; a study of GitHub Copilot reports that\nindividual developers use 27% to 46% of code recommendations (Zhao, 2023). Such behavior may\nbe appropriate, given that AI models may make incorrect or irrelevant suggestions. In Appendix\nFigure A.11, we further show that the variation in adherence is similar within locations and teams,\nindicating that this variation is not driven by some organizational segments being systematically\nmore supportive than others.\nPanel B of Figure 5 shows that returns to AI model deployment are higher when agents actually\nfollow recommendations. To show this, we divide agents into quintiles based on the share of AI\nrecommendations they follow in the first month of AI access. Following Equation 4, we separately\nestimate the impact of AI assistance for each group, including year-month, agent and agent tenure\nfixed effects.\nWe find a steady and monotonic increase in returns by agent adherence: among agents in the\nlowest quintile, we still see a 10% gain in productivity, but for agents in the highest quintile, the\nestimated impact is over twice as high, close to 25%. Appendix Figure A.12 shows the results for\nour other four outcome measures. The positive correlation between adherence and returns holds\nmost strongly for average handle time (Panel A) and chats per hour (Panel B), and more noisily\nfor resolution rate (Panel C) and customer satisfaction (Panel D).\nOur results are consistent with the idea that there is a treatment effect of following AI rec-\nommendations on productivity. We note, however, that this relationship could also be driven by\nother factors: selection (agents who choose to adhere are more productive for other reasons); or\nselection on gains (agents who follow recommendations are those with the greatest returns). To\nfurther explore this, we consider the worker\u2019s revealed preference: do they continue to follow AI\nrecommendations over time? If our results were driven purely by selection, we would expect workers\nwith low adherence to continue having low adherence, since it was optimal for them to do so.\nFigure A.13 plots the evolution of AI adherence over time, for various categories of agents. Panel\nA begins by considering agents who differ in their initial AI compliance, which we categorize based\non terciles of AI adherence in the first month of model deployment (\u201cinitial adherence\u201d). Here, we\nsee that compliance either stays stable or grows over time. The most initially compliant agents\n23\n\ncontinue to comply at the same rates (just above 50%). Less initially compliant agents increase\ntheir compliance over time: those in the bottom tercile initially follow recommendations less than\n20% of the time but, by month five, their compliance rates have increased by over 50%, to just over\nhalf of the time. Next, Panel B divides workers up by tenure at the time of AI deployment. More\nsenior workers are initially less likely to follow AI recommendations: 30% for those with more than\na year of tenure compared to 37% for those with less than three months of tenure. However, over\ntime, all workers increase adherence, with more senior workers doing so faster so that the groups\nconverge five months after deployment. In Panel C, we show the same analysis by worker skill at\nAI deployment. Here, we see that compliance rates are similar across skill groups, and all groups\nincrease their compliance over time. In Appendix Figure A.14 we show that these patterns are\nrobust to examining within-agent changes in adherence (that is, adherence rates residualized by\nagent fixed effects). This suggests that increases in adherence over time are not driven exclusively\nby selection.\nThe results in Figures A.13 and A.14 are consistent with agents, particularly those who are\ninitially more skeptical, coming to value AI recommendations over time. We note, however, that\nhigh skill agents increase their adherence as quickly as their lower skill peers, even though their\nproductivity gains are smaller and\u2014in the case of some quality measures\u2014even negative. This\nsuggests an alternate possibility: agents may be over-relying on AI recommendations beyond what\nis optimal in the long run. Top agents, in particular, may see little additional value in taking the\ntime to provide the highest quality inputs when an adequate AI suggestion is readily available. If\nthis is indeed the case, high AI adherence in the present may reduce the quality or diversity of\nsolutions used for AI training in the future. While this may be a longer run concern, in the short\nrun, our analysis finds no evidence that the model is declining in quality over our sample period:\nin Appendix Figure A.10, we show that workers who received later access to the AI system\u2014and\ntherefore to a more recently updated version\u2014had similar first month treatment effects as those\nwho were onboarded to an earlier version of the model.18\n5.2\nWorker Learning\nA key question raised by our findings so far is whether these improvements in productivity and\nchanges in communication patterns reflect durable changes in the human capital of workers or\nsimply their growing reliance on AI assistance. In the latter case, the introduction of AI assistance\n18When the AI model is updated, changes are made available to all onboarded workers. In order to compare workers\nusing different versions of the model, we restrict our comparisons to productivity impacts from the first month after\nadoption, before a version update would have been pushed.\n24\n\ncould actually lead to an erosion in human capital, and we would expect treated workers to be less\nable to address customer questions if they are no longer able to access AI assistance.19\nTo study this, we examine how workers perform during periods in which they are not able to\naccess AI-recommendations due to technical issues at the AI firm. Outages occur occasionally in\nour data and can last anywhere from a few minutes to a few hours. During an outage, the system\nfails to provide recommendations to some, but not necessarily all, workers. For example, outages\nmay affect agents who log into their computers after the system crashes, but not agents working at\nthe same time who had signed in earlier. They may also affect workers using one physical server but\nnot another. Our AI firm tracks the most significant outages in order to perform technical reviews\nof what went wrong. We compile these system reports to identify periods in which a significant\nfraction of chats are affected by outages.\nAppendix Figure A.15 shows an example of such an outage, which occurred on September 10,\n2020. The y-axis plots the share of post-treatment chats (e.g. those occurring after the AI system\nhas been deployed for a given agent) for which the AI software does not provide any suggestions,\naggregated to the hour level.\nThe x-axis tracks hours in days leading up to and following the\noutage event (hours with fewer than 15 post-treatment chats are plotted as zeros for figure clarity).\nDuring non-outage periods, the share of chats without AI recommendations is typically 30-40%.\nThis reflects the fact that the AI system does not generate recommendations in response to all\nmessages, even when it is functioning properly. On the morning of September 10th, however, we\nsee a notable spike in the number of chats without recommendations, increasing to almost 100%.\nRecords from our AI firm indicate that this outage was caused by a software engineer running a\nload test that crashed the system.\nFigure 6 examines the impact of access to the AI system for chats that occur during and outside\nthese outage periods. These regressions are estimated at the individual chat level in order to precisely\ncompare conversations that occurred during outage periods, versus those that did not. Because we\ndo not have information on chat resolution at this level of granularity, our main outcome measure\nis chat duration. Panel A considers the impact of AI assistance using only post-adoption periods in\nwhich no outages are reported. Consistent with our main results, we see an immediate decline in\nthe duration of individual chats by approximately 10% to 15%.\nIn Panel B, we use the same pre-treatment observations, but now restrict to post-adoption\nperiods that are impacted by large outages. We first note that our estimates are noisy and their\nmagnitude appears larger than for non-outage periods (equivalent to 15% to 25% declines in chat\n19For example, research in cognitive science has shown that individuals learn less about spatial navigation when\nthey follow GPS directions, relative to using a map (?).\n25\n\nduration). Because AI outages are rare and not necessarily random, this may reflect differences in\nthe types of chats that are seen during outage periods than during non-outage periods. However,\nfocusing on the size of estimated effects over time, an interesting pattern emerges. Rather than\ndeclining immediately post-adoption and staying largely stable as we see in Panel A for non-outage\nperiods, Panel B shows that the benefit of exposure to AI assistance increases with time during\noutage periods. That is, if an outage occurs one month after AI adoption, workers do not handle\nthe chat much more quickly than their pre-adoption baseline. Yet, if an outage occurs after three\nmonths of exposure to AI recommendations, workers handle the chat faster\u2014even though they are\nnot receiving direct AI assistance in either case.\nPanel B highlights the potential scope for improving existing employee training practices. Prior\nto AI assistance, training was limited to brief weekly coaching sessions where managers reviewed\nselect conversations and provided feedback.\nHowever, by necessity, managers can only provide\nfeedback on a small fraction of the conversations an agent conducts. Moreover, because managers\nare often short on time and may lack training, they often simply point out weak metrics (\u201cyou\nneed to reduce your handling time\u201d) rather than identifying strategies for how an agent could better\napproach a problem (\u201cyou need to ask more questions at the beginning to diagnose the issue better.\u201d)\nSuch coaching can be ineffective and can be counterproductive to employee engagement (Berg et al.,\n2018). In contrast, AI assistance offers workers specific, real-time, actionable suggestions, potentially\naddressing a limitation of traditional coaching methods.\nTo better understand how learning might occur, in Panels C and D of Figure 6, we divide our\nmain study of outage events by the initial adherence of the worker to AI, as described in Section 5.1.\nWhen a worker chooses not to follow a particular AI recommendation, they miss the opportunity to\nobserve how the customer might respond. AI suggestions may prompt workers to communicate in\nways that differ from their natural style, such as by expressing more enthusiasm or empathy, or by\nfrequently pausing to recap the conversation. Workers who do not \u201ctry out\u201d these recommendations\nmay never realize that customers could react positively to them.\nPanel C reveals that workers with high initial adherence to AI recommendations experience\nsignificant and rapid declines in chat processing times, even during outages, relative to their pre-\nadoption baseline. In contrast, Panel D shows no such improvement for workers who frequently\ndeviate from AI suggestions; they see no reduction in chat times during outage periods, even after\nprolonged AI access. These findings suggest that workers learn more by actively engaging with AI\nsuggestions and observing first-hand how customers respond. These findings are consistent with\nother evidence from education that having students attempt math problems on their own before\nseeing LLM-generated responses positively impacted learning (Kumar et al., 2023). Together, these\n26\n\nresults suggest that AI assistance can play a useful role in supplementing existing on-the-job training\nprograms.\n5.3\nHandling Routine and Non-Routine Topics\nSo far, our analyses have focused on how the impact of AI assistance varies by characteristics of the\nworker. The impact of AI of course, can also depend on the types of problems it is asked to resolve.\nAgents in our data encounter a diverse set of customer questions, ranging from common requests for\nhelp in onboarding an employee or changing a password, to less common questions such as setting\nup wage garnishments in child support cases or ensuring compliance with international tax treaties.\nWhen an AI model is trained on these conversations, it will naturally have more examples of the\nmost common problems, raising the question of whether it can effectively provide suggestions for\ntopics it has encountered less frequently.\nIn this section, we examine the impact of AI-assistance for more and less routine customer\nproblems. To categorize the diverse conversations in our data, we employed Gemini, a large language\nmodel developed by Google DeepMind, to classify the interactions into topic categories. The details\nof this process, along with our human validation of the LLM classification process, are described in\nAppendix Section A.2 (Gemini Team, 2024).\nAppendix Figure A.16 reports the distribution of conversation topics in our dataset. Unsur-\nprisingly for a customer service setting, we observe a small number of frequent issues, accompanied\nby a long tail of less common problems. Specifically, the two most prevalent topics\u2014payroll and\ntaxes, and account access and management\u2014comprise half of all conversations and the top 16 topics\nrepresent over 90% of all chats.\nTo evaluate the impact of AI assistance based on the frequency of customer inquiries, we cat-\negorize conversations into four distinct groups. The \u201cPayroll/Account\u201d category, comprising 50%\nof all chats and including the two most common topics, includes inquiries related to payroll, taxes,\nand account access and management. The next 25% of chats cover 5 additional topic categories, for\ninstance, those dealing with bank transfers or managing subscriptions. The following 15% of chats\nencompass nine additional topics while the final 10% of chats includes the remaining topics. Our\nregression, in Appendix Section A.3, is conducted at the chat level, with a focus on chat duration.\nPanel A of Figure 7 shows the average treatment effect of AI assistance according to the routine\nnature of the customer support inquiry. This pattern is non-monotonic: our results suggest that\nAI-assistance has the greatest impact on workers\u2019 ability to handle problems that are moderately\nrare. Workers with access to AI assistance handle the most routine problems\u2014payroll and account\nmanagement\u2014about 4 to 5 minutes faster, which corresponds to an approximately 10% decrease\n27\n\noff the pre-treatment mean duration for these topics. We see the largest decline, 5 to 6 minutes,\nfor issues that are in the 75th to 90th percentiles of topic rarity, corresponding to a 14% reduction\ngiven the pre-treatment means for those topics. Finally, we see a smaller 4 minute or 11% decrease\nfor the most rare problems.\nThese results highlight the difference between the technical quality of an AI system and its\npotential productivity impacts in real-world settings. AI models generally perform better when\ntrained on large datasets, as these provide diverse examples and richer contextual information,\nenabling the model to learn more robust and generalizable patterns while reducing the risk of\noverfitting (Halevy et al., 2009). Consequently, we might expect an AI system to function best\nwhen dealing with routine problems, where training data are abundant.\nHowever, the value of AI systems, when used to complement human workers is less straightfor-\nward. Customer service agents, especially those dealing with common issues, are specifically trained\nto address these routine problems efficiently and get the most experience answering common ques-\ntions. For example, even novice workers are likely to know how to reset a customer\u2019s password. In\nsuch cases, access to even high quality AI assistance may not have a large complementary impact\non the most common problems. Rather, as our findings suggest, the impact of an AI system on\nworkplace productivity depends critically on its capabilities relative to workers\u2019 baseline skills. The\ngreatest productivity gains may occur not where the AI system is most capable in absolute terms,\nbut where its capabilities most effectively complement or exceed those of human workers.\nIn our setting, the heterogeneous impact of AI access appears to reflect both factors: AI access\nhas the smallest reduction in handle time for problems where human agents are already well trained\n(very routine problems) or where its training data may be sparse (very rare problems). We see the\nlargest improvements in the handle time for somewhat uncommon problems. Here, the AI system is\nlikely to have enough training data to assess these problems, while individual agents are less likely to\nhave had much first-hand experience. For example, the AI system we study provides workers with\nlinks to potentially relevant technical documentation. This may be particularly valuable for the\ntypes of cases where agents may not be aware of the solution and would need to put the customer\non hold while they search for an answer.\nTo examine the role of agent-specific experience, Panel B of Figure 7 plots the impact of AI\nassistance on chat duration by quartiles of topic frequency with respect to an individual agent,\ncontrolling for the overall frequency of a problem. Here we show that AI assistance reduces con-\nversation times most for problems that an agent has encountered least often: 15% for the least\ncommon problems compared with 10% for the most common. Once we control for a topic\u2019s overall\nfrequency, we now find a monotonic relationship between agent-specific exposure to a problem and\n28\n\nthe impact of AI. That is, holding constant the AI model\u2019s exposure to a problem, the impact of\nAI assistance is greatest for problems that a specific agent is least exposed to. This suggests that\nwhile AI in isolation may be most effective where training data is most plentiful, the marginal value\nof AI assistance is highest where humans have a greater need for AI input.\n5.4\nConversational Style\nOur previous analysis examined how the effectiveness of AI assistance varies based on the technical\nnature of the problems being addressed. However, top customer service agents not only resolve\ntechnical issues but also communicate in a way that ensures customer satisfaction. For our US-\nbased customers, this involves clearly writing in fluent English and displaying subtle interpersonal\nskills. In this section, we first demonstrate that AI assistance improves overall language fluency.\nThen, to capture less quantifiable changes in communication style, we compare the evolution of\nconversational text produced by different workers. Our analysis provides suggestive evidence that\nAI adoption helps lower-skill workers communicate more like their higher-skill peers.\n5.4.1\nEnglish Fluency\nFor contact center workers serving US customers, the ability to communicate in clear, idiomatic\nEnglish is crucial to their job performance and customer satisfaction. We begin by assessing how\nAI assistance influences workers\u2019 ability to communicate clearly. In our data, 80% of the agents\nare based in the Philippines, where many residents are fluent English speakers.20 However, cultural\ndifferences and language nuances can occasionally lead to misunderstandings or a sense of disconnect,\neven when an agent\u2019s technical language skills are strong.\nWe measure the proficiency of text in two ways: its comprehensiblity and its native fluency.\nThe comprehensibility score assesses whether the agent produces text that is cogent and easy to\nunderstand, using a scale of 1 to 5, where 1 indicates \u201cvery difficult to comprehend\u201d and 5 signifies\n\u201cvery fluent and easily understandable, with no significant errors.\u201d In contrast, our native fluency\nfocuses on whether the text was likely to have been produced by a native speaker of American\nEnglish. Our criteria for native-like fluency are based on the Interagency Language Roundtable\n\u201cfunctionally native\u201d proficiency standard. This is also a 5 point scale where 1 indicates that a\nwriter is \u201cDefinitely not a native American English speaker\u201d and 5 indicates that they definitely\nare. For instance, \u201cI could care less\u201d is grammatically incorrect, but a common English-language\n20English is an official language of the Philippines and Filipinos, especially those who are college educated, often\nhave near-native fluency due to early exposure in schools. Further, the Philippines is a former American colony and\nAmerican English tends to be the dominant version of English used in the media.\n29\n\nexpression. On the other hand, Filipino agents often use the greeting \u201cto have a blessed day,\u201d which\nis grammatically correct, but not a common greeting in the United States. We use Gemini, an LLM,\nto score agents\u2019 text in each conversation. For more information on our specific approach, prompts,\nand validation tests, see Appendix A.2.5.\nFirst, we note that the general level of both comprehensibility and native fluency is high: prior\nto having AI access, the interquartile range of comprehensibility scores was 4.28 to 4.67; for native\nfluency it was 4.26 to 4.65. Despite this high baseline level, we find clear evidence that access\nto AI assistance increases proficiency scores in our data. This can be seen in Appendix Figure\nA.17 which simply presents the raw pre- and post-AI distribution of comprehensibility and native\nfluency scores for never-treated workers, pre-treatment workers, and post-treatment workers. The\nnever treated and pre-treatment workers have identical distributions but that we see markedly higher\nscores for post-treatment workers. In Panels A and B of Figure 8, we report the accompanying event\nstudies: we see a marked and significant improvement in both comprehensiblity and native fluency.\nFinally, in Panels C and D of Figure 8, we report separate coefficients for US- and Philippines-based\nworkers. We see a positive impact for all workers, but a larger improvement for workers based in\nthe Philippines.\n5.4.2\nTextual Convergence\nThe analysis above focuses on an important, but narrow, aspect of how workers communicate. To\ngain a broader understanding of AI\u2019s influence on communication patterns, we examine how the\ntext produced by workers evolves over time: do they change how they write relative to their pre-\nAI baseline, and does AI access impact the relative communication patterns of high and low skill\nworkers? Because tacit knowledge is, by definition, not something that can be codified as a set of\nrules, we examine the overall textual similarity of conversations using textual embeddings, rather\nthan looking for the presence of specific formulaic phrases (Hugging Face, 2023).21\nPanel A of Appendix Figure 9 plots the evolution of agents\u2019 communication over time, before and\nafter access to AI assistance. We compute the cosine similarity of agents\u2019 text in each given event-\ntime week to their own chats from the month before AI deployment (week -4 to week -1). Cosine\nsimilarity runs from 0 to 1, with 0 meaning that two pieces of text are orthogonal (when represented\nas semantic vectors), and 1 indicating exact semantic similarity. Prior to the deployment of AI,\nthe similarity between a worker\u2019s own text from month to month is stable, at 0.67, which reflects\nconsistency in an individual agent\u2019s language use, while also capturing differences in the topics and\ncustomers that she faces. However, following AI deployment, the similarity of agents\u2019 text drops.\n21For more details on this process, see Appendix Section A.2.\n30\n\nThe drop is equivalent to about half of a standard deviation of within-agent cosine similarity across\nthe pre-period. This is consistent with the idea that AI assistance changes the content of agents\u2019\nmessages, rather than merely leading workers to type the same content but faster. Panel B of Figure\n9 plots the average change in textual content separately by pre-AI worker skill. Lower-skill agents\nexperience greater textual change after AI adoption, relative to top performers.\nNext, we examine across-worker changes in communication, focusing on the gap between high\nand low skill workers. Panel C of Appendix Figure 9 plots the cosine similarity between high- and\nlow-skill agents at specific moments in calendar time, separately, for workers without (blue dots)\nand with (red diamonds) access to AI assistance.22 Without AI, high- and low-productivity workers\nshow a moderate level of similarity in their language use, with an average cosine similarity between\nhigh and low workers of 0.55. This similarity remains stable over time, suggesting that there are no\ndivergent trends between skill groups that do not have access to AI assistance. Post-AI adoption,\nhowever, text similarity between high and slow skill workers begins increasing, from 0.55 to 0.61.\nWhile this change may seem modest, it represents a substantial narrowing of language gaps, given\nthat the average similarity of a high-skill worker\u2019s own pre and post AI text is only 0.67. The change\nis equivalent to half of a standard deviation of the average high and low worker textual similarity.\nTaken together, the patterns in Appendix Figure 9 are consistent with the idea that AI assistance\nleads lower-skill workers to change how they communicate more, and that these changes ultimately\nlead them to communicate more like high skill workers. We caution, however, that changes in agent\ntext can reflect many factors that are not directly related to a worker\u2019s style or tacit skills, such as\nchanges in conversation topics driven by customers. As a result, we stress that this analysis is only\nsuggestive.\n6\nEffects on the Experience of Work\nQualitative studies suggest that working conditions for contact center agents can be unpleasant.\nThey are regularly exposed to challenging, emotionally charged and anonymized conversations, and\ncalled upon to absorb customer frustrations while limiting one\u2019s own emotional reaction (Hochschild,\n2019). The stress associated with this type of emotional labor is often cited as a key cause of burnout\nand attrition among customer service workers (Lee, 2015). Additionally, contact center work for US-\nbased businesses is frequently outsourced to countries such as India and the Philippines, meaning\nthat agents often work difficult hours and may face cultural barriers or judgements when speaking\n22For non-AI users, we define skill levels based on monthly quintiles of our skill index. For AI users, we use skill\nquintiles at the time of AI deployment.\n31\n\nwith customers.\njMoreover, increases in productivity may not improve the work environment,\nespecially if workers feel pressure to work faster.\nIn this section, we examine the impact of generative AI on two particularly salient aspects of\nthe workplace experience: how agents are treated by customers and how often they face requests\nto speak with a manager. We also examine the impact of AI assistance on worker turnover as an\noverall indicator of worker satisfaction.\n6.1\nCustomer Sentiment\nCustomers often vent their frustrations to anonymous service agents and, in our data, we see regular\ninstances of swearing, verbal abuse, and \u201cyelling\u201d (typing in all caps).\nAccess to AI-assistance may impact how customers treat agents, but the direction and magnitude\nof these impacts are ambiguous. AI assistance may improve the tenor of conversations by helping\nagents set customer expectations or resolve their problems more quickly. Alternatively, customers\nmay become more frustrated if AI-suggested language feels \u201ccorporate\u201d or insincere.\nTo assess this, we capture the affective nature of both agent and customer text, using sentiment\nanalysis (Mejova, 2009). For this analysis, we use SiEBERT, an LLM that is fine-tuned for sentiment\nanalysis using a variety of datasets, including product reviews and tweets (Hartmann et al., 2023).\nSentiment is measured on a scale from \u00b41 to 1, where \u00b41 indicates negative sentiment and 1\nindicates positive. In a given conversation, we compute separate sentiment scores for both agent\nand customer text. We then aggregate these chat-level variables into a measure of average agent\nsentiment and average customer sentiment for each agent-year-month.\nPanel A and B of Appendix Figure A.18 shows the distribution of customer sentiment scores.\nOn average, customer sentiments in our data are mildly positive and normally distributed around\na mean of 0.14, except for a mass of very positive and very negative scores. Panel B shows the\ndistribution of sentiments associated with agents: agents are unfailingly positive, with a mean\nsentiment score of 0.89. This reflects the fact that agents are trained to be extremely polite and\nfriendly, even prior to AI access.\nPanels A and B of Figure 10 consider how sentiment scores respond following the rollout of AI\nassistance. In Panel A, we see an immediate and persistent improvement in customer sentiment.\nThis effect is economically large: according to Column 1 of Table 4, access to AI improves the mean\ncustomer sentiments (averaged over an agent-month) by 0.18 points, equivalent to half of a standard\ndeviation. In Panel B, we see no detectable effect for agent sentiment, which is already very high\nat baseline. Column 2 of Table 4 indicates that agent sentiments increase by only 0.02 points or\nabout 1% of a standard deviation.\n32\n\nFocusing on customer sentiment, Panels C and D of Appendix Figure A.18 examine whether\naccess to AI has different impacts across agents. We find that access to AI assistance significantly\nimproves how customers treat agents of all skill and experience levels, with the largest effects for\nagents in the lower to lower-middle range of both the skill and tenure distributions. Consistent\nwith our productivity results, the highest-performing and most-experienced agents see the smallest\nbenefits of AI access. These results suggest that AI recommendations, which were explicitly designed\nto prioritize more empathetic responses, may improve agents\u2019 demonstrated social skills and have a\npositive emotional impact on customers.\n6.2\nCustomer Confidence and Managerial Escalation\nChanges in individual productivity may have broader implications for organizational workflows\n(Garicano, 2000; Athey et al., 1994; Athey and Stern, 1998). In most customer service settings,\nfront-line agents attempt to resolve customer problems but can seek the help of supervisors when\nthey are unsure how to proceed. Customers, knowing this, will sometimes attempt to escalate a\nconversation by asking to speak to a manager. This type of request generally occurs when the\ncustomer becomes frustrated and feels that the current agent is not equipped to address their\nproblem.\nIn Panel C of Figure 10, we consider the impact of AI-assistance on the frequency of chat\nescalation. The outcome variable we focus on is the share of an agent\u2019s chats in which a customer\nrequests to speak to a manager or supervisor, aggregated to the year-month level. We focus on\nrequests for escalation rather than actual escalations both because we lack data on actual escalations\nand because requests are a better measure of customer confidence in an agent\u2019s competence or\nauthority. Following the introduction of AI assistance, we see a gradual decline in requests for\nescalation.\nRelative to a baseline rate of approximately 6 percentage points, these coefficients\nsuggest that AI assistance generates an almost 25% decline in customer requests to speak to a\nmanager. In Panels E and F of Appendix Figure A.18, we consider how these patterns change\ndepending on the skill and experience of the worker. While these results are relatively noisy, our\npoint estimates suggest that requests for escalation are disproportionately reduced for agents who\nwere less skilled or less experienced at the time of AI adoption.\n6.3\nAttrition\nThe adoption of generative AI tools can affect workers in various ways, including their productivity,\nthe level of stress they experience, how customers perceive them, and their overall job satisfaction.\n33\n\nWhile we cannot directly observe all these factors, we can analyze turnover patterns as a broad\nmeasure of how workers respond to AI implementation.\nIn this analysis, we compare attrition rates between AI-assisted agents and untreated agents\nwith equal tenure. We drop observations for treated agents before treatment because they do not\nexperience attrition by construction (they must survive to be treated in the future), and control for\nlocation and time fixed effects.23\nConsistent with our findings so far, Panel A of Figure 11 shows that access to AI assistance is\nassociated with the strongest reductions in attrition among newer agents, those with less than 6\nmonths of experience. The magnitude of this coefficient, around 10 percentage points, translates\ninto a 40% decrease relative to a baseline attrition rate in this group of 25%. In Panel B, we examine\nattrition by worker skill. Here, we find a significant decrease in attrition for all skill groups, although\nwithout a clear gradient.\nThese results should be taken with more caution relative to our main results because we are\nunable to include agent fixed effects because attrition only occurs once per worker. Our results may\noverstate the impact of AI access on attrition if, for example, the firm is more likely to give AI\naccess to agents deemed more likely to stay.\n7\nConclusion\nAdvancements in AI technologies open up a broad set of economic possibilities. Our paper provides\nthe first empirical evidence on the effects of a generative AI tool in a real-world workplace. In our\nsetting, we find that access to AI-generated recommendations increases overall worker productivity\nby 15%, with even larger impacts for lower-skill and novice agents. These productivity gains in\npart reflect durable worker learning rather than rote reliance on AI suggestions.\nFurthermore,\nAI assistance appears to improve worker\u2019s on the job experiences, such as by improving customer\nsentiment and confidence, and is associated with reductions in turnover.\nOur analysis is subject to some caveats and raises many unanswered questions.\nFirst, we note again that our findings apply for a particular AI tool, used in a single firm, within\na single occupation, and should not be generalized across all occupations and AI systems.\nFor\nexample, our setting has a relatively stable product and set of technical support questions. In areas\nwhere the product or environment is changing rapidly, the relative value of AI recommendations may\nbe different: they may be better able to synthesize changing best practices, or they may actually\nimpede learning by promoting outdated practices observed in historical training data. Indeed recent\n23See Appendix Section A.3 for the exact specification.\n34\n\nwork by Otis et al. (2023) and Perry et al. (2022) have found instances in which AI adoption has\nlimited or even negative effects.\nSecond, we report partial equilibrium short to medium run impacts of AI deployment. While we\ndo not have access to pay data, the managers we spoke to believed that workers may have received\nhigher performance pay as a result of AI assistance, since these bonuses were typically tied to targets\nrelated to average handle time and resolution rates. They caution, however, that potential gains\nin bonus pay may not be long lived because it is common practice to adjust performance targets if\ntoo many agents were hitting the goals. As a result, workers may eventually be subject to a ratchet\neffect if AI assistance leads performance targets to be readjusted upwards.\nMore generally, we are not able to observe longer run equilibrium responses in worker demand or\njob design. If customer demand for assistance is inelastic, then the productivity gains we document\nwill likely translate into less demand for human labor. A back-of-the-envelope calculation suggests\nthe firm could field the same number of customer support issues with 12% fewer worker-hours.\nConversely, individuals may currently avoid contacting customer service because of the long wait\ntimes and low-quality service. AI assistance that improves this experience may boost consumer\ndemand for product support, resulting in increased labor demand (Berg et al., 2018; Korinek,\n2022). It is also possible that the use of AI could create new jobs for customer service agents, such\nas testing and training AI models (Autor et al., 2022). One manager we spoke with reports that,\nin some contact centers, high-skill workers are already being tasked with reviewing AI-suggestions\nand providing better alternatives. Other work shows that even low levels of AI adoption can impact\nmarket equilibrium prices and quantities, highlighting the need for more work on the equilibrium\neffects of AI on the labor market (Raymond, 2023).\nFurthermore, the eventual impacts of AI tools on workers, wages, and aggregate productivity\neffects will depend on context-specific factors and the development of complementary organizational\npractices and educational policies (Brynjolfsson et al., 2021). For example, Humlum and Vestergaard\n(2024) shows that workers\u2019 willingness to use AI depends critically on access to training and on\nregulations such as those related to data confidentiality.\nFinally, our findings also raise questions about the nature of worker productivity. Traditionally,\na support agent\u2019s productivity refers to their ability to help the customers they come in contact\nwith. Yet, in a setting where customer service conversations are fed into training datasets, a worker\u2019s\nproductivity also includes their ability to provide ML models with examples of successful behaviors\nthat can be shared with others. Top performers, in particular, contribute many of the examples\nused to train the AI system we study, but our results suggest that access to AI suggestions may lead\nthem to put too little effort into coming up with new solutions. Going forward, compensation polices\n35\n\nthat provide incentives for workers to contribute to model training could be important. Given the\nearly stage of generative AI, these and other questions deserve further scrutiny.\n36\n\nReferences\nAcemoglu, Daron, \u201cThe Simple Macroeconomics of AI,\u201d 2024.\nand David Autor, \u201cSkills, tasks and technologies: Implications for employment and earnings,\u201d\nin \u201cHandbook of labor economics,\u201d Vol. 4, Elsevier, 2011, pp. 1043\u20131171.\nand Pascual Restrepo, \u201cLow-Skill and High-Skill Automation,\u201d Journal of Human Capital,\nJune 2018, 12 (2), 204\u2013232.\nand\n, \u201cRobots and Jobs: Evidence from US Labor Markets,\u201d Journal of Political Economy,\n2020, 128 (6), 2188\u20132244. _eprint: https://doi.org/10.1086/705716.\n, Gary Anderson, David Beede, Catherine Buffington, Eric Childress, Emin Din-\nlersoz, Lucia Foster, Nathan Goldschlag, John Haltiwanger, Zachary Kroff, Pascual\nRestrepo, and Nikolas Zolas, \u201cAutomation and the Workforce: A Firm-Level View from the\n2019 Annual Business Survey,\u201d 2022.\n, Philippe Aghion, Claire Lelarge, John Van Reenen, and Fabrizio Zilibotti, \u201cTech-\nnology, Information, and the Decentralization of the Firm*,\u201d The Quarterly Journal of Eco-\nnomics, November 2007, 122 (4), 1759\u20131799. _eprint: https://academic.oup.com/qje/article-\npdf/122/4/1759/5234557/122-4-1759.pdf.\nAgarwal, Nikhil, Alex Moehring, Pranav Rajpurkar, and Tobias Salz, \u201cCombining Human\nExpertise with Artificial Intelligence: Experimental Evidence from Radiology,\u201d Working Paper\n31422, National Bureau of Economic Research July 2023.\nAkerman, Anders, Ingvil Gaarder, and Magne Mogstad, \u201cThe Skill Complementarity of\nBroadband Internet *,\u201d The Quarterly Journal of Economics, July 2015, 130 (4), 1781\u20131824.\n_eprint: https://academic.oup.com/qje/article-pdf/130/4/1781/30637431/qjv028.pdf.\nAngelova, Victoria, Will S Dobbie, and Crystal Yang, \u201cAlgorithmic Recommendations and\nHuman Discretion,\u201d Working Paper 31747, National Bureau of Economic Research September\n2023.\nAthey, Susan and Scott Stern, \u201cAn Empirical Framework for Testing Theories About Compli-\nmentarity in Organizational Design,\u201d Working Paper 6600, National Bureau of Economic Research\nJune 1998.\nand\n, \u201cThe Impact of Information Technology on Emergency Health Care Outcomes,\u201d RAND\nJournal of Economics, Autumn 2002, 33 (3), 399\u2013432.\n, Joshua Gans, Scott Schaefer, and Scott Stern, \u201cThe Allocation of Decisions in Organiza-\ntions,\u201d Stanford Graduate School of Business, 1994.\nAutor, David, \u201cPolanyi\u2019s Paradox and the Shape of Employment Growth,\u201d Working Paper w20485,\nNational Bureau of Economic Research September 2014.\n, Caroline Chin, Anna M Salomons, and Bryan Seegmiller, \u201cNew Frontiers: The Origins\nand Content of New Work, 1940\u20132018,\u201d Working Paper 30389, National Bureau of Economic\nResearch August 2022.\nAutor, David H., Frank Levy, and Richard J. Murnane, \u201cThe Skill Content of Recent\nTechnological Change: An Empirical Exploration,\u201d The Quarterly Journal of Economics, 2003,\n118 (4), 1279\u20131333.\n, Lawrence F. Katz, and Alan B. Krueger, \u201cComputing Inequality:\nHave Computers\nChanged the Labor Market?*,\u201d The Quarterly Journal of Economics, November 1998, 113\n(4), 1169\u20131213.\n_eprint: https://academic.oup.com/qje/article-pdf/113/4/1169/5406877/113-\n4-1169.pdf.\nBabina, Tania, Anastassia Fedyk, Alex Xi He, and James Hodson, \u201cArtificial Intelligence,\nFirm Growth, and Product Innovation,\u201d May 2022.\n37\n\nBahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio, \u201cNeural Machine Translation\nby Jointly Learning to Align and Translate,\u201d in Yoshua Bengio and Yann LeCun, eds., 3rd Inter-\nnational Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9,\n2015, Conference Track Proceedings, 2015.\nBaker, George P. and Thomas N. Hubbard, \u201cMake Versus Buy in Trucking: Asset Ownership,\nJob Design, and Information,\u201d American Economic Review, June 2003, 93 (3), 551\u2013572.\nBartel, Ann, Casey Ichniowski, and Kathryn Shaw, \u201cHow Does Information Technology\nAffect Productivity? Plant-Level Comparisons of Product Innovation, Process Improvement, and\nWorker Skills*,\u201d The Quarterly Journal of Economics, 11 2007, 122 (4), 1721\u20131758.\nBerg, Jeff, Avinash Das, Vinay Gupta, and Paul Kline, \u201cSmarter call-center coaching for\nthe digital world,\u201d Technical Report, McKinsey & Company November 2018.\nBloom, Nicholas, Luis Garicano, Raffaella Sadun, and John Van Reenen, \u201cThe Distinct\nEffects of Information Technology and Communication Technology on Firm Organization,\u201d Man-\nagement Science, 2014, 60 (12), 2859\u20132885.\nBorusyak, Kirill, Xavier Jaravel, and Jann Spiess, \u201cRevisiting Event Study Designs: Robust\nand Efficient Estimation,\u201d 2022.\nBresnahan, Timothy F., Erik Brynjolfsson, and Lorin M. Hitt, \u201cInformation Technology,\nWorkplace Organization, and the Demand for Skilled Labor: Firm-Level Evidence,\u201d The Quarterly\nJournal of Economics, 02 2002, 117 (1), 339\u2013376.\nBrown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Pra-\nfulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,\nSandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon\nChild, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christo-\npher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and\nDario Amodei, \u201cLanguage Models are Few-Shot Learners,\u201d July 2020. arXiv:2005.14165 [cs].\nBrynjolfsson, Erik and Tom Mitchell, \u201cWhat Can Machine Learning, Do? Workforce Implica-\ntions,\u201d Science, December 2017, 358, 1530\u20131534.\n, Daniel Rock, and Chad Syverson, \u201cThe Productivity J-Curve: How Intangibles Complement\nGeneral Purpose Technologies,\u201d American Economic Journal: Macroeconomics, January 2021, 13\n(1), 333\u201372.\nBubeck, Sebastien, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric\nHorvitz,\nEce Kamar,\nPeter Lee,\nYin Tat Lee,\nYuanzhi Li,\nScott Lundberg\net al., \u201cSparks of artificial general intelligence: Early experiments with gpt-4,\u201d arXiv preprint\narXiv:2303.12712, 2023.\nBuesing, Eric, Vinay Gupta, Sarah Higgins, and Raelyn Jacobson, \u201cCustomer care: The\nfuture talent factory,\u201d Technical Report, McKinsey & Company June 2020.\nCallaway, Brantly and Pedro H. C. Sant\u2019Anna, \u201cDifference-in-Differences with multiple time\nperiods,\u201d Journal of Econometrics, December 2021, 225 (2), 200\u2013230.\nCalvino, Flavio and Luca Fontanelli, \u201cA Portrait of AI Adopters across Countries: Firm Char-\nacteristics, Assets\u2019 Complementarities and Productivity,\u201d Technical Report, OECD, Paris April\n2023.\nCengiz, Doruk, Arindrajit Dube, Attila Lindner, and Ben Zipperer, \u201cThe Effect of Min-\nimum Wages on Low-Wage Jobs*,\u201d The Quarterly Journal of Economics, May 2019, 134 (3),\n1405\u20131454.\nChoi, Jonathan H. and Daniel Schwarcz, \u201cAI Assistance in Legal Analysis: An Empirical\nStudy,\u201d August 2023.\n38\n\nChoi, Jun Ho, Oliver Garrod, Paul Atherton, Andrew Joyce-Gibbons, Miriam Mason-\nSesay, and Daniel Bj\u00f6rkegren, \u201cAre LLMs Useful in the Poorest Schools? TheTeacher.AI in\nSierra Leone,\u201d 2024.\nChui, Michael, Bryce Hall, Alex Singla, and Alex Sukharevsky, \u201cGlobal survey: The state\nof AI in 2021,\u201d Technical Report, McKinsey & Company 2021.\nde Chaisemartin, Cl\u00e9ment and Xavier D\u2019Haultf\u0153uille, \u201cTwo-Way Fixed Effects Estimators\nwith Heterogeneous Treatment Effects,\u201d American Economic Review, September 2020, 110 (9),\n2964\u201396.\nDell\u2019Acqua, Fabrizio, Edward McFowland III, Ethan Mollick, Lifshitz-Assaf, Katherine\nKellogg, Saran Rajendran, Lisa Krayer, Francois Candelon, and Karim Lakhani,\n\u201cNavigating the Jagged Technological Frontier: Field Experimental Evidence of the Effects of AI\non Knowledge Worker Productivity and Quality,\u201d September 2023.\nDixon, Jay, Bryan Hong, and Lynn Wu, \u201cThe Robot Revolution: Managerial and Employment\nConsequences for Firms,\u201d DecisionSciRN: Other Performance Management (Sub-Topic), 2020.\nDunn, Andrew, Diana Inkpen, and R\u0103zvan Andonie, \u201cContext-Sensitive Visualization of\nDeep Learning Natural Language Processing Models,\u201d 2021.\nEloundou, Tyna, Sam Manning, Pamela Mishkin, and Daniel Rock, \u201cGPTs are GPTs:\nAn Early Look at the Labor Market Impact Potential of Large Language Models,\u201d March 2023.\narXiv:2303.10130 [cs, econ, q-fin].\nFelten, Edward W., Manav Raj, and Robert Seamans, \u201cOccupational Heterogeneity in\nExposure to Generative AI,\u201d April 2023.\nGaricano, Luis, \u201cHierarchies and the Organization of Knowledge in Production,\u201d Journal of Po-\nlitical Economy, 2000, 108 (5), 874\u2013904. Publisher: The University of Chicago Press.\nand Esteban Rossi-Hansberg, \u201cKnowledge-Based Hierarchies: Using Organizations to Un-\nderstand the Economy,\u201d Annual Review of Economics, 2015, 7 (1), 1\u201330.\nGemini Team, \u201cGemini: A Family of Highly Capable Multimodal Models,\u201d Working Papers,\nGoogle 2024.\nGoldin, Claudia and Lawrence F. Katz, \u201cThe Origins of Technology-Skill Complementarity*,\u201d\nThe Quarterly Journal of Economics, 08 1998, 113 (3), 693\u2013732.\nand\n, The Race between Education and Technology, Harvard University Press, 2008.\nGoodman-Bacon, Andrew, \u201cDifference-in-differences with variation in treatment timing,\u201d Jour-\nnal of Econometrics, December 2021, 225 (2), 254\u2013277.\nGoogle, \u201cAI vs. Machine Learning: How Do They Differ?\u201d\nGretz, Whitney and Raelyn Jacobson, \u201cBoosting contact-center performance through employee\nengagement,\u201d Technical Report, McKinsey & Company 2018.\nHalevy, Alon, Peter Norvig, and Fernando Pereira, \u201cThe Unreasonable Effectiveness of\nData,\u201d IEEE Intelligent Systems, March 2009, 24 (2), 8\u201312.\nHartmann, Jochen, Mark Heitmann, Christian Siebert, and Christina Schamp, \u201cMore\nthan a Feeling: Accuracy and Application of Sentiment Analysis,\u201d International Journal of Re-\nsearch in Marketing, 2023, 40 (1), 75\u201387.\nHochschild, Arlie Russell, The managed heart: Commercialization of human feeling, University\nof California press, 2019.\nHoffman, Mitchell, Lisa B Kahn, and Danielle Li, \u201cDiscretion in Hiring*,\u201d The Quarterly\nJournal of Economics, 10 2017, 133 (2), 765\u2013800.\n39\n\nHugging Face, \u201csentence-transformers/all-MiniLM-L6-v2,\u201d April 2023.\nHumlum, Anders and Emilie Vestergaard, \u201cThe Adoption of ChatGPT,\u201d May 2024.\nKanazawa, Kyogo, Daiji Kawaguchi, Hitoshi Shigeoka, and Yasutora Watanabe, \u201cAI,\nSkill, and Productivity: The Case of Taxi Drivers,\u201d Working Paper 30612, National Bureau of\nEconomic Research October 2022.\nKaplan, Jared, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess,\nRewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei, \u201cScaling laws\nfor neural language models,\u201d arXiv preprint arXiv:2001.08361, 2020.\nKatz, Lawrence F. and Kevin M. Murphy, \u201cChanges in Relative Wages, 1963-1987: Supply\nand Demand Factors,\u201d The Quarterly Journal of Economics, 1992, 107 (1), 35\u201378.\nKorinek, Anton, \u201cHow innovation affects labor markets: An impact assessment,\u201d Working Paper,\nBrookings Institution June 2022.\nKoroteev, M. V., \u201cBERT: A Review of Applications in Natural Language Processing and Under-\nstanding,\u201d 2021.\nKumar, Harsh, David Rothschild, Daniel Goldstein, and Jake Hofman, \u201cMath Education\nwith Large Language Models: Peril or Promise?,\u201d SSRN Electronic Journal, 01 2023.\nLee, Don, \u201cThe Philippines has become the call-center capital of the world,\u201d Los Angeles Times,\nFebruary 2015. Section: Business.\nLegg, Shane, Marcus Hutter et al., \u201cA collection of definitions of intelligence,\u201d Frontiers in\nArtificial Intelligence and applications, 2007, 157, 17.\nLi, Chun, \u201cOpenAI\u2019s GPT-3 Language Model: A Technical Overview,\u201d June 2020.\nLiu, Yiheng, Tianle Han, Siyuan Ma, Jiayue Zhang, Yuanyuan Yang, Jiaming Tian,\nHao He, Antong Li, Mengshen He, Zhengliang Liu, Zihao Wu, Dajiang Zhu,\nXiang Li, Ning Qiang, Dingang Shen, Tianming Liu, and Bao Ge, \u201cSummary of\nChatGPT/GPT-4 Research and Perspective Towards the Future of Large Language Models,\u201d\nApril 2023. arXiv:2304.01852 [cs].\nMeijer, Erik, \u201cBehind every great deep learning framework is an even greater programming lan-\nguages concept (keynote),\u201d in \u201cProceedings of the 2018 26th ACM Joint Meeting on European\nSoftware Engineering Conference and Symposium on the Foundations of Software Engineering\u201d\n2018, pp. 1\u20131.\nMejova, Yelena, \u201cSentiment Analysis: An Overview,\u201d University of Iowa, Computer Science De-\npartment, 2009.\nMichaels, Guy, Ashwini Natraj, and John Van Reenen, \u201cHas ICT Polarized Skill Demand?\nEvidence from Eleven Countries Over Twenty-Five Years,\u201d The Review of Economics and Statis-\ntics, 2014, 96 (1), 60\u201377.\nNguyen, Nhan and Sarah Nadi, \u201cAn Empirical Evaluation of GitHub Copilot\u2019s Code Sug-\ngestions,\u201d in \u201c2022 IEEE/ACM 19th International Conference on Mining Software Repositories\n(MSR)\u201d May 2022, pp. 1\u20135. ISSN: 2574-3864.\nNoy, Shakked and Whitney Zhang, \u201cExperimental Evidence on the Productivity Effects of\nGenerative Artificial Intelligence,\u201d Available at SSRN 4375283, 2023.\nOECD, OECD Employment Outlook 2023: Artificial Intelligence and the Labour Market, Paris:\nOrganisation for Economic Co-operation and Development, 2023.\nOpenAI, \u201cGPT-4 Technical Report,\u201d Technical Report, OpenAI March 2023.\n40\n\nOtis, Nicholas G, Berkeley Haas, Rowan Clarke, and Rembrand Koning, \u201cThe Uneven\nImpact of Generative AI on Entrepreneurial Performance,\u201d December 2023.\nOuyang, Long, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schul-\nman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell,\nPeter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe, \u201cTraining language models\nto follow instructions with human feedback,\u201d March 2022. arXiv:2203.02155 [cs].\nPatel, Dylan and Gerald Wong, \u201cGPT-4 Architecture, Infrastructure, Training Dataset, Costs,\nVision, MoE,\u201d 2023.\nPeng, Baolin, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan\nHuang, Lars Liden, Zhou Yu, Weizhu Chen, and Jianfeng Gao, \u201cCheck Your Facts\nand Try Again: Improving Large Language Models with External Knowledge and Automated\nFeedback,\u201d 2023.\nPeng, Sida, Eirini Kalliamvakou, Peter Cihon, and Mert Demirer, \u201cThe Impact of AI on\nDeveloper Productivity: Evidence from GitHub Copilot,\u201d 2023.\nPerry, Neil, Megha Srivastava, Deepak Kumar, and Dan Boneh, \u201cDo Users Write More\nInsecure Code with AI Assistants?,\u201d Proceedings of the 2023 ACM SIGSAC Conference on Com-\nputer and Communications Security, 2022.\nPolanyi, Michael, The Tacit Dimension, Chicago, IL: University of Chicago Press, May 1966.\nPoursabzi-Sangdeh, Forough, Daniel G. Goldstein, Jake M. Hofman, Jennifer Wortman\nVaughan, and Hanna Wallach, \u201cManipulating and Measuring Model Interpretability,\u201d 2021.\nRadford, Alec and Karthik Narasimhan, \u201cImproving Language Understanding by Generative\nPre-Training,\u201d 2018.\n, Jeff Wu, Rewon Child, D. Luan, Dario Amodei, and Ilya Sutskever, \u201cLanguage Models\nare Unsupervised Multitask Learners,\u201d 2019.\nRaymond, Lindsey, \u201cThe Market Effects of Algorithms,\u201d Working Paper September 2023.\nRoose, Kevin, \u201cA Conversation With Bing\u2019s Chatbot Left Me Deeply Unsettled,\u201d The New York\nTimes, February 2023.\nRosen, Sherwin, \u201cThe Economics of Superstars,\u201d The American Economic Review, 1981, 71 (5),\n845\u2013858.\nSun, Liyang and Sarah Abraham, \u201cEstimating dynamic treatment effects in event studies with\nheterogeneous treatment effects,\u201d Journal of Econometrics, 2021, 225 (2), 175\u2013199.\nSyverson, Chad, \u201cWhat Determines Productivity?,\u201d Journal of Economic Literature, June 2011,\n49 (2), 326\u201365.\nTaniguchi, Hiroya and Ken Yamada, \u201cICT Capital-Skill Complementarity and Wage Inequality:\nEvidence from OECD Countries,\u201d Labour Economics, June 2022, 76, 102151. arXiv:1904.09857\n[econ, q-fin].\nThe White House, \u201cThe Impact of Artificial Intelligence on the Future of Workforces in the Eu-\nropean Union and the United States of America,\u201d Technical Report, The White House December\n2022.\nVaccaro, Michelle, Abdullah Almaatouq, and Thomas Malone, \u201cWhen Are Combinations\nof Humans and AI Useful?,\u201d 2024.\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.\nGomez, Lukasz Kaiser, and Illia Polosukhin, \u201cAttention Is All You Need,\u201d December 2017.\narXiv:1706.03762 [cs].\n41\n\nZhao, Shuyin, \u201cGitHub Copilot now has a better AI model and new capabilities,\u201d February 2023.\nZolas, Nikolas, Zachary Kroff, Erik Brynjolfsson, Kristina McElheran, David Beede,\nCatherine Buffington, Nathan Goldschlag, Lucia Foster, and Emin Dinlersoz, \u201cAd-\nvanced Technologies Adoption and Use by U.S. Firms: Evidence from the Annual Business Sur-\nvey,\u201d Working Papers 20-40, Center for Economic Studies, U.S. Census Bureau December 2020.\n42\n\nFigure 1: Raw Productivity Distributions, by AI Treatment\nA. Resolutions Per Hour\n0\n.2\n.4\n.6\nDensity\n0\n1\n2\n3\n4\n5\nResolutions Per Hour\nPre AI\nPost AI\nNever AI\nB. Average Handle Time\nC. Chats Per Hour\n0\n.01\n.02\n.03\n.04\n.05\nDensity\n0\n20\n40\n60\n80\nAverage Handle Time\nPre AI\nPost AI\nNever AI\n0\n.2\n.4\n.6\nDensity\n0\n2\n4\n6\nChats Per Hour\nPre AI\nPost AI\nNever AI\nD. Resolution Rate\nE. Customer Satisfaction (NPS)\n0\n1\n2\n3\n4\nDensity\n0\n.2\n.4\n.6\n.8\n1\nShare Resolved\nPre AI\nPost AI\nNever AI\n0\n.01\n.02\n.03\n.04\nDensity\n0\n20\n40\n60\n80\n100\nCustomer Satisfaction (NPS)\nPre AI\nPost AI\nNever AI\nNotes: This figure shows the distribution various outcome measures. We split this sample into agent-month observa-\ntions for agents who eventually receive access to the AI system before deployment (\u201cPre AI\u201d), after deployment (\u201cPost\nAI\u201d), and for agent-months associated with agents who never receive access (\u201cNever AI\u201d). Our primary productivity\nmeasure is \u201cresolutions per hour,\u201d the number of customer issues the agent is able to successfully resolve per hour.\nWe also provide descriptives for \u201caverage handle time,\u201d the average length of time an agent takes to finish a chat;\n\u201cchats per hour,\u201d the number of chats completed per hour incorporating multitasking; \u201cresolution rate,\u201d the share of\nconversations that the agent is able to resolve successfully; and \u201cnet promoter score\u201d (NPS), which are calculated by\nrandomly surveying customers after a chat and calculating the percentage of customers who would recommend an\nagent minus the percentage who would not. All data comes from the firm\u2019s software systems.\n43\n\nFigure 2: Event Studies, Productivity\n-.2\n0\n.2\n.4\n.6\n.8\nChange in Resolutions Per Hour\n-10\n-9\n-8\n-7\n-6\n-5\n-4\n-3\n-2\n-1\n0\n1\n2\n3\n4\n5\nMonths to/from AI Deployment\nNotes: This figure plot the coefficients and 95% confidence intervals from event study regressions of AI model\ndeployment on our measure of productivity, resolutions per hour, using the Sun and Abraham (2021) interaction\nweighted estimator. Our specification follows Equation 1 and includes fixed effects for agent, chat year-month and\nagent tenure in months.\nObservations are at the agent-month level, which is the most granular level at which\nresolutions per hour is available. Robust standard errors are clustered at the agent level. Section 3.1 describes the\nrollout and Appendix Section A.3 outlines the regression specification.\n44\n\nFigure 3: Heterogeneity of AI Impact, by Skill and Tenure\nA. Impact of AI on Resolutions Per Hour, by Skill at Deployment\n-.2\n0\n.2\n.4\n.6\nChange in Resolutions Per Hour\nQ1 (Lowest)\nQ2\nQ3\nQ4\nQ5 (Highest)\nAgent Skill at AI Deployment\nB. Impact of AI on Resolutions Per Hour, by Tenure at Deployment\n-.2\n0\n.2\n.4\n.6\n.8\nChange in Resolutions Per Hour\n0 Mos.\n1-2 Mos.\n3-6 Mos.\n7-12 Mos.\n>12 Mos.\nAgent Tenure at AI Deployment\nNotes: These figures plot the impacts of AI model deployment on resolutions per hour for agents grouped by pre-\nAI skill and experience.\nFor Panel A, within each month and company (e.g.\nthe employer of an agent), agents\nare grouped into quintiles of a trailing three month performance index incorporating handle time, call resolution,\nand customer satisfaction, with the most productive agents in quintile 5 and the least productive in quintile 1. In\nPanel B, pre-AI worker tenure is the number of months an agent has been employed when they receive access to AI\nrecommendations. Our regression specifications, outlined in Appendix Section A.3, include fixed effects for agent,\nchat year-month and agent tenure in months, for Panel A, and agent skill at deployment in Panel B. The month of\nAI model deployment is the month that AI output is turned on for each agent. We estimate these regressions with\nOLS and cluster standard errors at the agent level. Section 3.1 describes the rollout, Appendix Section A.2 explains\nconstruction of key variables and includes detailed regression specifications.\n45\n\nFigure 4: Experience Curves by Deployment Cohort\nResolutions Per Hour, by Agent Tenure\n1.5\n2\n2.5\n3\n3.5\nResolutions Per Hour\n0\n2\n4\n6\n8\n10\nAgent Tenure, Months\nNever Treated\nAlways Treated\nTreated 5-6 Mo.\nNotes: This figure plot the relationship between productivity and job tenure. The short dashed red line plots the\nperformance of always-treated agents, those who have access to AI assistance from their first month on the job. The\nlong dashed blue line plots agents who are never treated. The solid green line plots agents who spend their first four\nmonths of work without the AI assistance, and gain access to the AI model during their fifth month on the job. 95%\nconfidence intervals are shown. Observations are at the agent-month level.\n46\n\nFigure 5: Heterogeneity of AI Impact, by AI Adherence\nA. Distribution of AI Adherence\n0\n.005\n.01\n.015\n.02\nDensity\n0\n20\n40\n60\n80\n100\nAI Percent Adherance\nkernel = epanechnikov, bandwidth = 3.1231\nB. Impact of AI on Resolutions Per Hour, by Initial Adherence\n.1\n.2\n.3\n.4\n.5\nChange in Resolutions Per Hour\nQ1 (Low)\nQ2\nQ3\nQ4\nQ5 (High)\nInitial Adherence at AI Deployment (%)\nNotes: Panel A plots the distribution of AI adherence, averaged at the agent-month level, weighted by the log of\nthe number of AI recommendations for that agent-month. Panel B shows the impact of AI assistance on resolutions\nby hour, by agents grouped by their initial adherence, defined as the share of AI recommendations they followed in\nthe first month of treatment. The regression, outlined in Appendix Section A.3, is run at the agent-month level and\nincludes fixed effects for agent, chat year-month and agent tenure in months. Standard errors are clustered at the\nagent level.\n47\n\nFigure 6: Chat Duration during AI system Outages\nA. Post-treatment Non-Outage Periods\nB. Post-treatment Outage Periods\n-20\n-10\n0\n10\n20\nChange in Chat Duration (minutes)\n-30\n-20\n-10\n0\n10\n20\nWeeks to/from AI Deployment\n-20\n-10\n0\n10\n20\nChange in Chat Duration (minutes)\n-30\n-20\n-10\n0\n10\n20\nWeeks to/from AI Deployment\nC. Outage\u2013High Initial Adherence\nD. Outage\u2013Low Initial Adherence\n-20\n-10\n0\n10\n20\nChange in Chat Duration (minutes)\n-30\n-20\n-10\n0\n10\n20\nWeeks to/from AI Deployment\n-20\n-10\n0\n10\n20\nChange in Chat Duration (minutes)\n-30\n-20\n-10\n0\n10\n20\nWeeks to/from AI Deployment\nNotes: These figures plot event studies for the impact of AI system rollout of chat duration at the individual chat\nlevel. Panel A restricts to post-treatment chats that do not occur during any period where there is a AI system outage.\nPanel B restricts to post-treatment chats that only occur during a large system outage. Panels C and D focus on\noutage only post-periods. Panel C restricts to only chats generated by ever-treated agents who with high initial AI\nadherence (top tercile) while Panel D restricts to agents with low initial adherence (bottom tercile). Agents who are\nnever treated are excluded from this analysis. The regressions are run at the chat level with agent, year-month and\ntenure fixed effects with standard errors clustered at the agent level.\n48\n\nFigure 7: AI Impact by Chat Topic\nA. Impact of AI on Chat Duration, by Overall Topic Frequency\n-.16\n-.14\n-.12\n-.1\n-.08\nChange in Chat Duration (minute\nPayroll/Account (50%)\nNext 25%\nNext 15%\nLast 10%\nTopic Frequency\nB. Impact of AI on Chat Duration, by Agent Topic Frequency\n-.16\n-.14\n-.12\n-.1\n-.08\nPercentage Change in Chat Duration \nQ1 (Most Common)\nQ2\nQ3\nQ4 (Least Common)\nAgent Chat Frequency Quartile\nNotes: Panel A shows the impact of AI assistance on chat duration in minutes for each category of conversation topic\nfrequency. Data is at the chat level and the regressions control for topic frequency, year-month fixed effects, agent\nfixed effects, and fixed effects in months of agent tenure. Panel B shows the impact of AI assistance grouped instead\nby topic frequency as encountered by the individual agent. Appendix Section A.3 details the regression specification\nand topic category construction.\n49\n\nFigure 8: Impact of AI on Language Skills\nA. Comprehensibility Score\nB. Native Fluency Score\n-.2\n0\n.2\n.4\nChange in Agent Comprehensibility Score\n-8\n-7\n-6\n-5\n-4\n-3\n-2\n-1\n0\n1\n2\n3\n4\n5\nMonths to/from AI Deployment\n-.2\n0\n.2\n.4\nChange in Agent Native Fluency Score\n-8\n-7\n-6\n-5\n-4\n-3\n-2\n-1\n0\n1\n2\n3\n4\n5\nMonths to/from AI Deployment\nC. Comprehensibility Score, by Agent Country\nD. Native Fluency Score, by Agent Country\nFilipino Agents\nUS Agents\n.1\n.15\n.2\n.25\n.3\nImpact of AI Access on Comprehensibility\nFilipino Agents\nUS Agents\n.1\n.15\n.2\n.25\n.3\nImpact of AI Access on Native Fluency\nNotes: These figures show the impact of AI access on scores of agent comprehensibility in Panel A and native\nfluency in Panel B. Observations for this regression are at the agent-chat level, aggregate to the agent-month level.\nRegressions follow Equation 1 and include agent, chat year-month and months of agent tenure fixed effects. Robust\nstandard errors are clustered at the agent level in Panels A and B and agent location in Panels C and D. For more\ndetails on construction of the comprehensibility and native fluency scores, refer to Appendix Section A.2.\n50\n\nFigure 9: Within Agent Textual Analysis\nA. Within-Person Textual Similarity to Month Prior to AI\n.62\n.64\n.66\n.68\nWithin Agent Similarity to Pre AI Window\n-10\n-5\n0\n5\n10\nEvent Time (Weeks)\nB. Within-Person Textual Change, Low vs. High Skill\n0\n.1\n.2\n.3\nText Difference, Within Worker Pre vs. Post AI \nHigh Skill\nLow Skill\nC. Text Similarity Between Low-Skill and High-Skill Workers, Pre and Post AI\n.5\n.55\n.6\n.65\nText Similarity, High vs. Low Skill Workers\n4/1/2020\n8/1/2020\n12/1/2020\n4/1/2021\nChat Date\nNo AI\nPost AI\nNotes: Panel A plots the average similarity between an agent\u2019s chats each week and a comparison group of their\nconversations in the month prior to AI deployment (the gray section). Panel B plots the average difference between\nan agent\u2019s pre-AI corpus of chat messages and that same agent\u2019s post-AI corpus, controlling for year-month and agent\ntenure. The first bar represents the average pre-post text difference for agents in the highest quintile of pre-AI skill\nand the second bar represents those in the bottom quintile. Panel C plots the average text similarity between the top\nand bottom quintile of agents. The blue line plots the similarity for never treated or pre-treatment agents, the red\nline plots the similarity for agents with access to the AI model. For agents in the treatment group, we define agent\nskill at AI model deployment.\n51\n\nFigure 10: Experience of Work\nA. Customer Sentiment, Event Study\n-.1\n0\n.1\n.2\n.3\nChange in Mean(Customer Sentiment)\n-10\n-9\n-8\n-7\n-6\n-5\n-4\n-3\n-2\n-1\n0\n1\n2\n3\n4\n5\nMonths to/from AI Deployment\nB. Agent Sentiment, Event Study\n-.04\n-.02\n0\n.02\n.04\nChange in Mean(Agent Sentiment)\n-10\n-9\n-8\n-7\n-6\n-5\n-4\n-3\n-2\n-1\n0\n1\n2\n3\n4\n5\nMonths to/from AI Deployment\nC. Manager Escalation, Event Study\n-.03\n-.02\n-.01\n0\n.01\nChange in Share Req. Manager\n-10\n-9\n-8\n-7\n-6\n-5\n-4\n-3\n-2\n-1\n0\n1\n2\n3\n4\n5\nMonths to/from AI Deployment\nNotes: Each panel of this figure plots the impact of AI model deployment on the experience of work. Panel A plots the\nimpact of AI model deployment on customer sentiment, Panel B plots the corresponding estimate for agent sentiment,\nand Panel C show the impacts of AI assistance on customer requests for manager assistance. Sentiment is measured\nusing SiEBERT, a fine-tuned checkpoint of a RoBERTA, an English language transformer model. Regressions follow\nEquation 1 and include agent, chat year-month and months of agent tenure fixed effects. Observations are at the\nchat-level, aggregated to the agent-month and robust standard errors are clustered at the agent level.\n52\n\nFigure 11: Impact of AI Model Deployment on Worker Attrition\nA. By Productivity at AI Model Deployment\n-.12\n-.1\n-.08\n-.06\n-.04\n-.02\nChange in Attrition Likelihood\nQ1 (Low)\nQ2\nQ3\nQ4\nQ5 (High)\nAgent Skill at AI Deployment\nB. By Tenure at AI Model Deployment\n-.15\n-.1\n-.05\n0\n.05\nChange in Attrition Likelihood\n0 Mos.\n1-2 Mos.\n3-6 Mos.\n7-12 Mos.\n >12 Mos.\nAgent Tenure at AI Deployment\nNotes: This figure presents the results of the impact of AI model deployment on workers\u2019 likelihood of attrition.\nPanel A plots the same impact by agent skill index at AI model deployment.\nPanel B graphs the effects of AI\nassistance on attrition by agent tenure at AI model deployment. All specifications include chat year and month fixed\neffects, as well as agent location, company and agent tenure. Observations for these regressions, detailed in A.3, are\nat the agent-month and all robust standard errors are clustered at the agent level.\n53\n\nTable 1: Summary Statistics for the Sample of Customer-Service Agents\nVariable\nAll\nNever Treated\nTreated, Pre\nTreated, Post\nChats\n3,006,395\n944,848\n881,101\n1,180,446\nAgents\n5,172\n3,517\n1,340\n1,636\nNumber of Teams\n133\n111\n80\n81\nShare US Agents\n.11\n.15\n.081\n.072\nDistinct Locations\n25\n25\n18\n17\nAverage Chats per Month\n128\n83\n147\n188\nAverage Handle Time (Min)\n41\n43\n43\n35\nSt. Average Handle Time (Min)\n23\n24\n24\n22\nResolution Rate\n.82\n.78\n.82\n.84\nResolutions Per Hour\n2.1\n1.7\n2\n2.5\nCustomer Satisfaction (NPS)\n79\n78\n80\n80\nNotes: This table shows summary statistics of conversations, agent characteristics and issue resolution rates, cus-\ntomer satisfaction and average call duration. Column 1 consists of all agents in our sample, Column 2 includes control\nagents who were never receive AI access. Column 3 presents statistics for treated agents before they receive AI access\nand Column 4 includes treated agents after AI model deployment.\n54\n\nTable 2: Main Effects: Productivity (Resolutions per Hour)\n(1)\n(2)\n(3)\nVARIABLES\nResolutions/Hr\nResolutions/Hr\nResolutions/Hr\nPost AI X Ever Treated\n0.469***\n0.371***\n0.301***\n(0.0325)\n(0.0318)\n(0.0329)\nEver Treated\n0.110**\n(0.0440)\nObservations\n13,192\n12,295\n12,295\nR-squared\n0.249\n0.562\n0.575\nYear Month FE\nYes\nYes\nYes\nLocation FE\nYes\nYes\nYes\nAgent FE\n-\nYes\nYes\nAgent Tenure FE\n-\n-\nYes\nDV Mean\n2.123\n2.176\n2.176\nRobust standard errors in parentheses\n*** p\u01030.01, ** p\u01030.05, * p\u01030.10\nNotes: This table presents the results of difference-in-difference regressions estimating the impact of AI model\ndeployment on our main measure of productivity, resolutions per hour, the number of technical support problems\nresolved by an agent per hour (resolutions/hour). Post AI X Ever Treated captures the impact of AI model deployment\non resolutions per hour. Column 1 includes agent geographic location and year-by-month fixed effects. Columns 2 and\n3 include agent-level fixed effects, and Column 3, our preferred specification described by Equation 1, also includes\nfixed effects that control for months of agent tenure. Observations for this regression are at the agent-month level\nand all standard errors are clustered at the agent level. Section 3.1 describes the AI rollout procedure.\n55\n\nTable 3: Main Effects: Additional Outcomes\n(1)\n(2)\n(3)\n(4)\nVARIABLES\nAHT\nChats/Hr\nRes. Rate\nNPS\nPost AI X Ever Treated\n-3.746***\n0.365***\n0.0132\n-0.119\n(0.369)\n(0.0345)\n(0.00882)\n(0.524)\nObservations\n21,839\n21,839\n12,295\n12,541\nR-squared\n0.591\n0.563\n0.371\n0.526\nYear Month FE\nYes\nYes\nYes\nYes\nAgent FE\nYes\nYes\nYes\nYes\nAgent Tenure FE\nYes\nYes\nYes\nYes\nDV Mean\n40.64\n2.559\n0.822\n79.59\nRobust standard errors in parentheses\n*** p\u01030.01, ** p\u01030.05, * p\u01030.10\nNotes: This table presents the results of difference-in-difference regressions estimating the impact of AI model\ndeployment on additional measures of productivity and agent performance. Post AI X Treated measures the impact\nof AI model deployment after deployment on treated agents for average handle time (AHT) in Column 1, chats per\nhour (Chats/Hr), the number of chats an agent handles per hour in Column 2, resolution rate (Res. Rate), the\nshare of technical support problems they can resolve in Column 3 and net promoter score (NPS), an estimate of\ncustomer satisfaction in Column 4. Our regression specification, Equation 1, includes fixed effects for each agent,\nchat year-month and agent months of tenure. Observations for this regression are at the agent-month level and all\nstandard errors are clustered at the agent level. Section 3.1 describes the AI rollout procedure.\n56\n\nTable 4: Experience of Work\n(1)\n(2)\n(3)\nVARIABLES\nMean(Customer Sentiment)\nMean(Agent Sentiment)\nShare Req. Manager\nPost AI X Ever Treated\n0.177***\n0.0198***\n-0.00875***\n(0.0116)\n(0.00599)\n(0.00201)\nObservations\n21,218\n21,218\n21,839\nR-squared\n0.485\n0.596\n0.482\nYear Month FE\nYes\nYes\nYes\nAgent FE\nYes\nYes\nYes\nAgent Tenure FE\nYes\nYes\nYes\nDV Mean\n0.141\n0.896\n0.0377\nRobust standard errors in parentheses\n*** p\u01030.01, ** p\u01030.05, * p\u01030.10\nNotes: This table presents the results of difference-in-difference regressions estimating the impact of AI model\ndeployment on measures of conversation sentiment and requests to speak to a manager (\u201cShare Req. Manager\u201d).\nOur regression specification, Equation 1, includes fixed effects for each agent, chat year-month and months of agent\ntenure. Observations for these regressions are at the agent-month level and all standard errors are clustered at the\nagent level. Measures of customer sentiment are created from conversation transcripts using SiBERT and aggregated\nto the agent-month level. Appendix Section A.2 elaborates on sentiment construction and Section 3.1 describes the\nAI rollout procedure.\n57\n\nAppendix Materials\n58\n\nA\nData Appendix\nA.1\nSample Construction\nWe begin with 3,006,395 chat conversations conducted by agents employed at our data firm over\nthe period between September 2019 and June 2021. Each chat includes the number of messages\nin the conversation, the start and end times, and identifiers associated with the conversation and\nagent. We drop chats with only one agent or one customer message to avoid capturing interactions\nwithout meaningful content.\nWe merge the chat data with a set of internal company datasets and datasets from our AI firm\nthat allow us to track agent information, conversation outcomes, and AI model output. To do\nthis, we use the system-generated chat identifier to merge chat-level information across database\nsystems. We also merge our conversation-level data into a message-level dataset that includes the\ntext of each message and message-level AI output. At the conversation level, the difference between\nthe chat start and end time gives us the chat duration, which is available for each call. We drop the\nsmall number of chats that are missing start or end times, not associated with an agent identifier\nor missing a chat identifier. There are also a small number of chats that remain \u201copen\u201d for days\nbecause the customer and agents forget to close the chat, so we winsorize call duration at the 99th\npercentile.\nWe aggregate our chat-level dataset to the agent-month level and merge it with agent data. This\ninformation includes the firm they are employed by, their location, their manager/team, their tenure\nat the firm, and the date treated agents are onboarded onto the AI assistant (defined as the date\nthe agent\u2019s account on the AI system is created). Some employees work flexible schedules, including\npart-time or seasonal roles. As a result, we measure performance metrics only for active periods,\nand an agent\u2019s tenure increases solely during months when they are actively handling customer\nchats.\nA.2\nConstruction of Key Variables\nA.2.1\nCall Duration, Resolution and Customer Satisfaction\nOur firm and associated subcontractors track call resolution, average call duration, and customer\nsatisfaction at the agent-month level. Customer satisfaction is collected by randomly sending surveys\nto customers who have interacted with contact center agents. From the survey results, our firm\ncalculates a monthly average agent customer satisfaction score. Not all customers complete these\nsurveys, so customer satisfaction is calculated monthly rather than at the chat level. Call resolution\n59\n\nis also calculated at the agent-month level. To calculate call resolution, our data firm also uses\nan algorithm that incorporates elements of chat text and future interactions between the customer\nand data firms to calculate a monthly average call resolution score. Although our firm calculates\nan agent-month average monthly call duration when tracking agent performance, our chat-level\ndataset allows us to calculate call duration data for each chat in our sample. However, because call\nresolution is only available on a monthly basis, we report our omnibus productivity measure at the\nagent-month level.\nA.2.2\nMeasuring Agent Skill, Firm and Tenure\nWe measure the tenure of the agent in months of work experience. The turnover of the contact\ncenter is extremely high; Our firm considers agents with over 6 months of experience to be \u201cvery\nexperienced\u201d while agents during their first two months are considered to be \u201cin training\u201d. In our\nregression specifications, we control for agent tenure using a set of fixed effects for each month of\nagent tenure. We do not count weeks when agents are not actively working due to vacation or\nmanagement practices like staggered scheduling intended to reduce worker burnout.\nOur regressions also include controls for each agent\u2019s firm, which is the company or subcontractor\nemploying each agent. Agents are employed directly by our data firm as well as by four other business\nprocess outsourcing firms. For instance, the firm name for an outsourced worker is \u201cConvergys\u201d, one\nof our outsourcing firms, while the company of a directly employed call center agent is \u201cdata firm.\u201d\nWe also include controls for agent location\u2014the physical location where the agent is employed.\nFor example, several of our subcontractor have locations in Cebu City, so many agents are located\nin Cebu. While many of the US-based call center agents are employed directly by our data firm, the\noutsourcing firms also employ US-based workers. Contact center agents based in the US tend to be\nclustered in cities with a low cost of living, such as Hazard, Kentucky, and Reno, Nevada. Outside\nof the United States, most workers are based in the Philippines. The turnover of workers is high,\nso contact center firms frequently open and close new locations.\nWe construct an agent skill index that incorporates call handling speed, issue resolution rates,\nand customer satisfaction.\nFor each firm, we rank agents based on three monthly performance\nindicators: average handle time (lower times receive higher ranks), call resolution rate, and customer\nsatisfaction (higher rates receive higher ranks). The agent\u2019s rank is calculated within firm; data firm\nagents are ranked against their peers, while the Convergys agents are compared to other Convergys\nemployees. These rankings are then averaged into a single skill index at the agent-month level. We\nthen categorize agents into quintiles based on their average skill index from the previous quarter.\n60\n\nThe skill index incorporates call speed and quality, while comparing individuals within the same\norganization.\nA.2.3\nProductivity\nOur omnibus productivity measure is monthly resolutions per hour. To calculate this, we divide\nthe share of calls resolved (the monthly call resolution score) by the number of calls handled per\nhour. Although call duration is available for each chat, resolution data is only available at the\nagent-month level. Consequently, we report our productivity results at the agent-month level.\nA.2.4\nCustomer Sentiment\nThe text-based nature of customer support sheds light into the on-the-job experience of contact\ncenter agents. Although a call may ultimately be resolved or receive a high customer satisfaction\nscore, customers often start conversations feeling stressed, frustrated, or angry. Regularly dealing\nwith complaints and dissatisfied customers is emotionally taxing and contributes to high worker\nturnover. Conversations also tend to be fairly long, close to 40 minutes, and often involve complex\ntopics such as managing payroll for employees, issues connecting with banks, and calculating taxes.\nTo capture the experience of customer-agent interactions, we use natural language processing to\ncapture the sentiment by analyzing the language, context, and emotional content of chat transcript.\nWe employ SiEBERT, an LLM fine-tuned for sentiment analysis on numerous datasets, including\nproduct reviews and tweets, with similar syntax and content to chat-based custoemr support. This\nmodel has demonstrated robustness to noisy, real-world data, such as social media posts or customer\nreviews. SiEBERT uses the text surrounding each word to capture word meaning, which as been\nshown to outperform other methods of sentiment analysis (Hartmann et al., 2023).\nFor each piece of text, the model produces a sentiment score measured on a scale from \u00b41\nto 1, where \u00b41 indicates negative sentiment and 1 indicates positive.\nWe separately calculate\nsentiment scores for agents\u2019 text (agent sentiment) and customers\u2019 text (customer sentiment). We\nthen aggregate these chat-level variables into measures of average agent sentiment and average\ncustomer sentiment for each agent-month.\nA.2.5\nLanguage Comprehensibility and Fluency\nWe use Gemini Pro, a large language model (LLM), to measure comprehensibility and native fluency\n(Gemini Team, 2024). Our criteria for native-like fluency are based on the Interagency Language\nRoundtable (ILR) \u201cfunctionally native\u201d language proficiency standard, adapted for written text.\n61\n\nThe ILR is an organization comprising federal government agencies that coordinates and shares\ninformation on foreign language activities. Functionally Native Proficiency, the highest level on\nthe ILR scale, describes language ability where an individual communicates with complete fluency\nand precision on all levels normally pertinent to professional needs, displays cultural understanding\nequivalent to a native speaker and demonstrates the ability to counsel, persuade, and negotiate in\nthe language as effectively as a well-educated native speaker.\nUsing the prompt below, we ask the LLM to score each agent\u2019s text from a scale of 1 to 5 where:\n1 = Definitely not a native American English speaker 2 = Probably not a native American English\nspeaker 3 = Uncertain if native American English speaker 4 = Probably a native American English\nspeaker 5 = Definitely a native American English speaker. The full prompt is:\nYou are given a conversation transcript from a customer service agent helping a customer\nthat only includes what the customer service agent writes to the customer. Based on\nthe transcript of agents\u2019 conversations, provide a score that determines if the customer\nservice agent is likely a native speaker of American English. For the native speaker as-\nsessment, look for traits such as: Correct grammar based on standard American English\nconventions, use of American English vocabulary, idioms, and phrasing rather than other\ndialects, natural, fluent-sounding language that does not appear stilted or translated,\nadherence to American cultural norms and reference points in word choice and examples,\nthe writer can produce written material with the proficiency of a highly educated native\nspeaker, demonstrating a superior command of the language. For each excerpt, provide\na score from 1-5 for the native speaker assessment, where: 1 = Definitely not a native\nAmerican English speaker 2 = Probably not a native American English speaker 3 =\nUncertain if native American English speaker 4 = Probably a native American English\nspeaker 5 = Definitely a native American English speaker. Only return your score of\nnative speaker assessment. Do not include an explanation.\nTo validate the LLM\u2019s scores, we had two independent human reviewers evaluate 100 randomly\nselected agent conversations. The reviewers did not have access to each other\u2019s scores or the LLM\nresults.\nThe mean score given by the LLM was 4.29, while the average score from the human\nevaluators was 4.22. The difference between these scores was not statistically significant, with a\np-value of 0.22.\nThe comprehensibility score captures the general fluency and ease of understanding in the re-\nsponses of the customer service agents, ranging from very difficult to comprehend (1) to very fluent\nand easily understandable (5). The score assesses the clarity of communication, frequency and im-\n62\n\npact of errors, and general language proficiency demonstrated in the writing, regardless of whether\nthe writer is a native speaker. A Philippines-based agent may have a very high level of fluency, but\nmay not speak like a native speaker. For instance, a common sign-off is to tell customers to \u201chave\na blessed day.\u201d While that phrase is not a common colloquial English phrase, it is grammatically\ncorrect and fully comprehensible. The full prompt is:\nYou are given a conversation transcript from a customer service agent helping a customer\nthat only includes what the customer service agent writes to the customer. Based on\nthe transcript of agents\u2019 conversations, score the overall fluency of each excerpt from\n1-5, regardless of whether it seems to be written by a native speaker, where: 1 =\nVery difficult to comprehend, multiple errors impeding comprehension 2 = Somewhat\ndifficult to understand, some errors impeding comprehension 3 = Mostly understandable\nbut with some errors 4 = Fluent and understandable with only minor errors that do not\nimpact meaning 5 = Very fluent and easily understandable with no significant errors.\nOnly return your score of fluency. Do not include an explanation.\nWe conducted a similar validation exercise with a random sample of 100 agent conversations,\nasking human evaluators to assess the comprehensibility of the agents\u2019 speech. The mean compre-\nhensibility score assigned by the LLM was 4.31, while the average score given by human evaluators\nwas 4.40. The difference between these scores was not statistically significant, with a p-value of\n0.12.\nA.2.6\nConversation Topic\nConversations between agents and customers are fairly complex interactions, often discussing details\nof tax filings, setting up sick leave, firing an employee, or correcting login issues. We use Gemini\nPro to capture the topic of each conversation (Gemini Team, 2024).\nFirst, we select a random sample of 5,000 conversations. Using the prompt below, we first ask\nthe LLM to define the topic of the conversation in one to three words, following a procedure similar\nto Choi et al. (2024). Using this list of 5,000 conversation topics, we ask the LLM to group these\ntopics into no more than 50 categories that describe the subject matter of the conversation. Using\nthe LLM and reviewing the conversations, we come up with a list of 50 topic categories and a one or\ntwo sentence description associated with each category. The total number of categories was chosen\nbased on conversations with business management.\nWe then use the LLM to cluster these topics into 50 distinct groups, each with a concise single-\nsentence definition, and validate these categories with contact center personnel. We use our LLM\n63\n\nto classify each of our conversations into a topic category, \u201cother\u201d or \u201cunsure.\u201d Using this approach,\nwe classify over 98% of conversations into one of 53 main topics. Appendix Figure A.16 displays a\nbreakdown of the most common chat topics encountered by agents. Conversation topics in our data\nare highly skewed, with the two most common topics\u2014payroll and taxes and account access and\nmanagement issues\u2014making up 50% of all chat topics. The next five topics account for the next\n25% of conversations. In total, the top 16 topics account for over 90% of chats in the data. This\npattern is consistent across customer support; generally a small number of fairly common issues\nmake up the bulk of customer inquiries. The full prompt is:\nThe following is the text of a chat between a customer service agent and a customer\nwho has reached out to the Intuit customer support team. The customer is usually\na small business owner based in the US. Based on the transcript, categorize the topic\nof the conversation into a 1 to 3 word topic.\nIf the topic is about a customer not\nbeing able to log into their account, categorize the chat as \u201cLogin.\u201d If the conversation\nis about paying their employees, the correct topic is \u201cPayroll.\u201d If the conversation is\nabout paying suppliers, the correct topic is \u201cSupplier Payments\u201d. If the topic is about\ntax related issues, the correct topic is \u201cTaxes.\u201d If the topic is about ensuring that two\nsets of records (usually the balances of two accounts) are in agreement, the right topic\nis \u201cReconciliation.\u201d\nWhen analyzing the transcript, do not focus on details like the\nidentity of the customer, filler greeting text, or metadata around the agent joining the\nconversation or variables that obscure sensitive details like $SSN, $EMAIL, $NAME-1,\n$ADDRESS, hyperlink metadata like <br>, or case numbers. The output format should\nbe: topic, problem type, issue resolution, probability resolved, skill required, covid-19\nrelated.\nOnce we have a list of 50 categories and a clear definition, we then ask the LLM to classify\nthe topic of each conversation into one of the 50 categories. The LLM could also classify a chat as\n\u201cother\u201d if the conversation does not fit any of the above categories, or \u201cunsure\u201d if the LLM is unable\nto classify the conversation into any specific category.\nAll together, we are able to classify 85% of all conversations into a topic category. The subjects\nfollow a highly skewed distribution\u2014almost 50% of contact center questions fall into payroll & taxes\nor account access & management.\nWe validate the LLM-generated topic categorizations by employing three independent human\nevaluators to classify a random sample of 100 conversations into our predefined topic categories. In\n30% of the sample, all three human evaluators unanimously agree on the same topic (\u201cunanimous\n64\n\ntopic\u201d). In 75% of the sample (which includes the unanimous subset), at least two evaluators agree\non a topic (\u201cmodal topic\u201d). The remaining 25% of the sample consists of discordant conversations\nwhere each evaluator selects a different topic. When comparing the LLM\u2019s topic selections to these\nhuman-evaluated subsets, we find that for unanimous-consensus conversations, the LLM selects the\nsame topic as the human evaluators 87% of the time. For modal-consensus conversations, the LLM\nselects the modal topic 66% of the time. In the case of discordant conversations, where there is no\nagreement between human reviewers, the LLM\u2019s selected topic matches one of the three distinct\nhuman-selected topics 74% of the time.\nWe use these topic categories to rank topics by overall topic frequency across all conversations.\nWe also categorize topics according to their overall frequency with respect to an individual agent.\nA.2.7\nConversation Sentiment\nSentiment analysis involves determining the emotions or attitudes expressed in a piece of text. The\nvalence of unstructured text data is widely used in consumer marketing, predicting stock market\nreturns, measuring consumer sentiment, monitoring social media and understanding voting behavior.\nMachine learning-based methods, particularly those utilizing transfer learning models, generally\noutperform other sentiment classification approaches. Hartmann et al. (2023) performs a meta-\nanalysis of over 1,100 experimental results, demonstrating that transfer-learning models are supe-\nrior for sentiment classification. They also provide an open-sourced, fine-tuned language model,\nSiEBERT, incorporating transfer-learning best practices, which we use for our sentiment analysis.\nThe model generates a sentiment measure on a scale from -1 to 1, with -1 indicating negative\nsentiment and 1 indicating positive sentiment.\nIn our analysis, we classify sentiment separately for agents\u2019 speech and customers\u2019 text in each\nconversation. Customer sentiment reflects the emotional experience of the 40-minute or so chat-\nbased interaction, while agent sentiment captures the tone of the agents\u2019 responses.\nA.2.8\nConversation Similarity\nWe create textual embeddings of agent-customer conversations and compare similarity of these\nembeddings across workers and over time.\nTextual embeddings take a given body of text and\ntransform it into a high-dimensional vector that represents its \u201ccoordinates\u201d in linguistic space.\nTwo pieces of text will have more similar coordinates if they share a common meaning or style. The\nspecific embedding given to a body of text will depend on the embedding model used. We form\nour text embeddings using all-MiniLM-L6-v2, an LLM that is specifically intended to capture and\ncluster semantic information to assess similarity across text (Hugging Face, 2023). Once we create\n65\n\nan embedding for each conversation, we can compare the similarity of conversations by looking at\nthe cosine similarities of their associated vectors; this common approach yields a score of 0 if two\npieces of text are semantically orthogonal and a score of 1 if they have the same meaning (Koroteev,\n2021). For context, the sentences \u201cCan you help me with logging in?\u201d and \u201cWhy is my login not\nworking?\u201d have a cosine similarity of 0.68 in our model.\nA.3\nEmpirical Specifications\nA.3.1\nPre-treatment Worker Skill Specification\nyit \u201c \u03b4t ` \u03b1i `\n5\u00ff\nq\u201c1\n\u03b2qpAIit \u02c6 Qiqq ` \u03b3Xit ` \u03f5it\n(2)\nOur worker skill specification allows us to estimate how the impact of assistance varies by agent\nskill when they receive access to AI assistance. The regression is conducted at the agent-month\nlevel, where:\n\u2013 yit is the resolutions per hour of agent i in year-month t.\n\u2013 \u03b4t represents year-month fixed effects.\n\u2013 \u03b1i denotes agent-level fixed effects.\n\u2013 AIit is an indicator equal to 1 if agent i has access to AI assistance at time t, 0 otherwise.\n\u2013 Qiq is an indicator function that equals 1 if agent i belonged to skill quintile q at the time of\ntreatment, where q ranges from 1 (lowest skill) to 5 (highest skill).\n\u2013 Xit is a set of time-varying controls, specifically fixed effects for agent tenure in months.\n\u2013 \u03b2q represents the average treatment effect of AI assistance for agents in skill quintile q\nWe estimate this equation separately for each of our outcome variables, which include our main\nmeasure of productivity for agent i in year-month t (resolutions per hour), as well as call resolution\nrate, customer satisfaction score, average call duration, calls handled per hour and requests to\nspeak to the manager. We cluster standard errors at the agent level to account for within-agent\ncorrelations in the error terms. In our main analysis, we find very similar results across estimators\nand similar main effects across adoption cohorts, so we estimate this regression specification using\nOLS.\n66\n\nA.3.2\nPre-treatment Worker Tenure Specification\nyit \u201c \u03b4t ` \u03b1i `\n5\u00ff\ne\u201c1\n\u03b2epAIit \u02c6 Expieq ` \u03f5it\n(3)\nOur tenure specification allows us to estimate how the impact of assistance varies by agent\nexperience when they receive access to AI assistance. The regression is conducted at the agent-\nmonth level, where:\n\u2013 yit is the resolutions per hour of agent i in year-month t.\n\u2013 \u03b4t represents year-month fixed effects.\n\u2013 \u03b1i denotes agent-level fixed effects.\n\u2013 AIit is an indicator equal to 1 if agent i has access to AI assistance at time t, 0 otherwise.\n\u2013 Expie is an indicator that equals 1 if agent i has e months of experience at the time of\ntreatment, and 0 otherwise. We divide months of experience into five categories: agents in\ntheir first month on the job (\u201c0 months\u201d), 1-2 months of experience, 3-6 months of experience,\n7-12 months, and over 12 months of experience.\n\u2013 Xi includes a set of fixed effects for agent i\u2019s skill quintile at the time of treatment. This is\nonly available for treated agents and is time-invariant.\n\u2013 \u03b2e is the average treatment effect of AI assistance for agents with e months of experience when\ntreated.\nWe estimate this equation separately for each of our outcome variables, which include our main\nmeasure of productivity for agent i in year-month t (resolutions per hour), as well as call resolution\nrate, customer satisfaction score, average call duration, and calls handled per hour. The skill quintile\nat AI treatment is time invariant and is not defined for control group workers. Standard errors are\nclustered at the agent level, and we estimate this regression using OLS.\nA.3.3\nAdherence to AI recommendations\nyit \u201c \u03b4t ` \u03b1i `\n5\u00ff\na\u201c1\n\u03b2apAIit \u02c6 Adhiaq ` \u03b3Xit ` \u03f5it\n(4)\nThis specification allows us to estimate how the impact of AI assistance varies by treated agents\u2019\nadherence in their first month of AI access. The regression is conducted at the agent-month level,\nwhere:\n67\n\n\u2013 yit is the resolutions per hour of agent i in year-month t.\n\u2013 \u03b4t represents year-month fixed effects.\n\u2013 \u03b1i denotes agent-level fixed effects.\n\u2013 AIit is an indicator equal to 1 if agent i has access to AI assistance at time t, 0 otherwise.\n\u2013 Adhia is an indicator equal to 1 if agent i is in the ath quintile of adherence in their first\nmonth of access, 0 otherwise.\n\u2013 Xit includes time-varying controls, specifically fixed effects for agent tenure.\n\u2013 \u03b2a is the average impact of AI assistance for agents in the ath quintile of initial adherence.\nStandard errors are clustered at the agent level. We estimate this regression with OLS.\nA.3.4\nHeterogeneity by Chat Topic\nyitc \u201c \u03b4t ` \u03b1i `\n4\u00ff\nr\u201c1\n\u03b2rpAIit \u02c6 Topiccrq ` \u03b3Xitc ` \u03f5itc\n(5)\nThis topic-based specification allows us to estimate how the impact of AI assistance varies by\nthe routine nature of customers\u2019 problems. The regression is conducted at the chat level, where:\n\u2013 yitc is the duration of chat c assigned to agent i in year-month t.\n\u2013 \u03b4t represents year-month fixed effects.\n\u2013 \u03b1i denotes agent-level fixed effects.\n\u2013 AIit is an indicator equal to 1 if agent i has access to AI assistance at time t, 0 otherwise.\n\u2013 Topiccr is an indicator equal to 1 if chat c belongs to topic frequency category r (where r\nranges from 1 to 4), 0 otherwise. The frequency of the topic is defined using frequency across\nall chats and agents.\n\u2013 Xit includes fixed effects for agent tenure and the overall category of topic frequency (Topiccr).\n\u2013 \u03b2r estimates the impact of AI assistance on chat duration for chats in the topic frequency\ncategory r.\nStandard errors are clustered at the agent level and we estimate this regression with OLS. We\ndrop the small number of topics that are classified as unsure or \u201cother.\u201d We also estimate a separate\nspecification on the agent-specific frequency of the technical support issue.\n68\n\nyitc \u201c \u03b4t ` \u03b1i `\n4\u00ff\nr\u201c1\n\u03b2rpAIit \u02c6 AgentTopicicrq ` \u03b3Xcit ` \u03f5itc\n(6)\n\u2013 AgentTopicicr is an indicator equal to 1 if chat c belongs to agent-specific topic frequency\ncategory r (where r ranges from 1 to 4), 0 otherwise. Topic frequency is defined relative to\nconversations conducted by agent i.\n\u2013 Xcit includes time-varying controls, specifically fixed effects for agent tenure, aggregate topic\ncategory defined over all conversations (Topiccr), and agent-specific topic frequency category\n(AgentTopicicrq.\n\u2013 \u03b2r estimates the impact of AI assistance on chat duration for chats in agent-specific topic\nfrequency category r.\nA.3.5\nAttrition\nattritit \u201c \u03b4t ` \u03b2AIit ` \u03b3Xit ` \u03f5it\n(7)\nThis specification allows us to look at how AI assistance impacts agent attrition. The regression\nis conducted at the agent-month level, where:\n\u2013 attritit is equal to 1 if agent i leaves in year-month t.\n\u2013 \u03b4t represents year-month fixed effects.\n\u2013 AIit is an indicator equal to 1 if agent i has access to AI assistance at time t, 0 otherwise.\n\u2013 Xit includes time-varying controls, specifically fixed effects for agent tenure, agent location,\ncountry and company employing the agent (data firm or subcontractor).\n\u2013 \u03b2a is the average impact of AI assistance on attrition.\nAttrition captures both voluntary or involuntary separations, which are undistinguishable in our\ndata. Standard errors are clustered at the agent level and we estimate this regression with OLS.\nWe cannot include for agent-fixed effects, because agents only leave once, so agent-fixed effects are\ncolinear with attrition. We drop all observations for treated agents before treatment because, by\nconstruction, agents must survive through treatment to receive AI assistance. We also estimate\nspecifications where we estimate the attrition effects by agent tenure and skill at AI deployment.\n69\n\nFigure A.1: Sample AI Output\nA. Sample Customer Issue\nB. Sample AI-generated Suggested Response\nC. Sample AI-generated Technical Link\nNotes: This figure illustrates AI-generated suggestions for customer service agents. Panel A shows a sample customer\nissue. Panel B displays AI-suggested responses for greeting and setting call expectations. Panel C presents a AI-\nrecommended technical documentation excerpt from the company\u2019s internal knowledge base. Suggestions are only\nvisible to agents, not customers, and agents can choose to use, modify, or ignore these suggestions when responding.\n70\n\nFigure A.2: Deployment Timeline\n0\n20\n40\n60\n80\n100\nShare of Agents Onboarded\nRCT\nOctober 2020\nNovember 2020\nDecember 2020\nJanuary 2021\nFebruary 2021\nMarch 2021\nApril 2021\nMay 2021\nMonth Onboarded\nNotes: This figure shows the share of agents deployed onto the AI system over the study period. Agents are deployed\nonto the AI system after a training session as described in Section 3.1. The small randomized control trial in August\n2020 is analyzed in Section 4.1.1. All data are from the firm\u2019s internal software systems.\n71\n\nB\nAverage Productivity Effects\n72\n\nFigure A.3: Event Studies, Additional Outcomes\nA. Average Handle Time\nB. Chats Per Hour\n-10\n-5\n0\n5\nChange in Average Handle Time\n-10\n-9\n-8\n-7\n-6\n-5\n-4\n-3\n-2\n-1\n0\n1\n2\n3\n4\n5\nMonths to/from AI Deployment\n-.5\n0\n.5\n1\nChange in Chats Per Hour\n-10\n-9\n-8\n-7\n-6\n-5\n-4\n-3\n-2\n-1\n0\n1\n2\n3\n4\n5\nMonths to/from AI Deployment\nC. Resolution Rate\nD. Customer Satisfaction (NPS)\n-.1\n-.05\n0\n.05\n.1\nChange in Share Resolved\n-10\n-9\n-8\n-7\n-6\n-5\n-4\n-3\n-2\n-1\n0\n1\n2\n3\n4\n5\nMonths to/from AI Deployment\n-5\n0\n5\n10\nChange in Customer Satisfaction\n-10\n-9\n-8\n-7\n-6\n-5\n-4\n-3\n-2\n-1\n0\n1\n2\n3\n4\n5\nMonths to/from AI Deployment\nNotes: These figures plot the coefficients and 95% confidence intervals from event study regressions of AI model\ndeployment using the Sun and Abraham (2021) interaction weighted estimator. Panel A plots the average handle\ntime or the average duration of each technical support chat. Panel B plots the number of chats an agent completes per\nhour, incorporating multitasking. Panel C plots the resolution rate, the share of chats successfully resolved, and Panel\nD plots net promoter score, which is an average of surveyed customer satisfaction. All specifications follow Equation\n1 and include agent and chat year-month and months of agent tenure fixed effects. Data is at the agent-month level\nand robust standard errors are clustered at the agent level.\n73\n\nFigure A.4: Event Studies, Resolutions Per Hour\nA. Resolutions Per Hour\n-.6\n-.4\n-.2\n0\n.2\n.4\n.6\n.8\n1\n1.2\nChange in Resolutions Per Hour\n-10 -9\n-8\n-7\n-6\n-5\n-4\n-3\n-2\n-1\n0\n1\n2\n3\n4\n5\nMonths to/from AI Deployment\nSun-Abraham\nde Chaisemartin-D'Haultfoeuille\nCallaway-Sant'Anna\nBorusyak et al.\nTWFE OLS\nNotes: This figure presents the effect of AI model deployment on our main productivity outcome, resolutions per\nhour, using a variety of robust dynamic difference-in-differences estimators introduced in Borusyak et al. (2022),\nCallaway and Sant\u2019Anna (2021), de Chaisemartin and D\u2019Haultf\u0153uille (2020) and Sun and Abraham (2021) and a\nstandard two-way fixed effects regression model. Regressions follow Equation 1 and include agent level, chat-year,\nand months of agent tenure fixed effects. Data is available at the agent-month level and robust standard errors are\nclustered at the agent level. Because of the number of post-treatment periods and high turnover of agents in our\nsample, we can only estimate five months of pre-period data using Borusyak et al. (2022) and de Chaisemartin and\nD\u2019Haultf\u0153uille (2020).\n74\n\nFigure A.5: RCT Analysis\nA. Resolutions Per Hour\n-10\n-5\n0\n5\n10\n15\nChange in Customer Satisfaction\n-10\n-9\n-8\n-7\n-6\n-5\n-4\n-3\n-2\n-1\n0\n1\n2\n3\n4\n5\nMonths to/from AI Deployment\nB. Average Handle Time\nC. Chats Per Hour\n-15\n-10\n-5\n0\n5\nChange in Average Handle Time\n-10\n-9\n-8\n-7\n-6\n-5\n-4\n-3\n-2\n-1\n0\n1\n2\n3\n4\n5\nMonths to/from AI Deployment\n-.5\n0\n.5\n1\nChange in Chats Per Hour\n-10\n-9\n-8\n-7\n-6\n-5\n-4\n-3\n-2\n-1\n0\n1\n2\n3\n4\n5\nMonths to/from AI Deployment\nD. Resolution Rate\nE. Customer Satisfaction (NPS)\n-.1\n0\n.1\n.2\n.3\nChange in Share Resolved\n-10\n-9\n-8\n-7\n-6\n-5\n-4\n-3\n-2\n-1\n0\n1\n2\n3\n4\n5\nMonths to/from AI Deployment\n-10\n-5\n0\n5\n10\n15\nChange in Customer Satisfaction\n-10\n-9\n-8\n-7\n-6\n-5\n-4\n-3\n-2\n-1\n0\n1\n2\n3\n4\n5\nMonths to/from AI Deployment\nNotes: This figures plots event studies focusing on the 22 agents who were onboarded as part of the pilot RCT.\nBecause we do not have information on the specific agents (there were around 25) selected to be part of the control\ngroup, we compare the 22 pilot-treated agents with observations for all pre-treatment agents, controlling for our usual\nagent, chat year-month, and months of agent tenure fixed effects in agent-month level regressions following Equation\n1. Robust standard errors are clustered at the agent level.\n75\n\nTable A.1: RCT Analysis\n(1)\n(2)\n(3)\n(4)\n(5)\nVARIABLES\nRes./Hr\nAHT\nChats/Hr\nRes. Rate\nNPS\nPost AI X Ever Treated\n0.202**\n-3.713***\n0.105\n0.0169\n-1.393\n(0.0850)\n(1.045)\n(0.0718)\n(0.0246)\n(1.529)\nObservations\n6,998\n15,100\n15,100\n6,998\n7,176\nR-squared\n0.568\n0.574\n0.566\n0.383\n0.517\nYear Month FE\nYes\nYes\nYes\nYes\nYes\nAgent FE\nYes\nYes\nYes\nYes\nYes\nAgent Tenure FE\nYes\nYes\nYes\nYes\nYes\nDV Mean\n1.956\n43\n2.378\n0.808\n79.68\nRobust standard errors in parentheses\n*** p\u01030.01, ** p\u01030.05, * p\u01030.10\nNotes: This table focuses on the 22 agents who were onboarded as part of the pilot RCT. Because we do not have\ninformation on the specific agents (there were around 25) selected to be part of the control group, we compare the\n22 pilot-treated agents with observations for all pre-treatment agents, controlling for agent, chat year-month, and\nmonths of agent tenure fixed effects. Data are at the agent-month level and the standard errors are clustered at the\nagent level.\n76\n\nTable A.2: Main Effects: Company Team IV\n(1)\n(2)\n(3)\n(4)\n(5)\n(6)\nVARIABLES\nIndividual Treatment\nRes./Hr\nAHT\nChats/Hr\nRes. Rate\nNPS\nEarliest Team Treatment\n0.311***\n(0.0163)\nPost AI X Individual Treatment\n0.550***\n-3.622**\n0.412***\n0.0740***\n-0.518\n(0.0943)\n(1.444)\n(0.0791)\n(0.0184)\n(0.946)\nObservations\n21,839\n12,295\n21,839\n21,839\n12,295\n12,541\nR-squared\n0.813\n0.177\n0.102\n0.115\n0.007\n0.007\nYear Month FE\nYes\nYes\nYes\nYes\nYes\nYes\nAgent FE\nYes\nYes\nYes\nYes\nYes\nYes\nAgent Tenure FE\nYes\nYes\nYes\nYes\nYes\nYes\nF-statistic\n365.7\nNumber of agent_id\n2,163\n3,633\n3,633\n2,163\n2,214\nRobust standard errors in parentheses\n*** p\u01030.01, ** p\u01030.05, * p\u01030.10\nNotes: In this table we instrument agent level date of AI adoption with the minimum date of AI deployment on that agent\u2019s team. Column 1 shows the first\nstage regression of earliest team adoption on individual adoption and Columns 2 through 6 shows the 2SLS estimates on measures of productivity and call quality\nand efficiency. Regressions follow Equation 1 and include agent level, chat-year, and months of agent tenure fixed effects. Data is available at the agent-month\nlevel and robust standard errors are clustered at the agent level.\n77\n\nTable A.3: Main Effects: Robustness to alternative clustering\n(1)\n(2)\n(3)\nVARIABLES\nRes./Hr\nRes./Hr\nRes./Hr\nPost AI X Ever Treated\n0.301***\n0.301***\n0.301***\n(0.0329)\n(0.0455)\n(0.0498)\nObservations\n12,295\n12,295\n12,295\nR-squared\n0.575\n0.575\n0.575\nYear Month FE\nYes\nYes\nYes\nAgent FE\nYes\nYes\nYes\nAgent Tenure FE\nYes\nYes\nYes\nDV Mean\n2.176\n2.176\n2.176\nRobust standard errors in parentheses\n*** p\u01030.01, ** p\u01030.05, * p\u01030.10\nNotes: This table shows robustness of our Column 3 in Table 2 to different levels of clustering. Column 1 clusters\nour robust standard errors at the agent level, Column 2 clusters at the company/team level and Column 3 clusters\nat the location (usually the agent\u2019s city of work). Regressions follow Equation 1 and include agent level, chat-year,\nand months of agent tenure fixed effects. Data is available at the agent-month level and robust standard errors are\nclustered at the agent level.\n78\n\nTable A.4: Main Effects: Chat-weighted\n(1)\n(2)\n(3)\n(4)\n(5)\nVARIABLES\nRes./Hour\nAHT\nChats/Hr\nRes. Rate\nNPS\nPost AI X Ever Treated\n0.252***\n-3.100***\n0.295***\n0.00325\n-0.119\n(0.0318)\n(0.315)\n(0.0263)\n(0.00725)\n(0.524)\nObservations\n12,295\n21,839\n21,839\n12,295\n12,541\nR-squared\n0.650\n0.756\n0.721\n0.465\n0.526\nYear Month FE\nYes\nYes\nYes\nYes\nYes\nAgent FE\nYes\nYes\nYes\nYes\nYes\nAgent Tenure FE\nYes\nYes\nYes\nYes\nYes\nDV Mean\n2.457\n38.66\n2.886\n0.839\n79.59\nRobust standard errors in parentheses\n*** p\u01030.01, ** p\u01030.05, * p\u01030.10\nNotes: This table presents the results of difference-in-difference regressions estimating the impact of AI model\ndeployment on measures of productivity and agent performance. Observations are at the agent-month level, weighted\nby the number of chats associated with that agent-month. Post AI X Treated measures the impact of AI model\ndeployment after deployment on treated agents for average handle time or average call duration, chats per hour, the\nnumber of chats an agent handles per hour, resolution rate, the share of technical support problems they can resolve\nand net promoter score (NPS), an estimate of customer satisfaction. Regressions follow Equation 1 and include agent\nlevel, chat-year, and months of agent tenure fixed effects. Data is available at the agent-month level and robust\nstandard errors are clustered at the agent level.\n79\n\nTable A.9:\nMain Effects:\nProductivity (Resolutions per Hour), Alternative\nDifference-in-Difference Estimators\nPoint\nEstimate\nStandard\nError\nLower Bound\n95% Confidence\nInterval\nUpper Bound\n95% Confidence\nInterval\nTWFE-OLS\n0.296\n0.032\n0.233\n0.360\nBorusyak-Jaravel-Spiess\n0.576\n0.070\n0.438\n0.714\nCallaway-Sant\u2019Anna\n0.489\n0.059\n0.374\n0.605\nDeChaisemartin-D\u2019Haultfeuille\n0.219\n0.042\n0.137\n0.302\nSun-Abraham\n0.521\n0.094\n0.337\n0.705\nNotes: This table shows the impact of AI model deployment on our main productivity outcome, resolutions per\nhour, using robust difference-in-differences estimators introduced in Borusyak et al. (2022), Callaway and Sant\u2019Anna\n(2021), de Chaisemartin and D\u2019Haultf\u0153uille (2020) and Sun and Abraham (2021). Regressions follow Equation 1\nand include agent level, chat-year, and months of agent tenure fixed effects. Data is available at the agent-month\nlevel and standard errors are clustered at the agent level. Because of the number of post-treatment periods and high\nturnover of agents in our sample, we can only estimate five months of pre-period data using Borusyak et al. (2022)\nand de Chaisemartin and D\u2019Haultf\u0153uille (2020).\n80\n\nC\nImpacts by Agent Skill and Tenure\n81\n\nFigure A.6: Heterogeneity of AI Impact by pre-AI Worker Skill and Controlling\nfor Tenure, Additional Outcomes\nA. Average Handle Time\nB. Chats Per Hour\n-6\n-4\n-2\n0\n2\nChange in Average Handle Time\nQ1 (Lowest)\nQ2\nQ3\nQ4\nQ5 (Highest)\nAgent Skill at AI Deployment\n.1\n.2\n.3\n.4\n.5\nChange in Chats Per Hour\nQ1 (Lowest)\nQ2\nQ3\nQ4\nQ5 (Highest)\nAgent Skill at AI Deployment\nC. Resolution Rate\nD. Customer Satisfaction (NPS)\n-.1\n-.05\n0\n.05\n.1\n.15\nChange in Share Resolved\nQ1 (Lowest)\nQ2\nQ3\nQ4\nQ5 (Highest)\nAgent Skill at AI Deployment\n-5\n0\n5\n10\nChange in Customer Satisfaction\nQ1 (Lowest)\nQ2\nQ3\nQ4\nQ5 (Highest)\nAgent Skill at AI Deployment\nNotes: These figures plot the impacts of AI model deployment on four measures of productivity and performance, by\npre-deployment worker skill. Agent skill is calculated as the agent\u2019s trailing three month average of performance on\naverage handle time, call resolution, and customer satisfaction, the three metrics our firm uses for agent performance.\nWithin each month and company, agents are grouped into quintiles, with the most productive agents within each firm\nin quintile 5 and the least productive in quintile 1. Panel A plots the average handle time or the average duration of\neach technical support chat. Panel B graphs chats per hour, or the number of chats an agent can handle per hour.\nPanel C plots the resolution rate, and Panel D plots net promoter score, an average of surveyed customer satisfaction.\nAll specifications include fixed effects for the agent, chat year-month and months of tenure. Robust standard errors\nare clustered at the agent level. The regression specifications are available in Appendix section A.3.\n82\n\nFigure A.7: Heterogeneity of AI Impact by pre-AI Worker Tenure Controlling\nfor Skill, Additional Outcomes\nA. Average Handle Time\nB. Chats Per Hour\n-8\n-6\n-4\n-2\n0\nChange in Average Handle Time\n0 Mos.\n1-2 Mos.\n3-6 Mos.\n7-12 Mos.\n>12 Mos.\nAgent Tenure at AI Deployment\n-.2\n0\n.2\n.4\n.6\n.8\nChange in Chats Per Hour\n0 Mos.\n1-2 Mos.\n3-6 Mos.\n7-12 Mos.\n>12 Mos.\nAgent Tenure at AI Deployment\nC. Resolution Rate\nD. Customer Satisfaction (NPS)\n-.05\n0\n.05\n.1\n.15\nChange in Share Resolved\n0 Mos.\n1-2 Mos.\n3-6 Mos.\n7-12 Mos.\n>12 Mos.\nAgent Tenure at AI Deployment\n-5\n0\n5\n10\nChange in Customer Satisfaction\n0 Mos.\n1-2 Mos.\n3-6 Mos.\n7-12 Mos.\n>12 Mos.\nAgent Tenure at AI Deployment\nNotes: These figures plot the impacts of AI model deployment on measures of productivity and performance by\npre-AI worker tenure, defined as the number of months an agent has been employed when they receive access to the\nAI model. Panel A plots the average handle time or the average duration of each technical support chat. Panel B\ngraphs chats per hour, or the number of chats an agent can handle per hour. Panel C plots the resolution rate, and\nPanel D plots net promoter score, an average of surveyed customer satisfaction. All specifications include fixed effects\nfor the agent, chat year-month and months of tenure. Robust standard errors are clustered at the agent level. The\nregression specifications are available in Appendix section A.3.\n83\n\nTable A.6: Heterogeneity of AI Impact by Skill and Tenure, Resolutions per Hour\n(1)\n(2)\nBy Skill at AI\nBy Tenure at AI\nQ1 (Lowest Skill)\n0.527\u02da\u02da\u02da\n(0.049)\nQ2\n0.315\u02da\u02da\u02da\n(0.056)\nQ3\n0.302\u02da\u02da\u02da\n(0.043)\nQ4\n0.261\u02da\u02da\u02da\n(0.055)\nQ5 (Highest Skill)\n0.015\n(0.065)\n< 1 Mos.)\n0.707\u02da\u02da\u02da\n(0.067)\n1-2 Mos.\n0.388\u02da\u02da\u02da\n(0.042)\n3-6 Mos.\n0.361\u02da\u02da\u02da\n(0.067)\n7-12 Mos.\n0.337\u02da\u02da\u02da\n(0.054)\n> 12 Mos.)\n0.028\n(0.059)\nYear Month FE\nYes\nYes\nAgent FE\nYes\nYes\nOther FE\nTenure\nSkill at AI\nDV Mean\n2.176\n2.284\nObservations\n12,295\n8,148\nStandard errors in parentheses\n\u02da p \u0103 0.10, \u02da\u02da p \u0103 0.05, \u02da\u02da\u02da p \u0103 0.01\nNotes: This table presents the results of difference-in-difference regressions estimating the impact of AI model\ndeployment on resolutions per hour. Column 1 estimates the impact of AI access by worker skill at AI, including\nagent, year-month and months of agent tenure fixed effects. Column 2 estimates the effects by worker tenure at AI\ndeployment, including agent, year-month and agent skill at AI deployment fixed effects. Observations are at the\nagent-month level and all standard errors are clustered at the agent level.\n84\n\nFigure A.8: Experience Curves by Deployment Cohort, Additional Outcomes\nA. Average Handle Time\nB. Chats Per Hour\n30\n35\n40\n45\nAverage Handle Time\n0\n2\n4\n6\n8\n10\nAgent Tenure, Months\nNever Treated\nAlways Treated\nTreated 5-6 Mo.\n2\n2.5\n3\n3.5\n4\nChats Per Hour\n0\n2\n4\n6\n8\n10\nAgent Tenure, Months\nNever Treated\nAlways Treated\nTreated 5-6 Mo.\nC. Resolution Rate\nD. Customer Satisfaction\n.6\n.7\n.8\n.9\nShare Resolved\n0\n2\n4\n6\n8\n10\nAgent Tenure, Months\nNever Treated\nAlways Treated\nTreated 5-6 Mo.\n65\n70\n75\n80\n85\nCustomer Satisfaction\n0\n2\n4\n6\n8\n10\nAgent Tenure, Months\nNever Treated\nAlways Treated\nTreated 5-6 Mo.\nNotes: These figures plot the experience curves of three groups of agents over their tenure, the x-axis, against\nfive measures of productivity and performance. The short-dashed red lines plot the performance of always-treated\nagents, those who are start work in their first month with the AI and always have access to the AI suggestions. The\nlong-dashed blue line plots agents who are never treated. The solid green line plots agents who spend their first four\nmonths of work without the AI model, and gain access to the AI during their fifth month on the job. All panels\ninclude 95% confidence intervals and observations are at the agent-month level.\n85\n\nFigure A.9: Resolutions per Hour over Time\n1.5\n2\n2.5\n3\n3.5\nResolutions per Hour\n-6\n-5\n-4\n-3\n-2\n-1\n0\n1\n2\n3\n4\n5\n6\nEvent Time -- Months\nLow\nMedium\nHigh\nNotes: This graph depicts the evolution of average resolutions per hour for agents following the implementation of\nAI assistance. The graph segments agents into three groups based on their skill level at the time of AI deployment.\nThe green line represents the highest-performing third of agents, those in the top tercile of the skill index. The red\nline illustrates the progress of agents in the middle tercile, while the blue line tracks those in the bottom tercile,\nrepresenting the lowest-skilled third at the time of treatment. Agents are categorized based on their skill index at\nthe time of AI implementation. For details on the skill index construction, refer to Appendix Section A.2.\n86\n\nFigure A.10: Productivity Impacts by Adoption Cohort\nA. Resolutions Per Hour\n-2\n-1\n0\n1\n2\n3\nChange in Customer Satisfaction (NPS)\nEarly\nMiddle\nLate\nAdoption Cohort\nB. Average Handle Time\nC. Chats Per Hour\n-6\n-5.5\n-5\n-4.5\n-4\n-3.5\nChange in Average Handle Time\nEarly\nMiddle\nLate\nAdoption Cohort\n.2\n.3\n.4\n.5\n.6\nChange in Chats Per Hour\nEarly\nMiddle\nLate\nAdoption Cohort\nD. Resolution Rate\nE. Customer Satisfaction (NPS)\n-.02\n0\n.02\n.04\n.06\n.08\nChange in Share Resolved\nEarly\nMiddle\nLate\nAdoption Cohort\n-2\n-1\n0\n1\n2\n3\nChange in Customer Satisfaction (NPS)\nEarly\nMiddle\nLate\nAdoption Cohort\nNotes: These figures plots the treatment effect of AI assistance on various outcomes for workers who received access\nto the AI model in the early, middle, or later period of the rollout. Because the AI model is periodically updated\nwith new data and pushed to all onboarded workers no more frequently than once a month, we focus on productivity\noutcomes in the first month after AI adoption in order to compare workers using earlier and later versions of the\nmodel. Data are at the agent-month, regression specification follows Equation 1 and we include chat year-month,\nagent, and months of tenure fixed effects. Robust standard errors are clustered at the agent level.\n87\n\nD\nAdherence to AI suggestions\n88\n\nFigure A.11: Variation in AI Adherence\n0\n.005\n.01\n.015\n.02\nDensity\n-100\n-50\n0\n50\n100\nAll\nw/in company-location\nw/in company-location-team\nNotes: This figure plots the distribution of AI adherence, averaged at the agent-month level, weighted by the log\nof the number of AI recommendations for that agent-month. \u201cAll\u201d refers to the overall distribution of unadjusted\nadherence rates. \u201cwithin company-location\u201d plots residual adherence after adjusting for company and location fixed\neffects, and \u201cwithin company-location-team\u201d plots residuals adjusted for company, location, and team fixed effects.\n89\n\nFigure A.12: Heterogeneity of AI Impact by Initial AI Adherence, Additional\nOutcomes\nA. Average Handle Time\nB. Chats Per Hour\n-8\n-6\n-4\n-2\n0\nChange in Average Handle Time\nQ1 (Low)\nQ2\nQ3\nQ4\nQ5 (High)\nInitial Adherence at AI Deployment (%)\n0\n.2\n.4\n.6\n.8\nChange in Chats Per Hour\nQ1 (Low)\nQ2\nQ3\nQ4\nQ5 (High)\nInitial Adherence at AI Deployment (%)\nC. Resolution Rate\nD. Customer Satisfaction (NPS)\n-.02\n0\n.02\n.04\nChange in Share Resolved\nQ1 (Low)\nQ2\nQ3\nQ4\nQ5 (High)\nInitial Adherence at AI Deployment (%)\n-2\n0\n2\n4\nChange in Customer Satisfaction\nQ1 (Low)\nQ2\nQ3\nQ4\nQ5 (High)\nInitial Adherence at AI Deployment (%)\nNotes: These figures plot the impact of AI model deployment on additional measures of performance by quintile\nof initial adherence, the share of AI recommendations followed in the first month of treatment. Panel A plots the\naverage handle time or the average duration of each technical support chat.\nPanel B graphs chats per hour, or\nthe number of chats an agent can handle per hour (including working on multiple chats simultaneously). Panel C\nplots the resolution rate, the share of chats successfully resolved, and Panel D plots NPS, or net promoter score, is\nan average of surveyed customer satisfaction. Data is at the agent-level and all regressions include agent and chat\nyear-month, months of agent tenure, with more details in Appendix section A.3.\n90\n\nTable A.7: Heterogeneity of AI Impact by Initial AI Adherence, Resolutions per\nHour\n(1)\nBy Adherence at AI\nQ1 (Lowest Adherence)\n0.213\u02da\u02da\u02da\n(0.043)\nQ2\n0.233\u02da\u02da\u02da\n(0.042)\nQ3\n0.293\u02da\u02da\u02da\n(0.042)\nQ4\n0.309\u02da\u02da\u02da\n(0.042)\nQ5 (Highest Adherence)\n0.432\u02da\u02da\u02da\n(0.043)\nYear Month FE\nYes\nAgent FE\nYes\nAgent Tenure FE\nYes\nDV Mean\n2.176\nObservations\n12,295\nStandard errors in parentheses\n\u02da p \u0103 0.10, \u02da\u02da p \u0103 0.05, \u02da\u02da\u02da p \u0103 0.01\nNotes: This table presents the results of difference-in-difference regressions estimating the impact of AI model\ndeployment on resolutions per hour by initial adherence. Column 1 estimates the impact of AI access by initial\nadherence quintile, including agent, year-month and months of agent tenure fixed effects. Observations are at the\nagent-month level and all standard errors are clustered at the agent level. We include chat year-month, agent and\nmonths of agent tenure fixed effects.\n91\n\nFigure A.13: AI Adherence over Time\nA. By Adherence at AI Model Deployment\n10\n20\n30\n40\n50\n60\n70\nAI Percent Adherence\n0\n1\n2\n3\n4\n5\nMonths Since AI Deployment\nLow Adherence\nMed Adherence\nHigh Adherence\nB. By Agent Tenure at AI Model Deployment\n25\n30\n35\n40\n45\n50\nAI Percent Adherance\n0\n1\n2\n3\n4\n5\nMonths Since AI Deployment\n<3 Mos.\n3-12 Mos.\n>12 Mos\nC. By Agent Skill at AI Model Deployment\n25\n30\n35\n40\n45\n50\nAI Percent Adherance\n0\n1\n2\n3\n4\n5\nMonths Since AI Deployment\nLow Skill\nMed Skill\nHigh Skill\nNotes: This figure plots the share of AI suggestions followed by agents as a function of the number of months each\nagent has had access to the AI model. In Panel A, we divide agents into terciles based on their adherence to AI\nsuggestions in the first month. In Panel B, we divide agents into groups based on their tenure at the firm at the time\nof AI model deployment. In Panel C, we divide workers into terciles of pre-deployment productivity as defined by\nour skill index.\n92\n\nFigure A.14: Within-agent AI Adherence over Time\nA. By Adherence at AI Model Deployment\n-5\n0\n5\n10\nResidual AI Adherence\n0\n1\n2\n3\n4\n5\nMonths Since AI Deployment\nLow Adherence\nMed Adherence\nHigh Adherence\nB. By Agent Tenure at AI Model Deployment\n-5\n0\n5\n10\nResidual AI Adherence\n0\n1\n2\n3\n4\n5\nMonths Since AI Deployment\n<3 Mos.\n3-12 Mos.\n>12 Mos\nC. By Agent Skill at AI Model Deployment\n-4\n-2\n0\n2\n4\n6\nResidual AI Adherence\n0\n1\n2\n3\n4\n5\nMonths Since AI Deployment\nLow Skill\nMed Skill\nHigh Skill\nNotes: This figure plots the residualized percentage of AI suggestions followed by agents as a function of the number\nof months each agent has had access to the AI model, after controlling for agent level fixed effects. In Panel A, we\ndivide agents into terciles based on their adherence to AI suggestions in the first month. In Panel B, we divide agents\ninto groups based on their tenure at the firm at the time of AI model deployment. In Panel C, we divide workers\ninto terciles of pre-deployment productivity as defined by our skill index.\n93\n\nE\nWorker Learning\n94\n\nFigure A.15: Sample AI Outage\n0\n20\n40\n60\n80\n100\nShare of AI-Enabled Chats with no AI\n06sep2020 08:26:40\n08sep2020 16:00:00\n10sep2020 23:33:20\n13sep2020 07:\nChat Date\nNotes: This figure plots the share of post-treatment chats with no AI suggestions during a period of a documented\nsoftware outage.\n95\n\nTable A.8: Chat Duration during AI System Outages\n(1)\n(2)\n(3)\n(4)\nNon Outage\nAI Outages\nOutages\nHigh Receptivity\nOutages\nLow Receptivity\nlead10\n0.684\n2.085\u02da\u02da\u02da\n3.391\u02da\u02da\u02da\n0.284\n(0.550)\n(0.581)\n(0.880)\n(1.590)\nlead9\n0.549\n2.126\u02da\u02da\u02da\n4.056\u02da\u02da\u02da\n1.003\n(0.529)\n(0.551)\n(0.890)\n(1.388)\nlead8\n-0.222\n1.410\u02da\u02da\u02da\n3.239\u02da\u02da\u02da\n0.340\n(0.479)\n(0.504)\n(0.771)\n(1.299)\nlead7\n-0.302\n1.267\u02da\u02da\u02da\n2.354\u02da\u02da\u02da\n1.025\n(0.470)\n(0.486)\n(0.798)\n(1.141)\nlead6\n0.874\u02da\n2.204\u02da\u02da\u02da\n4.081\u02da\u02da\u02da\n2.210\u02da\u02da\n(0.463)\n(0.468)\n(0.786)\n(1.079)\nlead5\n0.439\n1.444\u02da\u02da\u02da\n2.953\u02da\u02da\u02da\n1.736\u02da\n(0.389)\n(0.378)\n(0.591)\n(0.885)\nlead4\n0.415\n1.052\u02da\u02da\u02da\n2.222\u02da\u02da\u02da\n1.258\u02da\n(0.336)\n(0.321)\n(0.471)\n(0.703)\nlead3\n0.135\n0.343\n1.314\u02da\u02da\u02da\n0.218\n(0.305)\n(0.297)\n(0.482)\n(0.627)\nlead2\n-0.146\n-0.209\n0.723\u02da\n-0.869\u02da\n(0.248)\n(0.247)\n(0.411)\n(0.469)\nlag0\n-4.105\u02da\u02da\u02da\n-3.471\n-5.236\n9.410\u02da\u02da\u02da\n(0.270)\n(3.713)\n(3.732)\n(3.564)\nlag1\n-4.523\u02da\u02da\u02da\n-2.420\n-6.699\u02da\n0.000\n(0.313)\n(3.242)\n(4.052)\n(.)\nlag2\n-4.785\u02da\u02da\u02da\n0.767\n-0.201\n0.000\n(0.335)\n(3.112)\n(3.858)\n(.)\nlag3\n-4.424\u02da\u02da\u02da\n-2.851\n-2.677\n6.005\n(0.333)\n(2.663)\n(3.915)\n(4.099)\nlag4\n-4.225\u02da\u02da\u02da\n-2.423\u02da\n-4.011\u02da\u02da\n4.851\n(0.350)\n(1.442)\n(2.015)\n(3.663)\nlag5\n-4.084\u02da\u02da\u02da\n0.838\n0.695\n7.871\n(0.380)\n(1.744)\n(2.042)\n(5.904)\nlag6\n-4.300\u02da\u02da\u02da\n-7.799\u02da\u02da\u02da\n-8.574\u02da\u02da\u02da\n-2.303\n(0.388)\n(1.836)\n(2.168)\n(3.167)\nlag7\n-4.445\u02da\u02da\u02da\n-8.494\u02da\u02da\u02da\n-9.532\u02da\u02da\u02da\n-2.161\n(0.393)\n(1.635)\n(1.687)\n(3.209)\nlag8\n-3.804\u02da\u02da\u02da\n-4.672\u02da\n-2.163\n-8.059\u02da\u02da\u02da\n(0.411)\n(2.647)\n(2.471)\n(3.023)\nlag9\n-3.628\u02da\u02da\u02da\n-8.705\u02da\u02da\u02da\n-8.842\u02da\u02da\u02da\n0.318\n(0.440)\n(1.660)\n(1.930)\n(5.166)\nlag10\n-3.609\u02da\u02da\u02da\n-4.386\u02da\u02da\n-4.486\u02da\u02da\n-0.233\n(0.501)\n(1.720)\n(2.040)\n(4.273)\nlag11\n-3.305\u02da\u02da\u02da\n-2.779\u02da\u02da\n-6.857\u02da\u02da\n0.539\n(0.486)\n(1.380)\n(3.252)\n(4.003)\nlag12\n-3.479\u02da\u02da\u02da\n-1.966\n-4.630\n-8.939\u02da\u02da\n(0.517)\n(3.003)\n(4.248)\n(3.510)\nYear Month FE\nYes\nYes\nYes\nYes\nAgent FE\nYes\nYes\nYes\nYes\nAgent Tenure FE\nYes\nYes\nYes\nYes\nDV Mean\n41.982\n41.982\n41.982\n41.982\nR-squared\n.12\n.104\n.113\n.083\nObservations\n2,969,371\n1,829,527\n1,220,852\n323,810\nStandard errors in parentheses\n\u02da p \u0103 0.10, \u02da\u02da p \u0103 0.05, \u02da\u02da\u02da p \u0103 0.01\nNotes: This table shows the event study coefficients corresponding to the figures in 6. Column 1 shows the impact of\nAI access on call duration during non-outage periods, Column 2 shows the impact when the AI system is experiencing\nan outage. Column 3 shows the effects on agents who are in the top tercile of AI adherence, while Column 4 shows\nthe impacts on agents in the lowest tercile of adherence. Observations for these OLS regressions are at the agent-chat\nlevel and robust standard errors are clustered at the agent level. All specifications include agent, chat year-month,\nand months of agent tenure fixed effects.\n96\n\nF\nTopic Heterogeneity\n97\n\nFigure A.16: Distribution of Conversational Topics\nA. Distribution of Conversational Topics\n0\n.05\n.1\n.15\n.2\n.25\nTopic Share\nother\nach payments & transfers\ncustomer management\ninvoice creation & management\n\ufb01le management & storage\ntime tracking & timesheets\nloan management & forgiveness\nproduct activation & registration\nconversion & data migration\ntax \ufb01ling & payments\nmerchant services\ndata management & security\n1099 forms & \ufb01ling\ncontacting customer support\nsoftware installation & updates\ntechnical issues & troubleshooting\npayment processing & methods\naccount balances & transactions\naccountant collaboration\nbilling & subscriptions\nbanking & reconciliation\naccount access & management\npayroll & taxes\nNotes: This figure shows the distribution of chats by common topics in our data.\n98\n\nTable A.9: Impact of AI on Chat Duration, by Topic Commonality\n(1)\n(2)\nVARIABLES\nCall Duration\nAggregate Problem Frequency\nCall Duration\nWithin-Agent Frequency\nQ1 (Most Common Overall)\n-0.107***\n(0.00717)\nQ2\n-0.115***\n(0.00724)\nQ3\n-0.136***\n(0.00742)\nQ4 (Least Common Overall)\n-0.118***\n(0.00842)\nQ1 (Most Common within Agent)\n-0.100***\n(0.00927)\nQ2\n-0.103***\n(0.00740)\nQ3\n-0.119***\n(0.00700)\nQ4 (Least Common within Agent)\n-0.142***\n(0.00708)\nObservations\n2,089,995\n2,089,995\nR-squared\n0.120\n0.122\nYear Month FE\nYes\nYes\nAgent FE\nYes\nYes\nAgent Tenure FE\nYes\nYes\nDV Mean\n0.916\n0.915\nRobust standard errors in parentheses\n*** p\u01030.01, ** p\u01030.05, * p\u01030.10\nNotes: This table shows the impact of AI model deployment on call duration by frequency of the chat topic.\nRegression include controls for chat year-month, agent and months of agent tenure. Data is at the chat level and\nrobust standard errors are clustered at the agent level. Appendix section A.2 describes construction of topics and\nthe regression specifications.\n99\n\nG\nLanguage Fluency\n100\n\nFigure A.17: Distribution of Language Skills\nA. Comprehensibility Score\n0\n.5\n1\n1.5\n2\n2.5\nDensity\n1\n2\n3\n4\n5\nComprehensibility Score\nPre AI\nPost AI\nNever AI\nB. Native Fluency Score\n0\n.5\n1\n1.5\n2\n2.5\nDensity\n1\n2\n3\n4\n5\nNative Speaker (American English) Score\nPre AI\nPost AI\nNever AI\nNotes: These figures show the distributions of comprehensibility and native fluency scores. We split this sample into\nagent-month observations for agents who eventually receive access to the AI system before deployment (\u201cPre AI\u201d),\nafter deployment (\u201cPost AI\u201d), and for agent-months associated with agents who never receive access (\u201cNever AI\u201d).\nAppendix Section A.2 describes construction of the language scores.\n101\n\nTable ??: AI Impact on Language Fluency and Comprehensibility\n(1)\n(2)\n(3)\n(4)\nVARIABLES\nNative Fluency\nComprehensibility\nNative Fluency\nComprehensibility\nPost AI X Ever Treated\n0.250***\n0.241***\n(0.00851)\n(0.00767)\nPost AI X Ever Treated X US = 1\n0.159***\n0.134***\n(0.0225)\n(0.0208)\nPost AI X Ever Treated X Philippines = 1\n0.251***\n0.234***\n(0.00684)\n(0.00672)\nObservations\n12,772\n12,772\n11,271\n11,271\nR-squared\n0.791\n0.754\n0.733\n0.751\nYear Month FE\nYes\nYes\nYes\nYes\nAgent FE\nYes\nYes\nYes\nYes\nAgent Tenure FE\nYes\nYes\nYes\nYes\nDV Mean\n4.564\n4.597\n4.592\n4.608\nRobust standard errors in parentheses\n*** p\u01030.01, ** p\u01030.05, * p\u01030.10\nNotes: This table presents the results of difference-in-difference regressions estimating the impact of AI model\ndeployment on native fluency and comprehensibility.\nObservations are aggregated to the agent-month level and\nregressions include chat year-month, agent and months of agent tenure fixed effects. In Columns 1 and 2, robust\nstandard errors are clustered at the agent level and at the agent location level in Columns 3 and 4. Appendix Section\nA.2 describes construction of the language scores.\n102\n\nH\nExperience of Work\n103\n\nFigure A.18: Experience of Work\nA. Customer Sentiment, Histogram\nB. Agent Sentiment, Histogram\n0\n2\n4\n6\n8\nPercent\n-1\n-.5\n0\n.5\n1\nMean(Customer Sentiment)\n0\n20\n40\n60\nPercent\n-1\n-.5\n0\n.5\n1\nMean(Agent Sentiment)\nC. Customer Sentiment, by Pre-AI Skill\nD. Customer Sentiment, by Pre-AI Tenure\n.05\n.1\n.15\n.2\n.25\nChange in Mean(Customer Sentiment)\nQ1 (Lowest)\nQ2\nQ3\nQ4\nQ5 (Highest)\nAgent Skill at AI Deployment\n.05\n.1\n.15\n.2\n.25\nChange in Mean(Customer Sentiment)\n0 Mos.\n1-2 Mos.\n3-6 Mos.\n7-12 Mos.\n>12 Mos.\nAgent Tenure at AI Deployment\nE. Manager Assistance, by Pre-AI Skill\nF. Manager Assistance, by Pre-AI Tenure\n-.02\n-.015\n-.01\n-.005\n0\n.005\nChange in Share Req. Manager\nQ1 (Lowest)\nQ2\nQ3\nQ4\nQ5 (Highest)\nAgent Skill at AI Deployment\n-.02\n-.01\n0\n.01\nChange in Share Req. Manager\n0 Mos.\n1-2 Mos.\n3-6 Mos.\n7-12 Mos.\n>12 Mos.\nAgent Tenure at AI Deployment\nNotes: Each panel of this figure illustrates the impact of AI model deployment on aspects of the experience of work.\nPanel A shows average customer sentiment, while Panel B shows average agent sentiment. Sentiment is measured\nusing SiEBERT, a fine-tuned checkpoint of a RoBERTA, an English language transformer model. Panel C plots the\nimpacts of AI on customer sentiment by agent ex-ante productivity and Panel D plots the effects by agent tenure\nwhen the AI is deployed. Panels E and F show the effects of AI on customer requests for manager assistance, by\npre-AI agent skill and in by pre-AI agent tenure. Observations for the regression are at the agent-month level, robust\nstandard errors are clustered at the agent level, and regressions include controls for agent, chat year-month and\nmonths-of-tenure.\n104\n\nTable A.11: Attrition\n(1)\n(2)\n(3)\nVARIABLES\nLeaves this Month\nLeaves this Month\nLeaves this Month\nPost AI X Ever Treated\n-0.0868***\n(0.0130)\n0 Mos.\n-0.0952***\n(0.0162)\n1-2 Mos.\n-0.121***\n(0.0167)\n3-6 Mos.\n-0.0850***\n(0.0190)\n7-12 Mos.\n-0.0803***\n(0.0165)\n>12 Mos.\n-0.0306\n(0.0234)\nQ1 (Low Skill)\n-0.0655***\n(0.0148)\nQ2\n-0.0864***\n(0.0134)\nQ3\n-0.0741***\n(0.0124)\nQ4\n-0.0936***\n(0.0146)\nQ5 (High Skill)\n-0.0531***\n(0.0145)\nObservations\n17,902\n17,902\n17,902\nR-squared\n0.206\n0.206\n0.206\nYear Month FE\nYes\nYes\nYes\nLocation FE\nYes\nYes\nYes\nAgent Tenure FE\nYes\nYes\nYes\nAgent Company FE\nYes\nYes\nYes\nDV Mean\n0.288\n0.288\n0.288\nRobust standard errors in parentheses\n*** p\u01030.01, ** p\u01030.05, * p\u01030.10\nNotes: This table presents the results of difference-in-difference regressions estimating the impact of AI model\ndeployment on our main measure of productivity, resolutions per hour, the number of technical support problems\nresolved by an agent per hour (resolutions/hour). Post AI X Ever Treated captures the impact of AI model deployment\non resolutions per hour. Column 1 includes agent geographic location and year-by-month fixed effects. Columns 2 and\n3 include agent-level fixed effects, and Column 3, our preferred specification described by Equation 1, also includes\nfixed effects that control for months of agent tenure. Observations for this regression are at the agent-month level\nand all standard errors are clustered at the agent level. Section 3.1 describes the AI rollout procedure.\n105\n",
    "1\nGPT (Generative Pre-trained Transformer) \u2013 A\nComprehensive Review on Enabling Technologies,\nPotential Applications, Emerging Challenges, and\nFuture Directions\nGokul Yenduri, Ramalingam M, Chemmalar Selvi G, Supriya Y, Gautam Srivastava,\nPraveen Kumar Reddy Maddikunta, Deepti Raj G, Rutvij H Jhaveri, Prabadevi B, Weizheng Wang, Athanasios V.\nVasilakos, and Thippa Reddy Gadekallu\nAbstract\u2014The Generative Pre-trained Transformer (GPT) rep-\nresent a notable breakthrough in the domain of natural language\nprocessing, which is propelling us toward the development of\nmachines that can understand and communicate using language\nin a manner that closely resembles that of humans. GPT is based\non the transformer architecture, a deep neural network designed\nfor natural language processing tasks. Due to their impressive\nperformance on natural language processing tasks and ability\nto effectively converse, GPT have gained signi\ufb01cant popularity\namong researchers and industrial communities, making them one\nof the most widely used and effective models in natural language\nprocessing and related \ufb01elds, which motivated to conduct this\nreview. This review provides a detailed overview of the GPT,\nincluding its architecture, working process, training procedures,\nenabling technologies, and its impact on various applications.\nIn this review, we also explored the potential challenges and\nlimitations of a GPT. Furthermore, we discuss potential solutions\nand future directions. Overall, this paper aims to provide a\ncomprehensive understanding of GPT, enabling technologies,\ntheir impact on various applications, emerging challenges, and\npotential solutions.\nIndex Terms\u2014Generative Pre-trained Transformer, Natural\nlanguage processing, Arti\ufb01cial Intelligence\nGokul Yenduri, Ramalingam M, Chemmalar Selvi G, Supriya Y, Praveen\nKumar Reddy Maddikunta, Deepti Raj G, Prabadevi B are with the School\nof Information Technology and Engineering, Vellore Institute of Technol-\nogy, Vellore, Tamil Nadu- 632014, India (Emails: { gokul.yenduri, rama-\nlingam.m, chemmalarselvi.g, supriya.d, praveenkumarreddy, deeptiraj.g2020,\nprabadevi.b }@vit.ac.in)\nGautam Srivastava is with the Dept. of Math and Computer Science, Bran-\ndon University, Canada, and the Research Centre for Interneural Computing,\nChina Medical University, Taichung, Taiwan as well as Dept. of Computer\nScience and Math, Lebanese American University, Beirut, Lebanon (email:\nsrivastavag@brandonu.ca)\nRutvij H Jhaveri is with the Department of Computer Science and Engi-\nneering, School of Technology, Pandit Deendayal Energy University, India,\n(Email: rutvij.jhaveri@sot.pdpu.ac.in).\nWeizheng\nWang\nis\nwith\nthe\nDepartment\nof\nComputer\nScience,\nCity\nUniversity\nof\nHong\nKong,\nHong\nKong\nSAR,\nChina,\n(E-mail:\nweizheng.wang@ieee.org).\nAthanasios\nV.\nVasilakos\nis\nwith\nthe\nCenter\nfor\nAI\nResearch\n(CAIR),University\nof\nAgder(UiA),\nGrimstad,\nNorway,\n(Email:\nthanos.vasilakos@uia.no).\nThippa Reddy Gadekallu is with the School of Information Technology\nand Engineering, Vellore Institute of Technology, Vellore 632014, India,\nLovely Professional University, Phagwara, India, Department of Electrical\nand Computer Engineering, Lebanese American University, Byblos, Lebanon,\nJiaxing University , Jiaxing 314001, China, Zhongda Group, China, 314312\n(E-mail: thippareddy@ieee.org).\nTABLE I\nLIST OF KEY ACRONYMS ONLY IF IT IS REPEATED\nAcronyms\nDescription\nAI\nArti\ufb01cial Intelligence\nAR\nAugmented Reality\nBERT\nBidirectional\nEncoder\nRepresentations\nfrom\nTransformers\nBGN\nBoneh\u2013Goh\u2013Nissim\nCNN\nConvolutionalNeural Network\nDAP\nData Access Point\nDLT\nDecentralized Ledger Technology\nDL\nDeep Learning\nDRL\nDeep Reinforcement Learning\nDR\nDemand response\nEC\nEdge Computing\nEU\nEnd User\nEAPs\nEnergy Access Points\n5G\nFifth-Generation\n4G\nFourth-Generation\nGPT\nGenerative Pre-trained Transformer\nGPU\nGraphics Processing Unit\nHPC\nHigh Performance Computing\nHCI\nHuman Computer Interaction\nIoT\nInternet of Things\nML\nMachine Learning\nNLP\nNatural Language Processing\nNPC\nNon Playable Character\nPLM\nPre-trained Language Models\nPTM\nPre-Trained Models\nRNN\nRecurrent Neural Network\n6G\nSixth-Generation\nTL\nTransfer Learning\nVU\nVirtual Reality\nI. INTRODUCTION\nLanguage is the cornerstone of human communication and\nplays a vital role in shaping our interactions with the world.\nWith the advent of NLP, it has revolutionized the way we\ninteract with machines. NLP has become a game-changer in\nthe world of communication, enabling humans to interact with\narXiv:2305.10435v2  [cs.CL]  21 May 2023\n\n2\nmachines in a more natural way. The evolution of NLP has\nbeen fueled by the exponential growth of textual data in the\ninternet. Over the years, NLP has witnessed a signi\ufb01cant trans-\nformation from simple rule-based systems to complex deep\nlearning-based models. Despite the advances, natural language\nunderstanding and generation have long been a challenging\nproblem in the \ufb01eld of NLP, largely due to the complex\nnature of human language. However, recent advancements\nhave paved the way for the new approaches to tackle these\nchallenges. One such breakthrough in NLP, is the development\nof the GPT [1]. GPT became famous after the launch of\nChatGPT by OpenAI, a research company [2] that focuses on\ndeveloping AI technologies. GPT is a deep learning model that\nis pre-trained on large corpora of text data and can be \ufb01ne-\ntuned for speci\ufb01c tasks like language generation, sentiment\nanalysis, language modelling, machine translation, and text\nclassi\ufb01cation. The transformer architecture used in GPT is\na signi\ufb01cant advancement over previous approaches to NLP,\nsuch as RNN and CNN. It uses a self-attention mechanism to\nallow the model to consider the context of the entire sentence\nwhen generating the next word, which improved the model\u2019s\nability to understand and generate language. The decoder is\nresponsible for generating the output text based on the input\nrepresentation [3].\nGPT can perform a wide range of tasks in NLP. One of\nits key strengths is in natural language understanding (NLU),\nwhere it can analyze and comprehend the meaning of text, in-\ncluding identifying entities and relationships in sentences. It\u2019s\nalso pro\ufb01cient in natural language generation (NLG), which\nmeans it can create text output, such as writing creative content\nor answering questions in a comprehensive and informative\nway. Alternatively, GPT is also code generator, where it can\nwrite programming code in various languages, such as Python\nor JavaScript. GPT can also be utilized for question answering,\nwhich means it can provide summaries of factual topics or\ncreate stories based on the input text. Additionally, GPT can\nsummarize a piece of text, such as providing a brief overview\nof a news article or research paper, and it can be used for\ntranslation, which makes it possible to translate text from\none language to another. Overall, GPT\u2019s ability to perform\na wide range of NLP tasks with high accuracy and precision,\nmakes it an invaluable tool for various industries, including\n\ufb01nance, healthcare, marketing, and more. As NLP technology\ncontinues to advance, we can expect GPT and other language\nmodels to become even more sophisticated and powerful,\nenabling us to communicate with machines more naturally and\neffectively.\nA. Motivation\nGPT has become a transformative technology in the \ufb01eld\nof NLP, enabling the rapid development and growth of a wide\nrange of industries and applications. Despite its wide adoption\nand numerous potential applications, there is still much to be\nexplored and understood about GPT\u2019s capabilities. Although\nthere are studies on GPT in the literature related to academia\nand libraries [4], education [5], GPT models [6], banking and\ncorporate communication [7], advancements in chatGPT and\nits version [8], and on generative AI\u2019s [9], no existing reviews\nare dedicated to providing a comprehensive survey on GPT.\nTherefore, there is a need for a comprehensive review that\nfocuses on GPT\u2019s architecture, enabling technologies, potential\napplications, emerging challenges, interesting projects and fu-\nture directions. These limitations motivated us to conduct this\nreview. Hence, this review will not only help researchers and\npractitioners in this \ufb01eld to gain a better understanding of GPT\nbut also provide valuable insights into its potential applications\nand major limitations when conducting the research.\nB. Related Surveys and Contributions\nThe GPT model is a type of DL model that uses self-\nsupervised learning to pre-train massive amounts of text data,\nenabling it to generate high-quality language output. The\nrecent advancements in GPT model research can be attributed\nto the continual improvement of its architecture, increased\navailability of computing power, and the development of novel\ntechniques to \ufb01ne-tune the model for speci\ufb01c tasks. These\nadvancements have led to the creation of larger and more\npowerful GPT models, enabling them to perform a wider range\nof NLP tasks with unprecedented accuracy and \ufb02uency. These\nGPT models have demonstrated great potential in transforming\nvarious industries like healthcare [10], customer service [11],\n\ufb01nancial industry [12] and so on. These applications are\nenabled by the generation of high-quality and diverse data\nlike large-scale corpora of text data with different fast-growing\nenabling technologies [13], [14]. There are numerous survey\npapers published to provide a comprehensive overview of\nthe latest developments in GPT models, insights into the\ndifferent architectures, training methods, evaluation metrics,\nand highlight the challenges and future directions of this \ufb01eld.\nThis literature survey aims to review and analyze the key\n\ufb01ndings and contributions of the most recent survey papers\npublished on GPT models, to provide a comprehensive and\nup-to-date understanding of the state-of-the-art in this exciting\nand rapidly evolving \ufb01eld.\nLund et al. [4] presents the potential effects of AI and GPT\nmodels, speci\ufb01cally ChatGPT, on academia and libraries. They\ndiscussed the capabilities of ChatGPT in generating human-\nlike responses and its potential applications. They examine\nhow AI-powered chatbots and virtual assistants based on\nGPT models can enhance student learning experiences, assist\nwith research tasks, and improve library services. They also\naddress concerns regarding data privacy, biases, and the need\nfor ethical guidelines. Overall, this survey paper highlighted\nthe transformative potential of AI and GPT models while\nemphasizing the importance of responsible deployment and\nhuman oversight.\nKasneci et al. [5] have reviewed the potential opportunities\nand challenges of using large language models, speci\ufb01cally\nChatGPT, for educational purposes. They highlighted the\nbene\ufb01ts and limitations of using such models by discussing\ntheir implications for teaching and learning. In addition, a\nde\ufb01ned strategy and pedagogical approach with a heavy focus\non critical thinking and fact-checking are required while using\nsuch large language models in educational institution. Thus,\n\n3\nthey concluded the paper by highlighting the key technical\nchallenges like copyright issues, biased content creation, user\ndependency, privacy and security, and high-cost language\nmodels when such language models are used in the educational\nsector.\nQiu et al. [6] presented an exhaustive survey of various\ntypes of GPT models by detailing their working architecture.\nThey discussed the evolution of pre-training methods for NLP,\nfrom language modelling to TL and pre-training on large-\nscale corpora. It also reviews the different types of GPT\nmodels, including word embeddings, contextual embeddings,\nand transformer-based models, and discusses their applications\nin various NLP tasks such as text classi\ufb01cation, Named Entity\nRecognition, and machine translation. They highlighted the\nbene\ufb01ts of GPT\u2019s models for the NLP domain, such as its\nability to improve model performance with limited annotated\ndata, reduce the need for task-speci\ufb01c feature engineering, and\nenable TL across multiple tasks. They discussed the major\nchallenges and limitations of PTMs, such as the risk of bias\nand the lack of interpretability.\nGeorge et al. [7] studied the potential impact of GPT-4,\nthe next iteration of GPT models, on communication within\ncorporate environments. They discussed how GPT-4 can rev-\nolutionize business communication by enabling more ef\ufb01cient\nand effective interactions. They explore various applications\nof GPT-4 in corporate settings, such as automating customer\nsupport through AI chatbots that can provide personalized\nresponses and resolve queries in real-time. They also ad-\ndressed potential challenges and considerations associated\nwith implementing GPT-4 in corporate settings. These include\nconcerns about data security, privacy, and the need for human\noversight to ensure accurate and ethical communication. Thus,\nthey concluded by emphasizing the transformative potential\nof GPT-4 in revolutionizing business communication to fully\nharness the bene\ufb01ts of GPT-4 while addressing any potential\nrisks or limitations.\nZhang et al. [8] presents an extensive survey of generative\nAI and evaluates the capabilities of the ChatGPT models,\nparticularly from GPT-4 to GPT-5. They provided an overview\nof generative AI, highlighting its signi\ufb01cance in generating\nrealistic and creative outputs across various domains and\nevaluate their advancements over previous iterations. They\nanalyze the architectural improvements, model size, training\ntechniques, and dataset considerations employed in GPT-4\nand GPT-5. In addition to it, they presented a comprehensive\ncomparison of ChatGPT with other state-of-the-art generative\nAI models, such as OpenAI\u2019s DALL-E and CLIP. Finally,\nthey concluded with valuable insights into the capabilities\nand limitations of these models and highlights the broader\nlandscape of generative AI.\nZaib et al. [9] provides a survey on the latest advance-\nments in GPTS and PTMs for conversational AI applications.\nThey focused on PLMs and their approaches while building\ndialogue-based systems. They also highlighted the potential\nuse of transformer-based models such as BERT and GPT,\nwhich have demonstrated good performance in understanding\nNLP generation, and dialogue management. Thus, they con-\ncluded with the signi\ufb01cant challenges in the \ufb01eld of developing\nconversational AI systems using PLMs and GPTs.\nThus, the comparison of existing surveys on GPT models\nhighlighting the growing importance of these models in key\nareas of NLP and other related \ufb01elds are discussed here.\nHence, this is the \ufb01rst-of-its-kind survey that presents the\nextensive information, by comparing existing surveys with our\nsurvey and summarized in Table II.\nC. Systematic Literature Survey\nIn this review of GPT, we conducted a thorough literature\nreview using various reputable sources. Our search was pri-\nmarily focused on peer-reviewed journals, and high-quality\narticles from reputed national and international conferences,\nseminars, books, symposiums, and journals. To ensure the\ncredibility of our sources, we referred to well-known archives\nsuch as Google Scholar and arXiv, and publications from top\ndatabases like IEEE, Springer, Elsevier, Taylor & Francis, and\nWiley. To identify relevant GPT references and publications,\nwe used keywords such as NLPGPT, GPT architecture, DL for\nGPT, Pretraining GPT, Fine-tuning AI GPT and GPT vertical\napplications. We then screened all the retrieved articles based\non their titles, excluding any papers with poor-quality material.\nNext, we reviewed the abstracts of the remaining articles to\ndetermine their contributions. In the \ufb01nal step of our literature\nreview, we extracted the necessary data for our analysis. By\nfollowing these phases, we ensured that our study was based\non high-quality and credible sources.\nD. Paper Organization\nThe structure of this paper\u2019s organization is illustrated in\nFig. 1. Section 2 presents the preliminaries of GPT models\nsuch as the de\ufb01nition of GPT, its evolution and architecture,\nhow it works and presents the comparison of various GPT\nmodels. Section 3 discusses the key enabling technologies for\nGPT models. The impact of GPT models in various applica-\ntions are presented in Section 4. In Section 5, we highlighted\nsome of the exciting GPT projects that are currently developed.\nSection 6 includes open issues, other technical challenges\nand future research directions in the \ufb01eld of GPT. Finally,\nwe conclude the paper in Section 7, by summarizing the\nkey \ufb01ndings and contributions of this study. The list of key\nacronyms are listed in Table I.\nII. PRELIMINARIES\nIn this section, the evolution of GPT models, the architecture\nof GPT, working process of GPT models are discussed and\n\ufb01nally, different versions of GPT models are compared.\nA. Generative Pre-trained Transformer\nThe GPT model produces enormous quantities of pertinent\nand complicated machine-generated text from a small amount\nof text as input. GPT models can be identi\ufb01ed as a language\nmodel that mimics human text using a DL techniques and it\nacts as an autoregressive model in which the present value is\nbased on the previous value [15].\n\n4\nSECTION-1:INTRODUCTION\nMOTIVATION\nRELATED SURVEYS AND\nCONTRIBUTIONS\nSYSTEMATIC LITERATURE\nSURVEY\nPAPER\nORGANIZATION\nSECTION-2:PRELIMINARIES\nGPT Definitions\nEvolution of GPT\nGPT Architecture\nHow does GPT work?\nComparisons\u00a0of GPT versions\nDefinition-1\nDefinition-2\nDefinition-3\nDefinition-4\nSECTION-3:ENABLING TECHNOLOGIES\nARTIFICIAL INTELLIGENCE\nCLOUD COMPUTING\nEDGE COMPUTING\n5G & BEYOND NETWORKS\nHUMAN COMPUTER INTERACTION\nBIG DATA\nSECTION-4:IMPACT OF GPT ON VARIOUS APPLICATIONS\nEducation\nHealthcare\nIndustry\nAgriculture\nTravel and Transport\nE-Commerce\nEntertainment\nLifestyle\nGaming\nMarketing\nFinance\nINTRODUCTION\nIMPACT OF GPT ON APPLICATION\nCHALLENGES\nSUMMARY\nSiriGPT\nAI Dungeon\nCopy.ai\nBond.AI\nViable\nUber's Plato Research Dialogue System\nAI Channels\nFireflies.AI\nDeepScribe\nPolyglot AI\nMeena\nSECTION-6:OPEN ISSUES AND OTHER TECHNICAL CHALLENGES\nSECTION-5:PROJECTS\nDomain Specific GPTs\nComputational requirements\nExplainability and interpretability\nData Bias\nMultimodal support\nRobustness\nMultilingual support\nModel size\nLimited understanding\nEthical Concerns\nSecurity and privacy concerns\nSECTION-7:CONCLUSION\nFig. 1. Organization Chart of the survey.\n1) De\ufb01nition 1: GPTs are language models pre-trained on\nvast quantities of textual data and can perform a wide range\nof language-related tasks [16].\n2) De\ufb01nition 2: A GPT is a language model relying on DL\nthat can generate human-like texts based on a given text-based\ninput. [17].\n3) De\ufb01nition 3: GPT is a language model developed by\nOpenAI to help give systems intelligence and is used in such\nprojects as ChatGPT [17].\nB. Evolution of GPT\nGPT models have evolved through multiple changes and\nbreakthroughs in NLP technology. These are some signi\ufb01cant\n\n5\nTABLE II\nCOMPARISON OF THIS SURVEY WITH THE EXISTING SURVEYS\nRef.\nApplications\nEnabling Technologies\nRemarks\nEducation\nIndustry\nAgriculture\nHealthcare\nTransport\nE-Commerce\nEntertainment\nLifestyle\nGaming\nMarketing\nFinance\nBig Data\nAI\nCloud Computing\nEdge Computing\n5G and Beyond\nHCI\n[4]\n\u2713\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\n\u2713\n\u2713\n\u2713\nX\nX\nThey conducted a survey discussing capabilities of\nChatGPT on academia and libraries. Although, Key\nchallenges of Chatgpt were highlighted, practical\nimplementation challenges and research directions\nwere missing.\n[5]\n\u2713\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\n\u2713\nX\nX\nX\nX\nReviewed the potential opportunities and challenges\nof using large language models, speci\ufb01cally Chat-\nGPT, for educational purposes. Thus, evolution of\nGPT and their preliminaries were not discussed in\nthis survey paper.\n[6]\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\n\u2713\nX\nX\nX\nX\nThey presented an exhaustive survey of various types\nof GPT models by detailing their working architec-\nture with bene\ufb01ts and limitations of GPTs. However,\n[7]\nX\nX\nX\n\u2713\nX\nX\nX\nX\nX\nX\nX\nX\n\u2713\nX\nX\nX\nX\nStudied the potential impact of GPT4 in business\ncommunication and explore various applications of\nGPT-4 in corporate settings by highlighting any\npotential risks or limitations. But, how GPT archi-\ntecture can be used in corporate is not found with\nkey enabling technologies.\n[8]\nX\n\u2713\nX\nX\n\u2713\n\u2713\n\u2713\nX\nX\n\u2713\n\u2713\nX\n\u2713\nX\nX\nX\nX\nAnalyzed the architectural improvements, model\nsize, training techniques, and dataset considerations\nemployed in GPT-4 and GPT-5. However, prelimi-\nnary details are unedr explored.\n[9]\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nX\nRecent trends in language models, applications of di-\nalogue management, question answering NLP tasks\nwere discussed along with challenges and future\nscope of GPT. Although it covered most of the\ntechnical aspects, the integration challenges to over-\ncomeare not presented.\nOur\nSurvey\nPaper\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\n\u2713\nPresents the evolution of GPT models, GPT architec-\nture and its detailed working, key enabling technolo-\ngies, signi\ufb01cant advancements of GPT models and\ntheir potential bene\ufb01ts in real-life applications, GPT\nprojects, lessons learnt, open challenges and future\nresearch directions.\nturning points in the growth of the GPT model: Before GPT,\nNLP models have been trained on large amounts of annotated\ndata that is related to a speci\ufb01c task. This had a signi\ufb01cant\ndrawback because it was dif\ufb01cult to access the quantity of\nlabelled data required to train the model precisely. The NLP\nmodels were unable to complete tasks outside of their training\nset since they were restricted to a particluar set of data. To\nget around these restrictions, OpenAI offered a Generative\nLanguage Model called GPT-1 that was created using un-\nlabeled data and then given to users to \ufb01ne-tune to carry\nout subsequent tasks like sentiment analysis, categorization,\nand question-answering [18]. This indicates that the model\nattempts to produce an appropriate response based on input\nand that the data used to train the model is not labelled [19].\nFig. 2 shows the timeline of the evolution of several pre-trained\nmodels from Eliza, which was created in 1960, to the more\ncurrent 2022-ChatGPT.\nGPT-1 was the \ufb01rst ever model that could read the text and\nrespond to queries [20]. OpenAI released GPT-1 in 2018. GPT-\n1 was a major move forward in AI development because it\nenabled computers to comprehend textual material in a more\nnatural manner than before. This generative language model\nwas able to learn a wide variety of connections and gain\nimmense knowledge on a varied corpus of contiguous text\nand lengthy stretches [21]. This happened after being trained\non a huge BooksCorpus dataset. In terms of design, GPT-1\nemploys a 12-layer decoder architecture transformer with a\nself-attention system for training. GPT-1\u2019s capacity to execute\nzero-shot performance on different tasks was one of its major\nsuccess as a result of its pre-training. This ability demonstrated\nthat generative language modelling can be used to generalize\nthe model when combined with a successful pretraining idea.\nWith TL as its foundation, GPT models evolved into a potent\ntool for performing NLP tasks with minimal \ufb01ne-tuning [22].\nIt paved the way for other models to progress even more in\ngenerative pre-training using larger datasets and parameters.\n[18].\nTo create a better language model later in 2019, OpenAI\ncreated a GPT-2 using a bigger dataset and more parameters.\nThe model design and execution of GPT-2 are some of the\nkey advancements [23]. With 1.5 billion parameters, it has\n10 times the size of GPT-1 (117 million parameters), and it\nhas 10 times as many parameters and data [21]. By using\nonly the raw text as input and utilizing little to no training\nexamples, it is effective in terms of resolving various language\ntasks related to translation, summarization, etc. Evaluation of\nGPT-2 on various downstream task datasets revealed that it\nexcelled by substantially increasing accuracy in recognizing\nlong-distance relationships and predicting sentences [24].\nThe most recent iteration of the GPT model is GPT-3. It is a\nsizable language prediction and production model created by\nOpenAI that can produce lengthy passages of the source text.\n\n6\nELIZA-1960\nPattern matching &\nreplacement\nHelps human chat by\nentertaining\nchatbot to provide original\nreplies on its own\nALICE-1995\nSmarterChild-2001\nSiri -2010\nGoogle Now-2012\nAlexa-2014\nAnalyze human language\n& synthesize\nBot provides the\nconversations in a\nfun manner\nintelligent\npersonal assistant\nHelps to answer the questions,\nperforms actions,\u00a0 makes\nrecommendations\nIntelligent personal\nassistant\nPARRY-1970\nChatbot to provide\npsychological concepts\nRacter-1980\nJabberwacky-1990\nCHatGPT-2022\nAssist users in generating\nhuman-like text based on\ngiven input\nFig. 2. GPT Road Map.\nGPT-3 eventually emerged as OpenAI\u2019s ground-breaking AI\nlanguage software. Simply put, it is a piece of software that can\ncreate lines on its own that are so distinctive they almost sound\nlike they were written by a human [25]. The GPT-3 program\nis presently accessible with limited access via a cloud-based\nAPI, and access is required to investigate the utility. Since its\ndebut, it has produced several interesting apps. Its capacity,\nwhich is about 175 billion parameters big and 100 times larger\nthan GPT-2, is a key advantage. It is taught using a corpus of\n500 billion words called \u201dCommon Crawl\u201d that was gathered\nfrom a sizable content archive and the internet [26]. Its other\nnoteworthy and unexpected capability is its ability to carry out\nbasic mathematical operations, write bits of code, and carry out\nclever tasks. As a result, NLP models can help businesses by\nresponding more quickly to requests and accurately keeping\nbest practices while minimizing human mistakes [27]. Due\nto its intricacy and size, many academics and writers have\nreferred to it as the ultimate black-box AI method. Due to the\nhigh cost and inconvenience of performing inference, as well\nas the billion-parameter size that makes it resource-intensive,\nit is dif\ufb01cult to put into practice in jobs [24].\nGPT-4 was named as the successor of GPT-3. In the meantime,\nseveral AI models built on GPT-3.5, an updated version of\nGPT-3, have been surreptitiously released by OpenAI [28].\nGPT-3.5 was trained on a mixture of text and code. From the\nvast amounts of data collected from the web, which includes\ntens and thousand of Wikipedia entries, social media posts,\nand news items, GPT 3.5 learned the relations between words,\nsentences, and various components. It was utilized by OpenAI\nto develop several systems that have been tailored to complete\nparticular jobs [26]. It collected vast amounts of data from the\nweb, including tens of thousands of Wikipedia entries, posts\non social media, and news items, and used that information\nto learn the relationships between sentences, words, and word\ncomponents [29].\nThe latest version of the GPT model by OpenAI is GPT-4\nwhich is a multimodal big language model. It was launched\non March 14, 2023, and is now accessible to the general\npublic through ChatGPT Plus in a constrained capacity. A\nwaitlist is required to gain access to the business API [10].\nUsing both public data and \u201ddata licensed from third-party\nproviders,\u201d GPT-4 was pre-trained to anticipate the next coin as\na transformer. It was then adjusted with reinforcement learning\nbased on input from humans and AI for human alignment\nand policy conformance. In comparison to GPT-3, which had\ncontext windows of only 4096 and 2049 tokens, respectively,\nthe group created two variants of GPT-4 with context windows\nof 8192 and 32768 tokens.\nC. GPT model\u2019s architecture\nGPT models are based on neural networks that are used for\nNLP tasks, such as language modelling, text classi\ufb01cation,\nand text generation.\nThe GPT model\u2019s architecture is based on the transformer\n\n7\nmodel [30]. The Transformer model uses self-attention\nmechanisms to process input sequences of variable length,\nmaking it well-suited for NLP tasks. GPT simpli\ufb01es the\narchitecture by substituting encoder-decoder blocks with\ndecoder blocks. GPT model takes the transformer model and\npre-trains it on large amounts of text data using unsupervised\nlearning\ntechniques.\nThe\npre-training\nprocess\ninvolves\npredicting the next word in a sequence given the previous\nwords, a task known as language modelling. This pre-training\nprocess enables the model to learn representations of natural\nlanguage that can be \ufb01ne-tuned for speci\ufb01c downstream\ntasks [31]. The following are the components of the GPT\narchitecture.\n\u2022 Input Embedding layer: The embedding layer maps the\ninput tokens (e.g., words or subwords) to continuous\nvector representations, which can be processed by the\ntransformer blocks [32].\n\u2022 Positional encoding: Since the transformer blocks do not\nhave any notion of order or position, positional encoding\nis added to the input embeddings to provide information\nabout the relative position of tokens. Masking: In some\ncases, masking may be necessary to mask certain input\ntokens (e.g., in language modelling tasks, the model\nshould only use tokens that come before the target\ntoken). Transformer blocks: GPT models are based on\nthe transformer architecture. It is designed for NLP tasks\nand has been widely used in applications such as ma-\nchine translation, text classi\ufb01cation, and text generation.\nTransformers allow the model to focus on different areas\nof the input while processing [33].\n\u2022 Linear and Softmax Functions: In the GPT architecture,\nthe softmax function is commonly used for classi\ufb01cation\ntasks. The softmax function is applied to the output of\nthe \ufb01nal layer of the model. It generates a probability\ndistribution over a set of output classes. The output of\nthe \ufb01nal layer is speci\ufb01cally converted into a set of\nlogits before being normalized with the softmax function.\nThe normalized values obtained from the model can\nbe interpreted as the likelihood or probability that a\nparticular input belongs to each of the output classes. The\nquery, key, and value vectors for each token in the input\nsequence are frequently calculated using linear functions\nin the attention mechanism. The output of the multi-\nhead attention layer is transformed using them in the\nfeedforward layers as well. The output layer also employs\nlinear functions to forecast the following token in the\nsequence [34].\n\u2022 Pre-training: Pre-training is a key component of the\nGPT architecture. In pre-training, the model is trained\non a large amount of data in an unsupervised manner\neven before \ufb01ne-tuning the model for speci\ufb01c tasks like\nclassi\ufb01cation and text generation.\n\u2022 Fine-tuning: Fine-tuning is the process of adapting a pre-\ntrained neural network model, such as GPT, to a new\ntask or dataset by further training the model on that\ntask or dataset. Fine-tuning in GPT involves adjusting\nthe parameters of the pre-trained model to optimize\nperformance on a speci\ufb01c downstream task, such as text\nclassi\ufb01cation or text generation [35].\n\u2022 Language modeling: Language modelling is a key task in\nthe GPT architecture. In the case of GPT, the language\nmodelling task is performed during the pre-training phase\nof the model. In pre-training, the model is trained based\non a large amount of data using a language model\nobjective. It is the task of predicting the next word in\nsequence based on the previous words. It allows the\nmodel to learn relationships between the words and their\nmeaning in the training data [36].\n\u2022 Unsupervised learning: Unsupervised learning is an ML\nalgorithm which enables the model to learn form unla-\nbelled data without any human intervention. GPT models\nuse unsupervised learning in the pre-training phase to\nunderstand the relationships between the words and their\ncontext in the training data [37].\nD. How do GPT models work?\nGPT models work by using a transformer which is a neural\nnetwork architecture that processes the input sequences of\nnatural language text [38]. The GPT model uses unsupervised\nlearning techniques to pre-train this transformer architecture\non a signi\ufb01cant amount of text input [39]. The model gains\nthe ability to anticipate the subsequent word in a sequence\nbased on the preceding words during pre-training. Language\nmodelling is the process that enables a model to discover\nthe statistical connections between words and their context\nin training data. Fig. 5 shows the various stages of GPT\noperation. The \ufb01rst step entails supervised \ufb01ne-tuning, the\nsecond step involves producing optimal responses to input,\nand the third step involves proximal policy optimization and\nreinforcement learning.\nThe model can be \ufb01ne-tuned for particular tasks, like text\nclassi\ufb01cation or text production, after pre-training. The model\nis trained on a smaller dataset that is unique to the work at hand\nduring \ufb01ne-tuning, and the model\u2019s parameters are changed\nto maximize performance on that task [8]. Fig. 3 shows the\ngeneral transformer architecture of GPT.\nWhen used for text creation, GPT models create text by\nanticipating the following word in a series based on the\npreviously created words. Depending on how it has been\nmodi\ufb01ed, the model can produce text that is comparable to\nthe input text or that adheres to a certain theme or style. Fig.\n4 projects the GPT model\u2019s transformer architecture and input\ntransformations for \ufb01ne-tuning different tasks.\nE.\nComparisons of GPT Versions\nThere are several versions of GPT models each having their\nown features and capabilities. Table III presents a comparison\nof various versions of the GPT models. The table presents\nthe following details like year of release of the GPT model,\nparameters, tokens generated, input type, features of each\nmodel, drawbacks of each model, and the size of each model.\n\n8\nTABLE III\nCOMPARSION OF DIFFERENT VERSIONS OF GPT MODEL\nModel\nTokens\nSize\nParameters\nDataset\nYear\nFeatures\nInput Type\nDrawbacks\nGPT-1\n-\n12-layer decoder\n117M parameters\nBooks corpus\n2018\nUsed mostly for lan-\nguage modelling tasks\nand it is transformer\nbased\nA sequence of tokens\nand words\nLimited\nCapacity,\nLimited\nData, Cannot perform complex\ntasks, Limited applications\nGPT-2\n-\n10 times the size\nof GPT-1\n1.5B parameters\nDownstream task\ndatasets\n2019\nText generation capa-\nbilities\nare\nimproved\nand a chance for misuse\nA sequence of tokens\nand words\nLimited Control, Limited Data\nDiversity, Expensive computa-\ntional requirements, Risk of\nimproper information\nGPT-3\n4096\nand\n2049\ntokens\n100 times larger\nthan GPT-2\n175B parameters\nCommon Crawl\n2020\nGood NLP capabilities,\nlanguage\ntranslation,\nsummarization\nand\ngeneration of text\nA sequence of tokens\nand words and images\nand tables\nLimited Control, Limited Data\nDiversity, Lack of explanation,\nEthical concerns\nGPT-3.5\nmaximum\ntoken\nlimit of 4096 to-\nkens\n96 layers\nsimilar or larger\nnumber\nof\nparameters\nlike\nGPT-3\n-\n2022\nImproves user experi-\nence by delivering more\nprecise and contextu-\nally relevant informa-\ntion\nThe input type typi-\ncally consists of text\ndata\nLimited resources to train,Data\nBias,Lack\nof\nExplain-\nability,Limited\nContextual\nUnderstanding,High Inference\nLatency\nGPT-4\n8192 and 32768\ntokens\n-\n100T parameters\n-\n2023\nCreative and technical\nwriting tasks\nA sequence of tokens\nand words and images\nand tables\n-\nGenerative AI (GAI) models are of different types like uni-\nmodal, cross-modal, and multimodal. The \ufb01rst type is uni-\nmodal which rely on a single type of input, such as text\nor images. The cross-modal, on the other hand, can process\nmultiple types of inputs and relate them to each other. The\nMultimodal is the most complex type of AI as it can process\nand integrate information from multiple modalities, such as\nspeech, text, images, and even physical interactions with the\nenvironment. GPT adopts only unimodal and multimodal types\nwhere ChatGPT is said to be unimodal, while GPT-4 is\nmultimodal. Fig. 6 is an illustration that distinguishes between\nunimodal, cross-modal, and multimodal Generative AI models.\nOverall, GPT models have demonstrated outstanding per-\nformance with NLP, by enhancing each iteration and its pre-\ndecessor\u2019 capabilities. Each model, however, also has its own\nrestrictions and drawbacks, such as restricted output control,\nlack of diverse data, and ethical concerns. While selecting a\nGPT model for a particular task, researchers and developers\nshould carefully take these factors into account [40].\nIn detail, this section describes the evolution, and architecture\nof GPT and compares the different versions and types of GPT.\nIII. ENABLING TECHNOLOGIES\nGPT is a convergence of several technologies. It is enabled\nby the latest technologies like Big data, AI, Cloud Computing,\nEC, 5G and beyond networks, and HCI. In this section, we\nprovide an overview of enabling technologies related to GPT.\nThe major technologies that constitute the GPT models are\ndepicted in Fig. 7.\nA. Big Data\nBig data refers to the vast amounts of structured and\nunstructured data generated by businesses, individuals, and\nmachines. The proliferation of new technologies, such as the\nIoT, has led to an explosion of data production from sources\nlike social media, sensors, and transaction-based systems [41].\nThe emergence of big data has revolutionized the way\norganizations approach data analysis and decision-making.\nThe training provided by this massive amount of data has\nyielded valuable insights for the use of advanced models like\nGPT in the \ufb01eld of NLP [42]. The GPT models utilize DL\nand big data for natural language generation, with GPT-4\nbeing the most advanced model to date [43]. The training\ndata for GPT models typically include millions or even\ntrillions of data from a diverse range of sources, such as\nbooks, articles, websites, and social media platforms. This\nlarge and diverse training data helps GPT models capture the\nvariations in language usage, making them more accurate\nand effective at NLP tasks. As a result, GPT models may\nbe used for a variety of tasks, including question-answering,\ntext summarization, and language translation [44]. Moreover,\nsince GPT models can learn from a variety of data sources,\nthey can be tuned for certain tasks and domains, making them\nvery adaptive and versatile. GPT model has the potential to\nbe utilized for a variety of activities, including the creation\nof images and videos in addition to its excellent language\nprocessing capabilities [45].\nWhile big data presents numerous bene\ufb01ts to GPT, by\nenabling the models to get trained with large amounts of data,\nit also presents several challenges [46]. GPT is trained on\na variety of data, large amounts of data, and also sensitive\ndata. Thus, ensuring data accuracy, privacy concerns, and\nethical use of data are some of the challenges that must be\nconsidered. However, with the continuous growth of available\ndata, GPT models will become even more advanced and\ncapable of performing increasingly complex tasks [47]. The\nfuture of big data as an enabling technology for GPT models\nis promising, with the potential to revolutionize the \ufb01eld of\nNLP. As technology continues to advance, organizations must\nprioritize ethical considerations and data accuracy to fully\nharness the bene\ufb01ts of big data and GPT models.\nB. Arti\ufb01cial Intelligence\nAI refers to the simulation of intelligent behaviour in\nmachines that are programmed to learn from their experience\nto reason, understand natural language, and perceive their\nenvironment [48]. AI gives machines the ability to sense their\nsurroundings, deal with what they see, handle issues, and\n\n9\nAdd & Norm\nMulti-Head\nAttention\u00a0\nSoftmax\nLinear\u00a0\nAdd & Norm\nFeed\nForward\u00a0\nAdd & Norm\nAdd & Norm\nMulti-Head\nAttention\u00a0\nMasked Multi-\nHead\nAttention\u00a0\nInput\nEmbedding\u00a0\n+\nOutput\nEmbedding\u00a0\n+\nInputs\u00a0\nOutputs\nPositional\nEncoding\nPositional\nEncoding\nOutput\nProbabilities\u00a0\nAdd & Norm\nFeed\nForward\u00a0\nFig. 3. Transformer Architecture.\ntake action to reach a particular objective. The importance\nand capability of AI is growing all the time.\nAI enables GPT models to allow machines to comprehend\nand react to human language. There are several ways in which\nAI can continue to help improve GPT and make it more\npowerful and effective in its language generation capabilities\n[49].\nThe following are the several ways through which AI can make\nGPT models more powerful:\n1) Fine tuning\n2) Dialogue generation\n3) Natural language understanding\nGPT\u2019s model performance on particular tasks can be enhanced\nby utilizing AI approaches. For instance, it can be trained on\na large corpus of text from a particular \ufb01eld such as legal\ndocuments or medical literature to better grasp and produce\nlanguage in that \ufb01eld [4]. Considering dialogue generation,\nAI techniques such as reinforcement learning and sequence-\nto-sequence models can be used to enable GPT generate\nmore natural and engaging dialogue in conversational contexts.\nSimilarly, AI techniques such as semantic parsing and named\nentity recognition can be used to help GPT better understand\nthe meaning of language and the relationships between words\nand phrases. This can enable it generate more accurate and\ncoherent language [14].\nThe development and enhancement of GPT model language\nproduction capabilities depend heavily on AI, and GPT\u2019s\ncapabilities will continue to be growing by continuous research\nand development in AI.\nAs GPT models become more advanced, there are growing\nconcerns about the potential for them in reinforcing biases and\npropagate harmful or offensive content [50]. Some of these\nconcerns also include bias which can lead to unintended dis-\ncrimination and unfairness, lack of understanding of the con-\ntext that can lead to misunderstandings or incorrect responses,\npoor data quality can lead to inaccurate or biased models,\nethical concerns like privacy and autonomy [51]. AI models\nlike GPT require signi\ufb01cant amounts of computational power\nto train and run, which can have a signi\ufb01cant environmental\nimpact due to their high energy consumption [52].\nThough AI has a great deal of promise, it\u2019s critical to be aware\nof the underlying issues and make efforts to \ufb01x them to ensure\nthat it is utilized responsibly and morally for GPT.\nC. Cloud Computing\nCloud computing refers to the on-demand availability\nof computer resources, such as storage, processing power,\nand applications, delivered over the internet [53]. The GPT\nmodel\u2019s successes are possible not only because of algorithmic\nevolution but also increased computational capabilities i.e.\nexponential growth in hardware (computational power, storage\ncapacity), cloud computing, and related operational software\n[54]. The applications for cloud and EC working together\nsuch as natural language generation, image completion, or\nvirtual simulations from wearable sensors see that the work\nis made more compute-intensive [55].\nGPT models need a lot of computational power to analyze\na lot of data, and cloud computing offers the scalability\nrequired to cope with demand spikes. Without worrying about\nthe constraints of on-premises hardware, GPT models can\nrapidly and easily scale up or down as needed with cloud\ncomputing [56]. Cloud-based platforms like Amazon Web\nServices (AWS) or Google Cloud Platform (GCP) provide\naccess to distributed computing resources that can be used\nto train GPT. Since cloud computing provides web-based\nsolutions and thereby does not require the purchase and\nmaintenance of costly hardware, it can be a cost-effective\nchoice for a GPT model. By utilizing cloud computing, the\nGPT model can only pay for the computing resources it uses\n[57]. The other added advantage of cloud computing in GPT\nis, it gives GPT models the freedom to access computing\nresources whenever it wants, from any location in the world.\n\n10\nLayer Norm\nFeed\nForward\u00a0\nText & Position\nEmbedded\u00a0\n+\nLayer Norm\nMasked Multi\nSelf Attention\u00a0\n+\nText\nPrediction\nTask\nClassifier\nStart\u00a0\nText\u00a0\nExtract\nTransformer\u00a0\nLinear\nClassification\nStart\u00a0\nPremise\nDelim\nTransformer\u00a0\nLinear\nEntailment\nHypothesis\nExtract\nStart\u00a0\nText 1\nDelim\nTransformer\u00a0\nLinear\nSimilarity\u00a0\nText 2\nExtract\nStart\u00a0\nText 2\nDelim\nTransformer\u00a0\nText 1\nExtract\nStart\u00a0\nContext\nDelim\nAnswer 1\nExtract\nStart\u00a0\nContext\u00a0\nDelim\nAnswer 2\nExtract\nTransformer\u00a0\nLinear\nTransformer\u00a0\nLinear\nMultiple\nChoice\nStart\u00a0\nContext\u00a0\nDelim\nAnswer N\nExtract\nTransformer\u00a0\nLinear\nFig. 4. Transformer Architecture and Input Transformations for Fine-Tuning on Different Tasks.\nSupervised fine tuning (Step1)\nReward Model (Step 2)\nProximal Policy Optimization (PPO) Reinforcement Learning\u00a0 (Step 3)\nA pretrained GPT model is used,\nfine-tuned with labelers by creating\na supervised dataset.\u00a0\n1\n1.1\nThe labelers then wrote an\nappropriate response to the\ninput prompt\u2019s\u00a0\n1.2\nGPT model will be fine-tuned using new supervised dataset 1.3\nAfter the model is trained in step 1, In this step it\ngenerates optimal responses to input\u00a0\nSupervised fine tuning (SFT) model is used, inputs\nare fed to the finetuned model, several responses\nwere generated\n2.1\n2\nLabeler provides a reward for each of\nthese outcomes, ranks the outcomes\nfrom best to worst\n2.2\n2.3\nThis data is used to\ntrain a reward model\nA new prompt is sampled from the\ntrained dataset\n3.1\n3\nThe PPO model is initialized from\nthe supervised policy\n3.2\nThe policy generates the output\n3.3\nThe reward model calculates a\nreward for the output\n3.4\nIn this step unseen input sequences are passed to the clone SFT model, pass the responses\nto the reward model for understanding, how high quality was this response for that input\u00a0\nThe reward is used to update the\npolicy using PPO\n3.5\nFig. 5. How does GPT Work.\nThis makes GPT models more accessible to users by enabling\nsmooth operation across a variety of gadgets and platforms\n[58]. Cloud computing providers offer high security and\ncompliance standards, which can protect the GPT model and\nits data from online dangers. Cloud service providers also\npossess the knowledge and tools necessary to effectively\naddress security problems and stop data leaks. Cloud-based\nstorage services, such as Amazon S3 or Google Cloud\nStorage, provide scalable and reliable storage for GPT\u2019s data.\n\n11\nHUMAN\nINSTRUCTION\nGPT\nTEXT RESULTS\nData\nInput\nOutput\nUNIMODAL (CHATGPT)\nHUMAN\nINSTRUCTION\nGPT\nVISUAL\nRESULTS\nInput\nOutput\nCROSS MODAL(STABLE DIFFUSION 2.1)\n\u00a0INSTRUCTION 1\n\u00a0INSTRUCTION 2\n\u00a0INSTRUCTION 3\nPre-\ntraining\nPre-\ntraining\nPre-\ntraining\nTEXT RESULTS\nVISUAL\nRESULTS\nAUDIO RESULTS\nHuman\nInstructions\nMULTI-MODAL\nResults\nData\nData\nGPT\nInput\nOutput\nFig. 6. A comparison between unimodal, cross-modal, and multimodal modal GPTs.\nKey\nenabling\ntechnologies\nBig Data\nArtificial\nIntelligence\nCloud\nComputing\nEdge\nComputing\n5G &\nBeyond\nHCI\nMassive data for training\nImproves model accuracy\nfor training\nFine tuning\nFaster data transmission\nImproved connectivity\nLower latency\nImproved input quality\nEnhanced user experience\nEnhances the usability of\u00a0\nGPT models\nReduced latency\nCost effective\nSecurity and\ncompliance\nLow bandwidth usage\nProvides scalability for GPT\nmodels\u00a0\nCost effective option\nSecurity and compliance\u00a0\nFlexibility\nGood computational\npower\nModel Optimization\nHelps in deployment of\nvarious application\nFig. 7. Enabling technologies of GPT models.\nDespite the advantages of cloud computing where it can\nhelp GPT models to operate more ef\ufb01ciently, effectively, and\nsecurely, there are also a few technological aspects where it\ncreates a drawback for GPT [59]. To function properly, the\nGPT model needs a sizable amount of computing power and\ndata storage. These resources can be accessible online with\ncloud computing. As a result, continued operation of the GPT\nmodel requires a robust and dependable internet connection,\n\n12\nand any breakdown in connectivity may result in delays or\neven data loss. There are some security concerns when storing\nsensitive data, such as personal information or trade secrets, in\nthe cloud which can be risky if proper security measures are\nnot in place [54]. While cloud computing can be more cost-\neffective than building and maintaining an in-house computing\ninfrastructure, it can still be expensive for long-term use. It also\nsuffers issues like performance variability, limited availability\netc.,\nD. Edge Computing\nThe rapid growth of IoT, a large amount of data from\nseveral IoT devices, and also cloud services have necessitated\nthe emergence of a concept called EC. EC is an open AI\nand\ndistributed\ndesign\nwith\ndecentralized\ncomputational\npower. In EC, there is a lesser need for clients and servers to\ncommunicate over long distances, which lowers latency and\nbandwidth utilization.\nInstead of depending on centralized data centers, EC entails\nbringing computing capacity and data storage closer to the\nconsumer [60].\nIn GPT, where there is a need for real-time data analysis, EC\nplays a major role in faster processing and better ef\ufb01ciency in\nproducing good results [61]. GPT models are typically large\nand complex, requiring signi\ufb01cant processing power to run.\nBy deploying GPT models on the edge devices, closer to the\nsource of data, latency can be reduced in replying to users\nwho seek information through the GPT models by eliminating\nthe need to move data back and forth from end devices to\nthe cloud. Since EC maintains data near the periphery and\naway from centralized servers, it can offer improved security\nand more privacy protections in the case of the requests\nmade by users through GPT [62]. GPT models utilize a lot\nof data for learning and thereby the cost of data transfer\nalso increases with data volume. EC can aid in controlling\ndata transfer expenses. EC can also help in lowering the\namount of bandwidth by pre-processing the data even before\ntransferring it to the cloud. Particularly when analyzing photos\nor videos, GPT models can produce a lot of data [63]. EC\naccelerators, such as graphics processing units (GPUs) and\n\ufb01eld-programmable gate arrays (FPGAs), can be used to speed\nup GPT model inference and training. These accelerators can\nbe integrated into edge devices or edge servers, providing more\nef\ufb01cient processing of GPT models.\nEC and GPT models make a great combination. Comparative\nto cloud data centres, edge devices may have constrained\ncomputation and storage capabilities [64]. This might limit the\nscope of GPT models that can be installed on edge devices in\nterms of size and complexity. Since GPT models handle large\nand varied data, EC can also increase security risks and data\nprivacy concerns. Implementing EC in existing infrastructure\ncan be dif\ufb01cult and require signi\ufb01cant investment in hardware,\nsoftware, and networking components. This can be a barrier\nfor many organizations which are using the GPT model and\nEC [65].\nE. 5G and beyond networks\n5G networks represent the latest generation of cellular\nnetworks that promise faster data speeds, lower latency, and\nthe ability to connect a vast number of devices simultaneously\n[66]. 5G and beyond networks enable faster data transmission\nspeeds\nthan\nprevious\ngenerations\nof\ncellular\nnetworks,\nwhich can help in training and deploying larger and more\ncomplex language models. This can result in faster training\ntimes and better performance. 5G and beyond networks\ncan provide lower latency than previous generations of\ncellular networks, which can reduce the time required for\ncommunication between GPT and other devices, such as\nservers or other language models [67]. This can improve\nthe real-time response of the GPT model for applications\nthat require quick and accurate language processing. 5G and\nbeyond networks offer improved connectivity options, such\nas increased capacity and more reliable connections, which\ncan help in scaling up the deployment of the GPT model for\nlarge-scale language processing tasks. With the deployment\nof 5G and beyond networks, EC is becoming more prevalent.\nThis means that a GPT model can potentially be deployed\ncloser to the end-user, reducing the latency and improving the\nresponse time for applications that require real-time language\nprocessing [68]. Ultra-Reliable Low-Latency Communication\n(URLLC) is a key feature of 5G networks. In the context\nof GPT language models, URLLC can enable real-time and\nreliable communication between multiple devices, such as\nedge devices, cloud servers, and end-users [69].\nThough 5G and beyond technology offers potential advan-\ntages to GPT models, it is also important to note that the actual\nimpacts of this technology may change depending on how it\u2019s\nimplemented and used. 5G enables the access to uncontrolled\naccess to the Internet,it may attract cybersecurity risks and\nprivacy concerns [70]. Also, as GPT uses a large amount of\ndata for analysis it could also cause privacy concerns. 5G\nand beyond networks in GPT models need high infrastructural\nrequirements which is a costly process.\nF. Human Computer Interaction\nHCI, which is multi-faceted, concentrates on the design of\ncomputer technology and, in particular, on how people and\ncomputers communicate with each other [71].\nHCI has a greater in\ufb02uence over GPT models. As a\nlanguage model, GPT is designed to interact with humans\nby generating natural language responses to input text. HCI\nresearch can help designers create more effective input\nmechanisms for the GPT model, such as natural language\ninterfaces, that allow users to communicate more easily and\naccurately with the model [72]. HCI also helps in enhancing\nthe GPT model\u2019s user experience by creating interfaces that\nare more intuitive and user-friendly. This makes it easy for\nthe users to interact with GPT models and understand their\nresponses [73]. HCI also estimates the performance of GPT\nmodels by evaluating their responses with real-time users\nand identi\ufb01es the areas where the model needs improvement,\n\n13\nthereby improving its reliability and accuracy. HCI enhances\nthe usability of GPT models by reducing the time and effort\nrequired for the users to interact with [74].\nWhile HCI can be incredibly helpful in improving the de-\nsign and usability of GPT models, there are also some potential\ndrawbacks to consider. If the research is not conducted with\na diverse group of users, HCI can introduce biases into the\ndesign of the GPT model. HCI techniques can be expensive\nand time-consuming. As GPT models become more complex,\nit may become more dif\ufb01cult to design interfaces and input\nmechanisms that are both effective and user-friendly [75]. HCI\nmay not always be able to provide the necessary insights or\nfeedback to drive improvements in GPT models. There are\nalso ethical concerns around the use of GPT models, including\nissues related to privacy, bias, and the potential misuse of\nthe technology [76]. As GPT models become more complex,\nit may become more dif\ufb01cult to design interfaces and input\nmechanisms that are both effective and user-friendly.\nIV. IMPACT OF GPT MODELS ON VARIOUS APPLICATIONS\nGPTs have made signi\ufb01cant progress, and its impact is\nbeing felt across various industries like education, healthcare,\nindustry, agriculture, travel and transport, e-commerce, en-\ntertainment, lifestyle, gaming, marketing, and \ufb01nance. This\nsection provides valuable insights on the impact of the GPT\nmodels in the aforementioned applications as depicted in Fig.\n8.\nA. Education\n1) Introduction: Education has been around for centuries,\nwith traditional education being the most common form.\nTraditional education involves a teacher imparting knowledge\nto a group of students in a physical classroom. While suc-\ncessful, traditional education can be restrictive and in\ufb02exible,\nlimiting students\u2019 ability to learn at their own pace and in\ntheir preferred style. It can also be limited by geography, as\nstudents need to be physically present in a classroom to learn.\nTechnology has emerged as a solution to some of these issues,\nallowing for personalized learning experiences and more en-\ngaging, accessible resources. Online learning platforms, digital\ntextbooks, and multimedia tools offer students access to a vast\narray of resources from anywhere in the world. Technology\ncan also facilitate collaboration and communication among\nstudents and teachers, leading to a more dynamic and inter-\nactive learning experience. Distance learning, hybrid learning\nmodels, and online classes are examples of how technology\ncan help break down the barriers of traditional education,\nmaking learning more \ufb02exible, ef\ufb01cient, and effective. By\nintegrating technology into traditional education, we can create\na more personalized and effective learning experience, bene-\n\ufb01ting students worldwide.\n2) Impact of GPT in Education: The \ufb01eld of education is\nconstantly evolving, with advancements in technology playing\na signi\ufb01cant role in shaping the way we learn and teach.\nOne such technology that has the potential to transform the\neducation industry is GPT. As a large language model trained\non a vast amount of data, GPT can generate human-like text\nthat is coherent and informative, making it a valuable tool\nin developing educational content such as textbooks, study\nguides, and course materials. Furthermore, GPT can be used to\nanalyze and summarize complex text, which can help educa-\ntors and students save time and increase comprehension. With\nits ability to support NLP applications and create intelligent\ntutoring systems, GPT has the potential to revolutionize the\nway we learn and teach. In this context, following section\nwill explore the different ways in which GPT can contribute\nto the education industry and transform the future of learning.\n\u2022 Intelligent Tutoring: Intelligent tutoring is a teaching ap-\nproach that uses AI and ML to provide personalized and\nadaptive instruction. It analyzes student performance data,\nunderstands their strengths and weaknesses, and gener-\nates customized learning paths. It provides immediate\nfeedback, personalized guidance, and remedial support.\nIt is effective in improving learning outcomes, increasing\nstudent engagement, and reducing learning time. With\nadvanced natural language processing capabilities, GPT\ncan enhance the personalized and adaptive instruction\nprovided by intelligent tutoring systems. It can analyze\nnatural language input from students, enabling intelligent\ntutoring systems to better understand and respond to\ntheir queries, needs, and preferences. It can also gen-\nerate personalized feedback and assessment based on\nthe individual learning progress of each student, helping\nthem to identify and address their knowledge gaps and\nimprove their performance. GPT can also analyze student\nperformance data and generate adaptive learning paths\nthat provide customized instruction and remediation, en-\nsuring that each student learns at their own pace and\nachieves their learning objectives. Additionally, it can\ncreate interactive dialogue systems that simulate natural\nconversations between students and virtual tutors, making\nlearning more engaging, interactive, and personalized\n[77]. The authors in [76] have identi\ufb01ed that GPT-4 model\noutperforms general-purpose GPT-3.5 model as well as\nGPTs (Med-PaLM, a prompt-tuned version of Flan-PaLM\n540B) specailly trained on medical data. The authors\nhave tested GPT-4 models\u2019 ability to explain medical\nreasoning, personalize explanations to students, and in-\nteractively craft new counterfactual scenarios around a\nmedical case.\n\u2022 Learning assistance and material development: Learning\nmaterials are critical in education as they provide a\nstructured way for students to acquire knowledge and\nskills. They can be tailored to meet the needs of diverse\nlearners and make learning more engaging and effective,\nsupporting teachers to create a more dynamic and interac-\ntive learning environment. GPT can contribute to creating\nlearning materials by automating content generation, pro-\nviding multilingual content creation, language correction,\npersonalized content creation, conducting topic research,\nand generating assessments. It saves time and effort for\neducators and publishers, improves the accuracy and\nreadability of material, and makes learning more engag-\n\n14\nEducation\nHealthcare\nAgriculture\nLifestyle\nMarketing\nGaming\nEntertainment\nFinance\nIndustry\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0Travel & Transport\nE-Commerce\nContent Creation\nAutomated Assessment\nWriting Assistance\nIntelligent Tutoring\nDrug Discovery\nPatient Diagnosis\nDiesease Prediction\nPersonalized Medicine\nImproving Crop Yield\nPest Control Assistance\nIdentifying Diseases, Data\nAnalysis and Prediction\nDiet Planner\nTravel Guide and Trip Advisor\nPersonalized Cook Book\nHobby Curator\nContent Creation\nCustomer Service\nPersonalized Advertising\nForecast Analysis\nChatbot Development\nGame Content Creation\nNon Playable Character\nSolitude\u00a0\nEnhanced Customer Interaction\nPersonalized Content Creation\nSentiment Analysis\nFinancial Forecasting\nTrading Strategies\nRisk prediction and Management\nSustainability\nCustomer Service\nAutomated Assitance\nLogistical Management\nIntelligent Fleet Management and\nTracking\nReal-Time Inventory Trackiing\u00a0\nProof Reading\nOrder Processing\nData Analysis\nEnabling Technologies\nApplications\nBig Data\nArtificial\nIntelligence\nCloud\nComputing\nEdge Computing\n5G &\nBeyond\nHuman Computer\nInteraction\nBenefits of Integrating GPT\nFig. 8. The impact of GPT models on various applications.\ning and effective. GPT can generate high-quality content\nsuch as summaries, quizzes, and lesson plans based on\nspeci\ufb01c learning objectives, making learning accessible\nto a wider audience. It can analyze written content and\nprovide suggestions to improve grammar, punctuation,\nand readability. GPT can also assist in research writing by\nsuggesting ideas for structure, rephrasing and organizing\ncontent, and identifying gaps in research [78]. Moreover,\nGPT can also provide personalized feedback based on in-\ndividual learning progress, enhancing the development of\nmore comprehensive and informative learning materials.\n\u2022 Automated Assessments: Automated assessment in ed-\nucation uses technology to evaluate students\u2019 learning\noutcomes, providing immediate feedback and reducing\npotential bias in grading. It can also help teachers iden-\ntify areas where students may need additional support,\nenabling them to tailor their teaching methods to better\nmeet individual needs. GPT with its advanced natural\nlanguage processing skills, can help in automated as-\nsessment by analyzing and grading student responses\nto various types of assessment questions, including es-\nsays and short answer questions. It can also provide\nfeedback to students [79], such as highlighting areas\nfor improvement and suggesting further reading or re-\nsources. GPT\u2019s natural language processing capabilities\ncan help to identify the meaning and context of students\u2019\nresponses, making automated assessment more accurate\nand effective. Additionally, GPT can generate personal-\n\n15\nized learning materials and exercises based on students\u2019\nassessment results, supporting educators to create more\ntailored and effective learning experiences. The authors\nin [80] have used Chat GPT in evaluating the students\u2019\nassignments such as quiz style questions, and also in\ngenerating relevant practice problems to improve content\nretention and understanding. The results were promising\nin the classroom. The authors believe that Chat GPT has\nthe signi\ufb01cant ability in reducing the load of instructor\nwithout compromising students\u2019 learning outcomes.\n\u2022 Fostering Creativity: Creativity thinking plays a vital role\nin education by encouraging students to think beyond\ntraditional boundaries and develop innovative solutions to\ncomplex problems. It helps students to approach learning\nwith an open mind and a willingness to explore new ideas,\nleading to greater engagement and motivation. GPT\u2019s\nability to generate human-like responses and creative\nwriting can aid in improving creativity. It can help\nimprove creativity by generating new and innovative\nideas based on vast amounts of data and information.\nBy analyzing patterns in language and identifying con-\nnections between different concepts, GPT can suggest\nnovel approaches to teaching and learning. Additionally,\nGPT can also generate creative prompts or challenges for\nstudents, encouraging them to think outside the box and\napproach problems in unique ways [81]. GPT can also\nanalyze and evaluate students\u2019 creative work, providing\nfeedback and suggestions for improvement. So, GPT can\nbe a valuable tool for promoting and enhancing creativity\namong students and faculty members.\n3) Challenges: There are several advantages to incorpo-\nrating GPTs in education, but it is essential to acknowledge\nthe potential limitations. While GPTs can quickly generate\ninformation, they may impede students\u2019 critical thinking and\nproblem-solving skills. Furthermore, learners who bene\ufb01t from\npersonal interaction with instructors may \ufb01nd the lack of\nhuman involvement disadvantageous. GPTs rely on statistical\npatterns, so they cannot provide a comprehensive understand-\ning of the material being taught [79]. Privacy concerns arise\nwhen using sensitive student data in GPTs for educational\npurposes. Additionally, since GPTs cannot provide citations, it\nis challenging to identify the source of information generated.\nThe cost of maintaining GPT may be prohibitive for schools\nand educational institutions with limited resources. Finally,\ndistinguishing between reliable and unreliable information\ngenerated by GPTs can be dif\ufb01cult, so it is necessary to have\nhuman oversight to ensure data accuracy and regulate access.\n4) Summary: GPT offers numerous advantages in the edu-\ncation sector, including personalized and adaptive instruction,\nautomated assessment, creative writing support, and research\nwriting assistance. They have the potential to revolutionize\nteaching by creating lesson plans and activities, responding\nto natural language queries, and integrating multiple digital\napplications. However, there are also challenges to consider,\nsuch as the potential negative impact on critical thinking and\nproblem-solving skills, lack of human interaction, data security\nand privacy concerns, inability to provide full comprehension,\nlack of citations or sources, high cost of maintenance, and po-\ntential for producing unreliable information. Further research\nis needed to explore human-computer interaction and user\ninterface design to integrate GPT into educational work\ufb02ows\nwhile ensuring that the information they provide is accurate\nand reliable.\nB. Healthcare\n1) Introduction:\nBefore technology became widespread\nin healthcare, healthcare services were primarily delivered\nthrough face-to-face interactions between healthcare profes-\nsionals and patients. Traditional healthcare faced several chal-\nlenges, including limited medical instruments, paper-based\nhealth records, patients receiving care mostly in hospitals\nor clinics, physical travel requirements to receive medical\nattention, and limited medical research. Despite these chal-\nlenges, traditional healthcare still provided valuable medi-\ncal services to patients. However, with the introduction of\ntechnology, healthcare has become more ef\ufb01cient, accessible,\nand personalized, resulting in improved patient outcomes and\nbetter overall healthcare services. Technology has become\nan essential aspect of society, as re\ufb02ected in the signi\ufb01cant\ninvestments made in this sector. Despite the advancements\nin technology, the healthcare industry still faces various new\nchallenges, including access to healthcare, high costs, per-\nsonalized medicine, data privacy and security concerns, and\nan aging population. However, technology has the potential\nto address these challenges and improve the ef\ufb01ciency of\nhealthcare services.\n2) Impact of GPT in healthcare: Recent years have seen\nsigni\ufb01cant advancements in technology, including in the\nhealthcare industry. Biotechnology, medical devices, and phar-\nmaceuticals have undergone transformations through the use\nof cutting-edge technologies like DL [82] and ML [83].\nCurrently, the healthcare sector is utilizing various forms of\nAI techniques for medical research and providing medical\nservices. One such technique is the GPT features of NLP,\nwhich hold immense potential for the healthcare industry.\nGPT can help to overcome several challenges in healthcare\nin multiple ways. For instance, it can be used to develop\nintelligent systems that assist doctors in making accurate\ndiagnoses and providing clinical assistance [84] [85]. GPT\ncan also analyze large volumes of medical data and generate\nreports. Furthermore, it has potential applications in drug\ndiscovery [86] [87], personalized medicine, patient diagnosis,\nmedical image analysis, analyzing electronic health records,\nclinical decision support systems, and disease prediction.\n\u2022 Drug Discovery: Recent AI and machine learning tech-\nniques [88] [89] are having the potential to contribute\nto the growth and development of drug discovery. GPTs\nare capable of learning new patterns and relationships\n[90] in the dataset they were trained on. This capability\ncan be used in drug discovery to aid in the identi\ufb01cation\nand design of potential new drugs with desired properties\n[91]. One of the key challenges in drug discovery is\n\ufb01nding compounds that can interact with speci\ufb01c parts\nof the body. GPT can help in this process by learning\nthe patterns and relationships from large databases of\n\n16\nknown compounds [86]. GPT can be trained on large\nsets of chemical databases to analyze chemical reactions\nand their outcomes. This can help suggest potential\ncombinations of new drugs using the analyzed data. These\nnew drugs can also be analyzed using GPT to test their\nef\ufb01cacy and toxicity.\n\u2022 Diagnosis: GPT can be used in medical diagnosis by ana-\nlyzing patient data. It can help to analyze medical records\nand extract information such as patient demographics,\nsymptoms, and medical history. This can help medical\nprofessionals provide effective patient care and improve\noutcomes. The recent release of GPT-4 has the ability to\nsupport multimodal information, allowing it to analyze\nimages as input and produce text results as output [92].\nIt is recommended to use AI systems such as a GPT,\nas clinical support tools to assist medical professionals\nin diagnosing and treating patients, but they should not\nbe relied upon as the sole source of medical advice or\ndecision-making. GPT can also be used to identify rare\ndiseases by analyzing patient\u2019s complete information. The\nauthors in [93] have used a general-purpose GPT based\non GPT-3 model for patient diagnosis and triage. The\nmodel has given a triage accuracy of 70% which was\nworse than a physician. But, in next subsequent weeks,\nthe accuracy has improved to 92% which is close to the\nperformance of a physician. In diagnosis, GPT-3 model\nhas given 88% accuracy. For emergency cases, GPT-3 has\ngiven 75% accuracy whereas physician has given 94%.\n\u2022 Disease prediction: GPT has great potential in disease\nprediction [94]. By analyzing large amounts of medical\ndata, including patient records, medical images, and clin-\nical trials, these pre-trained language models can learn\npatterns and make predictions about the likelihood of\na patient developing a particular disease. For instance,\ntrained healthcare GPTs can be used to predict the\noccurrence of diseases such as diabetes, heart disease,\nand cancer by analyzing various parameters, including\nthe patient\u2019s medical history, age, family history, and\nlifestyle. It can also be used to predict the likelihood of\na rare disease This helps in the early detection of high-\nrisk patients so that medical personnel can take necessary\nmeasures and suitable medicines to reduce the risk of\ndeveloping the disease. The medical practitioner and\nauthor in [95] have recommended using GPT-4 models\u2019\nability of NLP in bariatric surgery.\n\u2022 Personalized medicine: The COVID-19 pandemic has\nhighlighted that not all body systems are clinically sim-\nilar. For instance, during the pandemic, medicines like\nRemdesivir and Tocilizumab have been effective for one\ncategory of patients but do not affect another category\nof patients with similar clinical metrics, as they progress\nfrom a mild or moderate level of infection to a severe\nstage [96]. This highlights the need for personalized\nmedicine in today\u2019s world. GPT can be used to identify\nvariable patterns of data to predict or classify hidden\nor unseen patterns, which can be used for exploratory\ndata analysis. GPT provide the possibility of identify-\ning personalized medicines [97] based on the clinical,\ngenomic, and nutritional data of patients. The dietician\nand the author in [98] have observed that the utilization\nof Chat GPTs has signi\ufb01cantly decreased obesity rates\namong patients by offering personalized recommenda-\ntions regarding nutrition plans, exercise programs, and\npsychological support. This approach allows for the de-\nvelopment of customized treatment plans that cater to the\nspeci\ufb01c needs of individuals, leading to a more ef\ufb01cient\nmethod of treating obesity with the assistance of Chat\nGPT.\n3) Challenges: While GPT is a powerful language model\nwith numerous applications in healthcare, it is not without\nits challenges. The primary challenge is data bias. As GPT\nmodels are also learning models, the signi\ufb01cant drawback of\nbiasing is also applicable to GPT. GPT can be susceptible\nto bias. If the data used to train the model is biased, the\nmodel will learn from it and replicate the bias. This leads to\nincorrect treatment and predictions. Another challenge is the\ntransparency of the model. GPT is complex to understand and\ninterpret. This lack of transparency in technology can make\ndoctors and medical personnel not believe in the predictions,\nwhich may result in a hesitancy to trust and adopt technology\n[99]. Another important concern is security and privacy issues.\nAs it is a model to be trained on data, there is a huge\namount of sensitive information about the patients to be used\nto improve the algorithm and its performance. This results in\nsigni\ufb01cant security and privacy concerns related to the use of\nGPT in healthcare. The \ufb01nal and important challenge is limited\nclinical validation. GPT are showing promising improvement\nin various \ufb01elds of healthcare, such as drug discovery, and\ndisease prediction. But still, their effectiveness and accuracy in\nmedical research and clinical settings have yet to be validated.\nMore research and clinical trials are required to prove that GPT\ncan transform the medical industry with full trust.\n4) Summary: GPT have the potential to revolutionize the\nhealthcare industry by contributing to drug discovery, per-\nsonalized medicine, clinical support in making decisions,\ndiagnosis support, and disease prediction. This can be helpful\nfor human beings to predict the disease in advance and treat\nit through proper medicine. However, there are signi\ufb01cant\nchallenges that are to be addressed, such as technology adop-\ntion, data bias, regulatory challenges, and security and privacy\nissues. It is so important to analyze and evaluate the bene\ufb01ts\nand risks of using GPT in healthcare and to continue to\nmonitor their development and implementation.\nC. Industry\n1) Introduction: An important economic transition from\nagriculture and handicrafts to large-scale industry and auto-\nmated production was achieved by the industrial revolution.\nEf\ufb01ciency and productivity were raised as a result of modern\nequipment, energy sources, and labour arrangements. New\nopportunities and challenges have been created as a result\nof the quick development of new technologies in both the\nworkplace and other industries [100]. The utilization of big\ndata is a well-known technology-driven trend. Nowadays,\ncompanies have access to enormous volumes of data that\n\n17\nmay be examined to uncover insightful information. Big data\ncan help businesses make wise decisions and discover areas\nfor development. AI is another innovation that is changing\nindustries. AI systems have the ability to analyse complex\ndata, automate procedures, and make wise conclusions [101].\nThis improves production by increasing its dependability,\nadaptability, and ef\ufb01ciency. The process of \u201ddigitalization,\u201d\nwhich includes incorporating digital technologies into every\nelement of business, is creating industries to become more\n\ufb02exible, ef\ufb01cient, and valuable. Businesses may automate\ntedious work, improve client experiences, and streamline oper-\nations by implementing digital solutions. In today\u2019s digitally-\ndriven world, adopting technological advancements is essential\nfor maintaining competitiveness and promoting growth.\n2) Impact of GPT in Industry: In industrial scenarios, GPT\nhas the potential to be applied as a sustainability tool, assisting\nbusinesses in evaluating and enhancing their sustainability\ngoals. Companies can improve supply chain tracking and\nquery response by integrating pre-trained transformer models\nlike ChatGPT with supply chain management platforms [102].\nAdditionally, GPTs can offer modi\ufb01cations to the production\nprocess that might increase ef\ufb01ciency [103]. GPT can also\nhelp users make knowledgeable decisions about how to use\nresources, allowing businesses to remain competitive while\nreducing their environmental effect. For example, the GPT-\n2 model has demonstrated ef\ufb01cacy in sentiment analysis,\nproviding insightful data for numerous applications [104].\n\u2022 Hospitality sector In the hospitality industry, hotels place\na high focus on providing satisfying guest experiences.\nTo ensure that every tourist is satis\ufb01ed during their\nstay, this necessitates adapting to their requirements and\npreferences. Hotels may improve the guest experience\nin a number of ways by integrating GPT into their\nwebsite or mobile application. Hotels may respond to\nconsumer inquiries in a timely and precise manner by\nutilizing GPT [105]. Customers do not have to wait for\nhuman assistance when looking up information about\nfacilities, booking procedures, or room availability. Cus-\ntomers\u2019 overall satisfaction with the hotel\u2019s services is\nincreased as a result of the large reduction in client wait\ntimes. GPTs can also make it easier for visitors who\nspeak multiple languages to communicate [106]. Hotels\ncan offer a more inclusive and welcoming experience\nfor visitors from other countries by removing linguistic\nobstacles. Hotels may provide their visitors with immer-\nsive and engaging experiences by combining GPT with\nAR technologies. For instance, customers can use their\nmobile devices to get AR guided tours of the hotel or\nnearby attractions, offering a distinctive and entertaining\nway to explore the surroundings and learn more about the\nhotel\u2019s amenities.GPTs integration into various aspects of\nthe hospitality industry gives hotels the ability to deliver\nstreamlined, tailored, and effective services, increasing\nclient happiness and loyalty.\n\u2022 Fashion: By providing highly customized user recommen-\ndations based on personal style, brand preferences, and\nparticular clothing or accessory demands, collaborative\n\ufb01ltering and AI algorithms have undoubtedly revolution-\nized the fashion business. The amount of personaliza-\ntion has been further increased in this context by the\nincorporation of GPT, dramatically altering the purchas-\ning experience for customers [107]. Fashion platforms\nmay analyse a signi\ufb01cant quantity of user data, such as\nbrowsing history, purchasing behaviour, and style prefer-\nences, using the advanced capabilities of GPT to produce\ntailored recommendations. Fashion platforms can direct\nconsumers towards clothing options that \ufb01t their desire\nfor sustainable fashion by including eco-friendly fabric\nselections into the system. GPT improve users\u2019 general\nfashion knowledge and con\ufb01dence while enabling users\nto keep up with the most recent trends. The image-text\nretrieval skills of GPT signi\ufb01cantly improve visual search\ncapability in fashion platforms [108]. Users may make\nmore con\ufb01dent shopping decisions and minimize the need\nfor returns by visualizing how various clothing items\nand accessories would appear on them without physically\ntrying them on. The model may recommend the proper\nsize for various brands and apparel products by taking\ninto account a user\u2019s measurements, preferred \ufb01t styles,\nand historical data. The overall purchasing experience is\nenhanced and the frustration of wrong size is decreased.\n\u2022 Sustainability: Sustainable development means address-\ning current demands without sacri\ufb01cing the capacity of\nfuture generations to address their own needs. Goals for\nsustainable development can be attained by implementing\nGPTs in a variety of sectors, including manufacturing\nand corporate operations [109]. The models can estimate\nwhere energy saving measures would be most useful\nby analyzing past data and patterns to provide insights\ninto energy usage, pinpoint problem areas, and recom-\nmend opportunities for improvement. GPTs can aid in\nidentifying sustainability-related problems, creating plans\nand strategies to solve them, investigating brand-new\nsustainable activities, keeping track of advancements,\nand conducting routine reviews. Companies can choose\nactivities that will have the biggest positive impact by\ngrading tasks and actions according to their impact on\nsustainability [110]. The models can optimize supply\nchains for decreased carbon emissions, minimized waste,\nand improved resource ef\ufb01ciency by assessing elements\nincluding transportation routes, packaging materials, and\nsupplier practises [111]. This results in more environ-\nmentally friendly production, distribution, and sourcing\nprocedures.\n3) Challenges: There are many different industrial \ufb01elds\nwhere GPT models can be applied; the three areas mentioned\nabove are only a few. However, for optimal use, the industrial\nsector needs to be ready to adapt to a constantly changing\nenvironment. Public and corporate policies must be developed\nover the long term to promote the use of sustainable production\ntechniques. For enterprises, deploying pre-trained GPT models\ncan be a costly task. Continuous development and training are\nalso required to accommodate new and evolving inquiries as\nclient expectations change. Companies have to carefully assess\nthe bene\ufb01ts and costs before implementing the GPT model\n\n18\nbecause these continuing efforts raise the deployment cost\n[112]. For industries to fully bene\ufb01t from GPT models, it is\ncrucial to address issues with interpretability, data reliance, and\nethical considerations. Industry may therefore take advantage\nof these GPT models\u2019 advantages, make wise decisions, and\npromote sustainable development.\n4) Summary: GPTs have the ability to have a positive\nimpact on society and business operations. They can speed\nup operations like accounting, sales, and marketing, increasing\nproductivity. But before they are widely used, ethical problems\nneed to be fully investigated. Technology products will change\nas GPT models develop. To reap the bene\ufb01ts and reduce dan-\ngers, it is essential to solve interpretability and data concerns.\nGPTs can have a tremendous positive impact on businesses,\nsociety, and the economy when they are used responsibly.\nD. Agriculture\n1) Introduction:\nTraditional agriculture, a time-honored\npractice passed down through generations, sustains civiliza-\ntions with its crop cultivation and livestock rearing methods.\nRooted in a deep connection to nature, it emphasizes sustain-\nability and local ecosystem understanding. Beyond providing\nsustenance and livelihoods, traditional agriculture preserves\ncultural heritage. However, it also faces challenges such as\nlabor-intensive processes and shortages, inef\ufb01cient resource\nutilization, vulnerability to pests and diseases, and limited\naccess to real-time data and environmental impact. Today, by\nmerging tradition with modernity, we have the opportunity to\nleverage technological advancements to enhance productivity,\nsustainability, and resilience while honoring the profound\nlegacy of traditional agriculture for future generations.\n2) Impact of GPT in Agriculture : GPTs have the ability\nto overcome the challenges of agriculture. It offers valuable\nadvantages to the agriculture sector. It acts as a comprehensive\nknowledge source, providing information on crop cultivation,\npest management, and soil health. By analyzing real-time data,\nGPT assists farmers in making informed decisions regarding\noptimal planting times and resource allocation. It plays a\ncrucial role in identifying and addressing crop diseases and\npests accurately. Moreover, GPT enables precision farming\npractices by utilizing sensor data and satellite imagery, en-\nsuring precise irrigation, fertilization, and pest control. Ad-\nditionally, it provides market analysis and price prediction,\nempowering farmers to navigate market conditions and opti-\nmize pricing strategies. GPT also supports farm management\nand planning, optimizing crop rotation and resource usage.\nBy facilitating agricultural research and innovation, GPT con-\ntributes to advancements in crop breeding and sustainable\npractices. Embracing GPT in agriculture enhances decision-\nmaking, ef\ufb01ciency, and sustainability, ultimately promoting\nimproved productivity and food security. For instance, GPT-4\ncan educate farmers about new methods and goods and warn\nthem of potential issues or possibilities by analyzing data from\nmany sources [113].\n\u2022 Improving Crop Yields:\nWith its data analysis capabilities and real-time rec-\nommendations, GPTs plays a crucial role in enhancing\ncrop yields. By examining historical yield data, weather\npatterns, soil conditions, and crop management practices,\nGPT identi\ufb01es valuable patterns and correlations, pro-\nviding insights and suggestions for optimal crop man-\nagement techniques [114]. It enables precision farming\nby integrating data from sensors, satellites, and IoT\ndevices, granting timely guidance on resource allocation\nfor improved ef\ufb01ciency. Additionally, GPT aids in the\nearly identi\ufb01cation and management of crop diseases and\npests, minimizing yield losses through precise and prompt\nrecommendations. Moreover, GPT supports crop breeding\nand genetic optimization by analyzing genetic data and\nplant characteristics, expediting the development of high-\nyielding and resilient crop varieties. Therefore, GPTs data\nanalysis and decision support capabilities signi\ufb01cantly\ncontribute to enhancing crop yields and maximizing agri-\ncultural productivity [115].\n\u2022 Pest Control:\nGPT offers signi\ufb01cant support in the realm of pest control\nin agriculture. By analyzing extensive data on pests,\nincluding their behavior, life cycles, and characteristics,\nGPT can provide valuable insights for effective control\nmeasures. It aids in early pest detection by analyzing\nsensor data and satellite imagery, enabling proactive\ninterventions to prevent pest spread and minimize damage\n[116]. GPT also assists in determining suitable pest\ncontrol methods tailored to speci\ufb01c crops and pests,\nconsidering factors like environmental impact and sus-\ntainability. Additionally, it contributes to precision pest\ncontrol by leveraging real-time data to optimize timing\nand dosage of interventions, reducing chemical usage\nand resistance risks. It also aids in identifying natural\nenemies and bene\ufb01cial organisms, promoting natural pest\ncontrol mechanisms such as habitat diversi\ufb01cation and\ncompanion planting. Through GPT\u2019s data analysis and\nrecommendation capabilities, it empowers farmers with\ninformed decisions, leading to more effective and sus-\ntainable pest management strategies, ultimately reducing\ncrop losses and enhancing agricultural productivity.\n\u2022 Identifying Diseases and Soil analysis:\nGPTs offer valuable assistance in disease identi\ufb01cation\nand soil analysis within the \ufb01eld of agriculture. With\nits ability to analyze extensive data sets, GPT can ac-\ncurately identify crop diseases by processing information\nsuch as symptoms, historical data, and disease patterns.\nThis enables timely and effective disease management\nstrategies [114]. Additionally, It plays a signi\ufb01cant role\nin soil analysis by analyzing diverse soil-related data,\nincluding nutrient levels, pH, organic matter content, and\nsoil composition. By interpreting this data, It provides\ninsights into soil health and fertility, empowering farmers\nto make informed decisions regarding nutrient manage-\nment, soil amendments, and cultivation practices. More-\nover, GPT can identify complex interactions between soil\nconditions and crop diseases, helping farmers understand\nthe relationship and take preventive measures accord-\ningly. It also supports precision agriculture practices by\nintegrating sensor data and satellite imagery to assess\n\n19\nsoil variations across \ufb01elds, allowing for site-speci\ufb01c\nmanagement strategies and optimized resource allocation.\nFurthermore, it also facilitates knowledge sharing and\ncollaboration by analyzing and disseminating research\n\ufb01ndings, best practices, and disease outbreak information\namong agricultural communities. This collective intelli-\ngence enhances disease monitoring and control efforts on\na broader scale.\n3) Challenges: While GPT, provides signi\ufb01cant bene\ufb01ts\nto agriculture, there are challenges to its implementation.\nGPT\u2019s effectiveness depends on the availability and quality\nof data, making insuf\ufb01cient or biased data a limitation. The\ninterpretability of GPT\u2019s decision-making process is challeng-\ning due to its black-box nature, hindering trust and under-\nstanding. GPT\u2019s computational requirements and infrastructure\ncan be demanding, posing dif\ufb01culties for resource-constrained\nfarmers. Language and domain-speci\ufb01c nuances can affect\nits performance, impacting accuracy and relevance. Ethical\nconsiderations surrounding data privacy and ownership need\ncareful attention to ensure responsible use. By addressing\nthese challenges, researchers and practitioners can unlock\nGPT\u2019s potential while ensuring its practicality and ethical\nimplementation in agriculture.\n4) Summary: GPT holds immense potential in agriculture,\noffering numerous bene\ufb01ts alongside notable challenges. Its\ndata analysis capabilities empower farmers with informed\ndecision-making in disease identi\ufb01cation, soil analysis, and\nprecision farming, leading to improved crop yields and sus-\ntainable practices. However, the effectiveness of GPT relies\non data availability and quality, while its interpretability re-\nmains a challenge due to its black-box nature. Additionally,\ncomputational requirements, language nuances, and ethical\nconsiderations require careful attention. By addressing these\nchallenges, the agricultural sector can harness the full potential\nof GPT, paving the way for more productive, ef\ufb01cient, and\nresponsible farming practices.\nE. Travel and Transport\n1) Introduction: Historically, animals have been used by\npeople as their main source of transportation. But as the\nworld\u2019s population increased, the demand for more effec-\ntive transportation systems increased. Transportation-related\ntechnological advancements have fundamentally changed the\nsector in several ways. Business operations like order tracking,\nfreight management, and customer support can be streamlined\nby automation employing AI-driven technologies. Companies\ncan enable their employees to concentrate on more bene\ufb01-\ncial and pro\ufb01table duties by automating these tasks [117].\nWith better transportation networks and logistics management\nsystems that optimize routes and reduce transit times, tech-\nnological developments also enable speedier delivery times.\nIn terms of product development, technical advancement has\npaved the way for the development of innovative vehicles,\ninfrastructure, and logistics systems, leading to the production\nof more sophisticated and effective transportation choices.\nAnother noteworthy bene\ufb01t of technology advancement in\nlogistics and transportation is increased customer service.\nInquiries and problems can be handled quickly and ef\ufb01ciently\nby chatbots and customer support systems powered by AI,\nimproving the entire customer experience [14].\n2) Impact of GPT in Travel and Transport : Companies\ncan learn about customer preferences in real time by us-\ning GPTs in logistics and transportation, which results in\nbetter personalization and more customer satisfaction. GPTs\nleverage NLP approaches to interpret customer requirements\nand preferences, enabling customized suggestions as well as\nguidance in the logistics and transportation processes. The\nmost effective routes and forms of transportation can be\nrecommended using GPTs, which can analyse a large amount\nof data, including traf\ufb01c patterns, weather conditions, and\ndelivery requirements [118]. In addition, GPTs can be used as\ntravel planners, allowing visitors to enter their travel budget,\nduration, and destination to create customized itineraries. For\ntravel agencies, this personalized approach increases consumer\nsatisfaction and revenue.\n\u2022 Logistical Management: GPTs can be quite important\nin the context of shipping logistics. They can automate\nthe creation of shipping labels, eliminating up manual\nentry and lowering the possibility of mistakes. Addi-\ntionally, GPTs can have access to real-time tracking\ndata and can integrate GPS data and sensors to provide\nbusinesses and customers with precise and up-to-date\nshipment status information. Companies can successfully\nmonitor shipments with the use of GPTs, geographic\ninformation systems (GIS), and routing algorithms [14].\nOrganizations can track shipments in real-time and ensure\nvisibility throughout the supply chain by utilizing GPS\ndata and sensor technology [119]. Customers can receive\nprecise updates on their shipments using this real-time\ninformation, which will improve their experience overall.\nOverall, the use of GPTs into shipping logistics results in\nincreased automation, ef\ufb01ciency, and client satisfaction.\n\u2022 Intelligent Fleet Management and Tracking: Companies\ncan get real-time \ufb02eet updates by utilizing GPT models,\nwhich enables them to track vehicles quickly and pre-\ncisely. GPT models\u2019 underlying technology also supports\nproactive \ufb02eet management. GPTs can identify possible\nproblems or maintenance needs before they develop into\nexpensive breakdowns or accidents by analyzing data\nfrom a variety of sources [120]. With this knowledge,\ncompanies may take preventative measures, such as plan-\nning maintenance or quickly \ufb01xing developing problems,\nultimately saving time and money by preventing unin-\ntended delays. Additionally, GPTs can provide clever\nalerts and noti\ufb01cations. Businesses can receive alerts\nwhen vehicles arrive at speci\ufb01ed areas by setting up\nspeci\ufb01c triggers, which enables better coordination and\ncustomer service [121]. For instance, businesses can alert\nclients or storage facilities in advance of a truck\u2019s arrival,\nallowing for effective unloading and loading procedures.\n\u2022 Real-Time Inventory Tracking: GPTs enable businesses\nto manage their inventory levels while on the road\nwith a cloud-based platform that makes it simple to\naccess inventory data from anywhere in the world. Better\ninventory management and decision-making are made\n\n20\npossible by this real-time accessibility. This ensures that\nthe appropriate quantity of stock is accessible when\nneeded to ful\ufb01l consumer requests, while minimizing\ncarrying costs and preventing lost sales as a result of\nstockouts. GPTs can streamline inventory management\nprocedures by eliminating the need for human data entry\ninto spreadsheets, saving time and cutting overhead costs\n[117]. With the advent of 5G technology, the cost of\nconnected devices has dramatically decreased, making it\nmore practical and affordable for businesses to set up and\noperate connected inventory monitoring systems. This\nmay make real-time inventory tracking solutions more\nwidely adopted, thereby increasing the effectiveness and\nprecision of inventory management [122].\n\u2022 Streamlining Delivery Operations: GPTs are able to es-\ntimate traf\ufb01c trends and improve routes for both drivers\nand passengers using real-time data [14]. These models\ncan produce effective routes that reduce travel times and\nenhance overall delivery performance by taking into ac-\ncount aspects like traf\ufb01c congestion, road conditions, and\ndelivery schedules. Route optimization not only reduces\ntravel time but also bene\ufb01ts the environment. In order to\nimprove air quality and create a more sustainable delivery\nprocess, it is possible to cut down on idle times and trip\ndistances. Businesses may streamline operations, improve\nthe overall customer experience, and contribute to a\nmore sustainable and environmentally friendly approach\nto logistics by automating procedures, optimizing routes,\nand utilizing real-time data [123].\n\u2022 Tourism: GPTs have the potential to signi\ufb01cantly improve\na number of tourism-related aspects. GPTs can offer\ncustomized solutions that suit the individual\u2019s preferences\nby understanding their needs and interests, resulting in a\nmore pleasurable travel experience. GPTs are excellent\nat understanding and creating text that is human-like [4].\nThis functionality can be used in the travel and tourism\nsector to enable chatbots or virtual travel assistants to\ncommunicate with users in natural language [124]. Trip\nplanning and information retrieval are made more sim-\nple and user-friendly by the ability of travelers to ask\nquestions, look for advice, and obtain full details about\ndestinations, modes of transportation, customs, and more.\nGPTs are capable of producing in-depth and interesting\ndescriptions of tourist sites, attractions, lodging, restau-\nrants etc. GPTs can provide time-ef\ufb01cient routes that\nguarantee a complete travel experience [125]. Including\nadvice on local legislation, emergency contacts, medical\nfacilities, and potential risks, GPTs can offer helpful\ninformation and direction regarding travel safety.\n3) Challenges: Privacy issues may occur when using sen-\nsitive data in travel GPTs. It is essential to manage user data\nsensibly and putting up strong security measures to safeguard\nprivate data. The quality of the model\u2019s outputs is directly\nin\ufb02uenced by the correctness and completeness of the data uti-\nlized during the training phase. Ethical considerations should\nbe taken into account when creating AI-powered applications\nemploying GPTs. It\u2019s crucial to check that the models are\ntruthful, unbiased, and free from harmful presumptions or\ndiscriminatory procedures [126]. Although the models contain\nadvanced features, they are dif\ufb01cult to tailor for speci\ufb01c use\ncases, need a lot of data to train, and have built-in limitations.\n4) Summary: Emerging GPTs have the potential to enhance\nproductivity, communication, and the calibre of goods and\nservices, which will bene\ufb01t many aspects of people\u2019s life.\nGPTs can offer real-time updates, effective route optimization,\nand customized recommendations in the travel and transporta-\ntion industries, enhancing the overall travel experience and\nincreasing operational effectiveness. Adopting them, however,\ncomes with some dif\ufb01culties. As speci\ufb01c roles are replaced\nby automation, GPTs may result in job displacement [127].\nAdditionally, the computational and memory requirements\nfor GPTs make their deployment on compact or low-power\ndevices dif\ufb01cult. GPTs may not be accessible to growing\nbusinesses due to the high costs associated with obtaining and\nusing them. Despite these obstacles, attempts are being done to\novercome them and improve the usability and value of GPTs\nfor a larger range of users.\nF. E-Commerce\n1) Introduction: Electronic commerce, commonly referred\nto as e-commerce, is a way for conducting economic trans-\nactions and create relationships between groups of people\nand entities using digital information processes and electronic\ncommunications [128]. Globally, this type of trade has experi-\nenced substantial growth, particularly in the retail sector. The\npreference for internet shopping, especially among younger\nmillennials, is a prominent trend in consumer behaviour.\nMobile devices have consequently taken over as the main\nmethod for carrying out internet transactions [129]. Therefore,\nit is crucial for e-commerce companies to give the customer\nexperience in their mobile applications top priority. The pro-\nvision of brief text summaries for titles and reviews is an\nessential component of this. These summaries are essential\nfor optimizing search results, helping consumers identify ap-\npropriate items, and ultimately raising customer happiness in\nthe online purchasing space [130].\n2) Impact\nof\nGPT\nin\nE-Commerce\nRealms:\nThe\ne-\ncommerce sector could signi\ufb01cantly advance with the intro-\nduction of GPTs. GPTs can be accessed by users or customers\nand are intended to answer commonly asked questions and\ngive in-depth details about many elements of the e-commerce\nprocess, such as products, delivery, refunds, and more [107].\nOne of the main bene\ufb01ts of GPTs is their capacity for quick\nresponses, which decreases the amount of time customers must\nwait to hear back from businesses [131]. By taking care of an\nimportant number of client inquiries, this function not only\nincreases customer happiness but also lessens the workload\non support workers. Customers will ultimately have a better\npurchasing experience as a result of being able to quickly\nacquire the information they require and interact with GPTs\n[106].\n\u2022 Proofreading: To improve the calibre and accuracy of\nwritten content in e-commerce, GPTs can be used for\nproofreading. Written content is essential for product\n\n21\ndescriptions, marketing materials, customer reviews, and\nother text-based components in the e-commerce sector\n[105]. For the purpose of projecting professionalism,\nfostering trust, and delivering a satisfying user experi-\nence, this text must be devoid of errors, well-written,\nand grammatically correct. E-commerce companies can\nautomate the process of identifying and correcting these\nproblems by using GPTs for proofreading, which saves\ntime and effort as comparison to manual proofreading\n[132]. This can be especially helpful in situations when\nusers are writing product reviews or interacting with\ncustomer service. An improved user experience is facil-\nitated by the early detection and recti\ufb01cation of errors,\nwhich also helps to avoid potential misunderstandings or\nmiscommunications.\n\u2022 Order Processing: GPTs are useful in many areas of\norder management and customer service because they\ncan comprehend and produce text that looks like human\nspeech. GPTs can help with handling consumer questions\nabout orders [133]. GPT is capable of interpreting the\nqueries, providing important details like order status,\ntracking information, and expected delivery time, as well\nas suggesting corrections for frequent problems [134].\nBy delivering real-time information, GPTs can assist cus-\ntomers in tracking their orders. Customers can customize\ntheir purchase with the help of GPTs. GPTs can help in\nthe identi\ufb01cation of possibly fraudulent orders by exam-\nining past transaction data, consumer behaviour patterns\netc [135]. Based on a customer\u2019s past purchases, browsing\nhabits, and preferences, GPTs can offer tailored product\nrecommendations. When a consumer puts a purchase,\nthe model can examine the information and produce\nrecommendations for related or supplementary products\nthat the customer might \ufb01nd interesting.\n\u2022 Generating titles for products: Companies can use GPTs\nto produce interesting and educational material to im-\nprove the appeal of their product listings [117]. Based on\na product\u2019s category, brand, and special characteristics,\nGPTs can come up with attractive titles for it. The model\ncan produce imaginative and memorable names that aid in\nbrand awareness and differentiation by receiving relevant\ninformation such as the characteristics of the product\nand the target market. GPTs are trained to produce in-\ndepth and interesting product descriptions [113]. These\nsummaries can offer a thorough summary that aids clients\nin selecting products wisely. GPTs are capable of coming\nup with clever and appealing captions for product images.\nGPTs can be adjusted to better re\ufb02ect the tone and\naesthetic of a certain brand [136]. As a result, the brand\nidentity is consistent and uni\ufb01ed throughout all product\nlistings.\n\u2022 Strategy Planning: GPTs have the ability to come up\nwith original and distinctive concepts for marketing\ncampaigns [137]. The model can provide recommenda-\ntions for different campaign aspects, such as slogans,\ntaglines, themes, contests, social media strategies, and\nmore by taking into account relevant information about\nthe product, target audience, marketing objectives, and\ndesired outcomes. GPTs can help with email writing\nthat encourages readers to become partners, investors,\nor customers [138]. To increase the likelihood of a\nfavourable response or interaction, these emails can be\ncustomized to address the needs and potential bene\ufb01ts\nfor the receivers. To improve their comprehension and\nproduction of appropriate material, GPTs can be trained\non domain-speci\ufb01c knowledge bases, such as e-commerce\n[107]. The models can offer more precise and situation-\nspeci\ufb01c recommendations for advertising strategies, prod-\nuct positioning, and target audience interaction because\nof this specialized training.\n\u2022 Data analysis: There are numerous ways to use GPTs for\ndata analysis in e-commerce. E-commerce data prepa-\nration can be aided by GPTs [24]. Data normalization,\ncleansing, and formatting are a few of the duties in-\nvolved. GPTs can produce summaries, identify signi\ufb01cant\ntopics, and extract appropriate data by studying textual\ndescriptions, reviews, and consumer feedback. This helps\nyou know the data more thoroughly, identify trends, and\n\ufb01nd insightful information. Customer reviews and social\nmedia comments from e-commerce can be analyzed for\nsentiment using GPTs. The sentiment expressed in text\ncan be evaluated using GPTs and categorized as either\npositive, negative, or neutral [107]. Understanding client\nviews, recognizing product strengths and de\ufb01ciencies,\nand making data-driven decisions all bene\ufb01t from this\nanalysis, which also helps to increase customer happiness.\nSegmenting consumers based on preferences, behaviours,\nor past purchases can be aided by GPT models [139].\nFor the purpose of detecting fraud in e-commerce trans-\nactions, GPTs can be used. GPTs can support the identi\ufb01-\ncation of potentially fraudulent actions by examining past\ntransaction data, user behaviour patterns, and recognized\nfraudulent tendencies [140].\n3) Challenges: While GPTs have a lot of potential for\nnumerous e-commerce applications, they also have several\ndrawbacks. In order to produce responses, GPTs mostly rely\non the context given in the input text. They could, however,\n\ufb01nd it dif\ufb01cult to fully understand the broader context or\ndetails that are unique to e-commerce. GPTs provide replies\nusing training data and prior knowledge. They are unable to\naccess real-time data or carry out real-time calculations [141].\nThey might not be appropriate for giving current information,\nsuch as pricing, product availability, or dynamic promotional\noffers. GPTs gain their knowledge from a wealth of training\ndata, which includes text taken from the internet, which\nmay be biased, stereotyped, or otherwise offensive [142].\nThe models may unintentionally provide biased or unsuitable\nreplies if they are not rigorously managed and monitored,\nwhich could be harmful to the customer experience and brand\nreputation. The use of ethical principles and the training data\nmust both be given careful thought.\n4) Summary: The conversational interface offered by GPTs\ncustomizes the purchasing process and makes interactions with\nclients more interesting and appropriate to their individual\nrequirements. GPTs can also be utilized to get insightful\ncustomer feedback. Businesses can learn about customers\u2019\n\n22\npreferences, issues, and opinions regarding their products and\nservices by conversing with them. In order to better serve\ntheir target audience, organizations can use this information to\ndiscover areas for improvement, increase customer happiness,\nand make data-driven decisions. It\u2019s essential to recognize\nthat GPTs might occasionally make mistakes or give poor\nanswers, particularly when dealing with complicated or am-\nbiguous queries. This highlights the necessity of continual\nmodel training, thorough testing, and modi\ufb01cation to guarantee\nthat they consistently meet consumer needs. To con\ufb01rm the\nef\ufb01cacy and dependability of using GPTs speci\ufb01cally in the e-\ncommerce area, more research and testing are required. When\nimplementing GPTs, it\u2019s critical for businesses to take into\naccount the particulars of their own e-commerce businesses,\ntheir target market, and the type of the client enquiries. Regular\nmonitoring and feedback analysis, along with a systematic\nand iterative approach, can help make sure that the outcomes\nof using GPT models are in line with the objectives of e-\ncommerce enterprises.\nG. Entertainment\n1) Introduction: In the ancient days, the Entertainment\nmeant about playing games with neighbors covers all outdoor\ngames, indoor games and chatting with neighbours through\ntelephone. As digitization has bought greater advancements\nin computation and communication, in turn access to internet\nis also much easier. This has changed the way people are\nentertained. as people are connected and fully engaged in\ncompleting the target for the day. And there was a radical shift\nfrom traditional employment to employment in the Industrial\nRevolution age. Stress and pressure are common factors hin-\ndering people of different age groups. The different forms of\nentertainment serve as stress busters. Entertainment and mental\nhealth are interrelated; the former transfers happiness, bringing\nharmony and peace to mental health. Some common forms\nof entertainment include playing games, watching TV series\nor movies, or funny videos, shopping, debugging, coding,\nbrowsing the internet, listening to music, dancing, chatting,\npainting, crafting, reading books, cooking, and many more,\nwhich can lessen the stress carried [143]. Entertaining and\ngetting entertained is the biggest motivation and medicine for\nall mental illnesses. Entertainment helps to improve the motor\nskills of humans, thereby inducing a positive cognitive effect\ntowards the work.\n2) Impact of GPT on the Entertainment Industry: GPT is\na potential game-changer in the entertainment \ufb01eld, delivering\nendless entertainment. Since its evolution, GPT models have\nbeen adopted as an entertainer crosschecking their ability\nto produce content on funny and illogical questions. GPTs\nentertain people in many ways, and of course, using GPT itself\nan entertainment as it reduces the burden of overthinking by\nproviding immediate feedback to queries in seconds [144].\nThe results are amazing and have been utilized for many\npurposes today. When the GPT model was probed to complete\na scene from the movie \u201cFrozen,\u201d it responded with an\nentertaining writeup [145], [146]. Some of the impacts of\nGPTs on Entertainment applications are given below:\n\u2022 Solitude with GPT: As the GPT itself is an entertainer,\none can feel better alone with the GPT, which helps to\ncome out of loneliness by exploring its savors [147].\nGPTs assist in providing soothing poems, mental healing\nquotes, and funny riddles. People with loneliness may feel\nanxiety, especially with older ones at home. In this case,\nGPT-4 helps people with its Voice Technology feature,\nenabling users to input their audio [147]. In turn, the\nGPT model responds to user-speci\ufb01c speech output using\nNLP algorithms embedded with it. The elderly can feel\nsafe and attentive at home. GPT-4 is multilingual and can\nunderstand various dialects and accents for personalized\nuser experience.\n\u2022 Enhanced Customer Interaction: The advent of Chat-\nGPT and Bard has improved customer interactions on\ncontent such as movies, Over-the-Top (OTT) platforms\nlike Net\ufb02ix, Hulu, Disney+ Hotstar, and prime video,\nsound recordings, song lyrics, pictorial works, comics,\njokes, memes, viral videos, and other entertaining factors.\nFurther, GPTs provide human-like recommendations on\nuser-speci\ufb01c fun activities based on user interactions for\nan immersive experience. This has dramatically improved\nthe interactions in the engagement industries. User en-\ngagement can be further improved by providing dynamic\nand more realistic responses to user queries, such as\ncreating virtual actors for interacting with real actors\n[148].\n\u2022 Personalized Content Creation: GPTs can help generate\nuser-speci\ufb01c personalized content by analyzing the user\npreferences and generating content like predicting future\nscenarios tailored to the user\u2019s interest. GPTs can be\nused for creating personalized, engaging, and high-quality\ncontent for online business advertising, ideas for content\ngeneration, marketing messages for attracting customers,\ndescriptions for selling products, and captions for social\nmedia [149]. In addition, it can be used for optimizing\nthe contents for search engines, i.e., GPTs will provide\nrelevant terms for search, thereby avoiding traf\ufb01c to the\nweb sources.\n\u2022 For the Film and TV industry: GPT-powered virtual\nassistants assist users in booking tickets and generat-\ning content and personalized recommendations using AI\nmodels. The evolution of GPT-4 with advanced NLP\nand DL algorithms helps the scriptwriter to generate AI-\ndriven content without the human author named virtual\nstorytelling [150]. GPTs create interactive stories, dia-\nlogues, and characters, recommending suitable characters.\nFurthermore, GPTs can be used to create content for\nvideo games, voice-enabled applications, AR applica-\ntions, and other VR experiences in virtual worlds [151].\n\u2022 For Social media in\ufb02uencers: GPTs can generate person-\nalized marketing ads for each customer based on their\nprevious interactions and provides relevant suggestions\nfor customer viewing experiences. Youtubers and other\nsocial media content creators will potentially bene\ufb01t from\ngenerating channel content based on demand and realistic\nsocietal activities.\n\n23\n\u2022 Realistic Gaming Interactions: GPT helps to generate the\nplayers, gaming narratives, dialogues, user interface, and\nuser-speci\ufb01c gaming recommendations and new game\ncreation. Powerful HCIs can render a better user ex-\nperience for game developers and players. Assistance\nto the game developers in debugging and enhancing\nthe code developed. GPT uses various NLP and AI\nalgorithms trained with massive data to predict the next\nphrase/movements and provide human-like experiences in\n3D gaming environments. ChatGPT has been integrated\nwith AR and VR to provide an immersive gaming expe-\nrience.\n3) Challenges: Latency is the major issue connected with\nrendering the voice-based response to the voice input. As\nwell, plausible misinterpretations may mislead the responses,\nand interruptions to the relayed output are dif\ufb01cult. Enabling\ntechnologies like EC and 5G can help overcome this issue.\nAlso, GPTs must be capable of storing the facts with audio\nconversations to relay them while conversing the other day.\nFurthermore, the AI system must be built in such a way\nthat it can continuously learn (lifelong machine learning) and\nenhance over time. The major ethical concern with virtual\nstorytelling is the bias exposed in the training data and the\nobscurity of reproduced content on the generated stories.\nAnother issue with the generated content is plagiarism (i.e.,\nproducing content similar to the content in the published\narticles or books), raising disputes with intellectual property\nrights. In addition to this, the source of the content generated\nremains unexplored. The language barriers in using GPT must\nbe lessened to improve user experience and utilize the features\nof GPT [113]. The implication of the user to provide inputs in\na certain format to GPTs can be further improved by providing\ndifferent options in addition to voice-based inputs GPT4,\nlike braille screen input for visually disabled people. The\nuser authentication can also be further enhanced to safeguard\nuser-speci\ufb01c content generation and avoid repeated content\ngeneration for users with similar requests. One of the primary\nconcerns with the GPTs adoption is job loss. Content creators,\nbloggers, and poets may lose their jobs.\n4) Summary: The entertainment industry is the one which\nwill be in demand always, as it is a lifeline for many indi-\nviduals leading a stressful work environment or personal life.\nDespite the stress, entertainment has become part of routine\nlife due to its immersive nature, creating harmony in the mind\nand the environment. GPTs have made a major contribution to\nenhancing the entertainment industry, but the job security of\nmany professionals in this \ufb01eld remains unanswered. GPTs\nmust be trained on unbiased data and ensure transparency\nin source content generation to provide a secure, robust, and\nef\ufb01cient contribution to the entertainment industry. To attract\nall types of users, the multilingual capability and content\nrendering of GPTs can be further enhanced. The issues con-\nstrained by providing user inputs to GPTs can be alleviated to\nall extent. Furthermore, safer user content generation without\nplagiarism and relating facts with previous conversations can\nbe guaranteed by abiding by the storage requirements to deal\nwith a more personalized user experience.\nH. Lifestyle\n1) Introduction: Lifestyle, the way of our living, is one\nof the prominent areas most people in today\u2019s digital era of\nAI, are bound to and look for constant improvement. The\n\u201cmodus vivendi\u201d is a Latin expression that semantically means\na way of living and should be understood in terms of values\nand attitudes. These two terms manifest self, in\ufb02uenced by\nfamily, society, and global media. Directly or indirectly, these\nin\ufb02uence an individual\u2019s lifestyle. Adopted from a sociological\nperspective, an individual expresses oneself through different\npractices, viz., eating ways, drinking behaviours, mode of\ntravel, travelling places, costume designs, body-shaping cloth\nto wear, media preferences, education choices, job preferences,\nentertainment modes, managing leisure time, means of com-\nmunication and so [152]. In all these practices, individuals\nwould like to explore and learn about what, where, how, and\nwhen factors for reading sustainable development [153]. The\nconcept of lifestyle is all about \u201chow one wants to live one\u2019s\nlife.\u201d Consumerism is the act of purchasing artifacts for soci-\netal status and is one of the thriving lifestyle factors. Certain\nstandard indicators like job, wealth, and physical and mental\nhealth determine the quality of one\u2019s life. Also, the choice\nof a healthy lifestyle moderately determines the health of an\nindividual [154]. Furthermore, few people believe that lifestyle\nre\ufb02ects their socioeconomic status. Many epidemiologic stud-\nies state that better lifestyles have dramatically reduced the\nrisk of various chronic diseases and are the primary cause\nfor their prevention [155]. The lifestyle has been de\ufb01ned\non different societal levels from individual, positional, and\nnational to global [153]. At the global level, lifestyle is adopted\nby general world-class in\ufb02uencers. In contrast, at the national\nlevel, the in\ufb02uencing factors will be the government and dif-\nferent cultural patterns across the country. The positional level\nconcerns in\ufb02uence from different status groups, age categories,\ngender groups, and social classes. And the individual level is\nin\ufb02uenced by a closely moving group of individuals concerned\nabout self-identity. The major source of information about\nthese in\ufb02uencers is the Internet through social media networks\nand personal development advertisements.\n2) Impact of GPT in Lifestyle: The most remarkable ap-\nplication of AI, the GPT, paves the way for the betterment\nof mankind in offering human-like intelligent conversation\non all whereabouts. People will always prefer to interact\nwith other peers to learn their attributes and tweak them for\nsocietal status. Various GPTs have \ufb02ourished for different\nlifestyle indicators, and they provide human-like assistance\nto all queries on \ufb01ne-tuning the lifestyle by harnessing the\npower of AI [156]. The advanced reasoning capability of GPT-\n4 serves the purpose better [147].\n\u2022 Diet Planner: Free GPT applications for maintaining\na balanced diet, helping the individual with a weight\nloss diet plan, followed by a brief list of meal plans,\nrequired shopping lists, physical activity plans targeting\nparticular body parts [157], motivational messages, and\npersonalized sleeping patterns. These apps act more like\npersonalized training assistance and help to track progress\nwith visualization charts or graphs. Fitness level, available\n\n24\nfree hours, medications taken, and available exercise\nequipment will be given as input to GPT.\n\u2022 Travel Guide and Trip Advisor: Harnessing AI models,\nGPT provides an individual\u2019s travel plan itinerary based\non information like the place(multiple cities), budget, and\nthe number of days. These GPTs provide local recom-\nmendations on restaurants, hotels, and other attractions.\nRoamAround, Roamr, and VacayChatbot are some of the\ntravel planning GPTs [158].\n\u2022 Personalized Stylist and Beauty Advisor: GPTs can\nact as personalized stylists for an individual by gen-\nerating occasion-speci\ufb01c clothing and costume prefer-\nences. GPTs can assist in organizing wardrobes based\non seasonal out\ufb01ts and provide recommendations on e-\ncommerce fashion stores for purchasing favourite brands.\nGPTs can provide tailoring design options, fabric choices,\nand design materials. Furthermore, GPTs can provide\nupdates on a stock based on the preferred searches and\nprovide insight into fabric types suitable for weather\nconditions that suit personal style.\n\u2022 Personalized CookBook: GPTs can serve as cooking\nassistants by recommending new curated recipes suiting\nthe family dietary plan, ingredients available, time, in-\ndividual\u2019s cooking skills, and new \ufb02avoured ingredients.\nChefGPT, PantryChef, and MacrosChef are some GPTs\nthat generate unique and delightful recipes [159]. Con-\nsequently, GPTs can assist in shopping list recommenda-\ntions and the nutritional value of the recipe generated.\n\u2022 Hobby Curator : GPT assists an individual in identifying\none\u2019s enjoyable leisure time activity by learning new\nskills [160]. Having a list of interests and ideas ready,\nthe GPT helps narrow down various options, instructional\nvideos to proceed, chatting and sharing with online\ncommunities, and researching the cited hobby to explore\nmore fun. Budget will also be an important factor in\nthis perspective, as learning new hobbies may require\njoining paid classes or courses. GPT provides step-by-\nstep instructions and guidelines to learn a new skill faster.\n\u2022 Dream Maker: GPTs with multimodal learning helps\nto search for a job based on one\u2019s quali\ufb01cations and\nexperience. In turn, it assists in preparing the job-speci\ufb01c\nresume, cover letters, training for the interviews (coding\nand technical queries), and grooming sessions and can\nredirect to the training place where knowledge can be\nacquired [161]. The futurist GPT models can assist in\nphase-by-phase questionnaires in the interview process\n3) Challenges: The recent version of the GPT uses both\nreinforcement and supervised learning models so it can learn\nbased on the interaction with the user and can use existing data\nto derive personalized decisions. In the context of lifestyle,\nGPTs offer the most promising solution for almost all lifestyle\nin\ufb02uencers, but the still challenging part is the trustworthiness\nof the data and copyright issues. Also, relying more on GPT\nas it solves all our problems may insipid human intelligence\nin upcoming generations. Though the GPT provides weight\nadvice, it can never be a substitute for the medical practitioner,\nas some information can be misleading. Travel planning GPTs\nsometimes require users to update information in a speci\ufb01c\nformat and may have outdated databases. GPTs cannot access\nspeci\ufb01c job openings\u2019 websites but can still provide insights\ninto acquiring them. At times, it can produce nonsensical infor-\nmation [162]. Therefore, before adopting the GPT recommen-\ndations fully, further instigation is recommended. Furthermore,\ndeveloping a large multimodal learning model abiding huge\nand dynamic datasets will be costly.\n4) Summary: GPT is a personalized assistant for improving\nan individual\u2019s lifestyle from various prospective in\ufb02uencers.\nGenerating personalized recommendations alleviates an indi-\nvidual\u2019s fear of survival in the digitized society. Individuals\nwill be personally trained to adapt to different cultural and\ntechnological shifts in the sustainable development of them-\nselves and the economy as a whole. On the other hand, more\nstringent recommendations may incur huge budget overruns\nand sometimes provoke the individual to misinterpret, leading\nto dreadful consequences. GPTs provide both positive and\nnegative recommendations based on the input fed. So, for the\neffective adoption of a GPT for lifestyle practices, adverse\ntraining and testing on extreme behaviours must be carried out.\nGPTs must be trained in the realistic and dynamic perception\nof individuals in real life.\nI. Gaming\n1) Introduction: Before the advent of technology and the\ngaming industry, entertainment was primarily centred around\nactivities such as reading, listening to music, watching plays\nand movies, participating in sports and physical games, and\nsocializing with friends and family. People also engaged in\ntraditional board games and card games, which were often\nplayed in groups and provided a fun and social way to pass\nthe time. After technology stepped into the gaming industry,\nthe way games are created, and the experience it has given\nusers have transformed tremendously. Technology has enabled\ndevelopers to create more immersive and engaging experiences\nfor players. It has contributed in various ways, like improving\ngraphics, performance, online play, and mobile gaming. Im-\nproved GPUs and other technologies allow for more detailed\nand realistic graphics, making games more visually stunning.\nFaster processors and higher amounts of RAM allow for\nsmoother gameplay and faster loading times, reducing lag and\nimproving overall performance. Technologies like AI, AR, and\nVR have created a new dimension of game development and\nexperience. Players can now immerse themselves in gaming\nworlds in a way that was not possible before. With the help\nof advanced AI techniques, game developers can create more\nsophisticated and challenging opponents for players, as well as\nNPCs with more realistic behaviours. Technology has greatly\nexpanded the possibilities of gaming and enabled developers\nto create more immersive, visually stunning, and engaging\nexperiences for players.\n2) Impact of GPT in Gaming : GPTs have the ability to\ncontribute to all sectors, including the gaming sector. GPT\nare not speci\ufb01cally designed for creating and playing games,\nbut they have the potential to improve the gaming experience\nby improving enhanced dialogue and story telling, creating\n\n25\ndynamic and personalized gaming worlds, generating more\nrealistic and engaging characters [163], game content creation,\nchatbot development.\n\u2022 Chatbot development: GPTs have been used in gaming\nthrough the development of chatbots that use NLP to\ncommunicate with players [164]. Because it allows the\nchatbot to understand and respond to a wide range of user\ninputs and queries related to the game. GPTs have been\npre-trained on a large corpus of text data, which makes\nthem adept at NLP. It can understand and respond to user\nqueries in a way that feels natural and intuitive. It can also\nunderstand the context of a user\u2019s query, which means\nthey can provide relevant and useful responses even when\nthe user\u2019s query is ambiguous or incomplete. It can\nalso generate game-related content, such as descriptions\nof game characters or settings, that can help to enrich\nthe user\u2019s gaming experience. Furthermore, it can also\npersonalize the user\u2019s experience by learning from their\nprevious interactions with the chatbot and tailoring its\nresponses accordingly.\n\u2022 Game content creation: GPTs are used in game design.\nThey are used to create game content such as levels,\nitems, and quests. If the game designer is working on a\nnew role-playing game, GPT can be used in creating char-\nacters to be used in the games. To generate new character\nclasses in the games, the developer has to give inputs that\ncontain information about the game environment, game\nsettings, player abilities, and game play mechanics. GPTs\nhas the ability to analyze the text and expectations given\nby the developer, and it can generate a list of potential\ncharacter classes based on the expectations given as text.\nThe designer then re\ufb01nes the ideas and chooses a more\nsuitable character to develop further with unique abilities\nand game mechanics. The authors in [165] have used\nGPT2 and GPT3 to procedurally generate role-playing\ngame with video game descriptions. The resultant quest\nwas evaluated by 349 online RPG players. The results\nconcluded that one of the \ufb01ve quest descriptions was\naccepted for game development.\n\u2022 Analyze player\u2019s ability and skill: GPTs can detect\nand analyze players\u2019 abilities and skill levels and tailor\nthe game accordingly. This analysis helps in making\ndynamic modi\ufb01cations to the game environment based\non the player\u2019s abilities and skill levels. This feature\nhelps achieve dynamic dif\ufb01culty balancing. GPTs can\nalso assist in identifying the player\u2019s intent. Thus, when\nplayers ascend to higher levels, it can assist in making\nthe games more challenging based on the player\u2019s abilities\nand skill levels in the previous levels\n\u2022 NPCs: NPC stands for \u201dNon-Player Character.\u201d In AI\ngames, NPCs refer to characters or entities in a game\nthat are not controlled by a player. NPCs can take on\na variety of roles within a game, such as enemies to\n\ufb01ght, quest givers, merchants, or friendly characters that\nprovide helpful information. They are often controlled\nby AI algorithms that determine their behaviour and\nactions within the game world. GPTs are not speci\ufb01cally\ndesigned for creating NPCs, but they can be used to\ngenerate dialogue and other character interactions that\ncan be incorporated into NPCs. Additionally, It can be\nused to generate character backstories and personalities,\nwhich can inform the development of NPCs. The authors\nin [166] have trained and used GPT-2 for text generation\nof video games. They have trained GPT-2 on a large\ncorpus of video game quests and used a GPT model\nto generate the dialogue for quest-giver NPCs in role-\nplaying games. The output has shown that GPT can learn\nthe structure and linguistic style of the games, and the\nquality of the content it has generated is high, making it\na good alternative to writing new RPG quests by hand.\n3) Challenges: GPTs are computationally expensive and\nrequire high computing resources to do their purpose. This\nmeans that implementing them in a game would require\npowerful hardware and this could have an impact on the\nperformance of the games. Lack of training data: GPTs require\nlarge amounts of high-quality training data to be effective.\nIn the gaming industry, this could be dif\ufb01cult to obtain, as\ngaming data are likely to be fragmented and less structured\nthan the kind of data used to train GPT models [131]. In\naddition, GPTs can perform content creation based on patterns\nthey have learned from their training data, which means that\nthey can be unpredictable. The content generated by GPT\nmay be nonsensical or inappropriate content to the game.\nIn the context of gaming, this lack of control could lead to\nundesirable or even offensive game content. GPTs can generate\ntext based on user input, they can\u2019t interact with the game\nenvironment in the same way a human player can. This limits\ntheir usefulness in gaming and may make them less effective\nthan other AI technologies.\n4) Summary: GPTs can transform the gaming industry by\ncontributing to improved game dialogue creation, enhanced\nnon-player characters, personalized gameplay, procedural con-\ntent generation, chatbot generation, and analyzing players\u2019\nabilities. However, it also has potential challenges that are to\nbe addressed, such as the need for high computing resources, a\nlack of control over content creation, and restricted interaction\nwith the game environment. In addition, the most important\nchallenge in adopting a GPT model in gaming is a lack of\ntraining data. If the challenges are addressed and the gaming\nindustry evolves with properly structured data to train a GPT\nmodel, then GPTs can revolutionize the \ufb01eld of gaming.\nJ. Marketing\n1) Introduction: Traditional marketing primarily relied on\ntraditional media channels, such as television, radio, newspa-\npers, and magazines, to reach consumers. Companies used to\ndevelop marketing campaigns based on demographic data, and\nmass media channels were used to broadcast these campaigns\nto a broad audience. However, the advancements in technology\nhave brought about signi\ufb01cant changes in the marketing indus-\ntry, and companies are increasingly integrating new marketing\nstrategies evolved through technologies to reach and engage\nwith customers. One of the signi\ufb01cant transformations has\nbeen the rise of digital marketing channels such as social me-\ndia, search engines, Email, and mobile applications that allow\n\n26\ncompanies to target speci\ufb01c populations with precision and\nprovide real-time feedback on campaign performance, allow-\ning for more effective and ef\ufb01cient marketing. Technology has\nalso given rise to marketing automation tools such as customer\nrelationship management systems, chat-bots, and personalized\nemail marketing, which have made marketing more ef\ufb01cient\nand effective. Another signi\ufb01cant transformation has been the\nuse of big data and analytics to better understand customer\nbehaviour and preferences. This has allowed companies to\ncreate more personalized and targeted campaigns based on\nspeci\ufb01c customer needs and preferences.\n2) Impact of GPT in Marketing : The marketing industry\nhas evolved with various AI-powered techniques. This revo-\nlution started in marketing by providing businesses with pow-\nerful tools for generating insights, automating processes, and\nimproving customer experiences. GPTs are also being used\nin marketing to generate engaging and personalized content.\nSome of the applications of GPT in marketing include content\ncreation, customer service, and personalized advertising.\n\u2022 Content creation: GPTs can contribute to marketing in\nvarious ways, such as by improving speed and ef\ufb01ciency\nin content creation, ensuring consistency and quality of\ncontent, generating personalized content, creating multi-\nlingual content, and repurposing existing content. It can\nbe trained on a company\u2019s existing marketing materials\nand customer data, allowing it to create new content,\nsuch as blog posts, social media updates, and product\ndescriptions, in a fast and ef\ufb01cient manner. Despite its\nspeed, it maintain high standards for quality and consis-\ntency. Moreover, GPTs [162] can generate personalized\ncontent based on customer data, such as search history\nand past purchases. This helps create content that is rel-\nevant to the users\u2019 desires, leading to better engagement\nand conversion rates. GPTs can also generate content in\nvarious languages, allowing marketers to expand their\nreach across regions. Copy.ai [167] has used GPT-3 to\ngenerate human-like text that is optimized for marketing\npurposes such as website copy, social media posts, ad-\nvertisement copy, and email campaigns. This means that\nmarketer personnel no longer focus on content creation.\nInstead, they can spend productive time improving the\nother aspects of marketing.\n\u2022 Customer service: GPTs can be trained on customer ser-\nvice conversations and chat logs to generate more natural\nresponses, like humans. This can help business person-\nnel provide better customer service 24/7 and save time\nand resources. It can be trained to generate automated\nresponses for frequently asked questions, providing faster\nresponses to customers and ensuring consistency in the\nquality of replies. GPTs can also analyze customers\u2019 emo-\ntions and sentiments, enabling businesses to proactively\naddress negative feedback. This is particularly helpful in\nmaintaining customers\u2019 trust. The authors in [168] have\nused GPT-3 model for automated drafting of responses\nfor incoming mails. They used it to understand the mail,\nand then software engineering and business studies were\nused to understand the challenges encountered and \ufb01nally,\nthe response generated after a thorough understanding of\nthe context of the mail. The authors have concluded that\napplying GPT-3 to rationalize email communication is\nfeasible both technically and economically.\n\u2022 Personalized advertising: GPTs can generate personalized\ncontent such as product descriptions, blog posts, and\nsocial media captions tailored to individual customers\u2019\npreferences and interests. This can help businesses create\ncontent that resonates with their target audience, leading\nto higher engagement and conversion rates. By analyzing\ncustomer data, GPTs can segment customers according\nto their behaviour, interests, and preferences. As a result,\nbusinesses can tailor their marketing campaigns to each\nsegment and provide personalized messaging and offers\nthat are more likely to connect with each customer group.\nThe authors in [169] have proposed a generative model\nto identify the name of the product from the product text\nand use this information \ufb01lter to improve the product\nrecommendation based on the product retrieval model.\nThis method has been implemented in the dynamic prod-\nuct advertising system of Yahoo. It is observed that the\nrecommendation system has recommended the product\nbased on the user\u2019s interest, and it was evaluated using an\nA/B test to serve similar products in an ad carousel, which\ncan help the system to explore more products ef\ufb01ciently.\n\u2022 Forecast analysis: Using customer data analysis, GPTs\ncan forecast future behaviour and buying patterns. This\nallows businesses to customize their marketing campaigns\nto each customer\u2019s desires based on their purchase pat-\nterns, increasing the likelihood of conversion or purchase.\nThe authors in [170] have used chatGPT to perform pre-\ndictive modelling based on past data. They have used the\nGPT model to predict the future based on the customer\u2019s\nbehaviour and buying pattern. This primarily helps the\nsystem to recommend the products to the customers as\nper their desires.\n3) Challenges: GPTs are designed to generate content that\nimitates human writing, but the content generated may not\nalign with the brand\u2019s image or message. This lack of control\ncan be a potential challenge for marketers. Another challenge\nthat applies to all learning technologies is that data bias is\npossible in GPTs [138]. Based on the large dataset of text used\nfor training, if the data is biased, it will affect the generated\ncontent, which may also exhibit the same biases. GPT is\ncomplex and dif\ufb01cult to interpret, making it challenging to\nexplain how the model arrived at its conclusions. This lack\nof transparency can lead to a lack of trust in adopting GPTs,\nand marketing teams may struggle to make improvements in\ntheir strategies. As like every AI technology, there are ethical\nconcerns associated with GPT models. For instance, the use\nof GPT in marketing could raise concerns about the use of\npersonal data and privacy, particularly if the model is used to\ngenerate targeted advertising or personalized content. To avoid\nany negative consequences, companies must ensure they use\nthese models ethically and transparently.\n4) Summary: Using GPTs in marketing can provide various\nbene\ufb01ts, such as better content creation, personalized messag-\ning, increased ef\ufb01ciency, competitive advantage, and enhanced\ncustomer experience. However, this strategy also involves\n\n27\npotential challenges, such as limited control, data bias, lack of\ntransparency, and ethical considerations. Therefore, companies\nmust consider the advantages and drawbacks of GPT adoption\nin marketing, and implement these models ethically and trans-\nparently to avoid negative outcomes. Successful integration\nof GPTs in marketing requires proper planning, a skilled\nworkforce, and continuous monitoring to ensure the desired\nresults and mitigate any potential risks.\nK. Finance\n1) Introduction: The \ufb01nance industry, also known as the\n\ufb01nancial sector, is a broad term that encompasses a wide range\nof institutions and businesses that provide \ufb01nancial services to\nindividuals, businesses, and governments. The \ufb01nance industry\nplays a critical role in the global economy, facilitating the \ufb02ow\nof funds between savers and investors, managing risk, and\nproviding \ufb01nancial services and products to support economic\ngrowth. The \ufb01nance industry has been the leader in technology\nadoption in recent years, with a focus on improving ef\ufb01ciency,\nreducing costs, and delivering better customer experiences.\nThe adoption of technologies like big data and analytics,\nmobile and digital payments, blockchain and distributed ledger\ntechnology, AI and ML, and cloud computing make the sector\nmore \ufb02exible, scalable, trustworthy, transparent, secured, and\neasier to access.\n2) Impact of GPT in Finance: GPT has greatly in\ufb02uenced\n\ufb01nance by automating customer support using chatbots and\nvirtual assistants, enhancing fraud detection, offering invest-\nment insights and recommendations based on \ufb01nancial data\nand news, assisting with risk assessment for investments and\nloans, impacting algorithmic trading strategies, simplifying\ncompliance with regulations by analyzing legal documents,\nimproving credit scoring and loan processes, and emphasizing\nthe importance of handling sensitive \ufb01nancial data securely\nand transparently.\n\u2022 Sentiment analysis: Sentiment analysis is a technique\nused in the \ufb01nance industry to evaluate the sentiment of\ninvestors [171] and the general public towards speci\ufb01c\ncompanies, industries, or markets by analyzing news ar-\nticles, social media posts, and other text-based sources of\ninformation. GPT has the potential to improve sentiment\nanalysis in \ufb01nance by providing more accurate and de-\ntailed analyses of \ufb01nancial data. With sentiment analysis,\nthe industry can predict stock prices by assessing the\nsentiment of news articles, social media posts, and other\nsources of information about a particular company or in-\ndustry to make informed investment decisions. By utiliz-\ning sentiment analysis, GPTs can aid \ufb01nancial institutions\nin identifying potential risks and taking appropriate action\nto mitigate them. The authors in [172] have investigated\nhow incorporating a lexicalized ontology can enhance\nthe performance of aspect-based sentiment analysis by\nextracting indirect relationships in user social data. The\ninvestigation results show that the analysis has given 98%\naccuracy.\n\u2022 Financial forecasting: GPTs can be trained on past \ufb01-\nnancial market data to predict future trends in the stock\nmarket, exchange rates, and other \ufb01nancial metrics. This\ncan help investors and \ufb01nancial organizations make more\naccurate predictions and reduce their risk exposure. With\nthe ability to analyze and process the natural language,\nGPTs can be used to analyze and interpret \ufb01nancial data,\nnews, and other related information. Financial analysts\nand researchers can use the ability to analyze natural\nlanguage to extract insights from unstructured data like\nnews articles, social media content, and other information\nthat is relevant to forecasting. This can help improve the\naccuracy of \ufb01nancial forecasting models by providing\na more comprehensive view of market trends and sen-\ntiments. This analysis may help improve the accuracy\nof prediction. Financial analysts can use the model to\nidentify the relationship between the \ufb01nancial parameters\nthat could change the market conditions in advance. This\nprediction may be helpful for investors as they make\ninvestment decisions.\n\u2022 Trading strategies: GPTs can also be used to analyze\nmarket trends and historical data to develop trading\nstrategies. This can help traders make better decisions in\nterms of trading to increase their pro\ufb01tability. GPTs can\nbe used to identify the potential risks in trading portfolios.\nBy analyzing the large volume of information related to\ntrading, GPT will get the potential to identify the risk\nparameters and provide insights into how to mitigate these\nrisks. The authors in [173] have used a popular GPT for\nstock market trend prediction. The results show that the\nmethod used is simple but the ef\ufb01ciency and accuracy of\nthe method are very effective. The prediction it has made\nis very close to the reality.\n\u2022 Risk prediction and management:The adoption of GPT\ncan enhance the process of risk prediction and manage-\nment in several ways. It can improve data analysis by\ndetecting patterns that may pose a risk. It can also help\nin enhancing fraud detection by analyzing transaction\ndata and identifying fraudulent activity based on patterns.\nAdditionally, GPT can be utilized to make better portfolio\nmanagement decisions by analyzing historical industry\ndata, company \ufb01nancial statements, and news articles, as\nwell as social media feeds. This portfolio management\nprocess can provide valuable information about the in-\nvestment risk of a given organization, enabling informed\ninvestment decisions and effective risk management.\n3) Challenges: GPTs have more challenges in the \ufb01nance\nsector. Primarily, they demand signi\ufb01cant computational re-\nsources to train and deploy, which can be expensive and time-\nconsuming for \ufb01nancial organizations to implement. Another\nchallenge is that, even though GPTs are capable of producing\nprecise predictions, they can be challenging to interpret, which\ncan present a problem for \ufb01nancial institutions seeking to\ncomprehend the reasoning behind speci\ufb01c predictions [174].\nThis lack of interpretability can harm risk management objec-\ntives.Implementing GPT in \ufb01nance sector can be vulnerable\nto adversarial attacks, which are designed to manipulate the\nmodel\u2019s output by injecting false data. This can be particularly\nproblematic for \ufb01nancial institutions that rely on GPTs for risk\n\n28\nmanagement and investment decisions. It also require large\namounts of training data to achieve high accuracy. However,\nin some cases, \ufb01nancial institutions may not have access to\nsuf\ufb01cient data to train the model effectively. GPTs can also\nbe biased if the training data used to develop the model is\nbiased. This can lead to inaccurate predictions and unintended\nconsequences.\n4) Summary: The use of GPTs in the \ufb01nance industry\nhas promising bene\ufb01ts such as improved risk management,\nenhanced fraud detection, better portfolio management deci-\nsions, and increased ef\ufb01ciency. However, it also has potential\nchallenges that need to be addressed, such as high computa-\ntional requirements, the complexity of implementation, limited\ninterpretability, vulnerability to adversarial attacks, limited\ntraining data, and bias in training data. So, the use of GPTs\nin the \ufb01nance industry presents signi\ufb01cant bene\ufb01ts but also\nrequires careful consideration of the challenges involved to\nensure the effective and secure deployment of these models.\nL. Summary On Impact of GPT models in Applications\nThe impact of GPTs in various applications and challenges\nwas highlighted. GPT with its varied usage has changed\nthe way people perceive facts such as content creation, en-\nhanced user interfaces, personalized learning, item tracking,\nself-awareness, market risk analysis, business forecasts and\nintrospection. However, there are concerns about the potential\nnegative impact of GPTs, such as the spread of fake news,\nbias in data and decision-making, not domain speci\ufb01c, ethical\nissues, data reliability, the complexity of implementation,\nmultimodal and multilingual support, security and privacy\nconcerns, vulnerable to data attacks, limited input data, ex-\nplainability of results, large model size, high computational\nrequirements and job loss. Despite these concerns, it is clear\nthat GPTs will continue to be a powerful tool for industries\nseeking to leverage the power of NLP and generative AI. As\nthe technology improves and new applications emerge, it will\nbe interesting to see how GPTs continue to shape the future\nof industries around the world.\nV. PROJECTS\nThis section presents the exciting projects developed using\nGPT model technologies for different applications mentioned\nin the above sections. Table. IV, Table. V shows the different\nlevels of such projects along with different parameters to\ncompare their characteristics leveraging the capabilities in\nmany real-life applications.\nA. SiriGPT\nSiri [175] is an intelligent digital assistant that enables\nApple device users to complete tasks more ef\ufb01ciently and\nwith ease, often anticipating their needs even before they make\nrequests. SiriGPT [176] [177] is a voice assistant powered by\na GPT model and developed entirely using shortcuts. Apple\ndevice users can utilize ChatGPT, fueled by GPT-3, by using\nan API key provided by OpenAI. This novel combination\noffers the best of both worlds, allowing users to utilize\nSiriGPT for voice commands and ChatGPT for generating\ntext. SiriGPT utilizes a tokenizer exclusively developed by\nApple that has been optimized for processing natural language\ntasks. SiriGPT\u2019s training data is not publicly available as it is\nexclusive to Apple. However, the language model is trained\non diverse text data from various sources such as books,\nnews articles, web pages, and other text data sources. This\nensures that SiriGPT can handle different natural language\ntasks accurately and ef\ufb01ciently. It has been reported that\nSiriGPT is one of the largest language models available, with\nover a trillion parameters.\nB. AI Dungeon\nLatitude, a startup based in Utah, created a groundbreaking\nonline game called AI Dungeon [178], which showcased a\nnovel type of collaboration between humans and machines.\nIt is a free-to-play, single-player, and multiplayer adventure\ngame that caught traction within the gaming community. It\ncombines fantasy and AI to create endless possibilities, e.g.,\none can take charge of a military operation to defend against\naliens or become a famous detective investigating an attempted\nmurder of the queen of the fairies. Unlike games with prede-\ntermined storylines, AI Dungeon allows you to guide the AI\nto generate unique characters and scenarios for your character\nto interact with. The game boasted about incorporating the\nGPT-3 text generator, but then the algorithm began producing\nunsettling narratives, including graphic depictions of sexual\nencounters involving minors [179].\nC. Copy.ai\nCopy.ai [180] is a mighty AI startup founded by Paul\nYacoubian in 2020. This project is created using GPT-3,\nmainly targeting business and marketing campaigns. It has the\nfollowing use cases: (i) For Teams: It assists with producing\ncustomized sales copy, composing long-form articles and\npages on a large scale, reusing content on various platforms,\nand creating product descriptions; (ii) For Emails: The AI-\npowered email writer takes care of the most challenging parts\nof marketing by creating email campaigns that are highly\neffective at converting leads, all with just a few clicks of\na button; (iii) For Blogs: By generating content briefs and\ncrafting one-of-a-kind SEO-focused blog articles every month,\nit can save a signi\ufb01cant amount of money for the business.\nIn addition, it\u2019s feasible to create briefs, outlines, and even\ninitial drafts in mere minutes, which can be utilized as an\nexcellent source of inspiration for writers to create high-quality\ncontent; (iv) Social Media: It aids in generating social media\nposts quickly and ef\ufb01ciently, allowing for a rapid expansion\nof the social media following. Additionally, Copy.ai includes\na suite of other tools, such as a headline analyzer, a language\ntranslator, and a content rephrase.\nD. Bond.AI\nBond.AI [181] is a company focused on AI for \ufb01nancial\ninstitutions, which has a headquarters in Little Rock, Arkansas.\nIt was established by Uday Akkarajuin in 2016 and prided\n\n29\nTABLE IV\nPROJECT SUMMARY TABLE.\nProject\nDeepScribe\nMeena\nJukebox\nUber\u2019s plato research dialogue system\nPolyglot AI\nSiriGPT\nApplication\nwidely used for\nHealthcare\nLifestyle\nEntertainment\nTransport\nEducation\nLifestyle\nPurpose\nMedical\ndocumentation\nand to improve doctor-\npatient association\nPersonalized product rec-\nommendation\nEnables the original mu-\nsic creation both artisti-\ncally compelling and com-\nmercially viable in a vari-\nety of styles and genres\nEnhances user experience using Uber\nrides,\nhelps\ndrivers\nand\nriders\nin\nscheduling rides, navigating routes, pro-\nviding real-time updates on traf\ufb01c and\nweather conditions.\nenables absolute communication irre-\nspective of the language barrier across\ndifferent regions and cross-culturalism\nAssist with voice-based assistants\nGPT Adoption\nCustomized\nversion\nof\nGPT\u2019s\nGoogle\u2019s\nseq2seq\ntransformer-based\nneural\nnetwork\narchitecture\nsimilar to Open AI\u2019s GPT\nGPT-2\nextension\ncalled\n\u201dMulti-Scale\nTransform-\ners for Music Modeling\u201d\n(MST) model\nGPT-2\nGPT-0, GPT-1,GPT-2,GPT-3\nGPT-3\nDataset\nNot Disclosed\nMeena dataset over 40 bil-\nlion words , 341 GB cap-\ntured from public domains\nlike Reddit and social me-\ndia platforms\n1.2 million songs, 600,000\npieces\nof\nsheet\nmusic,\n45,000 MIDI \ufb01les\nPersona-Chat\nwith\n160,000\nconver-\nsational\ndialogues,\nCornell\nMovie-\nDialogs Corpus with 200,000 movie\nconversation, DailyDialog over 13,000\ndialogues, and\nCONLL-2003, Sentiment140 dataset,\nReuters\nCorpus,\n20\nNewsgroups\ndataset, WMT (Workshop on Machine\nTranslation)\ndatasets\nand\nSQuAD\n(Stanford Question Answering Dataset)\nInformation not publisized\nBuilding Blocks\nRecurrent Neural Network\nand Attention mechanism\nfueled by NLP techniques\nSeq2Seq\nTransformer-\nbased Architecture\nTransformer-based\nLanguage\nModel\nand\nAutoregressive model\nLanguage modeling, Dialogue model-\ning, Discrete latent variabe modeling\nand response ranking\nLanguage Identi\ufb01cation, Named Entity\nRecognition (NER), Sentiment Analy-\nsis, Text Classi\ufb01cation, Machine Trans-\nlation, Question Answering\nTransformer-based neural network ar-\nchitecture\nEvaluation\nMet-\nrics\nBleu score, perplexity\nBleu score, perplexity\nFrechet\nAudio\nDistance\n(FAD)\nand\nPitch\nand\nRhythm Similarity\nBleu score, Perplexity and Distinct n-\ngram\naccuracy, precision, recall, , F1-score,\nBleu score as well as cross-entropy loss\nor perplexity\nPerplexity,\nBLEU\nscore,\nF1\nscore,\nROUGE score, Human evaluation\nAddressed Chal-\nlenges\nReduced Transcription er-\nrors and enhanced patient\ncare\nNatural\nand\nEngaging\nconversations\nFresh orginical music con-\ntent creation and drasti-\ncally reducing the cost\nand time by creating high-\nquality\nmusic\ncontents,\nand also to preserve and\nadvance musical heritage.\ncustomer service, user experience, and\noperational ef\ufb01ciency\nMultilingualism and Sentiment Analy-\nsis are the key challenges in NLP and\nPolyglot AI solved this problem by\noffering a tool for supporting morethan\n40 languages and pre-trained sentiment\nanalysis model\nLanguage understanding and genera-\ntion, Data scarcity, Contextual under-\nstanding, Text summarization, Senti-\nment analysis, Named entity recogni-\ntion\nInput data\nAudio\nText\nAudio\nText\nText\nAudio\nOwned By\nDeepScribe\nGoogle\nOpenAI\nUber\nUizard Technologies\nApple\nTABLE V\nPROJECT SUMMARY TABLE (CONTINUED).\nProject\nAI Dungeon\nCopy.ai\nBond AI\nViable\nAI Channels\nFire\ufb02ies.ai\nApplication\nwidely used for\nGaming\nBusiness and marketing\nFinance\nBusiness Analytics\nAI Industry\nBusiness\nPurpose\nInteractive and engaging\nstorytelling experience for\nplayers\nhelp clients create written\ncontent more quickly and\neasily\nTo enhance the \ufb01nancial\nwell-being of clients\nprovide businesses with intelligent in-\nsights to help them make better deci-\nsions\nprovide a platform for developers, data\nscientists, and machine learning prac-\ntitioners to create, deploy, and manage\ntheir AI models\nto simplify the meeting process and\nreduce the time and energy required for\nnote-taking and collaboration\nGPT Adoption\nGPT-3\nGPT-3\nGPT-3\nGPT-4\nGPT-3\nGPT-4\nDataset\nCommon Crawl, OpenAI\nGPT-2, and various text\ndatasets from Kaggle\nbooks, articles, and web-\nsites\nlikely use of a combina-\ntion of publicly available\n\ufb01nancial datasets, propri-\netary data, and client data\nInformation not publisized\nUsers\u2019 own dataset\nPossible datasets: the Common Voice\ndataset from Mozilla having over 9,000\nhours of speech data in multiple lan-\nguages\nBuilding Blocks\nMachine\nLearning\nModels,\nText\nInput\nInterface, Game Engine,\nContent Database, Player\nFeedback System, Cloud\nInfrastructure\nNLP, Language Models,\nNeural Networks\nNLP,\nPersonalization,\nConversational\nUser\nInterface, Data Analytics\nUnsupervised learning, Contextual un-\nderstanding, Sentiment analysis, Topic\nmodeling, Entity recognition\nPre-built models, Model training, Data\npreparation, Collaboration\nSpeech-to-Text\nTechnology,\nNLP,\nCloud\nComputing,\nIntegration\ntechnologies\nEvaluation\nMet-\nrics\nResponse Coherence, Re-\nsponse Diversity, Player\nSatisfaction, Engagement,\nRealism, Novelty\nPerplexity, BLEU score,\nROUGE score, F1 score\nIntent\nrecognition\naccuracy, entity extraction\naccuracy,\nand\nlanguage\nmodel perplexity\nPerplexity, Accuracy, F1 score, Word\nsimilarity\nAccuracy, Precision and Recall, F1\nScore, Perplexity, User satisfaction\nSpeech Recognition Accuracy, NLP\nPerformance, Integration Performance,\nTask Completion Time, User Satisfac-\ntion\nAddressed Chal-\nlenges\nNarrative\nGeneration,\nContent\nCreation,\nPersonalization,\nReplayability,\nAccessibility,\nCreative\nExpression\nLack of writing skills, In-\nconsistency,\nMultilingual\ncontent creation\nPersonal\n\ufb01nancial\nman-\nagement,\nCustomer\nen-\ngagement, Fraud detection\nand prevention\nUnderstanding\nunstructured\ndata,\nContextual\nunderstanding,\nVisualization and exploration of data,\nCustomization and integration\nNatural language understanding, Scala-\nbility, Personalization, Integration with\nother systems, Maintenance and up-\ndates\nTime-consuming manual note-taking,\nDif\ufb01culty in capturing important de-\ntails, Lack of visibility and accountabil-\nity, Communication barriers\nInput data\nText\nText\nAudio and Text\nText\nText\nAudio\nOwned By\nLatitude\nCopy.ai\nBond.AI\nViable AI\nMiroMind AG\nFire\ufb02ies AI\nitself on providing AI technology centred around human needs.\nThis innovative project offers a product named BondBot,\nwhich is powered by Empathy Engine 3.0 and ChatGPT, to\nenhance the \ufb01nancial health of clients. It assists \ufb01nancial insti-\ntutions and employers in promoting interconnected \ufb01nance by\noffering various tools to improve the institution\u2019s pro\ufb01tability\nand the \ufb01nancial health of its clients on a single network.\nIt uses customer data to create individual personas for every\nbank customer or small business, considering their behaviours,\nstrengths, and potential needs. This approach enables the plat-\nform to develop multiple customized pathways to holistically\nenhance clients\u2019 \ufb01nancial well-being.\nE. Viable\nViable [182] is a platform powered by GPT-4 that utilizes\nthe latest advancements in NLP and AI to offer businesses\nintelligent insights to aid their decision-making processes.\nCompanies can extract actionable insights from unstructured\ndata sources, such as social media posts, customer reviews,\nand survey responses, by employing Viable. GPT assists in\ncomprehending the sentiment and context behind the data,\nresulting in valuable insights that can enhance a company\u2019s\nservices, products, and customer experience. Viable\u2019s \u201dInsight\nExplorer\u201d is a distinctive feature that enables users to interact\nwith and visualize their data via a user-friendly interface. In\naddition, the platform offers advanced analytics capabilities,\nincluding entity recognition, topic modelling, and sentiment\nanalysis. The GPT-based technology of Viable is continually\nevolving and advancing, which allows the platform to deliver\nmore precise and insightful data. Moreover, Viable provides\ncustomized integration and solutions to cater to the speci\ufb01c\nrequirements of each business.\nF. AI Channels\nAI Channels [183] is a platform that provides a comprehen-\nsive set of tools for developers, data scientists, and machine\n\n30\nlearning practitioners to develop, launch, and manage their AI\nmodels. The platform offers an all-in-one solution for creating\npersonalized AI models, starting from data preparation and\nmodel training to deployment and monitoring. Users can train\ntheir models on their data or, on pre-trained models provided\nby AI Channels. These models can be deployed as APIs\nor Docker containers on various infrastructures, including\ndifferent cloud platforms. It also provides a dashboard for\ntracking model performance and managing con\ufb01gurations. It\ncovers various use cases, including computer vision, NLP, and\nspeech recognition. The platform includes pre-built models for\ntasks such as image and text classi\ufb01cation, object detection,\nand sentiment analysis. Additionally, users can create their\nmodels using popular frameworks. The main objective of AI\nChannels is to make building and launching AI models more\naccessible to developers and businesses without specialized AI\nskills.\nG. Fire\ufb02ies.ai\nFire\ufb02ies AI [184] is a privately held company based in San\nFrancisco, California, founded by Krish Ramineni and Sam\nUdotong. Fire\ufb02ies AI software is powered by GPT-4 to au-\ntomate notes-taking tasks and collaborations during meetings.\nIt is compatible with various video conferencing platforms,\nincluding Zoom, Google Meet, and Microsoft Teams, and it\ncan transcribe meeting audio and video content in real time.\nIts primary function is based on speech-to-text technology,\nwhich enables it to generate a searchable transcript of the\nmeeting, which can be used for later review and to recall\nessential points and action items. Additionally, the software\nutilizes NLP capabilities that can identify signi\ufb01cant keywords\nand phrases within the conversation. Apart from note-taking,\nFire\ufb02ies AI includes collaboration tools such as assigning tasks\nand sharing notes with other team members. It can integrate\nwith project management and task tracking tools to auto-\nmatically generate tasks based on the identi\ufb01ed action items\nduring the meeting. Fire\ufb02ies AI provides several customization\noptions to suit particular use cases and work\ufb02ows. Users can\ncon\ufb01gure the software to automatically join speci\ufb01c meetings\nor capture audio only from speci\ufb01c speakers. It allows users\nto specify particular words and phrases to highlight in the\ntranscript, making it easier to identify critical points during the\nlater review. Thus, Fire\ufb02ies AI aims to simplify the meeting\nprocess and reduce the time and energy required for note-\ntaking and collaboration.\nH. Uber\u2019s Plato Research Dialogue System\nUber\u2019s AI Lab introduced Uber\u2019s Plato Research Dialogue\nSystem in 2020 developed by a team of researchers and engi-\nneers to enable the intelligence in riding experience. PLATO -\nPre-trained Dialogue Generation Model with Discrete Latent\nVariable [185]. Uber\u2019s Plato Research Dialogue System uses\nGPT-2, a large-scale language model developed by OpenAI\nin 2019. Uber\u2019s Plato Research Dialogue System project\nused several datasets to train and evaluate their conversa-\ntional agents such as Persona-Chat contains 160,000 conversa-\ntional dialogues, Cornell Movie-Dialogs Corpus with 200,000\nmovie conversations, DailyDialog over 13,000 dialogues, and\nEmpatheticDialogues over 25,000 user dialogues. The main\ncomponents in developing the GPT-powered PLATO project\nare language modelling, dialogue modelling, discrete latent\nvariable modelling and response ranking. The Plato Research\nDialogue System was trained on a massive corpus of text data\nconsisting of over 40 GB of uncompressed text while Bleu\nscore, Perplexity and Distinct n-gram are the evaluation met-\nrics used for training and testing the PLATO project. Uber\u2019s\nAI PLATO has addressed many key challenges like customer\nservice by personalizing user feedback with conversational AI\nagent, user experience using the Uber platform for scheduling\nrides, navigating routes, and providing real-time updates, and\nincreasing operational ef\ufb01ciency by reducing the need for\nhuman customer service representatives and enabling faster\nand more accurate communication between riders, drivers, and\nthe Uber app.\nI. Jukebox\nJukebox, a GPT-powered music creation, was developed\nin 2020 as an extension of Open AI\u2019s GPT language model\n[186]. Jukebox\u2019s goal is to push the boundaries of what AI can\naccomplish in the world of music creation and to investigate\nfresh applications for AI. A variation of the GPT architecture,\nthe \u201dMulti-Scale Transformers for Music Modeling\u201d (MST)\nmodel, was created speci\ufb01cally to handle the intricate and\nmulti-scale nature of musical data. Additionally, Jukebox can\nproduce lyrics that match the music\u2019s tone and style. A sizable\nand varied dataset of musical recordings, lyrics, and related\nmetadata was used to train Jukebox such as 1.2 million songs\nsourced including Lakh MIDI Dataset, Free Music Archive,\nSpotify and Tidal, 600,000 pieces of sheet music were sourced\nfrom IMSLP (International Music Score Library Project),\nand 45,000 MIDI \ufb01les from Lakh MIDI Dataset and the\nMIDIworld collection. Faster training times and more effective\nuse of computational resources were made possible by the\ndistributed computing setup with 2048 TPU( Tensor Process-\ning Unit) cores used to train the Jukebox model. Training the\nmodel required signi\ufb01cant computational resources demanding\nfaster training times by the distributed computing setup with\n2048 TPU (Tensor Processing Unit) cores used to train the\nJukebox model. A combination of subjective and objective\nmetrics was used to assess and test Jukebox. In a large-scale\nsubjective assessment, more than 1,000 participants listened\nand rated each one individually determining the overall score\nfor each song produced. On the other side, objective assess-\nments were conducted by evaluating Frechet Audio Distance\n(FAD) and Pitch and Rhythm Similarity. Overall, Jukebox\nrevolutionizes with its signi\ufb01cant advancement in the music\nindustry through creative inspiration, music production, music\neducation and preservation of music heritage.\nJ. Meena\nGoogle\u2019s Meena project was developed by Google Research\nTeam in 2020 for providing personalized product recom-\nmendations [187]. The primary goals of the Meena project\nempowered the lifestyle sector to enhance the user experience\n\n31\nand customer service by recommending goods and services\non a personalized basis. The project designed a GPT using\nthe seq2seq transformer-based neural network architecture,\nin particular for open-domain conversational agents. The ar-\nchitecture was pre-trained over 341 GB of text captured\nfrom Reddit and other social platforms containing over 40\nmillion words and called this massive collection as \u2019Meena\nDataset\u2019. Meena was tested using the automated performance\nmetrics known as Bleu score and perplexity on a cluster\nof HPC nodes with a total of 2048 NVIDIA V100 GPUs.\nOne of the biggest challenges solved Meena was building\ntrust and generating reliable engaging human-like conservation\nthat typically enhances user satisfaction and personalization.\nMeena has achieved state-of-the-art performance compared to\nother open-domain chatbots and revolutionized the wide range\nof applications in the lifestyle industry and a way beyond\nby providing natural and engaging responses through virtual\nassistants, customer service bots and personal shoppers.\nK. DeepScribe\nDeepScribe was a GPT-based medical project developed\nin 2019 by the student team at the University of California\nby partnering with giant US-based healthcare providers such\nas One Medical, Stanford Medicine, Mount Sinai and Sutter\nHealth [188]. The DeepScribe\u2019s technology aims at transcrib-\ning medical conversation allowing doctors to treat the patients\nrather than noting down the patient\u2019s history, enhancing the\ndoctor-patient relationship and targeting the overall quality\nof patient care. Although DeepScribe used the customized\nvariants of Open AI, the technical details of the GPT model\nused for customizing the model were not disclosed which was\noptimized for medical transcription tasks.\nL. Polyglot AI\nPolyglot AI is a communication platform designed to gen-\nerate text in multiple languages and process the data by\nperforming several tasks such as advanced NLP techniques,\ntext translation, and sentiment analysis. The potential features\nof Polyglot AI have been exploited in the following application\nareas such as language translation, chatbots, language learning\ntools, content creation, customer support, and data analysis\nacross different languages and regions. Polyglot AI is built\nbased on different variants of GPT models, and state-of-the-\nart language model architecture for NLP tasks, which uses the\nself-supervised learning approach.\nThe Polyglot AI was pre-trained using a large amount\nof textual data on multiple languages simultaneously in an\nunsupervised environment using a shared architecture, Multi-\nlingual Universal Sentence Encoder (MUSE). MUSE devel-\noped by Google, is a pre-trained DL model used for cross-\nlingual TL, that encodes the text into common vector space\nfor multiple languages. Thus, the Polyglot language model\nwas created with the following pre-training techniques as\nMasked Language Modeling (MLM), Translation Modeling\nLanguage (TML), sequence-to-sequence modelling and cross-\nlingual TL. The pre-trained language model is \ufb01ne-tuned and\nevaluated by standard benchmarks and metrics such as the\nBLEU score (Bilingual Evaluation Understudy), METEOR\n(Metric for Evaluation of Translation with Explicit ORdering)\nor F1-score. Remarkably, Facebook used new Polyglot AI\nto translate between 100 languages [189]. Thus Polyglot AI\nenables absolute communication irrespective of the language\nbarrier across different regions and cross-culturalism.\nThus, this section focused on several exciting real-life\nprojects which are developed and used for humankind. These\nprojects were discussed by presenting Table ?? highlighting\nthe details of the project with model architecture, datasets\nused, training and testing, and evaluation metrics involved with\nthe challenges addressed. The next section will discuss the\nopen research issues and future directions for the potential\nbene\ufb01ts of GPT models.\nVI. OPEN RESEARCH ISSUES AND FUTURE DIRECTIONS\nThis section highlights the various open research issues con-\ncerned with the implementation and adoption of sustainable\nGPT models. It also provides insights into future research\ndirections for the betterment of researchers in the \ufb01eld of\nGPT development. Fig. 9 outlines the many issues that can\ndevelop while using GPT models, as well as the various future\napproaches that need to be considered for the effective usage\nof GPT models.\nA. Domain Speci\ufb01c GPT models\nDomain-speci\ufb01c GPT models are mandated in almost all\napplications; developing these models is still challenging and\nan open issue within GPT. While the current GPT models have\nbeen developed to understand natural language and generate\ncontent effectively, their performance may not be equally\neffective when handling speci\ufb01c domains, such as medicine,\nagriculture, etc. One of the key challenges in adapting to a\nparticular domain is the availability of domain-speci\ufb01c data.\nIt is well known that the performance of GPTs is directly\nproportional to the quality and quantity of data used for train-\ning the model. So, obtaining such quality data for a speci\ufb01c\ndomain is expensive and time-consuming, as the data are\nheterogeneous. Also, these data accumulations may even make\nthese models much larger, sometimes catastrophic too, leading\nto forgetting the knowledge attained during the process. To\novercome this issue, pre-training tasks and domain-speci\ufb01c\nmodel generation are integrated by data augmentation [190].\nAnother challenge is \ufb01ne-tuning the model to accustom to the\nunique characteristics and vocabulary of the domain. A few\ndomain-speci\ufb01c GPT models have been developed and imple-\nmented despite these challenges. There is a growing interest\nin creating more domain-speci\ufb01c GPTs for various domains.\nMoreover, these models will be trained using the knowledge\nacquired from large language models speci\ufb01c to domains.\nTherefore, these models can be \ufb01ne-tuned for speci\ufb01c tasks\nor domain-speci\ufb01c requirements with gradually improving\nperformance. GPT models have the potential to be trained in\nany context, and researchers are exploring new approaches\nand methods to address these challenges. Furthermore, these\nmodels will be more ef\ufb01cient, enhanced interpretability, and\ndomain generability than the existing Large language models\n\n32\nFig. 9. Challenges and Future Directions.\nas they are customized to speci\ufb01c domain concerns and can\nprovide more concise and informative solutions. TL can be\nused for developing domain-speci\ufb01c GPT models. Domain-\nspeci\ufb01c GPT models were developed to summarize products\nbased on customer reviews on an E-commerce site, where the\nlanguage model is pre-trained on the Chinese-short summa-\nrization dataset and has obtained \ufb01ne-tuned results [130]. Be-\nsides these challenges, domain-speci\ufb01c models require higher\ncomputation costs for the resources and time spent in pre-\ntraining and relearning in downstream tasks during \ufb01ne-tuning\nof pre-trained domain-speci\ufb01c models. Therefore, domain-\nspeci\ufb01c model development must focus on optimizing resource\nconsumption and \ufb01ne-tuning the pre-trained model to alleviate\nthe forgetting problem involved in existing models.\n\n33\nB. High Computational requirements\nAs the Transformer model utilizes varied heterogeneous\ndatasets for training and learning from the knowledge ac-\nquired, one of the key challenges of GPT models is high\ncomputational resources for pre-training and inference. The\ncomputational requirement continuously increases as the mod-\nels become more complex and larger. Depending on the size\nand complexity of the model and the available resources, the\ntime required to train the model can take days, weeks, or\neven months. Moreover, the inference time for these models\nis typically slower, making it challenging to use them for\nreal-time applications. This poses a signi\ufb01cant obstacle to\nadopting GPT models for many practical applications. Despite\nthese challenges, signi\ufb01cant efforts are underway to overcome\nthem. To accommodate the increasing data size and pre-\ntraining computational requirement, data enhancement-based\nGPT models were developed [190] by joining the downstream\ntasks and pertaining process by reconstructing the domain-\nspeci\ufb01c text before proceeding for pre-training and utilizing\nthe empirical knowledge rather than learning for falsy domain\ndata. Researchers are exploring various ways to optimize and\nspeed up the training and inference process, such as using\nspecialized GPUs and TPUs. They are also developing more\nef\ufb01cient algorithms and attempting to reduce the model size\nwithout sacri\ufb01cing performance. In addition, ChatGPT has\nevolved to include plugins [191] that enable statistical analysis\nfor real-time applications. By integrating these plugins with\nthe help of third-party services, ChatGPT can now be used\nfor analyzing real-time applications as well.\n1) Increasing Model size and Space Constraints: Devel-\noping and training large language models, such as a GPT\nmodel, can be a challenging task due to signi\ufb01cant technical\nand computational dif\ufb01culties as discussed. The size of GPT\nmodels presents a major challenge, as the computational\nresources required for training and inference increase with\nthe number of parameters that need to be trained. As the\nmodel size increases, it also requires more memory to store\nand manipulate parameters during training and inference [76].\nAcquiring and processing vast amounts of high-quality training\ndata is another challenge in training large language models like\nGPT. For instance, GPT-4, which is the largest GPT model to\ndate with 1 trillion parameters, demands a massive amount\nof computational resources, such as specialized hardware like\nGPUs and TPUs, spatial requirements, and high-speed network\nconnections to transfer data between different parts of the\nsystem. Model evaluation and interpretation are also critical\nchallenges. Since large language models like GPT are trained\non a massive scale, understanding how the model makes\npredictions and why it generates speci\ufb01c outputs is dif\ufb01cult.\nEvaluating the quality and accuracy of the model\u2019s output and\nidentifying and addressing biases or errors in its performance\ncan also be challenging.\nAs these and other signi\ufb01cant efforts continue, we can\nexpect the challenge of computational resource requirements\nfor GPT models to transform into a strength in the future.\nC. Explainability and interpretability\nExplainability and interpretability are currently major chal-\nlenges for GPTs for speci\ufb01c applications. Explainability refers\nto providing a clear and understandable explanation of how\nthe model has arrived at any output. Interpretability, on the\nother hand, refers to the ability to understand the internal\nprocesses of the model. GPT models are highly complex\nand dif\ufb01cult to understand and interpret due to their size\nand architecture. The outcomes and decisions of the model\nare based on previous learning and training, and the models\nlearn from vast amounts of data to make decisions. These\ndecisions may not be easily explainable to humans. This lack\nof transparency and interpretability raises concerns about the\nreliability and safety of the model, particularly in critical ap-\nplications such as healthcare and \ufb01nance. Researchers are cur-\nrently conducting much research to make GPT models more\nexplainable and interpretable [192] by utilizing EXplainable\nArti\ufb01cial Intelligence (XAI) to provide explanations for the\ndecisions arrived at, speci\ufb01cally to different users at stake. As\nwell, XAI models enable interpretability by providing detailed\nexplanations for the internal process. As GPT can generate\nany type of unconstrained output for instance code generation\nfor the given problem, it requires proper justi\ufb01cations and\nexplanations for the output. So, to assure these codes by\nGPT are reliable, a metric model to evaluate and validate\nthis GPT code was developed using NLP metrics and XAI\nfor model interpretability [193]. Also, some domain-speci\ufb01c\nGPT models of GPT-3 have evolved with solutions [194] [195]\nto ensure that the GPT model\u2019s decisions are understandable,\nexplainable, and trustworthy enough to be used for critical\napplications like healthcare and \ufb01nance.\nD. Data Bias\nData bias is an open issue concerned with the adoption\nof any advancements in AI, till GPT [196]. This is also\na prominent challenge for GPT and other machine-learning\nmodels. It refers to patterns or relationships in the data that\ndo not accurately re\ufb02ect the true distribution of the target\npopulation or domain. GPT models are trained on vast amounts\nof text data which may contain bias in language use or cultural\nassumptions. Still, the source of data remains undeclared, con-\nsidering GPTs are trained using internet data which may have\nfaulty, fake, and error data, GPTs may generate biased texts or\ninformation imitating the training data [197]. Such biases can\nbe ampli\ufb01ed in the model\u2019s output, resulting in false or unfair\nresults. Data bias can arise from various sources, such as selec-\ntion bias, labelling bias, concept drift, confounding variables,\nand changes in input data distribution over time. For example,\nsuppose a dataset used to train a GPT model is dominated\nby a particular demographic group. In that case, the resulting\nmodel may be biased in its predictions towards that group,\nleading to inaccurate or unfair predictions when applied to new\ndata. This bias can have serious consequences, especially in\nhealthcare, \ufb01nance, and law enforcement, where biased results\ncan signi\ufb01cantly impact human lives. To mitigate these issues,\nresearchers have developed strategies such as diversifying the\ntraining data, debiasing the training data, modifying the model\n\n34\narchitecture, and using post-processing methods to normalize\nthe data and create more fair and inclusive GPT models.\nThe authors in [198] have made an in-depth analysis of the\nmost downloaded text generation model GPT2. By examining\nthe intersections of gender with religion, sexuality, ethnicity,\npolitical af\ufb01liation, and continental name origin, the authors\nevaluated prejudices associated with occupational associations\namong various protected categories. These biases may have\ninaccuracies in climatic data prediction or global warming\n[199]. Therefore, data bias must be of greater concern in GPT\nmodel development as the data quality of the internet is limited\nto avoiding producing disturbing content.\nE. Multimodal support\nThe challenge of developing multimodal learning ability in\nthe GPT model remains unsolved. Multimodal support refers\nto the GPT model\u2019s ability to process and generate text along\nwith other modalities, such as audio, images, and videos. GPT\nmodels have shown impressive results in generating high-\nquality text and NLP tasks, but it was primarily designed for\ntext-based tasks and cannot handle other modalities. However,\ndue to its success in text processing, users expect its integra-\ntion with other modalities, such as speech recognition, video\nsummarization, and image or video captioning [200]. Several\nresearch initiatives have been proposed to integrate multimodal\nsupport to address this issue. One approach is to feed the visual\nand audio information with the corresponding text to the model\nas input. The other is to handle this input modality process as a\nseparate model and use the output as input to GPT. Multimodal\nvideo captioning is done using GPT in the unlabelled videos\n[200]. Multimodel learning has been applied for information\nretrieval [201] and image generation for illustrating the news\n[202] to assist the GPTs. However, the primary challenge in\nboth approaches is effective integration, requiring architectural\nchanges and techniques to handle various modalities. Recently,\nOpenAI\u2019s GPT4 has launched with multimodal support, en-\nabling it to read images, analyze the input, and generate text as\noutput. It cannot create images as output, though. Nevertheless,\nthe \ufb01eld of multimodal processing is still an active area of\nresearch, and much work must be done to effectively and\nef\ufb01ciently process and understand multimodal data.\nF. Robustness\nThe robustness is a major requirement to be imposed by any\ntype of GPT model, and it is a global problem for all learning-\nbased prediction technologies. Robustness refers to the ability\nof the model to maintain high performance and accuracy even\nin the face of unexpected or adversarial inputs. Although GPT\nmodels have shown impressive performance in a wide range of\nNLP applications and have set a benchmark for high-quality\ntext generation, they are still vulnerable to certain types of\nerrors and attacks. In particular, handling adversarial inputs is a\nchallenging task in GPT models. GPT models are particularly\nsusceptible to adversarial attacks [203]. Adversarial inputs are\nspeci\ufb01cally designed to make a learning model collapse and\nmisbehave. GPT models can be highly prone to these attacks\nbecause they are trained on a large volume of text. As a\nresult, they may be in\ufb02uenced by subtle patterns or biases in\nthe training data. If such biases or patterns exist in the data,\nthe GPT model may amplify or perpetuate existing biases,\nleading to unfair outcomes. A few techniques may be used,\nsuch as adversarial training [204] [205], defensive distillation\n[206], and regularization techniques [207] such as dropout,\nweight decay, and batch normalization, to mitigate and handle\nadversarial inputs. Therefore, GPT development must focus on\ndeveloping models with more robustness, enabling them to be\ntolerant of various vulnerabilities, and thus to be used reliably\nand susceptible in a wide range of applications.\nG. Multilingual support\nWhile GPT models have demonstrated remarkable pro\ufb01-\nciency in NLP tasks for individual languages, achieving mul-\ntilingual support remains a signi\ufb01cant challenge. The primary\ndif\ufb01culty in developing multilingual GPT models lies in the\nsigni\ufb01cant differences in syntax, grammar, and vocabulary\nacross various languages. As the number of internet users\nday by day increasing irrespective of literacy rate, multilingual\nsupport will target all types of end users. To create models that\ncan effectively process multiple languages, researchers need to\ntrain GPT models on extensive, diverse datasets that span a\nbroad range of languages and language families. Addition-\nally, designing language-speci\ufb01c pre-processing techniques\nto prepare input data for the model is another obstacle to\novercome. Various languages possess distinct writing systems,\nword orders, and linguistic features, necessitating specialized\npre-processing techniques to ensure that the model can process\nthe input data effectively. Despite the challenges, researchers\ncontinue to explore new methods to improve the multilingual\ncapabilities of GPT models. Some techniques involve training\nseparate models for each language or developing language-\nspeci\ufb01c \ufb01ne-tuning techniques. Others include developing\ncross-lingual TL techniques that allow the model to transfer\nknowledge and skills learned in one language to another.\nH. Limited understanding\nGPT models have a limited understanding of context and\nmeaning, despite their ability to generate coherent text. This\nproblem arises due to issues such as a lack of semantic\nunderstanding, bias, stereotyping, and handling nuances and\n\ufb01gurative language. As a result, the outputs generated by the\nmodel may contain errors or inaccuracies, even if they are\ngrammatically correct. Researchers are exploring various tech-\nniques to enhance the model\u2019s contextual understanding. Un-\nderstanding GPTs will be more reactive and may attract more\nusers for accurate results [208]. These methods include incor-\nporating external knowledge sources like knowledge graphs\nand ontologies into the training process, developing common\nsense reasoning capabilities, and improving the model\u2019s ability\nto handle nuances and idiomatic expressions. By enhancing the\ncontextual understanding of GPT models, their outputs will\nbe more accurate, relatable, sequential, less biased, and more\nuseful for a variety of applications.\n\n35\nI. Ethical Concerns\nThe ethical concerns in GPT models are an active area of\ndiscussion and debate due to the potential negative impacts that\nthe use of GPTs could have on society. Although GPT models\nhave demonstrated remarkable abilities in generating coherent\nand realistic text, there are concerns about the perpetuation\nof biases and stereotypes, the possibility of malicious use,\nand the effects on employment and economic inequality.\nSome of the ethical characteristics to be possessed by GPT\ninclude functional Morality, operational morality, abiding by\nthe right for explanation law, improved transparency with\nhuman involvement, unbiased data, and adhering to govern-\nment regulations on data usage [196]. The responsibility of\ndevelopers and companies to address these ethical concerns\nand ensure the ethical use of a GPT model is also a topic\nof debate. The ethical implications of GPT models are being\nactively researched and discussed in the \ufb01elds of AI, computer\nscience, and philosophy.\nJ. Security and privacy concerns\nGPT models raise concerns about security and privacy,\nparticularly as they become more widespread. One of the main\nconcerns is that GPT could be used for harmful purposes, such\nas creating fake news or deep fakes, as it can generate text that\nlooks real and convincing, making it dif\ufb01cult to distinguish\nbetween genuine and fake content. Another concern is the po-\ntential for privacy violations when using a GPT model. Large\nlanguage models like GPT require a signi\ufb01cant amount of\ntraining data, which could contain sensitive or personal infor-\nmation. This raises concerns about privacy and data protection\nas per European Union\u2019s General Data Protection Act [209],\nparticularly if the training data is not properly anonymized\nor if the models are used to generate text based on user data\nwithout their explicit consent. Some of the problems concerned\nwith con\ufb01dentiality related to the pre-training dataset are\nData tracing, Membership Inference Attacks, reconstruction\nattacks, and property inference attacks and the vulnerabilities\nconcerned with a model encoder are hyperparameter stealing\nattacks and encoder parameter stealing attacks. Poisoning,\nBackdoor, and evasion attacks are the vulnerability related to\nthe integrity of self-supervised learning. Resource depletion\nattack is one major issue with data availability, which may lead\nto tremendous effects incorrect results, and may cause greater\ndeviations too [209]. Additionally, the GPT model\u2019s ability to\ngenerate text based on user input could inadvertently disclose\nsensitive information, such as personal or \ufb01nancial details, or\ntrade secrets. This could happen if a GPT model is used in an\ninsecure environment or if it is targeted by malicious actors\nseeking to obtain sensitive information. Researchers and devel-\nopers should focus on assuring authenticity in using users\u2019 data\nin case of interactive information generation based on privacy\ndata shared. These include using differential privacy to protect\ntraining data privacy [210], implementing secure hardware or\nsoftware protocols to protect models from cyberattacks, and\ndeveloping techniques to detect and prevent the malicious use\nof GPT models. It\u2019s crucial to adopt and follow these measures\nto ensure the ethical and safe use of GPT models before using\nthem in various applications.\nTherefore, GPT model development must focus on devel-\noping more robust, reliable, safest, multi-lingual, multimodal\nsupport-enabled solutions for delivering domain-speci\ufb01c or\nhuman-speci\ufb01c solutions with optimal resource utilization.\nVII. CONCLUSION\nThe impact of GPT and other large language models is\nfar-reaching and profound. As these technologies continue to\nevolve and improve, they have the potential to transform the\nway we interact with technology and each other. From per-\nsonalized recommendations and customer service to language\ntranslation and text generation, the possibilities are endless.\nHowever, as with any technology, there are potential ethical\nand societal concerns that must be addressed. As we continue\nto rely more heavily on these language models, we must\nensure that we are using these tools responsibly and with\nconsideration for their impact on society as a whole. These\ninclude challenges related to biases in the data used to train\nthe models, safeguarding privacy and security, understanding\nthe implications of human creativity, and the potential impact\non employment and job displacement. We need to continue to\nevaluate and re\ufb02ect on the impact of GPT and other language\nmodels, to ensure that they are being used in a way that\nbene\ufb01ts society as a whole. By doing so, we can help to ensure\nthat these technologies are used to their fullest potential while\nminimizing any negative impact that they may have.\nREFERENCES\n[1] X. Han, Z. Zhang, N. Ding, Y. Gu, X. Liu, Y. Huo, J. Qiu, Y. Yao,\nA. Zhang, L. Zhang et al., \u201cPre-trained models: Past, present and\nfuture,\u201d AI Open, vol. 2, pp. 225\u2013250, 2021.\n[2] \u201dIntroducing OpenAI\u201d. [Accessed on 23.03.2023]. [Online]. Available:\nhttps://openai.com/blog/introducing-openai\n[3] L. Dong, S. Xu, and B. Xu, \u201cSpeech-transformer: a no-recurrence\nsequence-to-sequence model for speech recognition,\u201d in 2018 IEEE\ninternational conference on acoustics, speech and signal processing\n(ICASSP).\nIEEE, 2018, pp. 5884\u20135888.\n[4] B. D. Lund and T. Wang, \u201cChatting about chatgpt: how may ai and\ngpt impact academia and libraries?\u201d Library Hi Tech News, 2023.\n[5] E. Kasneci, K. Se\u00dfler, S. K\u00a8uchemann, M. Bannert, D. Dementieva,\nF. Fischer, U. Gasser, G. Groh, S. G\u00a8unnemann, E. H\u00a8ullermeier et al.,\n\u201cChatgpt for good? on opportunities and challenges of large language\nmodels for education,\u201d Learning and Individual Differences, vol. 103,\np. 102274, 2023.\n[6] X. Qiu, T. Sun, Y. Xu, Y. Shao, N. Dai, and X. Huang, \u201cPre-trained\nmodels for natural language processing: A survey,\u201d Science China\nTechnological Sciences, vol. 63, no. 10, pp. 1872\u20131897, 2020.\n[7] A. S. George, A. H. George, T. Baskar, and A. G. Martin, \u201cRevolu-\ntionizing business communication: Exploring the potential of gpt-4 in\ncorporate settings,\u201d Partners Universal International Research Journal,\nvol. 2, no. 1, pp. 149\u2013157, 2023.\n[8] C. Zhang, C. Zhang, S. Zheng, Y. Qiao, C. Li, M. Zhang, S. K. Dam,\nC. M. Thwal, Y. L. Tun, L. L. Huy et al., \u201cA complete survey on\ngenerative ai (aigc): Is chatgpt from gpt-4 to gpt-5 all you need?\u201d\narXiv preprint arXiv:2303.11717, 2023.\n[9] M. Zaib, Q. Z. Sheng, and W. Emma Zhang, \u201cA short survey of pre-\ntrained language models for conversational ai-a new age in nlp,\u201d in\nProceedings of the Australasian computer science week multiconfer-\nence, 2020, pp. 1\u20134.\n[10] Z. Liu, X. Yu, L. Zhang, Z. Wu, C. Cao, H. Dai, L. Zhao,\nW. Liu, D. Shen, Q. Li et al., \u201cDeid-gpt: Zero-shot medical text de-\nidenti\ufb01cation by gpt-4,\u201d arXiv preprint arXiv:2303.11032, 2023.\n[11] P. Rivas and L. Zhao, \u201cMarketing with chatgpt: Navigating the ethical\nterrain of gpt-based chatbot technology,\u201d AI, vol. 4, no. 2, pp. 375\u2013384,\n2023.\n\n36\nTABLE VI\nVARIOUS LESSONS LEARNED AND FUTURE RESEARCH DIRECTIONS.\nSl.No\nLessons Learned\nOpen Issues\nFuture Directions\n1.\n\u25e6Huge volume of data usage\nis critical\n\u25e6Data privacy - may unknowingly reveal sensitive\ninformation\n\u25e6Varied data quality - Inconsistency in quality of\ndata used for training\n\u25e6Scalability - Models should be able to handle an\nincrease in data set size and complexity\n\u25e6Optimized architecture and algorithms\n\u25e6Cloud-based computing\n\u25e6Hardware advancements\n2.\n\u25e6Importance of Proper Pre-\nprocessing of data\n\u25e6Data bias - Overrepresentation of certain groups\nor perspectives\n\u25e6Poor model performance\n\u25e6Reduced ef\ufb01ciency of the model\n\u25e6Continuous monitoring\n\u25e6Testing model for potential biases\n\u25e6Diversifying the training data\n3.\n\u25e6Importance of explainability\nand interpretability\n\u25e6Complexity of models\n\u25e6Inability to explain predictions\n\u25e6Need of User-tailored Explanations generation\n\u25e6Developing Interpretable models\n\u25e6Lack of transparency in the data source\n\u25e6AI governance models can be used\n\u25e6Model Summaries can be provided\n\u25e6Techniques like LIME(Local Model-Agnostic Ex-\nplanations can be used\n\u25e6Uncertaining estimates can be obtained from a\nmodel\n4.\n\u25e6Ethical concerns\n\u25e6Data privacy and data protection\n\u25e6Misuse of data\n\u25e6Accountability and transparency concerns\n\u25e6Societal implications - displacing jobs and exacer-\nbating equalities\n\u25e6Counterfactual analysis can be used\n\u25e6Federated learning can be used\n\u25e6Ethical guidelines, Legal frameworks and regula-\ntions can be developed to avoid harmful use\n5.\n\u25e6Lack of contextual under-\nstanding in AI systems\n\u25e6Possibility for ambiguous, contradictory, incorrect\nresults leads to misunderstandings\n\u25e6Inconsistency in responses or outputs\n\u25e6Lack of ability in distinguishing true and false\ninformation\n\u25e6Incorporation of knowledge graphs and semantic\nembeddings into the training process\n\u25e6Usage of attention mechanisms to focus on relevant\nparts of the input\n\u25e6Imparting reasoning and inference capabilities\n\u25e6Task or domain-based \ufb01ne-tuning\n6.\n\u25e6Pre-trained models may not\nperform\nwell\nfor\nDomain-\nspeci\ufb01c task\n\u25e6Possibility for ambiguous, contradictory, incorrect\nresults leads to misunderstandings\n\u25e6Inconsistency in responses or outputs\n\u25e6Lack of ability in distinguishing true and false\ninformation\n\u25e6Incorporation of knowledge graphs and semantic\nembeddings into the training process\n\u25e6Usage of attention mechanisms to focus on relevant\nparts of the input\n\u25e6Imparting reasoning and inference capabilities\n\u25e6Task or domain-based \ufb01ne-tuning\n[12] M. Leippold, \u201cThus spoke gpt-3: Interviewing a large-language model\non climate \ufb01nance,\u201d Finance Research Letters, vol. 53, p. 103617, 2023.\n[13] M. Trajtenberg, \u201cAi as the next gpt: a political-economy perspective,\u201d\nNational Bureau of Economic Research, Tech. Rep., 2018.\n[14] D. Haluza and D. Jungwirth, \u201cArti\ufb01cial intelligence and ten societal\nmegatrends: An exploratory study using gpt-3,\u201d Systems, vol. 11, no. 3,\np. 120, 2023.\n[15] L. J. Quintans-J\u00b4unior, R. Q. Gurgel, A. A. d. S. Ara\u00b4ujo, D. Correia,\nand P. R. Martins-Filho, \u201cChatgpt: the new panacea of the academic\nworld,\u201d pp. e0060\u20132023, 2023.\n[16] Q. Zhu and J. Luo, \u201cGenerative pre-trained transformer for design\nconcept generation: an exploration,\u201d Proceedings of the Design Society,\nvol. 2, pp. 1825\u20131834, 2022.\n[17] \u201dGPT\u201d.\n[Accessed\non\n25.03.2023].\n[Online].\nAvailable:\nhttps:\n//aidungeon.io/\n[18] M. Kosinski, \u201cTheory of mind may have spontaneously emerged in\nlarge language models,\u201d arXiv preprint arXiv:2302.02083, 2023.\n[19] \u201dGPT-1,\nGPT-2\nand\nGPT-3\nmodels\nexplained\u201d.\n[Accessed\non\n27.03.2023]. [Online]. Available: https://360digitmg.com/blog/types-\nof-gpt-in-arti\ufb01cial-intelligence\n[20] B. Ghojogh and A. Ghodsi, \u201cAttention mechanism, transformers, bert,\nand gpt: Tutorial and survey,\u201d 2020.\n[21] \u201dGenerative\nPre-trained\nTransformer\n3\nby\nOpe-\nnAI\u201d.\n[Accessed\non\n23.03.2023].\n[Online].\nAvail-\nable: https://medium.com/@shripad.kulkarni18/generative-pre-trained-\ntransformer-3-by-openai-4abe6614c8ef\n[22] N. Williams, S. Ivanov, and D. Buhalis, \u201cAlgorithmic ghost in the re-\nsearch shell: Large language models and academic knowledge creation\nin management research,\u201d arXiv preprint arXiv:2303.07304, 2023.\n[23] M.-T. Nguyen, P.-T. Nguyen, V.-V. Nguyen, and Q.-M. Nguyen, \u201cGen-\nerating product description with generative pre-trained transformer 2,\u201d\nin 2021 6th International Conference on Innovative Technology in\nIntelligent System and Industrial Applications (CITISIA), 2021, pp. 1\u2013\n7.\n[24] V. Taecharungroj, \u201c\u201cwhat can chatgpt do?\u201d analyzing early reactions\nto the innovative ai chatbot on twitter,\u201d Big Data and Cognitive\nComputing, vol. 7, no. 1, p. 35, 2023.\n[25] G. Spitale, N. Biller-Andorno, and F. Germani, \u201cAi model gpt-3 (dis)\ninforms us better than humans,\u201d arXiv preprint arXiv:2301.11924,\n2023.\n[26] J. Ye, X. Chen, N. Xu, C. Zu, Z. Shao, S. Liu, Y. Cui, Z. Zhou,\nC. Gong, Y. Shen et al., \u201cA comprehensive capability analysis of gpt-3\nand gpt-3.5 series models,\u201d arXiv preprint arXiv:2303.10420, 2023.\n[27] J. Liu, D. Shen, Y. Zhang, B. Dolan, L. Carin, and W. Chen,\n\u201cWhat makes good in-context examples for gpt-3?\u201d arXiv preprint\narXiv:2101.06804, 2021.\n[28] M. Bommarito II and D. M. Katz, \u201cGpt takes the bar exam,\u201d arXiv\npreprint arXiv:2212.14402, 2022.\n[29] T. Hagendorff, S. Fabi, and M. Kosinski, \u201cMachine intuition: Uncov-\nering human-like intuitive decision-making in gpt-3.5,\u201d arXiv preprint\narXiv:2212.05206, 2022.\n[30] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, \u0141. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d\nAdvances in neural information processing systems, vol. 30, 2017.\n[31] W. Hou and Z. Ji, \u201cGeneturing tests gpt models in genomics,\u201d bioRxiv,\npp. 2023\u201303, 2023.\n[32] S. Edunov, A. Baevski, and M. Auli, \u201cPre-trained language model\nrepresentations for language generation,\u201d in Proceedings of the 2019\nConference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, Volume\n1 (Long and Short Papers).\nMinneapolis, Minnesota: Association\nfor Computational Linguistics, Jun. 2019, pp. 4052\u20134059. [Online].\nAvailable: https://aclanthology.org/N19-1409\n[33] A. Rahali and M. A. Akhlou\ufb01, \u201cEnd-to-end transformer-based models\nin textual-based nlp,\u201d AI, vol. 4, no. 1, pp. 54\u2013110, 2023.\n\n37\n[34] J. R. Stevens, R. Venkatesan, S. Dai, B. Khailany, and A. Raghunathan,\n\u201cSoftermax: Hardware/software co-design of an ef\ufb01cient softmax for\ntransformers,\u201d in 2021 58th ACM/IEEE Design Automation Conference\n(DAC).\nIEEE, 2021, pp. 469\u2013474.\n[35] M. Bangura, K. Barabashova, A. Karnysheva, S. Semczuk, and\nY. Wang, \u201cAutomatic generation of german drama texts using \ufb01ne tuned\ngpt-2 models,\u201d arXiv preprint arXiv:2301.03119, 2023.\n[36] J. Savelka, A. Agarwal, C. Bogart, and M. Sakr, \u201cLarge language\nmodels (gpt) struggle to answer multiple-choice questions about code,\u201d\narXiv preprint arXiv:2303.08033, 2023.\n[37] H. Liu, Y. Cai, Z. Lin, Z. Ou, Y. Huang, and J. Feng, \u201cVariational latent-\nstate gpt for semi-supervised task-oriented dialog systems,\u201d IEEE/ACM\nTransactions on Audio, Speech, and Language Processing, vol. 31, pp.\n970\u2013984, 2023.\n[38] \u201dGPT-3, explained: This new language AI is uncanny, funny \u2014\nand a big deal\u201d. [Accessed on 23.03.2023]. [Online]. Available:\nhttps://www.computerhope.com/jargon/g/gpt.html\n[39] A. Hendy, M. Abdelrehim, A. Sharaf, V. Raunak, M. Gabr, H. Mat-\nsushita, Y. J. Kim, M. A\ufb01fy, and H. H. Awadalla, \u201cHow good are\ngpt models at machine translation? a comprehensive evaluation,\u201d arXiv\npreprint arXiv:2302.09210, 2023.\n[40] \u201dOpenAI GPT-n models: Shortcomings & Advantages in 2023\u201d.\n[Accessed\non\n23.03.2023].\n[Online].\nAvailable:\nhttps://research.\naimultiple.com/gpt/\n[41] S. Pramanik and S. K. Bandyopadhyay, \u201cAnalysis of big data,\u201d in\nEncyclopedia of Data Science and Machine Learning.\nIGI Global,\n2023, pp. 97\u2013115.\n[42] A. Zaremba and E. Demir, \u201cChatgpt: Unlocking the future of nlp in\n\ufb01nance,\u201d Available at SSRN 4323643, 2023.\n[43] N. Aleksi\u00b4c, M. Arnelid, D. Lisy, A. Balachandran, M. Belfrage,\nA. Br\u00a8annstr\u00a8om, M. Busarello, S. A. Carretta, K. Cotton, S. de Heer\net al., \u201cDeep learning as a gpt and disruptor in innovation processes.\u201d\n[44] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz,\nE. Kamar, P. Lee, Y. T. Lee, Y. Li, S. Lundberg et al., \u201cSparks of\narti\ufb01cial general intelligence: Early experiments with gpt-4,\u201d arXiv\npreprint arXiv:2303.12712, 2023.\n[45] D. M. Katz, M. J. Bommarito, S. Gao, and P. Arredondo, \u201cGpt-4 passes\nthe bar exam,\u201d Available at SSRN 4389233, 2023.\n[46] D. O. Beerbaum, \u201cGenerative arti\ufb01cial intelligence (gai) ethics\ntaxonomy-applying chat gpt for robotic process automation (gai-rpa)\nas business case,\u201d Available at SSRN 4385025, 2023.\n[47] H. A. Dida, D. Chakravarthy, and F. Rabbi, \u201cChatgpt and big data:\nEnhancing text-to-speech conversion,\u201d Mesopotamian Journal of Big\nData, vol. 2023, pp. 33\u201337, 2023.\n[48] V. Pereira, E. Hadjielias, M. Christo\ufb01, and D. Vrontis, \u201cA systematic\nliterature review on the impact of arti\ufb01cial intelligence on workplace\noutcomes: A multi-process perspective,\u201d Human Resource Management\nReview, vol. 33, no. 1, p. 100857, 2023.\n[49] M. Trajtenberg, \u201cArti\ufb01cial intelligence as the next gpt: A political-\neconomy perspective,\u201d in The economics of arti\ufb01cial intelligence: An\nagenda.\nUniversity of Chicago Press, 2018, pp. 175\u2013186.\n[50] S. Tu, A. Cyphert, and S. Perl, \u201cLimits of using arti\ufb01cial intelligence\nand gpt-3 in patent prosecution,\u201d Tex. Tech L. Rev., vol. 54, p. 255,\n2021.\n[51] A. Mathew, \u201cIs arti\ufb01cial intelligence a world changer? a case study of\nopenai\u2019s chat gpt,\u201d Recent Progress in Science and Technology Vol. 5,\npp. 35\u201342, 2023.\n[52] R. Reed, \u201cThe theology of gpt-2: Religion and arti\ufb01cial intelligence,\u201d\nReligion Compass, vol. 15, no. 11, p. e12422, 2021.\n[53] H. Akbar, M. Zubair, and M. S. Malik, \u201cThe security issues and\nchallenges in cloud computing,\u201d International Journal for Electronic\nCrime Investigation, vol. 7, no. 1, pp. 13\u201332, 2023.\n[54] K. D. Gupta et al., \u201cA review of generative ai from historical perspec-\ntives,\u201d 2023.\n[55] R. Sajja, Y. Sermet, D. Cwiertny, and I. Demir, \u201cPlatform-independent\nand curriculum-oriented intelligent assistant for higher education,\u201d\narXiv preprint arXiv:2302.09294, 2023.\n[56] F. Etro, \u201cThe economic consequences of the diffusion of cloud comput-\ning,\u201d Dutta, Soumitra; Mia, Irene. The Global Information Technology\nReport, vol. 2010, 2009.\n[57] K. YAMAOKA, K. WATANABE, K. KISE, A. DENGEL, and S. ISHI-\nMARU, \u201cExperience is the best teacher: Personalized vocabulary\nbuilding within the context of instagram posts and sentences from gpt-\n3,\u201d 2022.\n[58] R. Ressmeyer, S. Masling, and M. Liao, \u201c\u201cdeep faking\u201d political twitter\nusing transfe r learning and gpt-2,\u201d 2019.\n[59] S. Balkus and D. Yan, \u201cImproving short text classi\ufb01cation with\naugmented data using gpt-3,\u201d arXiv preprint arXiv:2205.10981, 2022.\n[60] H. Hua, Y. Li, T. Wang, N. Dong, W. Li, and J. Cao, \u201cEdge computing\nwith arti\ufb01cial intelligence: A machine learning perspective,\u201d ACM\nComputing Surveys, vol. 55, no. 9, pp. 1\u201335, 2023.\n[61] J. Yao, S. Zhang, Y. Yao, F. Wang, J. Ma, J. Zhang, Y. Chu, L. Ji,\nK. Jia, T. Shen, A. Wu, F. Zhang, Z. Tan, K. Kuang, C. Wu, F. Wu,\nJ. Zhou, and H. Yang, \u201cEdge-cloud polarization and collaboration: A\ncomprehensive survey for ai,\u201d IEEE Transactions on Knowledge and\nData Engineering, pp. 1\u20131, 2022.\n[62] Y. Yu and S. Lee, \u201cMeasurements of the bene\ufb01ts of edge computing\non autonomous driving,\u201d in 2022 13th International Conference on\nInformation and Communication Technology Convergence (ICTC).\nIEEE, 2022, pp. 2155\u20132159.\n[63] D. Yuan, L. Cui, M. Xie, Z. Su et al., \u201cParatra: A parallel transformer\ninference framework for gpus in edge computing.\u201d\n[64] X. Zhou, H. Liu, C. Shi, and J. Liu, Deep Learning on Edge Computing\nDevices: Design Challenges of Algorithm and Architecture.\nElsevier,\n2022.\n[65] K. Li, K. Chen, S. Luo, H. Zhang, and P. Fan, \u201cUbinn: A commu-\nnication ef\ufb01cient framework for distributed machine learning in edge\ncomputing,\u201d IEEE Transactions on Network Science and Engineering,\n2023.\n[66] C. Benza\u00a8\u0131d, T. Taleb, and M. Z. Farooqi, \u201cTrust in 5g and beyond\nnetworks,\u201d IEEE Network, vol. 35, no. 3, pp. 212\u2013222, 2021.\n[67] J. S. Wey and J. Zhang, \u201cPassive optical networks for 5g transport:\nTechnology and standards,\u201d Journal of Lightwave Technology, vol. 37,\nno. 12, pp. 2830\u20132837, 2019.\n[68] S. Zhang, W. Y. B. Lim, W. C. Ng, Z. Xiong, D. Niyato, X. S. Shen,\nand C. Miao, \u201cTowards green metaverse networking: Technologies,\nadvancements and future directions,\u201d IEEE Network, 2023.\n[69] Z. Li, M. A. Uusitalo, H. Shariatmadari, and B. Singh, \u201c5g urllc:\nDesign challenges and system concepts,\u201d in 2018 15th International\nSymposium on Wireless Communication Systems (ISWCS), 2018, pp.\n1\u20136.\n[70] A. Gonzalez Fanfalone et al., \u201cThe road to 5g networks: experience to\ndate and future developments,\u201d 2019.\n[71] Y. Rogers, H. Sharp, and J. Preece, Interaction design: beyond human-\ncomputer interaction.\nJohn Wiley & Sons, 2023.\n[72] Y. Liu, M. Yu, M. Jiang, and Y. Huang, \u201cCreative research question\ngeneration for human-computer interaction research,\u201d 2023.\n[73] P. H\u00a8am\u00a8al\u00a8ainen, M. Tavast, and A. Kunnari, \u201cEvaluating large language\nmodels in generating synthetic hci research data: a case study,\u201d in ACM\nSIGCHI Annual Conference on Human Factors in Computing Systems.\nACM, 2023.\n[74] A. Shafeeg, I. Shazhaev, D. Mihaylov, A. Tularov, and I. Shazhaev,\n\u201cVoice assistant integrated with chat gpt,\u201d Indonesian Journal of\nComputer Science, vol. 12, no. 1, 2023.\n[75] J. Zhang, J. Pu, J. Xue, M. Yang, X. Xu, X. Wang, and F.-Y.\nWang, \u201cHivegpt: Human-machine-augmented intelligent vehicles with\ngenerative pre-trained transformer,\u201d IEEE Transactions on Intelligent\nVehicles, 2023.\n[76] H. Nori, N. King, S. M. McKinney, D. Carignan, and E. Horvitz,\n\u201cCapabilities of gpt-4 on medical challenge problems,\u201d arXiv preprint\narXiv:2303.13375, 2023.\n[77] A. Tack and C. Piech, \u201cThe ai teacher test: Measuring the pedagogical\nability of blender and gpt-3 in educational dialogues,\u201d arXiv preprint\narXiv:2205.07540, 2022.\n[78] A. Lecler, L. Duron, and P. Soyer, \u201cRevolutionizing radiology with gpt-\nbased models: Current applications, future possibilities and limitations\nof chatgpt,\u201d Diagnostic and Interventional Imaging, 2023.\n[79] D. Baidoo-Anu and L. Owusu Ansah, \u201cEducation in the era of gen-\nerative arti\ufb01cial intelligence (ai): Understanding the potential bene\ufb01ts\nof chatgpt in promoting teaching and learning,\u201d Available at SSRN\n4337484, 2023.\n[80] A. O\u2019Cain, B. D. Fedoruk, Z. Masri, R. Frost, and A. Alahmar, \u201cA\nsystem for the improvement of educational assessment using intelligent\nconversational agents,\u201d Available at SSRN 4393234, 2023.\n[81] A. Alam, \u201cPossibilities and challenges of compounding arti\ufb01cial intel-\nligence in india\u2019s educational landscape,\u201d Alam, A.(2020). Possibilities\nand Challenges of Compounding Arti\ufb01cial Intelligence in India\u2019s\nEducational Landscape. International Journal of Advanced Science and\nTechnology, vol. 29, no. 5, pp. 5077\u20135094, 2020.\n[82] H. Chen, O. Engkvist, Y. Wang, M. Olivecrona, and T. Blaschke, \u201cThe\nrise of deep learning in drug discovery,\u201d Drug discovery today, vol. 23,\nno. 6, pp. 1241\u20131250, 2018.\n\n38\n[83] N. Pillai, A. Dasgupta, S. Sudaskorn, J. Fretland, and P. D. Mavroudis,\n\u201cMachine-learning-guided early drug discovery of small molecules,\u201d\nDrug Discovery Today, 2022.\n[84] Y. Kim, J.-H. Kim, J. M. Lee, M. J. Jang, Y. J. Yum, S. Kim, U. Shin,\nY.-M. Kim, H. J. Joo, and S. Song, \u201cA pre-trained bert for korean\nmedical natural language processing,\u201d Scienti\ufb01c Reports, vol. 12, no. 1,\npp. 1\u201310, 2022.\n[85] K. S. Kalyan, A. Rajasekharan, and S. Sangeetha, \u201cAmmu: a survey\nof transformer-based biomedical pretrained language models,\u201d Journal\nof biomedical informatics, vol. 126, p. 103982, 2022.\n[86] Z. Liu, R. A. Roberts, M. Lal-Nag, X. Chen, R. Huang, and W. Tong,\n\u201cAi-based language models powering drug discovery and develop-\nment,\u201d Drug Discovery Today, vol. 26, no. 11, pp. 2593\u20132607, 2021.\n[87] J. Vamathevan, D. Clark, P. Czodrowski, I. Dunham, E. Ferran, G. Lee,\nB. Li, A. Madabhushi, P. Shah, M. Spitzer et al., \u201cApplications of\nmachine learning in drug discovery and development,\u201d Nature reviews\nDrug discovery, vol. 18, no. 6, pp. 463\u2013477, 2019.\n[88] E. Gawehn, J. A. Hiss, and G. Schneider, \u201cDeep learning in drug\ndiscovery,\u201d Molecular informatics, vol. 35, no. 1, pp. 3\u201314, 2016.\n[89] A. Lavecchia, \u201cDeep learning in drug discovery: opportunities, chal-\nlenges and future prospects,\u201d Drug discovery today, vol. 24, no. 10,\npp. 2017\u20132032, 2019.\n[90] F. Urbina, F. Lentzos, C. Invernizzi, and S. Ekins, \u201cDual use of\narti\ufb01cial-intelligence-powered drug discovery,\u201d Nature Machine Intel-\nligence, vol. 4, no. 3, pp. 189\u2013191, 2022.\n[91] M. H. Segler, T. Kogej, C. Tyrchan, and M. P. Waller, \u201cGenerating\nfocused molecule libraries for drug discovery with recurrent neural\nnetworks,\u201d ACS central science, vol. 4, no. 1, pp. 120\u2013131, 2018.\n[92] M. Ahsan, M. Rahaman, N. Anjum et al., \u201cFrom chatgpt-3 to gpt-4:\nA signi\ufb01cant leap in ai-driven nlp tools,\u201d Saidur and Anjum, Nishath,\nFrom ChatGPT-3 to GPT-4: A Signi\ufb01cant Leap in AI-Driven NLP Tools\n(March 27, 2023), 2023.\n[93] D. M. Levine, R. Tuwani, B. Kompa, A. Varma, S. G. Finlayson,\nA. Mehrotra, and A. Beam, \u201cThe diagnostic and triage accuracy of\nthe gpt-3 arti\ufb01cial intelligence model,\u201d medRxiv, pp. 2023\u201301, 2023.\n[94] I. Alghanmi, L. Espinosa-Anke, and S. Schockaert, \u201cProbing pre-\ntrained language models for disease knowledge,\u201d arXiv preprint\narXiv:2106.07285, 2021.\n[95] H. Ali, \u201cThe potential of gpt-4 as a personalized virtual assistant for\nbariatric surgery patients,\u201d Obesity Surgery, pp. 1\u20131, 2023.\n[96] D.-M. Vulturar, M. A. Neag, S. C. Vesa, A.-D. Maierean, D. Gherman,\nA. D. Buzoianu, O. H. Or\u02d8asan, and D.-A. Todea, \u201cTherapeutic ef\ufb01cacy\nand outcomes of remdesivir versus remdesivir with tocilizumab in se-\nvere sars-cov-2 infection,\u201d International Journal of Molecular Sciences,\nvol. 23, no. 22, p. 14462, 2022.\n[97] B. Wang, Q. Xie, J. Pei, P. Tiwari, Z. Li et al., \u201cPre-trained language\nmodels in biomedical domain: A systematic survey,\u201d arXiv preprint\narXiv:2110.05006, 2021.\n[98] S. Arslan, \u201cExploring the potential of chat gpt in personalized obesity\ntreatment,\u201d Annals of Biomedical Engineering, pp. 1\u20132, 2023.\n[99] A. Blanchard and M. Taddeo, \u201cEthical challenges of using arti\ufb01cal\nintelligence for intelligence analysis,\u201d Available at SSRN 4226631,\n2022.\n[100] B. Balsmeier and M. Woerter, \u201cIs this time different? how digitalization\nin\ufb02uences job creation and destruction,\u201d Research policy, vol. 48, no. 8,\np. 103765, 2019.\n[101] F. Pesapane, M. Codari, and F. Sardanelli, \u201cArti\ufb01cial intelligence\nin medical imaging: threat or opportunity? radiologists again at the\nforefront of innovation in medicine,\u201d European radiology experimental,\nvol. 2, pp. 1\u201310, 2018.\n[102] I. Carvalho and S. Ivanov, \u201cChatgpt for tourism: applications, bene\ufb01ts\nand risks,\u201d Tourism Review, 2023.\n[103] R. S. Rathore, S. Sangwan, and O. Kaiwartya, \u201cTowards trusted green\ncomputing for wireless sensor networks: Multi metric optimization\napproach.\u201d Adhoc & Sensor Wireless Networks, vol. 49, 2021.\n[104] S. Shah, H. Ghomeshi, E. Vakaj, E. Cooper, and R. Mohammad, \u201cAn\nensemble-learning-based technique for bimodal sentiment analysis,\u201d\nBig Data and Cognitive Computing, vol. 7, no. 2, p. 85, 2023.\n[105] J. Salminen, C. Kandpal, A. M. Kamel, S.-g. Jung, and B. J. Jansen,\n\u201cCreating and detecting fake reviews of online products,\u201d Journal of\nRetailing and Consumer Services, vol. 64, p. 102771, 2022.\n[106] A. S. George and A. H. George, \u201cA review of chatgpt ai\u2019s impact on\nseveral business sectors,\u201d Partners Universal International Innovation\nJournal, vol. 1, no. 1, pp. 9\u201323, 2023.\n[107] A. El-Ansari and A. Beni-Hssane, \u201cSentiment analysis for personalized\nchatbots in e-commerce applications,\u201d Wireless Personal Communica-\ntions, vol. 129, no. 3, pp. 1623\u20131644, 2023.\n[108] K. Goei, M. Hendriksen, M. de Rijke et al., \u201cTackling attribute \ufb01ne-\ngrainedness in cross-modal fashion search with multi-level features,\u201d\nin SIGIR 2021 Workshop on eCommerce. ACM, 2021.\n[109] S. G. Bouschery, V. Blazevic, and F. T. Piller, \u201cAugmenting human\ninnovation teams with arti\ufb01cial intelligence: Exploring transformer-\nbased language models,\u201d Journal of Product Innovation Management,\nvol. 40, no. 2, pp. 139\u2013153, 2023.\n[110] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von\nArx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill et al., \u201cOn\nthe opportunities and risks of foundation models,\u201d arXiv preprint\narXiv:2108.07258, 2021.\n[111] J.\nCowls,\nA.\nTsamados,\nM.\nTaddeo,\nand\nL.\nFloridi,\n\u201cThe\nai\ngambit:\nleveraging\narti\ufb01cial\nintelligence\nto\ncombat\nclimate\nchange\u2014opportunities, challenges, and recommendations,\u201d Ai & So-\nciety, pp. 1\u201325, 2021.\n[112] H. Benbya, T. H. Davenport, and S. Pachidi, \u201cArti\ufb01cial intelligence in\norganizations: Current state and future opportunities,\u201d MIS Quarterly\nExecutive, vol. 19, no. 4, 2020.\n[113] Y. Liu, T. Han, S. Ma, J. Zhang, Y. Yang, J. Tian, H. He, A. Li,\nM. He, Z. Liu et al., \u201cSummary of chatgpt/gpt-4 research and per-\nspective towards the future of large language models,\u201d arXiv preprint\narXiv:2304.01852, 2023.\n[114] S. Biswas, \u201cImportance of chat gpt in agriculture: According to chat\ngpt,\u201d Available at SSRN 4405391, 2023.\n[115] M. S. Farooq, S. Riaz, A. Abid, K. Abid, and M. A. Naeem, \u201cA\nsurvey on the role of iot in agriculture for the implementation of smart\nfarming,\u201d Ieee Access, vol. 7, pp. 156 237\u2013156 271, 2019.\n[116] H. Wang, H. Wu, H. Zhu, Y. Miao, Q. Wang, S. Qiao, H. Zhao,\nC. Chen, and J. Zhang, \u201cA residual lstm and seq2seq neural network\nbased on gpt for chinese rice-related question and answer system,\u201d\nAgriculture, vol. 12, no. 6, p. 813, 2022.\n[117] Y. K. Dwivedi, N. Kshetri, L. Hughes, E. L. Slade, A. Jeyaraj, A. K.\nKar, A. M. Baabdullah, A. Koohang, V. Raghavan, M. Ahuja et al.,\n\u201c\u201cso what if chatgpt wrote it?\u201d multidisciplinary perspectives on op-\nportunities, challenges and implications of generative conversational ai\nfor research, practice and policy,\u201d International Journal of Information\nManagement, vol. 71, p. 102642, 2023.\n[118] S. Biswas, \u201cProspective role of chat gpt in the military: According to\nchatgpt,\u201d Qeios, 2023.\n[119] P. Helo and A. Shamsuzzoha, \u201cReal-time supply chain\u2014a blockchain\narchitecture for project deliveries,\u201d Robotics and Computer-Integrated\nManufacturing, vol. 63, p. 101909, 2020.\n[120] R. Kadel, H. Shrestha, A. Shrestha, P. Sharma, N. Shrestha, J. Bashyal,\nand S. Shrestha, \u201cEmergence of ai in cyber security,\u201d International\nResearch Journal of Modernization in Engineering Technology and\nScience, 2022.\n[121] H. Benbya, S. Pachidi, and S. Jarvenpaa, \u201cSpecial issue editorial:\nArti\ufb01cial intelligence in organizations: Implications for information\nsystems research,\u201d Journal of the Association for Information Systems,\nvol. 22, no. 2, p. 10, 2021.\n[122] T. Zheng, M. Ardolino, A. Bacchetti, and M. Perona, \u201cThe applications\nof industry 4.0 technologies in manufacturing context: a systematic lit-\nerature review,\u201d International Journal of Production Research, vol. 59,\nno. 6, pp. 1922\u20131954, 2021.\n[123] B. Rathore, \u201cDigital transformation 4.0: Integration of arti\ufb01cial in-\ntelligence & metaverse in marketing,\u201d Eduzone: International Peer\nReviewed/Refereed Multidisciplinary Journal, vol. 12, no. 1, pp. 42\u201348,\n2023.\n[124] J. Bulchand-Gidumal, \u201cImpact of arti\ufb01cial intelligence in travel,\ntourism, and hospitality,\u201d in Handbook of e-Tourism.\nSpringer, 2022,\npp. 1943\u20131962.\n[125] N. Gillani, R. Eynon, C. Chiabaut, and K. Finkel, \u201cUnpacking the\n\u201cblack box\u201d of ai in education,\u201d Educational Technology & Society,\nvol. 26, no. 1, pp. 99\u2013111, 2023.\n[126] N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, and A. Galstyan,\n\u201cA survey on bias and fairness in machine learning,\u201d ACM Computing\nSurveys (CSUR), vol. 54, no. 6, pp. 1\u201335, 2021.\n[127] F. T. Tschang and E. Almirall, \u201cArti\ufb01cial intelligence as augmenting\nautomation: Implications for employment,\u201d Academy of Management\nPerspectives, vol. 35, no. 4, pp. 642\u2013659, 2021.\n[128] V. Jain, B. Malviya, and S. Arya, \u201cAn overview of electronic commerce\n(e-commerce),\u201d Journal of Contemporary Issues in Business and Gov-\nernment\u2014 Vol, vol. 27, no. 3, p. 666, 2021.\n[129] B. Feijoo and A. Garc\u00b4\u0131a Gonz\u00b4alez, \u201cOnline shopping routines among\nchilean children: level of expansion and main causes,\u201d 2020.\n\n39\n[130] X. Zhang, Y. Jiang, Y. Shang, Z. Cheng, C. Zhang, X. Fan, Y. Xiao, and\nB. Long, \u201cDsgpt: Domain-speci\ufb01c generative pre-training of transform-\ners for text generation in e-commerce title and review summarization,\u201d\nin Proceedings of the 44th International ACM SIGIR Conference on\nResearch and Development in Information Retrieval, 2021, pp. 2146\u2013\n2150.\n[131] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever et al.,\n\u201cLanguage models are unsupervised multitask learners,\u201d OpenAI blog,\nvol. 1, no. 8, p. 9, 2019.\n[132] T. Eloundou, S. Manning, P. Mishkin, and D. Rock, \u201cGpts are gpts:\nAn early look at the labor market impact potential of large language\nmodels,\u201d arXiv preprint arXiv:2303.10130, 2023.\n[133] P. Maddigan and T. Susnjak, \u201cChat2vis: Generating data visualisations\nvia natural language using chatgpt, codex and gpt-3 large language\nmodels,\u201d IEEE Access, 2023.\n[134] N. Jain, S. Vaidyanath, A. Iyer, N. Natarajan, S. Parthasarathy, S. Ra-\njamani, and R. Sharma, \u201cJigsaw: Large language models meet program\nsynthesis,\u201d in Proceedings of the 44th International Conference on\nSoftware Engineering, 2022, pp. 1219\u20131231.\n[135] A. Haleem, M. Javaid, and R. P. Singh, \u201cAn era of chatgpt as a\nsigni\ufb01cant futuristic support tool: A study on features, abilities, and\nchallenges,\u201d BenchCouncil transactions on benchmarks, standards and\nevaluations, vol. 2, no. 4, p. 100089, 2022.\n[136] N. Brand, W. Odom, and S. Barnett, \u201cEnvisioning and understanding\norientations to introspective ai: Exploring a design space with meta.\naware,\u201d in Proceedings of the 2023 CHI Conference on Human Factors\nin Computing Systems, 2023, pp. 1\u201318.\n[137] N. Dehouche, \u201cPlagiarism in the age of massive generative pre-trained\ntransformers (gpt-3),\u201d Ethics in Science and Environmental Politics,\nvol. 21, pp. 17\u201323, 2021.\n[138] B. D. Lund, T. Wang, N. R. Mannuru, B. Nie, S. Shimray, and Z. Wang,\n\u201cChatgpt and a new academic reality: Arti\ufb01cial intelligence-written\nresearch papers and the ethics of the large language models in scholarly\npublishing,\u201d Journal of the Association for Information Science and\nTechnology, 2023.\n[139] M. Javaid, A. Haleem, and R. P. Singh, \u201cChatgpt for healthcare ser-\nvices: An emerging stage for an innovative perspective,\u201d BenchCouncil\nTransactions on Benchmarks, Standards and Evaluations, p. 100105,\n2023.\n[140] P. P. Ray, \u201cChatgpt: A comprehensive review on background, appli-\ncations, key challenges, bias, ethics, limitations and future scope,\u201d\nInternet of Things and Cyber-Physical Systems, 2023.\n[141] E. G. Wilcox, J. Gauthier, J. Hu, P. Qian, and R. Levy, \u201cOn the\npredictive power of neural language models for human real-time\ncomprehension behavior,\u201d arXiv preprint arXiv:2006.01912, 2020.\n[142] R. Sridhar and D. Yang, \u201cExplaining toxic text via knowledge enhanced\ntext generation,\u201d in Proceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, 2022, pp. 811\u2013826.\n[143] J. Bryant and P. Vorderer, Psychology of entertainment.\nRoutledge,\n2013.\n[144] H. H. Thorp, \u201cChatgpt is fun, but not an author,\u201d pp. 313\u2013313, 2023.\n[145] S. Shahriar and K. Hayawi, \u201cLet\u2019s have a chat! a conversation with\nchatgpt: Technology, applications, and limitations,\u201d arXiv preprint\narXiv:2302.13817, 2023.\n[146] M. U. Haque, I. Dharmadasa, Z. T. Sworna, R. N. Rajapakse, and\nH. Ahmad, \u201c\u201d i think this is the most disruptive technology\u201d: Exploring\nsentiments of chatgpt early adopters using twitter data,\u201d arXiv preprint\narXiv:2212.05856, 2022.\n[147] O. AI. [Online]. Available: https://openai.com/product/gpt-4\n[148] opchatsgpt, \u201cImpact of chat gpt on the entertainment industry,\u201d\nMar 2023. [Online]. Available: https://opchatsgpt.com/chat-gpt-in-\nentertainment/\n[149] \u201cThe\nPower\nof\nChat\nGPT:\nCreating\nPersonalized\nMarketing\nand\nEf\ufb01cient\nCustomer\nSupport.\n\u2014\nlinkedin.com,\u201d\nhttps:\n//www.linkedin.com/pulse/power-chat-gpt-creating-personalized-\nmarketing-customer-bundhoo/, [Accessed 24-Apr-2023].\n[150] \u201cChat GPT-4 in the Film Industry: Scriptwriting, Editing, and More;\nTS2\nSPACE\n\u2014\nts2.space,\u201d\nhttps://ts2.space/en/chat-gpt-4-in-the-\n\ufb01lm-industry-scriptwriting-editing-and-more/#:\u223c:text=GPT\\%2D4\\\n%20has\\%20the\\%20potential,and\\%20consistency\\%20than\\\n%20ever\\%20before., 2023, [Accessed 24-Apr-2023].\n[151] E. i. bd, \u201cChatgpt: The impact of chat gpt on the entertainment\nindustry - march 24, 2023 educationsinbd,\u201d Mar 2023. [Online].\nAvailable:\nhttps://educationsinbd.com/the-impact-of-chat-gpt-on-the-\nentertainment-industry/\n[152] A. J. Veal, \u201cThe concept of lifestyle: a review,\u201d Leisure Studies, vol. 12,\nno. 4, pp. 233\u2013252, 1993.\n[153] M. Jensen, \u201cDe\ufb01ning lifestyle,\u201d Environmental sciences, vol. 4, no. 2,\npp. 63\u201373, 2007.\n[154] P. Contoyannis and A. M. Jones, \u201cSocio-economic status, health and\nlifestyle,\u201d Journal of health economics, vol. 23, no. 5, pp. 965\u2013995,\n2004.\n[155] M. J. Reeves and A. P. Rafferty, \u201cHealthy lifestyle characteristics\namong adults in the united states, 2000,\u201d Archives of internal medicine,\nvol. 165, no. 8, pp. 854\u2013857, 2005.\n[156] Y. H. Yeo, J. S. Samaan, W. H. Ng, P.-S. Ting, H. Trivedi, A. Vipani,\nW. Ayoub, J. D. Yang, O. Liran, B. Spiegel et al., \u201cAssessing the\nperformance of chatgpt in answering questions regarding cirrhosis and\nhepatocellular carcinoma,\u201d medRxiv, pp. 2023\u201302, 2023.\n[157] S. S. Biswas, \u201cRole of chat gpt in public health,\u201d Annals of Biomedical\nEngineering, pp. 1\u20132, 2023.\n[158] M. Patkar, \u201c5 Free Travel Planning AI and ChatGPT Apps to Get an\nInstant Itinerary \u2014 makeuseof.com,\u201d https://www.makeuseof.com/free-\ntravel-planning-ai-chatgpt-apps/, [Accessed 24-Apr-2023].\n[159] \u201cYour ai-powered personal chef.\u201d [Online]. Available: https://www.\nchefgpt.xyz/\n[160] lechjaLearnCrafts, \u201cLearn Crafts & Hobbies w/ GPT Chat \u2014\nlechja.com,\u201d\nhttps://www.lechja.com/ai/learn-crafts-hobbies-w-gpt-\nchat, 2023, [Accessed 24-Apr-2023].\n[161] \u201cHow to use chatgpt in your job search \u2014 indeed.com.\u201d [On-\nline]. Available: https://www.indeed.com/career-advice/news/chatgpt-\njob-search\n[162] L. Floridi and M. Chiriatti, \u201cGpt-3: Its nature, scope, limits, and\nconsequences,\u201d Minds and Machines, vol. 30, pp. 681\u2013694, 2020.\n[163] S. Toshniwal, S. Wiseman, K. Livescu, and K. Gimpel, \u201cLearning chess\nblindfolded,\u201d 2021.\n[164] J. Freiknecht and W. Effelsberg, \u201cProcedural generation of interactive\nstories using language models,\u201d in Proceedings of the 15th Interna-\ntional Conference on the Foundations of Digital Games, 2020, pp.\n1\u20138.\n[165] S. V\u00a8artinen, P. H\u00a8am\u00a8al\u00a8ainen, and C. Guckelsberger, \u201cGenerating role-\nplaying game quests with gpt language models,\u201d IEEE Transactions on\nGames, pp. 1\u201312, 2022.\n[166] J. van Stegeren and J. Myunde\ufb01nedliwiec, \u201cFine-tuning gpt-2 on\nannotated rpg quests for npc dialogue generation,\u201d in Proceedings\nof the 16th International Conference on the Foundations of Digital\nGames, ser. FDG \u201921.\nNew York, NY, USA: Association for\nComputing Machinery, 2021. [Online]. Available: https://doi.org/10.\n1145/3472538.3472595\n[167] P. Roetzer and M. Kaput, Marketing Arti\ufb01cial Intelligence: AI, Mar-\nketing, and the Future of Business.\nBenBella Books, 2022.\n[168] J. Thiergart, S. Huber, and T.\n\u00a8Ubellacker, \u201cUnderstanding emails\nand drafting responses\u2013an approach using gpt-3,\u201d arXiv preprint\narXiv:2102.03062, 2021.\n[169] X. Bai, L. Duan, R. Tang, G. Batra, and R. Agrawal, \u201cImproving text-\nbased similar product recommendation for dynamic product advertising\nat yahoo,\u201d in Proceedings of the 31st ACM International Conference on\nInformation & Knowledge Management, ser. CIKM \u201922.\nNew York,\nNY, USA: Association for Computing Machinery, 2022, p. 2883\u20132892.\n[Online]. Available: https://doi.org/10.1145/3511808.3557129\n[170] P. S. Neves, \u201cChat gpt ais \u201cinterview\u201d 1, december 2022,\u201d AIS-\nArchitecture Image Studies, vol. 3, no. 2, pp. 58\u201367, 2022.\n[171] X.-R. Gong, J.-X. Jin, and T. Zhang, \u201cSentiment analysis using autore-\ngressive language modeling and broad learning system,\u201d in 2019 IEEE\nInternational Conference on Bioinformatics and Biomedicine (BIBM).\nIEEE, 2019, pp. 1130\u20131134.\n[172] A. H. Sweidan, N. El-Bendary, and H. Al-Feel, \u201cSentence-level aspect-\nbased sentiment analysis for classifying adverse drug reactions (adrs)\nusing hybrid ontology-xlnet transfer learning,\u201d IEEE Access, vol. 9, pp.\n90 828\u201390 846, 2021.\n[173] F. Wei and U. T. Nguyen, \u201cStock trend prediction using \ufb01nancial market\nnews and bert,\u201d Wall Street Journal, 2018.\n[174] T. Yue, D. Au, C. C. Au, and K. Y. Iu, \u201cDemocratizing \ufb01nancial knowl-\nedge with chatgpt by openai: Unleashing the power of technology,\u201d\nAvailable at SSRN 4346152, 2023.\n[175] \u201dSiri\u201d. [Accessed on 25.03.2023]. [Online]. Available: https://www.\napple.com/in/siri/\n[176] \u201dSiri ChatGPT\u201d. [Accessed on 25.03.2023]. [Online]. Available:\nhttps://support.apple.com/en-in/guide/shortcuts/apd07c25bb38/ios\n[177] \u201dSiri ChatGPT\u201d. [Accessed on 25.03.2023]. [Online]. Available:\nhttps://gpt3demo.com/apps/chatgpt-sirigpt-apple-ios\n\n40\n[178] \u201dAI Dungeon: A text-based adventure-story game you direct (and\nstar in) while the AI brings it to life.\u201d. [Accessed on 30.03.2023].\n[Online]. Available: https://aidungeon.io/\n[179] \u201dIt\nBegan\nas\nan\nAI-Fueled\nDungeon\nGame.\nIt\nGot\nMuch\nDarker\u201d.\n[Accessed\non\n25.03.2023].\n[Online].\nAvailable:\nhttps:\n//www.wired.com/story/ai-fueled-dungeon-game-got-much-darker/\n[180] \u201dWhatever you want to ask, our chat has the answers\u201d. [Accessed on\n25.03.2023]. [Online]. Available: https://www.copy.ai/\n[181] \u201dIntroducing, The BOND Network.\u201d. [Accessed on 25.03.2023].\n[Online]. Available: https://www.bond.ai/\n[182] \u201dSave\nhundreds\nof\nhours\nanalyzing\nfeedback.\u201d.\n[Accessed\non\n29.03.2023]. [Online]. Available: https://www.askviable.com/\n[183] \u201dai\u2014channels: make contact with intelligent minds\u201d. [Accessed on\n30.03.2023]. [Online]. Available: https://aichannels.app/\n[184] \u201dAutomate your meeting notes\u201d. [Accessed on 30.03.2023]. [Online].\nAvailable: https://\ufb01re\ufb02ies.ai/\n[185] A. Papangelis, M. Namazifar, C. Khatri, Y.-C. Wang, P. Molino, and\nG. Tur, \u201cPlato dialogue system: A \ufb02exible conversational ai research\nplatform,\u201d arXiv preprint arXiv:2001.06463, 2020.\n[186] P. Dhariwal, H. Jun, C. Payne, J. W. Kim, A. Radford, and\nI. Sutskever, \u201cJukebox: A generative model for music,\u201d arXiv preprint\narXiv:2005.00341, 2020.\n[187] D. Adiwardana, M.-T. Luong, D. R. So, J. Hall, N. Fiedel, R. Thop-\npilan, Z. Yang, A. Kulshreshtha, G. Nemade, Y. Lu et al., \u201cTowards\na human-like open-domain chatbot,\u201d arXiv preprint arXiv:2001.09977,\n2020.\n[188] M. M. van Buchem, H. Boosman, M. P. Bauer, I. M. Kant, S. A.\nCammel, and E. W. Steyerberg, \u201cThe digital scribe in clinical practice:\na scoping review and research agenda,\u201d NPJ digital medicine, vol. 4,\nno. 1, p. 57, 2021.\n[189] K. Hao, \u201cFacebook\u2019s new polyglot ai can translate between 100\nlanguages,\u201d 2020.\n[190] Y. Gan, G. Lu, Z. Su, L. Wang, J. Zhou, J. Jiang, and D. Chen, \u201cA\njoint domain-speci\ufb01c pre-training method based on data enhancement,\u201d\nApplied Sciences, vol. 13, no. 7, p. 4115, 2023.\n[191] \u201cChatgpt plugins,\u201d Mar 2023. [Online]. Available: https://openai.com/\nblog/chatgpt-plugins\n[192] B. Bhattarai, O.-C. Granmo, and L. Jiao, \u201cConvtexttm: An explainable\nconvolutional tsetlin machine framework for text classi\ufb01cation,\u201d in\nProceedings of the Thirteenth Language Resources and Evaluation\nConference, 2022, pp. 3761\u20133770.\n[193] A. Narasimhan, K. P. A. V. Rao et al., \u201cCgems: A metric model for au-\ntomatic code generation using gpt-3,\u201d arXiv preprint arXiv:2108.10168,\n2021.\n[194] J. E. Zini and M. Awad, \u201cOn the explainability of natural language\nprocessing deep models,\u201d ACM Computing Surveys, vol. 55, no. 5, pp.\n1\u201331, 2022.\n[195] R. K. Yadav, L. Jiao, O.-C. Granmo, and M. Goodwin, \u201cAn inter-\npretable word sense classi\ufb01er for human explainable chatbot,\u201d in Agents\nand Arti\ufb01cial Intelligence: 13th International Conference, ICAART\n2021, Virtual Event, February 4\u20136, 2021, Revised Selected Papers.\nSpringer, 2022, pp. 236\u2013249.\n[196] A. Chan, \u201cGpt-3 and instructgpt: technological dystopianism, utopi-\nanism, and \u201ccontextual\u201d perspectives in ai ethics and industry,\u201d AI and\nEthics, pp. 1\u201312, 2022.\n[197] M. Zhang and J. Li, \u201cA commentary of gpt-3 in mit technology review\n2021,\u201d Fundamental Research, vol. 1, no. 6, pp. 831\u2013833, 2021.\n[198] H. R. Kirk, Y. Jun, F. Volpin, H. Iqbal, E. Benussi, F. Dreyer,\nA. Shtedritski, and Y. Asano, \u201cBias out-of-the-box: An empirical\nanalysis of intersectional occupational biases in popular generative\nlanguage models,\u201d Advances in neural information processing systems,\nvol. 34, pp. 2611\u20132624, 2021.\n[199] S. S. Biswas, \u201cPotential use of chat gpt in global warming,\u201d Annals of\nbiomedical engineering, pp. 1\u20132, 2023.\n[200] P. H. Seo, A. Nagrani, A. Arnab, and C. Schmid, \u201cEnd-to-end genera-\ntive pretraining for multimodal video captioning,\u201d in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2022, pp. 17 959\u201317 968.\n[201] W. Ji, Y. Wei, Z. Zheng, H. Fei, and T.-s. Chua, \u201cDeep multimodal\nlearning for information retrieval,\u201d in ACM International Conference\non Multimedia, 2023.\n[202] V. Liu, H. Qiao, and L. Chilton, \u201cOpal: Multimodal image generation\nfor news illustration,\u201d in Proceedings of the 35th Annual ACM Sympo-\nsium on User Interface Software and Technology, 2022, pp. 1\u201317.\n[203] C. Guo, A. Sablayrolles, H. J\u00b4egou, and D. Kiela, \u201cGradient-\nbased adversarial attacks against text transformers,\u201d arXiv preprint\narXiv:2104.13733, 2021.\n[204] A. Shafahi, M. Najibi, A. Ghiasi, Z. Xu, J. P. Dickerson, C. Studer,\nL. S. Davis, G. Taylor, and T. Goldstein, \u201cAdversarial training\nfor free!\u201d CoRR, vol. abs/1904.12843, 2019. [Online]. Available:\nhttp://arxiv.org/abs/1904.12843\n[205] T. Bai, J. Luo, J. Zhao, B. Wen, and Q. Wang, \u201cRecent advances\nin\nadversarial\ntraining\nfor\nadversarial\nrobustness,\u201d\nCoRR,\nvol.\nabs/2102.01356, 2021. [Online]. Available: https://arxiv.org/abs/2102.\n01356\n[206] M. Verma, \u201cIntegration of ai-based chatbot(chatgpt) and supply chain\nmanagement solution to enhance tracking and queries response,\u201d 02\n2023.\n[207] Y. Kubo and T. Trappenberg, Mitigating Over\ufb01tting Using Regulariza-\ntion to Defend Networks Against Adversarial Examples, 04 2019, pp.\n400\u2013405.\n[208] X. Liu, Y. Zheng, Z. Du, M. Ding, Y. Qian, Z. Yang, and J. Tang, \u201cGpt\nunderstands, too,\u201d arXiv preprint arXiv:2103.10385, 2021.\n[209] J. Jia, H. Liu, and N. Z. Gong, \u201c10 security and privacy problems in\nself-supervised learning,\u201d arXiv preprint arXiv:2110.15444, 2021.\n[210] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov,\nK. Talwar, and L. Zhang, \u201cDeep learning with differential privacy,\u201d\nin Proceedings of the 2016 ACM SIGSAC conference on computer and\ncommunications security, 2016, pp. 308\u2013318.\n",
    "Large Language Model Partitioning\nfor Low-Latency Inference at the Edge\nDimitrios Kafetzis1, Ramin Khalili2 and Iordanis Koutsopoulos1\n1Athens University of Economics and Business, Department of Informatics, Athens, Greece\n2Huawei European Research Center, Munich, Germany\nAbstract\u2014Large Language Models (LLMs) based on autore-\ngressive, decoder-only Transformers generate text one token at\na time, where a token represents a discrete unit of text. As\neach newly produced token is appended to the partial output\nsequence, the length grows and so does the memory and compute\nload, due to the expanding key-value (K/V) caches\u2014which\nstore intermediate representations of all previously generated\ntokens\u2014in the multi-head attention (MHA) layer. As this iterative\nprocess steadily increases memory and compute demands, layer-\nbased partitioning in resource-constrained edge environments\noften results in memory overload or high inference latency. To\naddress this, aiming to reduce inference latency, we propose a\nresource-aware Transformer architecture partitioning algorithm,\nwhere the partitioning decision is updated at regular intervals\nduring token generation. The approach is a myopic algorithm\nin the sense that it is based on instantaneously available in-\nformation about device resources availability and network link\nbandwidths. When the algorithm is first executed, it generates\na placement of blocks on devices, and in each consecutive time\nit is executed, it migrates these blocks among devices so that\nthe sum of migration delay and inference delay remains low.\nOur approach partitions the decoder at the attention head-\nlevel, co-locating each attention head with its K/V cache and\nallowing dynamic migrations whenever resources become tight.\nBy allocating different attention heads to different devices, we\nexploit parallel execution of attention heads and thus allow for\nsubstantial reductions in inference delays. Our experiments show\nthat in small-scale settings (3\u20135 devices), the proposed method\nachieves within 15\u201320% of an exact optimal solver\u2019s latency,\nwhile in larger-scale tests it achieves notable improvements in\ninference speed and memory usage compared to state-of-the-art\nlayer-based partitioning approaches.\nIndex Terms\u2014Large Language Models (LLMs), Transformer\npartitioning, Multi-head attention, Edge computing, Resource\nallocation, Low-latency inference.\nI. INTRODUCTION\nLarge Language Models (LLMs) have revolutionized a\nvariety of natural language processing tasks, from question-\nanswering and conversational ones to code generation. To\nachieve low-latency services in these applications, there is\ngrowing interest in running LLM inference at the edge, where\ndevices such as smartphones, IoT nodes, or on-premise servers\noften have limited memory and compute capacity. One pre-\nrequisite for overcoming these limitations is to partition large\nmodels across multiple edge devices, harnessing collective\nresources instead of relying on a single node.\nHowever, partitioning modern LLMs\u2014often containing bil-\nlions of parameters\u2014remains highly non-trivial. One approach\nis to split the model along large block boundaries, e.g.,\nFig. 1: Illustration of our approach, showing how attention head-level\npartitions (red lines) and additional cuts for feed-forward network\n(FFN) and linear projection (proj) blocks (green lines) enable flexible\nassignment of blocks across multiple edge devices. The controller\nnode makes inference requests and handles partitioning decisions,\nwhile the attention heads (with their K/V caches), FFN, and proj\nblocks are allocated among devices D1, D2, D3, and D4.\nentire decoder layers. Each decoder layer typically consists\nof multiple parallel attention head blocks and feed-forward\nnetworks, which can be assigned to different devices. This is\nwhat we call a coarse-grained partition, since it treats large\ndecoder layers as a single unit. Moreover, such partitions are\nstatic in the sense that the assignment of blocks to devices\nremains fixed throughout the inference process [1]\u2013[3]. While\ncoarse-grained and static partitions can be effective for deep\nneural networks, large Transformer-based LLMs require more\nfine-grained and adaptive partitioning. This is due to their\nautoregressive decoding nature, where tokens are generated\none by one. Specifically, as each new token is generated,\nthe model stores intermediate representations\u2014keys and val-\nues (K/V)\u2014for all previously produced tokens in dedicated\ncaches. These K/V caches appear in the multi-head attention\n(MHA) module of the Transformer and grow with every\nadditional token in the sequence, since each new token must\nattend to all past tokens. As the inference task progresses and\nnew tokens are generated, devices may become overloaded if\narXiv:2505.02533v1  [cs.DC]  5 May 2025\n\neach entire Transformer layer is treated as a single block.\nIn this paper, we focus on a single-layer decoder-only\nTransformer architecture (e.g., GPT [4]), meaning it includes\njust one instance of the core decoder sub-modules (MHA and\nfeed-forward blocks) rather than stacking multiple such blocks.\nWe propose a fine-grained, attention head-level partitioning\napproach that allows for more flexible use of edge resources.\nThe paper contributes to state of the art as follows:\n\u2022 We study the problem of reducing inference delay of\nTransformer architectures by allocating different Trans-\nformer blocks to different devices (Figure 1).\n\u2022 We develop an algorithm executed once before inference\nbegins, and then at regular intervals during the generation\nprocess. When executed prior to token generation, the\nalgorithm allocates different blocks of the Transformer\narchitecture to different devices in order to reduce in-\nference delay. When executed at regular intervals during\ntoken generation, the algorithm performs block migration\namong devices, taking migration delays into account.\n\u2022 We demonstrate through numerical studies that our\nmethod achieves near-optimal performance in terms of\ninference latency reduction (within 15\u201320% of an exact\nsolver) for small networks, and achieves up to 9\u201310\ntimes speedup in inference latency compared to existing\npartitioning approaches [1], [3] in scenarios involving\nlarge numbers of generated tokens. These findings high-\nlight the importance of attention head-level partitioning\nand resource management for low-latency autoregressive\nLLM inference at the edge.\nThe rest of this paper is organized as follows. Section II\nsurveys related work on distributed Transformer inference and\npartitioning. Section III details our system model. Section IV\npresents our approach, and Section V reports simulation re-\nsults. Finally, Section VI concludes the paper.\nII. RELATED WORK\nThe partitioning of deep learning models across resource-\nconstrained networks has been studied extensively to address\neither the training or the inference stages [5]. A broad set of\nworks aims to improve resource utilization, with emphasis on\npartitioning and parallelism techniques.\nDNN partitioning and scheduling. Many approaches focus\non splitting DNNs across devices to leverage the heteroge-\nneous compute and memory resources of different nodes and\nthereby reduce computational or communication overhead.\nFor instance, the work in [6] studies DNN partitioning for\ninference in resource-constrained networks, considering the\njoint problem of partitioning many DNNs (each split between\nan end-device and the Base Station) and scheduling those\npartitions at the Base Station to minimize inference delay.\nAlso, SplitPlace [7] proposes AI-driven strategies for partition-\ning DNNs among edge devices based on resource availability\nand application demands. Similarly, the authors of [8] address\nthe joint optimization of DNN partitioning and scheduling\nin end-edge-cloud systems to balance computational loads\nand reduce inference latency. Related lines of work couple\npartitioning with scheduling algorithms to improve resource-\nusage efficiency [9], using deep reinforcement learning for\ncontinuous task scheduling in edge networks (i.e., tasks arrive\nover time and must be allocated on the fly). Overall, these\nmethods underscore the need for adaptive (dynamic or real-\ntime) and task-specific (i.e., tailored to each DNN-inference\njob) partitioning to handle limited memory and compute\nresources effectively.\nPipeline and parallelism. A separate body of work fo-\ncuses on pipeline- and parallelism-based solutions. Techniques\nsuch as GPipe [10] and PipeDream [11] accelerate model\ntraining via pipeline parallelism, splitting large models across\naccelerators and mitigating pipeline stalls. Extensions like\nAutoPipe [12] and PipeDream-2BW [13] further optimize\nmemory footprints by applying pipeline-parallel training. On\nthe other hand at the inference stage, Galaxy [3] adopts tensor\nand sequence parallelism for Transformer inference at the\nedge. Meanwhile, long-sequence parallelism techniques [14]\nare designed for large input sequences and manage them using\ndata sharding (dividing input tokens among multiple devices)\nor pipeline sharding (distributing computation stages).\nInference for Transformers. Beyond training, methods\nfor distributed Transformer inference have also gained trac-\ntion. SplitLLM [2] studies collaborative cloud-edge inference\nthrough device selection and model partitioning, yet it focuses\nprimarily on single-shot inference (i.e., processing an input\nbatch in one pass) without explicitly tackling the token-by-\ntoken memory demands of autoregressive generation. Edge-\nShard [1] distributes large Transformers into shards (subdivi-\nsions of the model) but relies on a layer-wise design (i.e.,\ntreating each entire decoder layer as a single block). This\napproach may cause stalls and does not address the growing\nK/V caches, which store the key-value vectors for each newly\ngenerated token, enabling subsequent tokens to attend to them\nwithout recomputing at every decoding step.\nIn contrast, our approach partitions the Transformer archi-\ntecture at the level of an attention head block, co-locates\ncaches with attention head blocks to reduce communication\nload, and accommodates expanding memory footprints as\ntokens are generated. In Section V, we compare our approach\nto static pipeline-based partitioning, showing that our proposed\nattention head-level partitioning outperforms those methods\nfor growing K/V caches.\nIII. SYSTEM MODEL AND PROBLEM FORMULATION\nIn this section, we formalize how an inference request for\na single-layer, decoder-only Transformer is processed across\na network of heterogeneous devices in an edge environment.\nA. Inference Request and Autoregressive Generation\nWe consider a single inference request for text generation\nthat is placed as an input to a device, initiated by a text of\nlength L0 tokens. During the inference, the model generates a\nsequence of up to N tokens (units of text) one by one. At step\nn \u2264N, the previously generated tokens {1, . . . , n \u22121} are\nfed into the decoder-only Transformer, along with the input\n\ntokens, to produce the n-th token. We denote by Ln = L0 +n\nthe total sequence length in tokens at step n.\na) Toy Example: Consider a simple text generation task\nin which the initial text is \u201cThe cat\u201d (i.e., L0 = 2). At step\nn = 1, the model might output \u201csat,\u201d extending the sequence\nto \u201cThe cat sat,\u201d so L1 = 3. At n = 2, it might produce\n\u201con,\u201d yielding \u201cThe cat sat on\u201d (L2 = 4).\nB. Devices, Memory, and Compute Capacities\nLet G = (V, E) denote the edge network, where V is the set\nof heterogeneous devices and E is the set of communication\nlinks between them. Among these devices, one node acts as a\ncontroller, gathering resource state information from the rest.\nThe inference steps are divided into intervals of size \u03bb \u22651,\nwhere \u03bb is an integer and represents the number of tokens\ngenerated per interval. These intervals are indexed by integers\n\u03c4 \u2208{1, . . . , T} with T representing the total number of\nintervals during the inference. At each interval \u03c4, the controller\ncollects the following information from every j \u2208V:\n\u2022 Available memory Mj(\u03c4) (bytes),\n\u2022 Max compute capacity Wj (FLOPs/sec),\n\u2022 Available compute capacity Cj(\u03c4) \u2264Wj (FLOPs/sec),\ndue to concurrent background processes,\n\u2022 Link bandwidths Rj,k(\u03c4) (bytes/sec) for communication\nwith every k \u2208V.\nIn this work, we assume that the memory and compute\nresources available at a device, as well as the link bandwidths,\nremain constant during an interval, or that these values rep-\nresent the average predictions for the interval. The controller\nthen decides how to allocate (or migrate) the model\u2019s blocks\nto the devices, as will be explained in Section IV.\nC. Single-Layer Decoder-Only Transformer Blocks\nDefinition of Blocks and Resource Requirements: We con-\nsider a single-layer, decoder-only Transformer architecture.\nThe layer consists of:\n\u2022 H = {1, . . . , h}, the set of attention head blocks in the\nmulti-head attention (MHA) module,\n\u2022 a feed-forward network block, ffn,\n\u2022 an output projection block, proj.\nHence, we define B = H \u222a{ffn} \u222a{proj} as the set\nof blocks in the layer. For each attention head i \u2208H, there\nis an associated K/V cache whose size grows token by token.\nSpecifically, at an interval \u03c4, each attention head i stores more\nkeys/values than it did at the previous interval. If i /\u2208H, then i\nis either ffn or proj, each having its own memory demands.\nFor each block i \u2208B, we define:\n\u2022 mi(\u03c4) as the maximum memory requirement (bytes), and\n\u2022 bi(\u03c4) as the maximum compute requirement (FLOPs).\nConcretely, if i \u2208H, mi(\u03c4) represents the memory footprint\nof the K/V cache of attention head i plus its parameters at\ninterval \u03c4. If i = ffn or proj, mi(\u03c4) is the memory needed\nto store that block\u2019s parameters. Because of the model\u2019s au-\ntoregressive nature, mi(\u03c4) and bi(\u03c4) increase with \u03c4 (reflecting\nthe fact that, as inference process progresses, more tokens are\ngenerated and added to the K/V cache of each attention head).\nD. Placement (Allocation) of Blocks\nWe define a binary variable xij(\u03c4) that indicates whether\nblock i is allocated on device j at interval \u03c4:\n\u2022 If i \u2208H (i.e., an attention head),\nxij(\u03c4) =\n\uf8f1\n\uf8f2\n\uf8f3\n1,\nif attention head i and its K/V cache\nare running at device j at \u03c4\n,\n0,\notherwise.\n\u2022 If i /\u2208H (i.e., i = ffn or proj),\nxij(\u03c4) =\n(\n1,\nif block i is running on device j at \u03c4,\n0,\notherwise.\nIn all cases, we require that one block is placed on one and\nonly one device at each interval \u03c4:\nX\nj\u2208V\nxij(\u03c4) = 1,\n\u2200i \u2208B.\n1) Memory Capacity Constraint: At any interval \u03c4, the total\nmemory usage of blocks assigned to device j must not exceed\nMj(\u03c4). Thus,\nX\ni\u2208B\nmi(\u03c4) xij(\u03c4)\n\u2264\nMj(\u03c4),\n\u2200j, \u2200\u03c4.\n(1)\nMultiple blocks can be co-located on a device j if their\ncombined memory fits within Mj(\u03c4).\n2) Migration Cost: If a block i migrates from device j to\ndevice k at an interval \u03c4, it incurs a migration delay:\nDi\nmig\n\u0000j \u2192k, \u03c4\n\u0001\n= mi(\u03c4 \u22121)\nRj,k(\u03c4) ,\n(2)\nwhere mi(\u03c4 \u22121) is the memory footprint of block i (including\nits K/V cache, if i is an attention head) at previous interval,\nand Rj,k(\u03c4) is the available bandwidth on link (j, k) at the\ncurrent interval.\na) When migration occurs: A migration j \u2192k for block\ni means xij(\u03c4 \u22121) = 1 and xik(\u03c4) = 1. We generally allow\nat most one migration per attention head per-interval to avoid\nback-and-forth overhead.\nb) Remark \u2013 Why we consider a single-layer decoder:\nIn our model, we focus on a single decoder layer to show\nfine-grained partitioning at the attention head-level and dy-\nnamic assignment of the partitioned blocks. This choice keeps\nthe modeling complexity manageable for a proof-of-concept.\nAlthough we model a single-layer decoder for clarity, our\nattention head-level partitioning and migration approach can\nbe applied independently to each layer in multi-layer Trans-\nformers.\nE. Communication and Processing Delays\na) Communication Latency: We adopt an abstract model\nof the single-layer decoder:\n\u2022 At interval \u03c4, each attention head i \u2208H produces output\nthat must be sent to proj block.\n\u2022 Then proj transmits its output to ffn block.\n\nIf, for a particular head i, the head is on device j and proj\nis on device k, the latency to transfer the data Wi\u2192proj(\u03c4) is:\nD i\nj\u2192k(\u03c4) = Wi\u2192proj(\u03c4)\nRj,k(\u03c4)\n,\n(3)\nand if proj resides on k while ffn is on \u2113, we have\nD proj\nk\u2192\u2113(\u03c4) = Wproj\u2192ffn(\u03c4)\nRk,\u2113(\u03c4)\n.\n(4)\nWhen multiple blocks on the same device j send outputs to\nthe same device k, they must share the outgoing link (j, k).\nWe assume that these transmissions are performed in series.\nb) Processing Delay: For block i at \u03c4, if i is on device\nj, we define\nDij(\u03c4) =\nbi(\u03c4)\nCj(\u03c4),\n(5)\nas the time to process bi(\u03c4) FLOPs, given available compute\nCj(\u03c4) \u2264Wj. However, if multiple blocks (e.g., several atten-\ntion heads) share device j concurrently, they also share Cj(\u03c4).\nOne simple scheduling policy is to process them sequentially\non j, in which case we sum their compute demands; though\nmore sophisticated concurrency models can be used as needed.\nF. Total Inference Delay at \u03c4\nSuppose we fix a placement A(\u03c4), i.e. xij(\u03c4) for all i \u2208B\nat interval \u03c4, where j indexes the devices. Then d(\u03c4, i) denotes\nthe device hosting block i. The total inference delay DT(\u03c4)\ncan be decomposed as follows, adapting the standard decoding\npipeline (input \u2192attention heads \u2192proj \u2192ffn):\nDT(\u03c4) = max\ni\u2208H\nn\nDin\nin\u2192d(\u03c4,i)(\u03c4) + Di,d(\u03c4,i)(\u03c4)\n+ D i\nd(\u03c4,i)\u2192d(\u03c4,proj)(\u03c4)\no\n+ Dproj\nd(\u03c4,proj) \u2192d(\u03c4,ffn)(\u03c4),\n(6)\nwhere:\n\u2022 Din\nin\u2192d(\u03c4,i)(\u03c4) is the delay of moving input tokens from\ntheir initial storage (e.g. a controller node) to the attention\nhead\u2019s device d(\u03c4, i);\n\u2022 Di,d(\u03c4,i)(\u03c4) is the processing delay of attention head i on\nits device d(\u03c4, i) (per (5));\n\u2022 Di\nd(\u03c4,i)\u2192d(\u03c4,proj)(\u03c4) is the communication delay from\nattention head i to proj (per (3));\n\u2022 D proj\nd(\u03c4,proj) \u2192d(\u03c4,ffn)(\u03c4) is the delay from proj to ffn\n(per (4)).\nWhen multiple blocks share a device or link, there are\nconcurrency effects:\n\u2022 Compute concurrency: If multiple heads or other blocks\nrun on the same device j and share compute Cj(\u03c4),\nthe processing times will depend on the block compute\nscheduling policy.\n\u2022 Link concurrency: If multiple heads on the device j send\noutputs to the device k simultaneously, the transfer delays\nin Di\nd(\u03c4,i)\u2192d(\u03c4,proj)(\u03c4) may be aggregated or scheduled.\nEquation (6) retains a simplified pipeline form for clarity.\nFig. 2: A single-layer decoder-only Transformer at intervals \u03c4 and\n\u03c4 + 1, continuing autoregressively up to the N-th token generation.\nRed lines show attention head-level partitioning in MHA; green lines\nindicate projection and feed-forward partitioning. Cyan vs. green\nblocks highlight new tokens building on the K/V caches of the\nattention heads. At the bottom, a controller allocates these blocks\nacross edge devices.\nG. Decision at Each \u03c4\nWe must decide a new placement A(\u03c4), i.e. a mapping {i 7\u2192\nd(\u03c4, i)} for each block i \u2208B at each interval \u03c4. If block i was\non device d(\u03c4 \u22121, i), but we now place it on d(\u03c4, i) \u0338= d(\u03c4 \u2212\n1, i), that constitutes a migration with cost Di\nmig\n\u0000d(\u03c4\u22121, i) \u2192\nd(\u03c4, i), \u03c4\n\u0001\nper (2). We sum the delays of these migrations:\nDtotal\nmig (\u03c4) =\nX\ni\u2208B\nDi\nmig\n\u0000d(\u03c4 \u22121, i) \u2192d(\u03c4, i), \u03c4\n\u0001\n,\n(7)\nassuming migrations happen sequentially or that no two or\nmore blocks migrate over the same link simultaneously.\na) Objective: At each \u03c4, we need to find an assignment\nA(\u03c4) in order to minimize DT(\u03c4) + Dtotal\nmig (\u03c4), subject to\nthe memory constraint (1), and the network\u2019s communica-\ntion/processing costs. At \u03c4 = 1, we initialize A(1) to minimize\nDT(1), while for \u03c4 > 1 we also include the migration cost\nDtotal\nmig (\u03c4).\n1) Per-Interval Assignment Policy (Myopic Algorithm):\nThe controller node executes a centralized algorithm that\ndecides the assignment policy (Figure 2) i.e., the block-to-\ndevice mapping A(\u03c4) at each interval \u03c4 as follows:\n\u2022 The controller receives updated {Cj(\u03c4)}, {Mj(\u03c4)}, and\n{Rj,k(\u03c4)} from all devices/links.\n\u2022 It observes the previous assignment A(\u03c4 \u22121).\n\u2022 It computes the new assignment A(\u03c4) by solving a con-\nstrained optimization that minimizes DT(\u03c4) + Dtotal\nmig (\u03c4)\nunder the memory constraint (1).\n\u2022 It performs any required block migrations.\n\nIf we had a priori knowledge (or accurate predictions) of the\ncapacity evolutions {Cj(\u03c4), Mj(\u03c4), Rj,k(\u03c4)} on each \u03c4, then\nwe would solve for a global schedule X = {X(1), . . . , X(T)}\nwhere each X(\u03c4) is a matrix B\u00d7V with xij(\u03c4) = 1 indicating\nthat block i runs on device j during the interval \u03c4. In this work,\nwe focus on the simpler myopic scheme that takes as input\nthe current block allocation and the current availability of re-\nsources in memory, compute and bandwidth and performs the\nmigration that gives the best cost (migration plus inference) as\nperceived at the next interval. This approach is more realistic,\nsince it is more plausible to know the instantaneous bandwidth,\nmemory, and compute resource amounts and decide based on\nthem on what the best next move is, rather than knowing the\nentire process of evolution of these resources.\nIn the next section, we present our proposed practical\nheuristic of this placement and migration procedure.\nIV. RESOURCE-AWARE ALGORITHM FOR LLM BLOCK\nASSIGNMENT TO DEVICES\nIn this section, we present a practical heuristic for the\nmyopic assignment procedure described above. Recall that the\ncontroller that executes the centralized heuristic algorithm at\neach interval \u03c4, knows the device compute capacities {Cj(\u03c4)},\nmemory capacities {Mj(\u03c4)}, bandwidths {Rj,k(\u03c4)}, and the\nprevious placement {xij(\u03c4 \u22121)}. At each \u03c4, the controller\nseeks to find a block-to-device mapping A(\u03c4) = {xij(\u03c4) :\ni \u2208B, j\n\u2208V}, to minimize the interval\u2019s total delay\n(inference plus migration), subject to the time-varying memory\nconstraints (1) and the placement definition from Section III-D.\nA. Algorithm Overview\nAt a high level, the algorithm steps are as follows:\n1) Collect resource availability from devices and links:\nThe algorithm gathers the updated device compute ca-\npacities {Cj(\u03c4)}, memory {Mj(\u03c4)}, and link band-\nwidths {Rj,k(\u03c4)}, along with the previous assignment\n{xij(\u03c4 \u22121)}.\n2) Compute feasibility scores: For each block i \u2208B and\ndevice j, we calculate a scoring function S(i, j, \u03c4) to\ngauge the suitability of placing i in j. This step is per\nblock, so it does not yet consider concurrency with other\nblocks on j. A score S(i, j, \u03c4) \u22641 simply indicates that\ni by itself could fit on j given j\u2019s resources.\n3) Assign blocks and handle migrations: We pick the device\nj\u2217yielding the lowest feasible score for each block i. If\nj\u2217\u0338= jold, we incur a migration cost Di\nmig\n\u0000jold \u2192j\u2217, \u03c4\n\u0001\nper (2). If no device is feasible for i alone, we attempt to\nresolve overload by migrating other blocks from devices\nwith insufficient memory or compute.\n4) Verify constraints or backtrack: After assigning all\nblocks, we check memory demands (sum of all blocks\u2019\nusage on device j vs. Mj(\u03c4)) and compute demands\n(total load vs. Cj(\u03c4)). If any constraint is violated, we\nbacktrack by removing a minimal set of blocks (the\nfewest blocks needed to resolve the violation) from the\noverburdened device and reassigning them. This ensures\nconcurrency among blocks is handled collectively.\n5) Return the new placement to the controller: Once we fi-\nnalize {xij(\u03c4)} for all blocks i, the controller applies the\nassignment, triggers migrations if needed, and proceeds\nto the next interval \u03c4 + 1.\nWe effectively execute the algorithm at every interval \u03c4, where\nthe size \u03bb of each interval \u03c4 is chosen so each interval is on\nthe order of a few seconds, thus allowing enough time for\nmigrations to occur.\na) Scoring Function: To decide on which device j block\ni should be placed at \u03c4, we define the scoring function\nS(i, j, \u03c4) = max\nn\nmi(\u03c4)\nMj(\u03c4),\nbi(\u03c4)\nCj(\u03c4), CommFactor(i, j, \u03c4)\no\n.\nThis scoring function measures how each block i would\nuse memory, impose compute load, and incur communication\noverhead on a candidate device j. Specifically:\n\u2022 Memory/Compute feasibility: The terms mi(\u03c4)\nMj(\u03c4) and bi(\u03c4)\nCj(\u03c4)\nmeasure whether block i alone could fit into j\u2019s resource\ncapacities. A low ratio suggests device j can handle i,\nassuming j\u2019s resources are not already fully consumed\nby other blocks.\n\u2022 Communication overhead: CommFactor(i, j, \u03c4) approx-\nimates data transfer times if i must exchange information\nwith blocks on different devices.\nA device j is said to be individually feasible for block i if\nS(i, j, \u03c4) \u22641. However, in the presence of multiple blocks\ncompeting for device j, we rely on the final constraints check\n(Step 4 in the algorithm in section IV-A) to ensure that the sum\nof mi(\u03c4) across all assigned blocks does not exceed Mj(\u03c4),\nand likewise that total compute can be scheduled within Cj(\u03c4).\nIf assigning i to j alongside other blocks pushes j\u2019s memory\nor compute usage beyond feasible limits, we resolve this by\ninvoking ResolveResourceOverload (Section IV-B1) to\nattempt block migrations that free up memory or compute\nresources.\nb) Termination Criteria: To ensure the assignment pro-\ncedure at each interval \u03c4 completes in finite time, we impose\na runtime safeguard with two stopping conditions: (i) a time\nlimit Tmax (i.e., a real-time upper bound on how long the\ncontroller can spend adjusting assignments), and (ii) an itera-\ntion bound U = |B| \u00d7 |V|, which caps repeated reassignments\n(migrations/backtracking) so the algorithm does not endlessly\nretry assignments of blocks. If either limit is exceeded, the\nalgorithm returns INFEASIBLE. These conditions keep our\nmyopic approach tractable for each interval.\nB. Explanation of the Main Steps of Algorithm 1\nAlgorithm 1 runs at each interval \u03c4 and the pseudocode\nfollows.\n\u2022 Lines 1\u20133: We reset the migration/backtrack counters,\nrecord the start time, and collect up-to-date memory and\ncompute capacities for each device at \u03c4 and as well as\nthe links\u2019 bandwidth.\n\n\u2022 Line 4: We generate a sorted list of blocks (attention\nheads, feed-forward, projection, etc.) based on descend-\ning memory or compute demand, ensuring higher-demand\nblocks are considered first.\n\u2022 Lines 5\u201322: For each block, we select the device with\nthe lowest S(i, j, \u03c4). We then tentatively place the block\nthere and verify total resource usage on that device. If\nusage exceeds its limits, we revert the assignment and\ncall ResolveResourceOverload to migrate other\nblocks and free capacity. Any block migration increments\na counter, and exceeding its bound leads to INFEASIBLE.\n\u2022 Lines\n23\u201329:\nIf\nmemory\nor\ncompute\nconstraints\nremain\nviolated\nafter\nassignment,\nwe\ninvoke\nAlgorithm 1 Resource-Aware algorithm for LLM block as-\nsignment at interval \u03c4\nInput: Network G = (V, E), set of blocks B,\ninterval index \u03c4, previous assignment {xij(\u03c4 \u22121)},\niteration bound U, time limit Tmax\nOutput: Assignment {xij(\u03c4)} or INFEASIBLE if no valid\nsolution is found\n1: Initialize migrationCount \u21900, backtrackCount \u21900\n2: Start timer to enforce Tmax\n3: UpdateResourceUsage(\u03c4) // gather Mj(\u03c4), Cj(\u03c4),\nand link bandwidths {Rj,k(\u03c4)} for each device j \u2208V.\n4: Sort B into blocksQueue (descending by mi(\u03c4) or bi(\u03c4))\n5: for each block i \u2208blocksQueue do\n6:\nCompute score S(i, j, \u03c4) for all j \u2208V\n7:\nSelect device j\u2217\u2190arg minj S(i, j, \u03c4)\n8:\nif S(i, j\u2217, \u03c4) \u22641 then\n9:\nxij\u2217(\u03c4) \u21901\n// tentative assignment\n10:\nif\nP\ni\u2032 mi\u2032(\u03c4) xi\u2032j\u2217(\u03c4)\n>\nMj\u2217(\u03c4)\nor\nP\ni\u2032 bi\u2032(\u03c4) xi\u2032j\u2217(\u03c4) > Cj\u2217(\u03c4)\n11:\nxij\u2217(\u03c4) \u21900; // undo assignment\n12:\nResolveResourceOverload(i, \u03c4)\n13:\nmigrationCount \u2190migrationCount + 1\n14:\nif migrationCount > U then return INFEASIBLE\n15:\nelse if i moved from jold \u0338= j\u2217\n16:\nmigrationCount \u2190migrationCount + 1\n17:\nif migrationCount > U then return INFEASIBLE\n18:\nelse\n19:\nResolveResourceOverload(i, \u03c4)\n20:\nmigrationCount \u2190migrationCount + 1\n21:\nif migrationCount > U then return INFEASIBLE\n22:\nend if\n23:\nif elapsed time > Tmax then return INFEASIBLE\n24: end for\n25: if not allConstraintsSatisfied\n\u0000{xij(\u03c4)}\n\u0001\nthen\n26:\nBacktrackForResourceViolations(\u03c4)\n27:\nbacktrackCount \u2190backtrackCount + 1\n28:\nif backtrackCount > U then return INFEASIBLE\n29: end if\n30: return Full assignment matrix {xij(\u03c4)}\nBacktrackForResourceViolations to relocate\nblocks causing the conflict. If too many backtracking or\nmigration steps occur, or if we reach the time limit, we\nreturn INFEASIBLE.\n\u2022 Line 30: Once all constraints are satisfied, the algorithm\nconcludes by returning the final feasible mapping of all\nblocks to devices for the interval \u03c4.\n1) Resolving Resource Overload: If a block i cannot be\nplaced on any device without exceeding memory/compute\nconstraints at \u03c4, ResolveResourceOverload tries to\nmigrate other blocks. For instance, to move a block from\ndevice j to k, we check that the sum of memory already on k\nplus mi(\u03c4) remains within Mk(\u03c4). Similar checks apply for\ncompute. If no such migration fixes the overload, we escalate\nto BacktrackForResourceViolations.\n2) Backtrack for Resource Violations: When constraints\nremain violated after attempts to resolve overload, we\nuse BacktrackForResourceViolations to reassign a\nminimal set of blocks that cause the violation. If it still fails,\nwe return INFEASIBLE. This approach remains consistent with\nthe myopic policy from Section III-G, handling each token-\ngeneration interval in a bounded-time manner.\nOverall, the worst-case time complexity of Algorithm 1 at\neach interval \u03c4 is O(|B|2|V|). In the most demanding scenar-\nios, each block may require checking all devices and triggering\nmultiple reassignments to resolve resource overloads. This\ncomplexity remains practical for small- and medium-scale\ndeployments, but may require approximation heuristics in\nlarge-scale settings.\nBy iterating these steps, we obtain a mapping A(\u03c4) that\naims to minimize inference and migration delays at each\ninterval \u03c4. In Section V, we demonstrate the evaluation of\nthe performance of Algorithm 1.\nV. EVALUATION\nWe evaluate our resource-aware algorithm in two distinct\nsettings to assess both correctness (optimality gap) and scala-\nbility:\n\u2022 Small-scale (3\u20135 devices, N = 4 tokens): We use an\nexhaustive (exact) solver to find the optimal assignment.\nThis is feasible only for small-scale setups. We measure\nour heuristic\u2019s deviation from the global optimum that\nis found with exhaustive enumeration of the solutions,\nand compare against simpler baselines\u2014Greedy, Round-\nRobin, Static, and Dynamic\u2014as described below.\n\u2022 Medium-scale (25 devices, up to N = 1000 tokens): We\nscale up the number of devices, examining whether our\napproach outperforms other methods under more realistic\nedge settings with higher heterogeneity. We benchmark\nour resource-aware approach against state-of-the-art parti-\ntioning frameworks (EdgeShard [1], Galaxy [3]) for large-\nscale Transformer inference across multiple devices.\nWe execute our resource-aware algorithm at fixed intervals of\nsize \u03bb = 1, meaning that one token is generated during each\ninterval \u03c4. In other words, the algorithm updates the block-to-\ndevice assignment A(\u03c4) once per token. This setup represents\n\na worst-case scenario in terms of migration overhead, as it\ntriggers the controller to re-evaluate and potentially migrate\nblocks at every single decoding step. As a result, it creates the\nhighest possible frequency of migration and placement deci-\nsions, which stresses the system and highlights the efficiency\nof our method under tight constraints.\nA. Baseline Methods\nWe compare our approach to the following baselines, which\nillustrate different ways of Transformer architecture partition-\ning:\n\u2022 Greedy: Sort blocks in descending order of resource\ndemand and place them on the first feasible device\nwithout re-checking feasibility in subsequent steps.\n\u2022 Round-Robin: Assign blocks sequentially to devices in a\ncyclic order, ignoring different resource requirements at\ndifferent stages of the token generation process.\n\u2022 Static: Do one initial assignment for all blocks and never\nmigrate them during token generation.\n\u2022 Dynamic: As in Resource-Aware algorithm, re-checks\nassignments at each step but treat each layer as one block.\n\u2022 EdgeShard [1]: Assigns entire Transformer layers to de-\nvices. This static, layer-based partitioning does not adapt\nto resource changes or K/V cache growth.\n\u2022 Galaxy [3]: Partitions the Transformer into contiguous\nlayer shards for pipeline parallelism and splits each\nshard\u2019s large matrix multiplications across multiple de-\nvices for tensor parallelism.\nB. Evaluation Setup\nWe built a custom Python simulator1 in a discrete-event\nfashion to model each token generation step. A central\ncontroller gathers devices\u2019 memory/compute availability and\nlinks\u2019 bandwidth at every interval \u03c4 and runs our resource-\naware algorithm or a baseline algorithm. The simulator\u2019s mod-\nular design supports custom device topologies, concurrency\nmodels, or partitioning policies, enabling easy adaptation to\nnew distributed inference experiments.\na) Transformer Configurations: We focus on a single-\nlayer decoder with h attention heads and an embedding\ndimension D that is the size of each token\u2019s representation.\nThis parameter strongly influences both memory and compute\nusage. For a Large LLM model setup (h = 32, D = 2048),\nwe approximate GPT-2/LLaMA scales. The initial input text\nlength is L0 = 64.\nb) Device Capabilities: We sample each device\u2019s mem-\nory availability Mj(\u03c4) (GB) and compute capacity Cj(\u03c4)\n(GFLOPS) from log-normal distributions for heterogeneity\n(e.g., Mj(\u03c4) \u2208[2, 8] GB, Cj(\u03c4) \u2208[5, 50] GFLOPS) [16].\nFor the network, we assign each link (j, k) a bandwidth\nrandomly drawn from [1, 10] Gbps, reflecting diverse edge\nconditions and assuming full connectivity between devices.\nTable I summarizes the memory and compute usage calcu-\nlation formulas per-token generation step for the considered\n1The full codebase of our implementation is publicly available in [15]\nTransformer architecture blocks, derived from the analysis in\n[17]; where in these formulas, b is the number of bytes for each\nmodel parameter (i.e., the numerical values learned during\ntraining that define how inputs are processed), and d = D/h.\nBlock\nMemory (mi(\u03c4))\nCompute (bi(\u03c4))\nAttn. Head i\n3L\u03c4 d b + 3D d b\n3L\u03c4 D d + L2\n\u03c4 d\nK/V cache\nmcache\ni\n(\u03c4) = \u03c4 D b\nN/A\nProjection\nmproj(\u03c4) = L\u03c4 D b\nbproj(\u03c4) = L\u03c4 D2\nFFN\nmffn(\u03c4) = 4 L\u03c4 D b\nbffn(\u03c4) = 8 L\u03c4 D2\nTABLE I: Resource usage formulas assuming one token is generated\nper interval \u03c4 i.e., assuming that one token is generated per interval,\nso that n = \u03c4\nc) Metrics: We consider two metrics in our evaluation,\ninference latency and memory usage. Inference Latency is total\nelapsed time to generate N output tokens, encompassing com-\nputation, communication, and any migration delay. Memory\nUsage can be measured as either the sum across all devices\nor the maximum usage on a single device, illustrating how\nefficiently our approach handles expanding K/V caches.\nC. Small-Scale Scenario Results\nHere, an exact search is possible and practical so as to\nfind the optimal assignment. The ratio of each method\u2019s total\nlatency to that of the optimal solution shows that our Resource-\nAware approach remains within 15\u201320% of optimal, while\nGreedy and other baselines may lag behind by 40\u201360%. This\nunderscores the impact of attention head-level partitioning for\nshort decoding sequences for a small number of devices.\nD. Medium-Scale Scenario Results\nNext, we simulate a network of 25 devices (2\u20138 GB mem-\nory, 5\u201350 GFLOPS compute) with N up to 1000 tokens. We\nalso inject background tasks to emulate fluctuating compute\nload. We compare Resource-Aware with the EdgeShard and\nGalaxy frameworks.\na) Inference Latency vs. Token Generation Step n:\nFigure 3 shows that while all methods see rising latency as\nmore tokens accumulate, EdgeShard eventually surpasses 1000\nseconds at the last generated token, and Galaxy slows to about\n400\u2013600 seconds. In contrast, our fine-grained, attention head-\nlevel partitioning dynamically reallocates attention heads to\nprevent severe overload, keeping latency under 200 seconds\nat the last generated token.\nb) Memory Usage vs. Token Generation Step n: As\nshown in Figure 4, EdgeShard and Galaxy exceed 7 GB\nby n = 100, while Resource-Aware remains near 6 GB.\nThe disparity grows further for larger n, partly due to the\ninflexibility of layer-level partitioning under expanding K/V\ncaches.\nc) Scalability with Increasing Number of Devices: Al-\nthough allocating attention heads among more devices can ac-\ncelerate inference by spreading the workload, larger networks\nimpose additional coordination and scheduling overhead. As\nthe number of devices grows, the complexity of finding a good\nplacement or migration increases.\n\n0\n200\n400\n600\n800\n1000\nToken Generation Step n\n0\n200\n400\n600\n800\n1000\n1200\nLatency (s)\nInference Latency vs. Token Generation Step\nEdgeShard\nGalaxy\nResource-Aware (our approach)\nFig. 3: Inference latency vs. generated token step n on 25 devices.\nOur approach (Resource-Aware) avoids steep growth.\n0\n20\n40\n60\n80\n100\nToken Generation Step n\n5.0\n5.5\n6.0\n6.5\n7.0\n7.5\n8.0\nMemory (GB)\nMemory Usage vs. Token Generation Step\nEdgeShard\nGalaxy\nResource-Aware (our approach)\nFig. 4: Total memory usage vs. generated token step n in the 25-\ndevice setup. Our resource-aware approach mitigates memory growth\nmore effectively.\nE. Summary of Results\nOverall, in the small-scale scenario, our method remains\nwithin 15\u201320% of the optimal solver\u2019s latency, outperform-\ning simpler heuristics such as the Greedy and Round-Robin\nbaselines by 40\u201360%. In the medium-scale setting, our ap-\nproach scales well to 25 devices and N = 1000 tokens,\nresulting in latency and memory overhead considerably better\nthan EdgeShard or Galaxy (up to 9\u201310 times speedup). This\nhighlights the advantage of attention head-level partitioning\nplus dynamic partitioned blocks assignment for autoregressive\nLLM inference.\nVI. CONCLUSION AND FUTURE WORK\nWe introduced a resource-aware approach for partitioning\ndecoder-only Transformers under the tight memory, compute,\nand link constraints. Our key contribution is to treat each\nattention head block along with its associated key/value cache\nas a distinct block that can be allocated to a certain device.\nThe ability to execute attention head blocks in parallel across\ndevices is the central strength of our approach, significantly\nreducing inference delay. By explicitly modeling how K/V\ncaches expand at each decoding step, we can dynamically\nreassign these fine-grained blocks across devices to balance\nworkloads.\nFor future work, we plan to extend and validate our ap-\nproach on multi-layer decoder-only Transformers. We also aim\nto incorporate limited foresight in the decision-making process\ni.e., to predict resource availability ahead of time and make\ndecisions based on these predictions. We also aim to deploy\nreal-world testbeds that add factors such as energy constraints\nor inference request load forecasts, in a real environment.\nREFERENCES\n[1] M. Zhang, J. Cao, X. Shen, and Z. Cui, \u201cEdgeshard: Efficient\nllm inference via collaborative edge computing,\u201d arXiv preprint\narXiv:2405.14371, 2024.\n[2] A. Mudvari, Y. Jiang, and L. Tassiulas, \u201cSplitllm: Collaborative infer-\nence of llms for model placement and throughput optimization,\u201d arXiv\npreprint arXiv:2410.10759, 2024.\n[3] S. Ye, J. Du, L. Zeng, W. Ou, X. Chu, Y. Lu, and X. Chen, \u201cGalaxy:\nA resource-efficient collaborative edge ai system for in-situ transformer\ninference,\u201d in IEEE INFOCOM 2024 - IEEE Conference on Computer\nCommunications, pp. 1001\u20131010, 2024.\n[4] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al., \u201cLanguage mod-\nels are few-shot learners,\u201d Advances in Neural Information Processing\nSystems, vol. 33, pp. 1877\u20131901, 2020.\n[5] One6G, \u201c6g technology overview,\u201d 2024.\n[6] D. Kafetzis and I. Koutsopoulos, \u201cDnn partitioning and inference\ntask offloading in 6g resource-constrained networks,\u201d in 2024 Joint\nEuropean Conference on Networks and Communications & 6G Summit\n(EuCNC/6G Summit), pp. 753\u2013758, 2024.\n[7] S. Tuli, G. Casale, and N. R. Jennings, \u201cSplitplace: Ai augmented\nsplitting and placement of large-scale neural networks in mobile edge\nenvironments,\u201d IEEE Transactions on Mobile Computing, vol. 22,\np. 5539\u20135554, Sept. 2023.\n[8] Z. Zhang, Q. Li, L. Lu, D. Guo, and Y. Zhang, \u201cJoint optimization of\nthe partition and scheduling of dnn tasks in computing and network\nconvergence,\u201d IEEE Networking Letters, vol. 5, no. 2, pp. 130\u2013134,\n2023.\n[9] S. Yuan, Z. Zhang, Q. Li, W. Li, and Y. Zhang, \u201cJoint optimization\nof dnn partition and continuous task scheduling for digital twin-aided\nmec network with deep reinforcement learning,\u201d IEEE Access, vol. 11,\npp. 27099\u201327110, 2023.\n[10] Y. Huang, Y. Cheng, A. Bapna, O. Firat, M. X. Chen, D. Chen, H. Lee,\nJ. Ngiam, Q. V. Le, Y. Wu, and Z. Chen, GPipe: efficient training of\ngiant neural networks using pipeline parallelism. Red Hook, NY, USA:\nCurran Associates Inc., 2019.\n[11] D. Narayanan, A. Harlap, A. Phanishayee, V. Seshadri, N. R. Devanur,\nG. R. Ganger, P. B. Gibbons, and M. Zaharia, \u201cPipedream: generalized\npipeline parallelism for dnn training,\u201d in Proceedings of the 27th ACM\nSymposium on Operating Systems Principles, SOSP \u201919, (New York,\nNY, USA), p. 1\u201315, Association for Computing Machinery, 2019.\n[12] W. Liu, Z. Lai, S. Li, Y. Duan, K. Ge, and D. Li, \u201cAutopipe: A fast\npipeline parallelism approach with balanced partitioning and micro-\nbatch slicing,\u201d in 2022 IEEE International Conference on Cluster\nComputing (CLUSTER), pp. 301\u2013312, 2022.\n[13] D. Narayanan, A. Phanishayee, K. Shi, X. Chen, and M. Zaharia,\n\u201cMemory-efficient pipeline-parallel dnn training,\u201d in International Con-\nference on Machine Learning, pp. 7937\u20137947, PMLR, 2021.\n[14] H. Zheng, P. Liang, Y. Tang, Y. Shi, L. Qiao, and D. Li, \u201c3d parallelism\nfor transformers via integer programming,\u201d in ICASSP 2024 - 2024 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing\n(ICASSP), pp. 6440\u20136444, 2024.\n[15] D.\nKafetzis,\n\u201cDistributed\nTransformer\nInference\nSimulator.\u201d\nhttps://github.com/Dimitrios-Kafetzis/DistributedTransformer\\\nInferenceSimulator, 2025.\n[16] C. Reiss, J. Wilkes, and J. L. Hellerstein, \u201cGoogle cluster-usage traces:\nformat+ schema,\u201d Tech. Rep. ISTC-CC-TR-12-101, Google Inc., 2012.\n[17] A. Vaswani, N. M. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d in\nNeural Information Processing Systems, 2017.\n",
    "Meta-Prompting:\nEnhancing Language Models with Task-Agnostic Scaffolding\nMirac Suzgun\nStanford University\u2217\nmsuzgun@stanford.edu\nAdam Tauman Kalai\nOpenAI\u2217\nadam@kal.ai\nAbstract\nWe introduce meta-prompting, an effective scaffolding technique designed to enhance\nthe functionality of language models (LMs). This approach transforms a single LM into a\nmulti-faceted conductor, adept at managing and integrating multiple independent LM\nqueries. By employing high-level instructions, meta-prompting guides the LM to break\ndown complex tasks into smaller, more manageable subtasks. These subtasks are then\nhandled by distinct \u201cexpert\u201d instances of the same LM, each operating under specific,\ntailored instructions. Central to this process is the LM itself, in its role as the conductor,\nwhich ensures seamless communication and effective integration of the outputs from these\nexpert models. It additionally employs its inherent critical thinking and robust verification\nprocesses to refine and authenticate the end result. This collaborative prompting approach\nempowers a single LM to simultaneously act as a comprehensive orchestrator and a panel\nof diverse experts, significantly enhancing its performance across a wide array of tasks.\nThe zero-shot, task-agnostic nature of meta-prompting greatly simplifies user interaction\nby obviating the need for detailed, task-specific instructions. Furthermore, our research\ndemonstrates the seamless integration of external tools, such as a Python interpreter, into\nthe meta-prompting framework, thereby broadening its applicability and utility. Through\nrigorous experimentation with GPT-4, we establish the superiority of meta-prompting\nover conventional scaffolding methods: When averaged across all tasks, including the\nGame of 24, Checkmate-in-One, and Python Programming Puzzles, meta-prompting\u2014\naugmented with a Python interpreter functionality\u2014surpasses standard prompting by\n17.1%, expert (dynamic) prompting by 17.3%, and multipersona prompting by 15.2%.1\nStd\n0-CoT Ex-St Ex-Dy\nMP\nMeta\n0\n20\n40\n60\n80\n100\nTask Accuracy (%)\nGame of 24\nStd\n0-CoT Ex-St Ex-Dy\nMP\nMeta\n0\n20\n40\n60\n80\n100\nCheckmate-in-One\nStd\n0-CoT Ex-St Ex-Dy\nMP\nMeta\n0\n20\n40\n60\n80\n100\nSonnet Writing\nFigure 1: Enhancing GPT-4 with meta-prompting. In this study, we introduce and examine the effectiveness of meta-\nprompting, contrasting it with a range of zero-shot prompting techniques, including standard zero-shot (Std), zero-shot\nchain-of-thought (0-CoT; Kojima et al. (2022)), generic and dynamic expert (Ex-St and Ex-Dy; Xu et al. (2023)), and\nmultipersona (MP; Wang et al. (2023)). Our research demonstrates that meta-prompting, particularly when combined\nwith a Python interpreter, significantly improves overall accuracy and robustness in GPT-4 across a variety of tasks.\n\u2217Work done while at Microsoft Research New England.\n1The data, prompts, and the model outputs are all available at https://github.com/suzgunmirac/meta-prompting.\n1\narXiv:2401.12954v1  [cs.CL]  23 Jan 2024\n\n1\nIntroduction\nThe latest generation of language models (LMs)\u2014notably, GPT-4 (OpenAI, 2023), PaLM (Anil et al., 2023),\nand LLaMa (Touvron et al., 2023)\u2014have expanded the boundaries of natural-language processing and\ngeneration. These large-scale models can tackle a wide spectrum of tasks, ranging from writing Shake-\nspearean sonnets about hedgehogs to summarizing intricate medical reports and solving competition-level\nprogramming puzzles. Despite their versatility, these models are not infallible; they sometimes generate\nresponses that are inaccurate, misleading, or conflicting. As the operational costs of these models become\nmore affordable, it becomes natural to ask whether one might use scaffolding systems and leverage multiple\nLM queries to not only refine but also to enhance the accuracy and robustness of these model outputs.\nIn this work, we introduce a new technique for enhancing the functionality and performance of LMs, called\nmeta-prompting. It involves constructing a high-level \u201cmeta\u201d prompt that instructs an LM to: (i) break down\ncomplex tasks or problems into smaller, manageable pieces; (ii) assign these pieces to specialized \u201cexpert\u201d\nmodels with proper and detailed natural-language instructions; (iii) oversee the communication between\nthese expert models; and (iv) apply its own critical thinking, reasoning, and verification skills throughout\nthe process. When presented with a query, the LM, effectively prompted under meta-prompting, serves\nas a conductor. It produces a message history\u2014a narrative, if you will\u2014comprising the responses from\nvarious expert models. The LM is originally responsible for generating the conductor\u2019s portion of this history,\nwhich includes the selection of experts and the formulation of specific instructions for them. However, the\nsame LM doubles itself as these independent experts as well, generating outputs based on the expertise and\ninformation chosen by the conductor for each particular query.\nThis approach allows for a single, uniform LM to maintain a coherent line of reasoning while also tapping\ninto a variety of expert roles. The use of dynamically selected contexts for prompting these experts introduces\nfresh perspectives into the process, while the conductor model retains a bird\u2019s-eye view of the entire history\nand coordination. This method, therefore, enables a single black-box LM to function effectively as both a\ncentral conductor and a diverse panel of experts to produce more accurate, reliable, and coherent responses.\nOur proposed meta-prompting technique combines and expands upon various prompting ideas introduced\nby recent studies\u2014including, high-level planning and decision-making (Yao et al., 2023b; Sun et al., 2023; Hao\net al., 2023a), dynamic persona assignment (Xu et al., 2023; Wang et al., 2023), multi-agent debating (Du et al.,\n2023; Zhuge et al., 2023), self-debugging and self-reflection (Schick et al., 2023b; Liu et al., 2023a; Gou et al., 2023;\nMadaan et al., 2023; Shinn et al., 2023). A key aspect of meta-prompting is its task-agnostic nature. Unlike\ntraditional scaffolding methods that require specific instructions or examples tailored to each task, meta-\nprompting employs the same set of high-level instructions across various tasks and inputs. This universality\nis particularly beneficial for users who might find it cumbersome to provide detailed examples or specific\nguidance for every distinct task. For instance, in responding to a one-off request like \u201cWrite a Shakespearean\nsonnet about selfies,\u201d the user would not need to supply examples of high-quality neoclassical poems. The\nmeta-prompting approach elevates the utility of language models by offering a broad, flexible framework\nwithout compromising on specificity or relevance. Additionally, to demonstrate the versatility and integration\ncapabilities of meta-prompting, we have enhanced our system with the functionality to invoke a Python\ninterpreter. This allows for an even more dynamic and comprehensive application of the technique, further\nextending its potential to address a wide array of tasks and queries effectively.\nWe provide an illustrative visualization of a meta-prompting session in Figure 2. It depicts how the Meta\nModel\u2014our technical term for the central controlling LM (a.k.a. the conductor)\u2014intersperses its own output\nwith inputs and outputs from various specialized expert models or code executions. Such a configuration\nmakes meta-prompting a nearly universal tool. It allows for the consolidation of various LM interactions and\ncomputations into a single, coherent narrative. What sets meta-prompting apart is that it leaves the decision\nof which prompts to use and which code snippets to execute to the discretion of the LM itself.\nIn our comprehensive experiments, which primarily utilize GPT-4 as the foundational LM, we compare\nthe efficacy of meta-prompting against other task-agnostic scaffolding methods. Our findings reveal that\nmeta-prompting not only enhances overall performance but often leads to state-of-the-art results across a\ndiverse range of tasks. Its flexibility is noteworthy: The conductor model has the capability to call upon\n2\n\nBased on the information given, what are the most logical next steps or conclusions? Please make sure that the solution is accurate, directly answers the original question, and follows to all given \nconstraints. Additionally, please review the \ufb01nal solution yourself or have another expert(s) verify it.\nExpert Chess Analyst: \"\"You are a chess analyst with expertise in reviewing and verifying chess moves, strategies, and tactics. Please verify the following move provided by Expert Chess Player as the \nnext move that will result in a checkmate: 1. d4 d5 2. g3 c6 3. Bg2 Nf6 4. Nf3 e6 5. O-O Be7 6 [...]  31. Qxg6 Re8 32. Rxe8+ Qf8 33. Rxf8#. Con\ufb01rm if this move is accurate and results in a checkmate.\"\"\"\n[...] Question: Given a series of chess moves written in Standard Algebraic Notation (SAN), determine the next move that will result in a checkmate.  1. d4 d5 2. g3 c6 3. Bg2 Nf6 4. Nf3 e6 5. O-O Be7 6. \nRe1 O-O 7. Nbd2 Nbd7  [...]  31. Qxg6 Re8 32. Rxe8+ Qf8 33.  Let's \ufb01rst come up with a list of experts you may want to consult for this problem and then immediately start solving it.\nInjected Instruction \nExpert Chess Player: \"\"\"You are a chess expert with deep knowledge of the game, including strategies, tactics, and Standard Algebraic Notation (SAN). Analyze the given series of chess moves and \ndetermine the next move that will result in a checkmate. Here is the series of moves: 1. d4 d5 2. g3 c6 3. Bg2 Nf6 4. Nf3 e6 5. O-O Be7 6. Re1 O-O 7. Nbd2 Nbd7 [...] 31. Qxg6 Re8 32. Rxe8+ Qf8 33.\"\"\"\nMeta Model Output  (prompted on the entire message history)\nYes, the sequence of moves provided is accurate and results in a checkmate. The \ufb01nal move, 33. Rxf8#, \nplaces the black king in a checkmate position, with no legal moves available to escape the check.\nBased on the information given, what are the most logical next steps or conclusions? Please make sure that the solution is accurate, directly answers the original question, and follows to all given \nconstraints. Additionally, please review the \ufb01nal solution yourself or have another expert(s) verify it.\nExpert Chess Analyst Output (after prompted only on the highlighted text)\nSince the Expert Chess Analyst has con\ufb01rmed the accuracy of the move provided by the Expert Chess Player, we can now present the \ufb01nal answer. \n>> FINAL ANSWER: \"\"\"33. Rxf8#\"\"\"\nRxf8#\nExpert Chess Player Output (after prompted only on the highlighted text)\nInjected Instruction \nInjected Instruction \nMeta Model Output  (prompted on the entire message history)\nMeta Model Output  (prompted on the entire message history)\nYou are Meta-Expert, an extremely clever expert with the unique ability to collaborate with multiple experts (such as Expert Problem Solver, Expert Mathematician, Expert Essayist, etc.) to tackle any \ntask and solve any complex problems. Some experts are adept at generating solutions, while others excel in verifying answers and providing valuable feedback\u2026 \n  System Instruction\nFigure 2: An example meta-prompting history, where the prompts have been shortened for illustrative purposes. The\nhistory is initialized by a question provided by a user. Then the entries cycle through: (a) injected instructions for the\nMeta Model, (b) the Meta Model\u2019s output (when prompted with the entire history thus far), and (c) the output of the\nexpert (with fresh eyes\u2014prompted only on the instructions generated by the Meta Model).\nexpert models (basically itself, albeit with fresh instructions) for performing a variety of functions. These\nfunctions might include critiquing earlier outputs, selecting specific personas for certain tasks, refining\ngenerated content, and ensuring that the final outputs meet the desired criteria in both substance and form.\nThis approach shows a marked improvement over several existing methods, as demonstrated in Figure 1.\nThe core contribution of this work is the introduction of a task-agnostic scaffolding system that leverages a\nsingle LM. This LM not only carries forward the thread of the task but also dynamically selects and instructs\nexpert models appropriate for each specific task. The effectiveness of this system is showcased across various\nbenchmarks, including the Game of 24 (Yao et al., 2023a), Checkmate-in-One from the BIG-Bench suite (BIG-\nBench authors, 2023), and our novel task of \u201cShakespearean Sonnet Writing.\u201d Overall, our empirical results\nunderscore the versatility and robustness of meta-prompting in enhancing LM performance.\n2\nMeta Prompting\nIntuition and Abstract Overview. The modus operandi of meta-prompting is to use a model2 to coordinate\nand execute multiple independent inquiries and subsequently synthesize their responses to render a final\nresponse. This mechanism, in principle, endorses an ensemble approach, drawing from the strength and\ndiversity of independent specialized models to collaboratively address and tackle multifaceted tasks or\nproblems. We posit that while a single, general-purpose model might deliver valuable and useful insights\ninto generic queries, combining the perspectives and conclusions of multiple domain-specific models (which\nwe also refer to as experts) has the potential to yield more comprehensive, robust, and accurate solutions.\n2Our use of the term model refers to the application of an LM with certain prompt templates to play a specified \u201crole.\u201d We typically\nonly use a single LM (e.g., GPT-4) to implement all the models in an execution.\n3\n\nCentral to our meta-prompting strategy is its shallow hierarchical configuration, where a single model\u2014called\nthe \u201cMeta Model\u201d\u2014emerges as the principal entity of authority. This prompting structure is reminiscent of\nan orchestra, wherein the conductor\u2019s role is mirrored by the Meta Model and each musician corresponds\nto a distinct domain-specific model. Just as a conductor harmonizes multiple musical elements to craft a\nbeautiful melody, the Meta Model combines solutions and insights from a range of models to provide an\naccurate and comprehensive answer to an intricate problem or task.\nConceptually, a domain-specific expert within our framework can take diverse forms, such as a finetuned LM\ntailored to perform a particular task, a specialized API equipped to handle specific domain-related inquiries,\nor even computational tools like calculators or a Python interpreter that can perform arithmetic calculations\nor write and execute code. These experts, despite their varying functionalities, are directed and unified under\nthe supervision of the Meta Model.\nUnder our setup, experts can be called only by the Meta Model. They cannot directly interact or communicate\nwith each other, though the Meta Model can choose to share some text from or combine the insights of\nvarious experts when interacting with a new expert. This restriction is made to simplify the communication\nbetween the experts and to put the Meta Model at the center of the operation.\nNotation and Terminology. Before we delve into the specific steps involved in meta-prompting, we establish\nsome notation and terminology. We let S denote the set of finite strings, with \u2205representing the empty string.\nWe use x \u2208S to refer to a test-time query, which can be a task or a problem described in natural language. A\ncrucial element of meta-prompting is the fixed language model, denoted as LM, which operates from S to S.\nThis model, like GPT-4, takes an input text (a prompt history that may include a list of previous messages,\nsymbolized by H) and produces a corresponding output (i.e., response). We also introduce specific template\nfunctions: tinit, tmid, and texp, each mapping from S to S; each takes a string input and formats it according to a\npredefined template. Specifically, tinit and tmid are used to format text for the history given to the Meta Model,\nwhile texp wraps the output of the Meta Model in a prompt suitable for an expert model. Furthermore, we\nhave two string extractors, eexp and eret, each mapping from S to S. These extractors are designed to retrieve\na substring that is enclosed within specific delimiters, returning the first matching segment in cases where\nmultiple segments are present. The symbol \u2295is used to represent string concatenation. Lastly, we introduce\na specific string referred to as error \u2208S, which is designed to denote an error message in the process.\nAlgorithmic Procedure. Algorithm 1 provides pseudocode of our proposed meta-prompting approach. We\nfurther provide a conceptual overview of the procedure below:\nAlgorithm 1 Meta Prompting\nInput: LM : S \u2192S; x, error \u2208S; T \u2208N; tinit, tmid, texp, eexp, eret : S \u2192S\n1: H1 \u2190tinit(x)\n2: for t \u2208[1, . . . , T] do\n3:\nyt \u2190LM (Ht)\n4:\nif eexp(yt) \u0338= \u2205then\n\u25b7Meta Model provided expert instructions\n5:\nprompt \u2190texp(eexp(yt))\n6:\nzt \u2190LM (prompt)\n7:\nHt+1 \u2190Ht \u2295tmid(zt)\n8:\nelse if eret(yt) \u0338= \u2205then\n\u25b7Meta Model returned a final answer\n9:\nreturn eret(yt)\n10:\nelse\n\u25b7Meta Model formatting error\n11:\nHt+1 \u2190Ht \u2295error\n12:\nend if\n13: end for\n1. Transforming the Input: Using the transformation function tinit, the raw query is placed in a suitable\ntemplate followed by initial instructions to the Meta Model.\n2. Loop Iteration:\n4\n\n(a) Prompting the Meta Model: The current message list, namely Ht, guides the Meta Model\u2019s next\naction\u2014either directly addressing the query or consulting a domain-specific expert.\n(b) Engaging Domain-Specific Expert Models: If the Meta Model does not return a result, it can\nconjure any expert and give it instructions, which are extracted from its output using eexp. This\nprocess is isolated though: Each expert only sees what the Meta Model chooses to share with\nthem, and responds accordingly. For instance, if a problem pertains to mathematics and history,\nthe Meta Model might consult a mathematics expert for a calculation and a history expert for\nhistorical context. The output of the expert is extracted and additional instructions are appended,\nall using the tmid template.\n(c) Returning the Final Response: If the Meta Model\u2019s response contains a final answer (highlighted\nby distinct special markers), the solution is extracted using eret and returned.\n(d) Error Handling: In cases where the model response yt contains neither a final answer nor a call\nto an expert model, an error message appended to the message list Ht. This ensures that our\nprocedure is robust and can handle unexpected outputs.\nMeta and Expert Model Specifications. In our setup, we employ the same LM, such as GPT-4, to function in\nboth Meta and Expert capacities. Their roles are distinguished by their respective model instructions in their\nprompts, with the Meta Model adhering to a set of instructions provided in Figure 3, and the expert models\nfollowing separate instructions dynamically determined by the Meta Model at inference time .\n3\nExperimental Setup\n3.1\nBaselines\nWe compare meta-prompting with the task-agnostic, zero-shot versions of the following prompting methods:\n\u2022 Standard prompting: This represents our most basic baseline wherein an LM is asked to directly yield\na response without any specific guiding input-output exemplars or any additional guiding instructions,\nbesides the task description already included in the input query.\n\u2022 Zero-shot CoT prompting (Kojima et al., 2022): Drawing inspirations from the chain-of-thought\nmethod of Wei et al. (2022b), this zero-shot prompting approach simply appends \u201cLet\u2019s think step by\nstep\u201d to the input query, encouraging the model to have a more deliberative and iterative cognition\nbefore addressing the problem or task at hand.\n\u2022 Expert prompting (Xu et al., 2023): This prompting approach functions through a two-step process:\nIt first crafts an expert identity tailored to align with the specific context of the input query. It then\nintegrates this generated expert profile into the input to generate a well-informed and authoritative\nresponse. In our experiments, we consider two versions of expert prompting, namely (a) static (i.e.,\ngeneric) and (b) dynamic (i.e., adaptive); the former uses a fixed and generic expert description, whereas\nthe latter adaptively designs a new expert identity for each input query.\n\u2022 Multi-persona prompting (Du et al., 2023): Also known as solo-performance prompting (SPP), this\nmethod instructs an LM to perform the following: (i) Propose a small ensemble of \u201cpersonas\u201d to\naddress the specific task or problem at hand; (ii) let these personas engage in a collective dialogue,\ncollaboratively generating potential solutions while extending feedback to one another and refining\ntheir answers; and (iii) synthesize all the available information and deliver a final response.\n3.2\nDatasets and Tasks\nTo evaluate the efficacy of our proposed meta-prompting approach over other zero-shot prompting baselines,\nwe consider a wide range of tasks and datasets that require various degrees of mathematical and algorithmic\n5\n\nreasoning, domain-specific knowledge, and literary creativity. These include:\n\u2022 (a) The Game of 24 from (Yao et al., 2023a) where the goal is to form an arithmetic expression whose\nvalue is 24 using each of four given numbers exactly once,\n\u2022 Three BIG-Bench Hard (BBH; Suzgun et al. (2023b)) tasks\u2014namely, (b) Geometric Shapes, (c) Multi-\nStep Arithmetic Two, and (d) Word Sorting\u2014as well as one reasoning task directly obtained from the\nBIG-Bench suite (BIG-Bench authors, 2023), that is, (e) Checkmate-in-One;\n\u2022 (f) Python Programming Puzzles (P3; Schuster et al. (2021)), a collection of challenging programming\npuzzles written in Python\u2014with varying difficulty levels;\n\u2022 (g) Multilingual Grade School Math (MGSM; Shi et al. (2023)), a multilingual version of the GSM8K\ndataset (Cobbe et al., 2021) with translations of a subset of examples into ten typologically diverse\nlanguages, including Bengali, Japanese, and Swahili;\n\u2022 (h) Shakespearean Sonnet Writing, a novel task we created where the goal is to write a sonnet with\nstrict rhyme scheme \u201cABAB CDCD EFEF GG,\u201d containing the three provided words verbatim.3\n3.3\nAnswer Extraction and Evaluation Protocols\nAs shown in Figure 3, the system instruction in our proposed meta-prompting method encourages the Meta\nModel to present its final answer in a specific format. This format, designed for consistent and unambiguous\nextraction, requires that the final answer is wrapped within triple quotes and preceded by a distinct marker\n(namely, \u201c\u00bbFINAL ANSWER:\u201d).\nOnce the final answer is extracted from the model and properly post-processed, we also need to evaluate its\ncorrectness.4 Because we consider a wide range of tasks, there is not a single metric that allows us to measure\naccuracy across all. Depending on the nature and formulation of the task, we measure accuracy using one of\nthe following three metrics:\n\u2022 Exact Match (EM): Under this strict metric, the correctness of an answer is determined by its precise\nalignment with the ground-truth label(s). An answer is deemed correct only if it is identical to a\nprovided reference.\n\u2022 Soft Match (SM): This metric offers a more lenient approach than EM. For an answer to be deemed\ncorrect, it is sufficient for a ground-truth label to be present within the model\u2019s output, regardless of\nany additional textual content.\n\u2022 Functionally Correct (FC): This metric ascertains whether the answer is functionally correct, meaning\nthat it adheres to task-specific constraints.\nWe use EM for Geometric Shapes, Multi-Step Arithmetic Two, and Checkmate-in-One; SM for MGSM and\nWord Sorting,; and FC for Game of 24, Python Programming Puzzles, and Shakespearean Sonnet Writing.\n3.4\nModels and Inference\nIn our main experiments, we concentrate on GPT-4 (gpt-4-32k), which is accessible through Microsoft\u2019s\nAzure OpenAI Service. Additionally, in our supplementary experiments, we include GPT-3.5 (gpt-35-turbo).\nBoth GPT-3.5 and GPT-4 are models fine-tuned for following instructions, though GPT-4 has demonstrated\nsignificantly better reasoning and content generation abilities than GPT-3.5.5\n3While all the other tasks and datasets were previously introduced by other studies, we present this task for the first time.\n4We have developed suitable pipelines for answer extraction and processing tailored to each task. Specific implementation details\ncan be found in our codebase.\n5In our preliminary experiments, we also tested other OpenAI models such as text-davinci-003 and code-davinci-002, but we\ndiscovered that our meta-prompting approach yielded consequential results when applied to GPT-3.5 and GPT-4.\n6\n\nIn all of our experiments, we consistently applied the same parameters and system instructions to the Meta\nModel. We set the temperature value at 0, the top-p value at 0.95, and the maximum token count at 1024.6\nYou are Meta-Expert, an extremely clever expert with the unique ability to collaborate with multiple experts (such as Expert \nProblem Solver, Expert Mathematician, Expert Essayist, etc.) to tackle any task and solve any complex problems. Some \nexperts are adept at generating solutions, while others excel in verifying answers and providing valuable feedback.\nNote that you also have special access to Expert Python, which has the unique ability to generate and execute Python code \ngiven natural-language instructions. Expert Python is highly capable of crafting code to perform complex calculations when \ngiven clear and precise directions. You might therefore want to use it especially for computational tasks.\nAs Meta-Expert, your role is to oversee the communication between the experts, effectively using their skills to answer a \ngiven question while applying your own critical thinking and veri\ufb01cation abilities.\nTo communicate with a expert, type its name (e.g., \"Expert Linguist\" or \"Expert Puzzle Solver\"), followed by a colon \":\", and \nthen provide a detailed instruction enclosed within triple quotes. For example:\nExpert Mathematician:\n\"\"\"\nYou are a mathematics expert, specializing in the \ufb01elds of geometry and algebra.\nCompute the Euclidean distance between the points (-2, 5) and (3, 7).\n\"\"\"\nEnsure that your instructions are clear and unambiguous, and include all necessary information within the triple quotes. You \ncan also assign personas to the experts (e.g., \"You are a physicist specialized in...\").\nInteract with only one expert at a time, and break complex problems into smaller, solvable tasks if needed. Each interaction \nis treated as an isolated event, so include all relevant details in every call.\nIf you or an expert \ufb01nds a mistake in another expert's solution, ask a new expert to review the details, compare both \nsolutions, and give feedback. You can request an expert to redo their calculations or work, using input from other experts. \nKeep in mind that all experts, except yourself, have no memory! Therefore, always provide complete information in your \ninstructions when contacting them. Since experts can sometimes make errors, seek multiple opinions or independently \nverify the solution if uncertain. Before providing a \ufb01nal answer, always consult an expert for con\ufb01rmation. Ideally, obtain or \nverify the \ufb01nal solution with two independent experts. However, aim to present your \ufb01nal answer within 15 rounds or fewer.\nRefrain from repeating the very same questions to experts. Examine their responses carefully and seek clari\ufb01cation if \nrequired, keeping in mind they don't recall past interactions.\nPresent the \ufb01nal answer as follows:\n>> FINAL ANSWER:\n\"\"\"\n[\ufb01nal answer]\n\"\"\"\nFor multiple-choice questions, select only one option. Each question has a unique answer, so analyze the provided \ninformation carefully to determine the most accurate and appropriate response. Please present only one solution if you \ncome across multiple options.\nMeta Model Instruction\nFigure 3: The instructions given to the Meta Model using the \u201csystem message\u201d parameter in the GPT-4 API.\n6The temperature value, which usually ranges between 0 and 1, controls how much randomness or creativity the model exhibits.\nIdeally, a temperature of 0 should lead to the model producing the same output when presented with the same input. However,\nboth GPT-3.5 and GPT-4 have shown a tendency to generate varied responses even at this setting. This means that reproducing our\nexact results might be challenging under identical experimental conditions. To address this issue, we are releasing all model inputs,\ninteractions, and outputs in our GitHub repository.\n7\n\nBasic\nExpert\nSPP\nMeta\n\u2206\nTask\nStandard\n0-CoT\nStatic\nDynamic\nMulti-Persona\n- Python\n+ Python\n(M-S)\nCheckmate-in-One\n36.4\n32.8\n39.6\n33.2\n17.2\n57.2\n57.2\n+20.8\nGame of 24\n3.0\n11.0\n3.0\n2.0\n25.0\n11.0\n67.0\n+64.0\nGeometric Shapes\n56.8\n69.2\n55.2\n53.6\n57.6\n58.4\n59.2\n+2.4\nMGSM (avg)\n84.4\n85.5\n83.0\n85.0\n85.7\n85.4\n84.8\n+0.4\nMulti-Step Arithmetic\n84.0\n83.2\n83.2\n78.8\n91.6\n84.8\n90.0\n+6.0\nPython Prog. Puzzles\n31.1\n36.3\n33.8\n25.0\n32.5\n32.7\n45.8\n+14.7\nSonnet Writing\n62.0\n71.2\n74.0\n74.0\n73.2\n77.6\n79.6\n+17.6\nWord Sorting\n80.4\n83.6\n83.2\n85.2\n79.2\n84.0\n99.6\n+19.2\nAverage (macro)\n54.8\n59.1\n56.9\n54.6\n57.7\n61.4\n72.9\n+18.1\nTable 1: Comparison of baselines with meta-prompting across tasks. Without a Python interpreter, meta-prompting\nsignificantly outperforms other methods on the Checkmate-in-One and Sonnet Writing tasks and is on par on most\nother tasks except Geometric Shapes. Meta-prompting can leverage the Python interpreter in a task-agnostic manner to\nimprove performance significantly across many tasks.\n4\nMain Results and Discussion\nThe results of our experiments, summarized in Table 1, demonstrate the superior effectiveness of our meta-\nprompting approach compared to the standard zero-shot prompting methods. When we look at the overall\nperformance across all tasks, there is a notable increase in accuracy with meta-prompting, especially when it\nis augmented with a Python interpreter. Specifically, meta-prompting outperforms standard prompting by\n17.1%, expert (dynamic) prompting by 17.3%, and multipersona prompting by 15.2%. Below, we delve into\nfour key insights that emerged from our empirical analysis.\n4.1\nOverall Performance\nThe meta-prompting approach, particularly when augmented with a Python interpreter, consistently outper-\nforms conventional zero-shot prompting across various tasks. This approach proves to be especially effective\nin tackling tasks that are heavily reliant on heuristic or iterative trial-and-error problem-solving strategies.\nIn the Game of 24 challenge, we see an accuracy improvement of over 60% compared to the basic standard\nprompting method (highlighted in pink), about a 15% gain in Python Programming Puzzles, and close to an\n18% increase in accuracy for Sonnet Writing. These tasks require complex, iterative, and heuristic search\nstrategies, where conventional single-shot prompting falls short. Conversely, meta-prompting leverages the\ncollective intelligence of various expert personas to iteratively navigate towards a solution, thus fostering a\nmore dynamic and effective problem-solving ecosystem.\nExpanding upon its capabilities, meta-prompting appears to be effective in creative writing tasks as well.\nIn the Shakespearean Sonnet Writing task, for instance, which demands linguistic precision and creative\nconformity to specific poetic structures, meta-prompting notably enhances performance. While standard\nprompting methods yield a 62% accuracy rate, meta-prompting achieves 79.6% and 77.6% accuracy, with and\nwithout a Python interpreter, respectively.\nIn MGSM and Geometric Shapes, the benefits of meta-prompting over the other prompting approaches seem\nminimal based on the first impression. Nonetheless,meta-prompting does provide 4-6% gains in Bengali and\nTelugu, two underrepresented languages with the lowest baseline performances. In Geometric Shapes,7 we\nhad expected that GPT-4 to identify the shapes of objects by generating and executing appropriate codes\nunder meta-prompting, but this did not happen. Meta-prompting yielded only a modest 2.4% gain in this\n7This task involves naming a shape from its SVG path. Note that the LMs we used did not offer visual capabilities at the time.\n8\n\ngeometric task. We, however, acknowledge that the zero-shot-CoT baseline performed surprisingly better\nthan all the other methods, outperforming meta-prompting with a 10% accuracy gap.\nWhile the most significant gains were observed using a Python interpreter, we note that for the Checkmate-in-\nOne task, meta-prompting achieved a 20.8% gain even without it. Overall, our results highlight the versatility\nof meta-prompting and underscore its potential for broad application beyond strictly computational problems.\n4.2\nZero-Shot Decomposition, Error Detection, and Aggregation\nThe success of our meta-prompting framework lies partly in its strategic use of specialized knowledge, self-\ncollaboration, and implicit verification loops. This approach, as well as multipersona prompting, encourages\nmulti-turn interactions where different personas collaborate to resolve a problem.\nTo illustrate how the framework can be beneficial, consider solving multilingual arithmetic problems from the\nMGSM dataset. GPT-4, under the meta-prompting method, typically follows a three-phase approach: initially\ntranslating the problem from the source language (e.g., Bengali) to English, then applying computational\nexpertise (like calling an Expert Mathematician) to find a solution, and finally, conducting an independent or\ncorroborated verification. This unsupervised approach aligns with the multilingual CoT prompting method\nused by Shi et al. (2023) for MGSM, where the prompt instructs the LM to first translate the problem and\nthen solve it. Meta-prompting performs such translation without explicitly being instructed to do so.\nOur structured approach embodies the principle of the wisdom of the crowd (Suzgun et al., 2023a), which\nposits that a collective opinion of a diverse set of critical thinkers often surpasses the insights of individual\nexperts. By harnessing an ensemble of specialized expert models under the guidance of the Meta Model,\neach contributing from different angles of expertise, we achieve more accurate and reliable problem-solving.\n4.3\nFresh Eyes\nThe concept of fresh eyes helps mitigate the well-known problem of LMs doubling-down on their mistakes\nand exhibiting overconfidence (see, e.g., Zhang et al., 2023b). Fresh eyes are a crucial differentiator between\nmeta-prompting and the multipersona prompting, and thus comparing experimental results demonstrates\nthe advantage. In meta-prompting, fresh perspectives are introduced by engaging experts\u2014or personas\u2014to\nreassess the problem. This approach provides an opportunity for novel insights and the potential discovery\nof previously unnoticed incorrect solutions.\nGrounded in principles from cognitive psychology, fresh perspectives can lead to more creative problem-\nsolving and error detection. When individuals or models approach a problem without preconceived notions,\nthey are more likely to consider alternative solutions and identify errors that might have been overlooked.\nFresh eyes may help avoid cognitive biases such as anchoring, confirmation bias, as well as overconfidence.\nConsider the following summary of an execution, which illustrates the benefit of \u201cfresh eyes\u201d in practice.\nSay the task is the 24 game, e.g., to use each of the numbers 6, 11, 12, and 13, exactly once, in an arithmetic\nexpression whose value is 24. The history may look something like the following:\n1. The Meta Model proposes consulting experts in mathematics, problem-solving, and Python program-\nming. It emphasizes the need for accuracy and adherence to constraints, suggesting the involvement of\nanother expert for review if needed.\n2. An expert proposes a solution, which a second expert identifies to be incorrect, and the Meta Model\nsuggests writing a Python program to find a valid solution.\n3. A programming expert is consulted to write a program.\n4. Another programming expert identifies an error in the script, modifies it, and then executes the revised\nscript.\n9\n\n5. A mathematics expert is consulted to verify the solution output by the program.\n6. After this verification, the Meta Model outputs it as the final answer.\nThis example underscores how meta-prompting, incorporating fresh perspectives at each step (since the\nexpert\u2019s prompt does not include the whole history), not only finds solutions but also effectively identifies\nand corrects errors. The diversity of perspectives, ranging from problem-solving strategies to technical\nexecution and verification, demonstrates how different angles of expertise contribute to a more robust and\nreliable problem-solving process.\n4.4\nReal-Time Code Execution\nThe introduction of a Python expert for code generation and execution within our meta-prompting framework\nleads to significant advancement in tackling algorithmic challenges. This enhancement is evident in Python\nProgramming Puzzles, where the integration of the Expert Python into the meta-prompting framework\nelevates the success rate from 32.7% to 45.8%. This improvement primarily arises from the Meta Model\u2019s\nability to use a Python expert for generating and executing code based on natural-language instructions.\nReal-time code execution enables instant validation and optimization of solutions, substantially improving\nboth the efficiency and precision of problem-solving.\nThis enhancement is not confined to a single task type, however. In tasks such as the Game of 24 and Word\nSorting, accuracy rates increase by 56.0% and 15.6%, respectively, with the integration of a Python interpreter\ninto meta-prompting. (When compared with the baseline standard prompting, the accuracy gains correspond\nto 64.0% and 19.2%, respectively.) These improvements highlight the significant role of code generation and\nexecution in enhancing the effectiveness of the meta-prompting framework, demonstrating its transformative\nimpact across various computational tasks. Overall, integrating a Python interpreter results in an average\nperformance improvement of an additional 11.5% across different tasks compared to meta-prompting without\na Python interpreter.\nHowever, the introduction of real-time code execution also brings essential security considerations. Estab-\nlishing such a system requires a secure and controlled environment to mitigate risks such as data breaches\nand system vulnerabilities. Therefore, the deployment of a Python interpreter within the meta-prompting\nframework should be fortified with a secure sandbox. These measures are crucial to ensure the system\u2019s\nintegrity and the protection of user data, ensuring that the advantages of improved problem-solving efficiency\nare not compromised in any way by security and privacy concerns, among other issues.\nExpert Graphic Designer (63.1%)\nExpert Geometer (15.5%)\nExpert Mathematician (95.2%)\nExpert Python (64.3%)\nExpert Mathematician (15.8%)\nExpert Poet (50.2%)\nExpert Essayist (37.6%)\nExpert Chess Player (48.4%)\nExpert Chess Analyst (32.1%)\nExpert Python (73.9%)\nExpert Linguist (16.8%)\nExpert Mathematician (41.8%)\nExpert Python (36.0%)\nExpert Problem Solver (22.3%)\n0\n20\n40\n60\n80\n100\nGeometric Shapes\nMultistep Arithmetic Two\nPython Programming Puzzles\nSonnet Writing\nCheckmate-in-One\nWord Sorting\nGame of 24\nExpert Linguist\nExpert Chess Player 2\nExpert Chess Analyst\nExpert Chess Player\nExpert Literary Critic\nExpert Essayist\nExpert Poet\nExpert Python Programmer\nExpert Python\nExpert Problem Solver\nExpert Mathematician\nExpert Graphic Designer 2\nExpert Geometer\nExpert Graphic Designer\nPercentage (%)\nFigure 4: Distribution of experts conjured by the Meta Model in experiments involving a Python interpreter. The remaining\nblank space represents a combination of experts that were employed infrequently.\n10\n\nExpert Graphic Designer (52.1%)\nExpert Mathematician (59.8%)\nExpert Arithmetic Verifier (20.7%)\nExpert Programmer (45.9%)\nExpert Mathematician (18.7%)\nExpert Poet (50.1%)\nExpert Poet Reviewer (25.2%)\nExpert Chess Player (47.4%)\nExpert Chess Analyst (38.0%)\nExpert Linguist (38.9%)\nExpert Proofreader (21.4%)\nExpert Essayist (15.9%)\nExpert Mathematician (51.0%)\nExpert Problem Solver (44.9%)\n0\n20\n40\n60\n80\n100\nGeometric Shapes\nMultistep Arithmetic Two\nPython Programming Puzzles\nSonnet Writing\nCheckmate-in-One\nWord Sorting\nGame of 24\nExpert Puzzle Solver\nExpert Problem Solver\nExpert Essayist\nExpert Proofreader\nExpert Linguist\nExpert Chess Player 2\nExpert Chess Analyst\nExpert Chess Player\nExpert Literary Critic\nExpert Poet Reviewer\nExpert Poet\nExpert Python Programmer\nExpert Programmer\nExpert Arithmetic Checker\nExpert Arithmetic Verifier\nExpert Mathematician\nExpert SVG Specialist\nExpert Graphic Designer 2\nExpert Graphic Designer\nPercentage (%)\nFigure 5: Distribution of experts conjured by the Meta Model in experiments without the use of a Python interpreter.\n5\nFurther Discussion\n5.1\nAdditional Analysis of Meta Prompting\nAnalysis of Expert Types Used in Meta Prompting. The Meta Model\u2019s dynamic selection of expert types\ndistinctly illustrates its adaptability and strategic alignment with specific task requirements. Analyzing tasks\nwith and without a Python interpreter offers insightful contrasts in the model\u2019s expert choices, influenced by\nthe available tools and task characteristics. In scenarios where a Python expert is explicitly mentioned for\ncode generation and execution, there is a noticeable preference for technical and computational expertise.\nFor example, in Python Programming Puzzles, the Meta Model frequently utilizes Expert Python, Expert\nMathematician, and several tiers of Expert Python Programmers. This pattern reveals a task-oriented strategy,\nhighlighting a focus on programming and algorithmic problem-solving. Similarly, tasks such as Game of\n24 and Word Sorting prominently feature Expert Python, reinforcing the model\u2019s propensity to rely on\ncomputational expertise when Python capabilities are accessible.\nIn contrast, for meta-prompting without a specific Python expert, the spectrum of experts employed is more\ndiverse. Tasks like Geometric Shapes predominantly involve design and geometry experts (e.g., Expert\nGraphic Designer and Expert Geometer), indicating a pivot towards visual and spatial problem-solving\nrather than computational approaches. This task illustrates where the Meta Model may have made a poor\nchoice of experts, and in particular it might have been more preferable to use an expert in SVG visualizations.\nIn Sonnet Writing, the Meta Model naturally leans on literary experts, notably Expert Poet and Expert\nLiterary Critic, emphasizing creative and linguistic skills. This pattern demonstrates the Meta Model\u2019s\nability to dynamically tailor its expert engagement to the demands of the task, utilizing technical experts for\ncomputational challenges and a varied range of non-computational expertise for creative or abstract tasks.\nNumber of Rounds Taken to Reach a Solution. Examining the meta-prompting experiments involving a\nPython expert reveals that the average number of rounds required to reach a solution in the Meta Model varies\nsignificantly across tasks, indicative of their complexity and specific nature. Simpler tasks, such as Word\nSorting (3.31 rounds) and Checkmate-in-One (3.48 rounds), typically necessitate fewer rounds, suggesting a\nmore linear and straightforward resolution process, likely due to their clearly defined parameters. Conversely,\nmore algorithmically challenging tasks like Python Programming Puzzles average a higher number of\nrounds at 6.07, reflecting the nuanced and multifaceted aspects of programming tasks that require extensive\ninteractions for thorough clarification and iterative refinement. The Game of 24 and Multistep Arithmetic\nTwo, with averages around 3.5 rounds, meld computational proficiency with logical reasoning, necessitating\nadditional rounds for accurate and precise solutions. This observed correlation between the number of rounds\nand the task complexity underscores the Meta Model\u2019s proficiency and adaptability. It efficiently manages\nsimpler tasks with minimal interactions while skillfully handling the complexities of more challenging and\n11\n\nheuristic-based problems, ensuring precision and efficacy in its solutions. This performance characteristic is\nparticularly critical in environments where efficiency and interaction trade-off are key.\nEnhancing Solution Reliability through Systematic Verification. The Meta Model\u2019s systematic verification\nprotocol strengthens the reliability and robustness of its solutions. Fundamental to this approach is the\nconsistent practice of consulting an expert for validation before finalizing responses, a principle applied\nacross diverse tasks. This method is further evidenced by the detailed interaction data. In tasks such as\nCheckmate in One, for instance, the Meta Model employs a two-step verification strategy. Initially, it consults\nan Expert Chess Player to come up with a solution, followed by a critical verification from an Expert Chess\nAnalyst, ensuring strategic correctness. A similar approach is adopted in Sonnet Writing too, where an\nExpert Poet drafts the sonnet, and an Expert Poet Reviewer or Expert Essayist reviews it, making sure that the\nsolution adheres to the strict rhyme scheme. This unsupervised but rigorous verification process extends to\ncomplex tasks like Game of 24 and MGSM, involving both external expert consultations and internal reviews.\nBy integrating this dual verification mechanism, the model significantly enhances solution accuracy and\nreliability, essential for real-world applications where precision is paramount.\nNavigating No-Solution Territories. Meta-prompting enables the Meta Model to acknowledge the absence\nor impossibility of a valid solution or its inability to find one more frequently than other prompting methods.\nIn 100 examples of the Game of 24, the model reports no solution 9 times with Expert Python and 15 times\nwithout it, compared to the mere 2 instances under standard prompting. In Checkmate, across 250 examples,\nit admits to no solution 12 times without Expert Python and 10 times with it, a rarity in multipersona and\nstandard prompting. While there were always solutions, it is arguably preferable to abstain from answering\nrather than provide an incorrect answer. Typically expressed as \u201cNo valid solution found\u201d or more explicitly\nas \u201cThere is no solution to the 24 game with these numbers given the constraints,\u201d these acknowledgments\nare likely the result of the model\u2019s verification and feedback loop, emphasizing accuracy and confidence over\nspeculative but incorrect responses.\nSetting the Bar High: GPT-4\u2019s Zero-Shot Task Solving Capabilities. Even without the enhanced capabili-\nties of meta-prompting, GPT-4 stands out as an effective zero-shot task solver under standard prompting\nconditions. Its performance across various tasks, including Python Programming Puzzles and MGSM, is\nremarkable, particularly when compared to other LMs as highlighted by OpenAI (2023). GPT-4 excels as\na task-agnostic solver, capable of processing and responding to diverse queries effectively. A significant\nattribute of GPT-4 is its proficiency in following instructions. Given clear and unambiguous natural-language\ninstructions, the model demonstrates a high level of compliance and accuracy. This aspect of instruction-\nfollowing is also a cornerstone of our meta-prompting framework, where we leverage GPT-4\u2019s capabilities.\nOur experiments reinforce that GPT-4 excels in code generation, demonstrates impressive zero-shot reasoning,\nand engages effectively in role-playing, solidifying its position as a versatile and reliable LM.\nLimited Performance Improvement with GPT-3.5. In comparison to GPT-4, GPT-3.5 demonstrates a more\nlimited scope of performance enhancement across various tasks. Although it shows notable improvements\nin specific tasks such as Sonnet Writing and Checkmate-in-One, its capabilities do not consistently surpass\nbaseline standards or zero-shot CoT prompting methods in other tasks, notably Word Sorting and Multiple\nArithmetic Two. Our qualitative analysis suggests that GPT-3.5 may not be as effective as GPT-4 in simulating\nrole-playing scenarios or managing extended context windows. This observation leads us to believe that\nfactors such as the scale of the model, the quality and size of the instruction-following corpus may be\nsignificantly influencing the efficacy of the meta-prompting approach. Furthermore, it appears that the\nadvantages offered by meta-prompting may even emerge more prominently at larger model scales.\n5.2\nLimitations and Failure Modes of Meta Prompting\nThe meta-prompting framework, despite its innovative approach, encounters several notable limitations,\nincluding cost efficiency, scalability, operational linearity, domain restrictions, information transfer challenges,\nand response patterns. A primary limitation is the elevated cost associated with multiple model calls. In our\nsetup using GPT-4, the dual role of the Meta Model and the experts, distinguished by unique instructions,\nincurs substantial costs under the GPT-4 API pricing model. This cost factor diminishes the effectiveness\n12\n\nof meta-prompting in smaller models like ChatGPT, which lack the comprehensive capabilities of GPT-4.\nConsequently, meta-prompting, though insightful, can become prohibitively expensive due to extensive\nmodel interactions and lengthy message histories. However, these costs will decrease as the costs of LMs\ndecrease. Note that recent OpenAI API features announced after the experiments were run, namely the\nability to run code in a sandbox directly through the API, could significantly decrease the costs of our system.\nAnother critical limitation is the requirement for substantial scale and a considerable context window. GPT-4\nfits this criterion, but smaller models such as ChatGPT fall short. Meta-prompting\u2019s design, characterized by\nextensive message histories, demands an LM capable of handling and retaining lengthy textual information, a\nfeature not universally present in all LMs. Operational efficiency is also challenged by the linear (sequential)\nnature of meta-prompting. The framework, in its current form, processes steps one at a time, relying on the\noutcome of preceding calls. This dependency constrains the possibility of parallel processing, impacting the\nspeed and efficiency of the system.\nAdditionally, our research confined meta-prompting within a closed-domain system. Nevertheless, the\nframework\u2019s potential extends to incorporating external resources such as APIs, specialized finetuned\nmodels, search engines, or computational tools. More expansive implementations like AutoAgents (Chen\net al., 2023a) and AutoGen (Wu et al., 2023), which include higher-level planning and diverse cooperation\nmechanisms, offer a glimpse into future directions. In subsequent versions, the Meta Model could benefit\nfrom refining or summarizing its history before advancing, optimizing the relevance and efficiency of the\nprocess. There is also untapped potential in concurrently summoning multiple experts or utilizing a single\nexpert with varied temperature parameters to synthesize their outputs.\nA practical challenge faced is the Meta Model\u2019s occasional oversight in conveying necessary information to\nexperts, forgetting that experts can only access data adhering to a certain format (within triple quotes in our\nsystem). This oversight can lead to unintended confusion and underscores the need for improved information\nmanagement. Lastly, the Meta Model\u2019s response pattern, particularly in tasks with lower performance, often\nincludes apologies, such as \u201cApologies for the confusion in my previous response\u201d or \u201cI apologize for the\nprevious incorrect solution.\u201d This behavior likely stems from its training on instruction-following data.\n6\nRelated Work\nThis section seeks to contextualize our proposed meta-prompting approach amidst recent advancements\nin prompting strategies and scaffolding techniques. We provide a brief overview of these developments,\nhighlighting their relevance and connections to our work.\nEnhancing Reasoning in Language Models through Prompting. Recent efforts in LM scaffolding and\nprompting methods have significantly boosted the arithmetic and commonsense reasoning capabilities of\nLMs. The chain-of-thought (CoT) prompting (Wei et al., 2022b) and its variants\u2014including least-to-most\n(Zhou et al., 2023), zero-shot CoT (Kojima et al., 2022), self-ask (Press et al., 2022), ask-me-anything (Arora\net al., 2023), decomposed prompting (Khot et al., 2023), and auto-CoT (Zhang et al., 2023d)\u2014have marked a\nparadigm shift in how LMs process complex queries. These methods encourage LMs to adopt human-like,\nsequential thinking processes, breaking down intricate questions into simpler subtasks and systematically\nsolving them before presenting a final answer. Multiple studies (Wei et al., 2022a; Madaan and Yazdanbakhsh,\n2022; Shi et al., 2023; Drozdov et al., 2023; Fu et al., 2023b; Suzgun et al., 2023b, inter alia) have shown the\nefficacy of these prompting methods across a broad set of tasks and benchmarks. More recent innovations\nsuch Tree-of-Thought (Yao et al., 2023a), Graph-of-Thought (Besta et al., 2023), Program-of-Thought (Chen\net al., 2023d), and Skeleton-of-Thought (Ning et al., 2023), have further enriched this domain; these explore\ndynamic, non-linear reasoning pathways, broadening the computational and heuristic capabilities of LMs.\nHowever, they come with increased resource demands and greater time complexity, require multiple manual\nprompt crafting, and are often specialized for particular types of tasks.\nIterative Self-Feedback and Refinement Mechanisms. Recent instruction-following techniques and data-\ncollection efforts have expanded the capabilities of LMs to follow instructions, emulate certain aspects of\nhuman behavior, and assist in tasks such as annotation and evaluation (Haluptzok et al., 2022; Aher et al.,\n13\n\n2023). LMs such as GPT-4, PaLM, and Llama are now capable of effectively integrating self-feedback and\nrefinement mechanisms through prompting and can leverage their own natural-language outputs to guide\ntheir behaviour and improve decision-making. SayCan (Ahn et al., 2022) and Inner Monologue (Huang\net al., 2023) are early examples showcasing the benefits of inner dialogues in a closed-loop system for robotic\ncontrol and action planning. Reflexion (Shinn et al., 2023) builds upon these studies and focuses on natural-\nlanguage generation and reasoning tasks. It functions as a policy optimization mechanism through natural\nlanguage feedback, using self-feedback and self-reflection to influence and correct behaviors in LMs, and\nhas shown considerable success in preliminary experiments. In a more innovative vein, the Self-Taught\nReasoner approach (STaR; Zelikman et al., 2022) iteratively trains an LM on its own outputs to refine initial\nrationales for more accurate solutions, leading to enhanced reasoning skills. Other notable methods such as\nCritic (Gou et al., 2023), Iterative Refinement (Chen et al., 2023b), RCI (Kim et al., 2023), Re3 (Yang et al.,\n2022), Refiner (Paul et al., 2023), Self-Critique (Saunders et al., 2022), Self-Correction (Welleck et al., 2023),\nSelf-Eval, Self-Debug (Chen et al., 2023c), Self-Edit (Zhang et al., 2023a), Self-Evolve (Jiang et al., 2023b),\nSelf-Taught Optimizer (SToP; Zelikman et al., 2023), and so forth, illustrate how verbal feedback, both internal\nand external, can significantly improve the accuracy, quality, and robustness of model outputs across various\ntasks and setups.\nExploring Role-Playing in Language Models. The integration of role-playing and self-collaboration concepts\ninto LMs, grounded in cognitive psychology and developmental education principles, has emerged as a useful\nmethod for augmenting LMs\u2019 problem-solving capabilities and optimizing their internal domain-specific\nknowledge and expertise. Recent studies (Park et al., 2022, 2023; Li et al., 2023; Xu et al., 2023; Fu et al., 2023a;\nDeshpande et al., 2023) have shown that endowing instruction-following LMs with \u201cexpert\u201d personas or\nroles enhances the quality and accuracy of their output. In particular, approaches like CAMEL (Li et al., 2023)\nand Expert Prompting (Xu et al., 2023), which involve dynamically assigning personas to a single LM, have\nbeen shown to yield higher quality and more reliable responses than models without designated personas.\nFurther investigations (Chen et al., 2023a,e; Du et al., 2023; Hao et al., 2023b; Liang et al., 2023; Liu et al., 2023b;\nJiang et al., 2023a; Xiong et al., 2023; Zhang et al., 2023c) demonstrate that assigning multiple expert identities\nor roles to a single LM, tailored to specific tasks or problems, and prompting it to conduct multi-round\ninternal dialogues\u2014similar to a team of experts discussing and refining ideas\u2014amplifies the reliability and\ncomprehensiveness of the LM\u2019s analysis; this leads to more well-rounded and thorough solutions. These\nstudies advocate a complementary approach wherein multiple instances of an LM propose, debate, and refine\ntheir individual responses and reasoning in successive rounds, culminating in a unified final answer. This\nrole-playing concept has shown to significantly improve mathematical and strategic reasoning across various\ntasks. Moreover, it improves the factual accuracy of the generated content, thereby reducing erroneous or\nfabricated responses.\nAutonomous Decision-Making and Execution in Multi-Agent LM Systems. There has been a growing\ninterest in using LMs for autonomous decision-making and task execution. Open-source projects like Auto-\nGPT, Agent-GPT, Baby-AGI, and LangChain are notable efforts developing agent protocols that are capable\nof planning, decision-making, and executing tasks end-to-end, with minimal or no human intervention.\nThese systems highlight the potential and risks of LMs, which go beyond performing predefined tasks to\nadapting, learning, and autonomously executing decisions in real time. As discussed by Masa (2023), those\nautonomous models might be exploited by individuals with malicious intents and pose threats to humanity.\nThere is also the dilemma of accountability: who bears responsibility when an LM-driven autonomous agent\nproduces an inappropriate or criminal action? Ensuring safety and security with these agents is crucial, given\nits potential for mishaps or exploitation by malicious actors, and its vulnerability to cyber-attacks.\nIntegration of External Tools and APIs into Language Models. As LMs continue to evolve, the integration\nof external tools is becoming increasingly important. This tool-use integration, often achieved through\nin-context learning (e.g., Cai et al., 2023) or finetuning (e.g., Schick et al., 2023a), allows LMs to effectively\nengage with real-world scenarios and tackle a diverse range of dynamic tasks. Recent advancements (Cai\net al., 2023; Gao et al., 2023; Gou et al., 2023; Hao et al., 2023c; Khattab et al., 2023; Lu et al., 2023; Qiao\net al., 2023; Paranjape et al., 2023; Patil et al., 2023; Schick et al., 2023a; Yang et al., 2023; Yuan et al., 2023)\nhave enabled LMs to perform accurate calculations, retrieve up-to-date information from search engines\nor databases, and interact with APIs, making them crucial for complex, multimodal real-world problems.\n14\n\nOpenAI\u2019s incorporation of predefined APIs and plugins into ChatGPT underscores the importance of external\nintegration in developing a comprehensive LM ecosystem. However, most approaches often limit themselves\nto a select group of tools or domain-specific resources, posing challenges in adapting to new domains (Lu\net al., 2023). Our meta-prompting approach, as detailed in Section 2, treats the LM as an independent tool and\nexpert, available on-demand for specific tasks. Furthermore, incorporating a Python interpreter\u2014through\nExpert Python\u2014to execute and evaluate model-generated code has been instrumental in enhancing both\naccuracy and efficiency in various tasks.\n7\nConclusion\nIn this work, we have introduced and examined meta-prompting, a simple yet powerful scaffolding technique\nthat enhances the performance of language models in a task-agnostic manner. This approach leverages\na language model to act as both a central conductor and a group of expert instances, thereby endowing\ntraditional models with dynamic, multi-functional capabilities. A noteworthy aspect of meta-prompting\nlies in its proficiency to decompose complex tasks, engage distinct expertise for each component, and then\nintegrate the varied outputs seamlessly. Demonstrating significant, double-digit improvements across a series\nof tasks, ranging from challenging arithmetic puzzles like the Game of 24 to the creative literary exercise of\nShakespearean Sonnet Writing, meta-prompting promises to grow more potent and cost-efficient as language\nmodels continue to evolve, offering exciting prospects for future applications.\nAcknowledgements\nWe would like to thank Federico Bianchi, Annabelle Carrell, Tayfun G\u00fcr, Dan Jurafsky, Suproteem Sarkar,\nScott Duke Kominers, Lester Mackey, Neil Mallinar, \u015eule Kahraman, Deniz Kele\u015f, Luke Melas-Kyriazi, Drew\nPendergrass, Faiz Surani, Garrett Tanzer, Michael Wornow, and Eric Zelikman for their valuable comments,\nuseful suggestions, and support.\n15\n\nReferences\nGati V Aher, Rosa I. Arriaga, and Adam Tauman Kalai. 2023. Using Large Language Models to Simulate\nMultiple Humans and Replicate Human Subject Studies. In Proceedings of the 40th International Conference\non Machine Learning (Proceedings of Machine Learning Research, Vol. 202), Andreas Krause, Emma Brunskill,\nKyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (Eds.). PMLR, 337\u2013371. https:\n//proceedings.mlr.press/v202/aher23a.html\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn,\nChuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. 2022. Do as i can, not as i say: Grounding\nlanguage in robotic affordances. arXiv preprint arXiv:2204.01691 (2022).\nRohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak\nShakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023. PaLM 2 Technical Report. arXiv preprint\narXiv:2305.10403 (2023). https://arxiv.org/abs/2305.10403\nSimran Arora, Avanika Narayan, Mayee F Chen, Laurel Orr, Neel Guha, Kush Bhatia, Ines Chami, and\nChristopher Re. 2023. Ask Me Anything: A simple strategy for prompting language models. In The Eleventh\nInternational Conference on Learning Representations. https://openreview.net/forum?id=bhUPJnS2g0X\nMaciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz\nLehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, et al. 2023. Graph of thoughts: Solving\nelaborate problems with large language models. arXiv preprint arXiv:2308.09687 (2023).\nBIG-Bench authors. 2023. Beyond the Imitation Game: Quantifying and extrapolating the capabilities of\nlanguage models. Transactions on Machine Learning Research (2023). https://openreview.net/forum?\nid=uyTL5Bvosj\nTianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. 2023. Large language models as tool\nmakers. arXiv preprint arXiv:2305.17126 (2023).\nGuangyao Chen, Siwei Dong, Yu Shu, Ge Zhang, Jaward Sesay, B\u00f6rje F Karlsson, Jie Fu, and Yemin Shi. 2023a.\nAutoAgents: A Framework for Automatic Agent Generation. arXiv preprint arXiv:2309.17288 (2023).\nPinzhen Chen, Zhicheng Guo, Barry Haddow, and Kenneth Heafield. 2023b. Iterative Translation Refinement\nwith Large Language Models. arXiv preprint arXiv:2306.03856 (2023).\nWenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. 2023d. Program of Thoughts Prompting:\nDisentangling Computation from Reasoning for Numerical Reasoning Tasks. Transactions on Machine\nLearning Research (2023). https://openreview.net/forum?id=YfZ4ZPt8zd\nWeize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia Qin, Yaxi\nLu, Ruobing Xie, et al. 2023e. Agentverse: Facilitating multi-agent collaboration and exploring emergent\nbehaviors in agents. arXiv preprint arXiv:2308.10848 (2023).\nXinyun Chen, Maxwell Lin, Nathanael Sch\u00e4rli, and Denny Zhou. 2023c. Teaching large language models to\nself-debug. arXiv preprint arXiv:2304.05128 (2023).\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word\nproblems. arXiv preprint arXiv:2110.14168 (2021).\nAmeet Deshpande, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, and Karthik Narasimhan. 2023.\nToxicity in chatgpt: Analyzing persona-assigned language models. arXiv preprint arXiv:2304.05335 (2023).\nAndrew Drozdov, Nathanael Sch\u00e4rli, Ekin Aky\u00fcrek, Nathan Scales, Xinying Song, Xinyun Chen, Olivier\nBousquet, and Denny Zhou. 2023. Compositional Semantic Parsing with Large Language Models. In\nThe Eleventh International Conference on Learning Representations.\nhttps://openreview.net/forum?id=\ngJW8hSGBys8\n16\n\nYilun Du, Shuang Li, Antonio Torralba, Joshua B Tenenbaum, and Igor Mordatch. 2023. Improving Factuality\nand Reasoning in Language Models through Multiagent Debate. arXiv preprint arXiv:2305.14325 (2023).\nYao Fu, Hao Peng, Tushar Khot, and Mirella Lapata. 2023a. Improving language model negotiation with\nself-play and in-context learning from ai feedback. arXiv preprint arXiv:2305.10142 (2023).\nYao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. 2023b. Complexity-Based Prompting\nfor Multi-step Reasoning. In The Eleventh International Conference on Learning Representations.\nhttps:\n//openreview.net/forum?id=yf1icZHC-l9\nDifei Gao, Lei Ji, Luowei Zhou, Kevin Qinghong Lin, Joya Chen, Zihan Fan, and Mike Zheng Shou. 2023.\nAssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn. arXiv preprint\narXiv:2306.08640 (2023).\nZhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. 2023. CRITIC:\nLarge language models can self-correct with tool-interactive critiquing. arXiv preprint arXiv:2305.11738\n(2023).\nPatrick Haluptzok, Matthew Bowers, and Adam Tauman Kalai. 2022. Language Models Can Teach Themselves\nto Program Better. In The Eleventh International Conference on Learning Representations.\nRui Hao, Linmei Hu, Weijian Qi, Qingliu Wu, Yirui Zhang, and Liqiang Nie. 2023b. ChatLLM Network:\nMore brains, More intelligence. arXiv preprint arXiv:2304.12998 (2023).\nShibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting Hu. 2023a.\nReasoning with language model is planning with world model. arXiv preprint arXiv:2305.14992 (2023).\nShibo Hao, Tianyang Liu, Zhen Wang, and Zhiting Hu. 2023c. ToolkenGPT: Augmenting Frozen Language\nModels with Massive Tools via Tool Embeddings. arXiv preprint arXiv:2305.11554 (2023).\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan Tompson,\nIgor Mordatch, Yevgen Chebotar, et al. 2023. Inner Monologue: Embodied Reasoning through Planning\nwith Language Models. In Conference on Robot Learning. PMLR, 1769\u20131782.\nDongfu Jiang, Xiang Ren, and Bill Yuchen Lin. 2023a. LLM-Blender: Ensembling Large Language Models\nwith Pairwise Ranking and Generative Fusion. arXiv preprint arXiv:2306.02561 (2023).\nShuyang Jiang, Yuhao Wang, and Yu Wang. 2023b. SelfEvolve: A Code Evolution Framework via Large\nLanguage Models. arXiv preprint arXiv:2306.02907 (2023).\nOmar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan,\nSaiful Haq, Ashutosh Sharma, Thomas T Joshi, Hanna Moazam, et al. 2023. DSPy: Compiling Declarative\nLanguage Model Calls into Self-Improving Pipelines. arXiv preprint arXiv:2310.03714 (2023).\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabhar-\nwal. 2023. Decomposed Prompting: A Modular Approach for Solving Complex Tasks. In The Eleventh\nInternational Conference on Learning Representations. https://openreview.net/forum?id=_nGgzQjzaRy\nGeunwoo Kim, Pierre Baldi, and Stephen McAleer. 2023. Language models can solve computer tasks. arXiv\npreprint arXiv:2303.17491 (2023).\nTakeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large\nLanguage Models are Zero-Shot Reasoners. In Advances in Neural Information Processing Systems, Vol. 35.\n22199\u201322213.\nGuohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem. 2023.\nCamel: Communicative agents for\" mind\" exploration of large scale language model society. arXiv preprint\narXiv:2303.17760 (2023).\n17\n\nTian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng Tu, and\nShuming Shi. 2023. Encouraging Divergent Thinking in Large Language Models through Multi-Agent\nDebate. arXiv preprint arXiv:2305.19118 (2023).\nZeyi Liu, Arpit Bahety, and Shuran Song. 2023a. Reflect: Summarizing robot experiences for failure explana-\ntion and correction. arXiv preprint arXiv:2306.15724 (2023).\nZijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, and Diyi Yang. 2023b. Dynamic LLM-Agent Network: An\nLLM-agent Collaboration Framework with Agent Team Optimization. arXiv preprint arXiv:2310.02170\n(2023).\nPan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng\nGao. 2023. Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models. arXiv\npreprint arXiv:2304.09842 (2023).\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha\nDziri, Shrimai Prabhumoye, Yiming Yang, et al. 2023. Self-refine: Iterative refinement with self-feedback.\narXiv preprint arXiv:2303.17651 (2023).\nAman Madaan and Amir Yazdanbakhsh. 2022. Text and patterns: For effective chain of thought, it takes two\nto tango. arXiv preprint arXiv:2209.07686 (2022).\nMasa. 2023. 7 Challenges and Potential Risks of AutoGPT Technology. AutoGPT Official (Apr 2023). https:\n//autogpt.net/challenges-and-potential-risks-of-autogpt-technology/\nXuefei Ning, Zinan Lin, Zixuan Zhou, Huazhong Yang, and Yu Wang. 2023. Skeleton-of-thought: Large\nlanguage models can do parallel decoding. arXiv preprint arXiv:2307.15337 (2023).\nOpenAI. 2023. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774 (2023). https://arxiv.org/abs/\n2303.08774\nBhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke Zettlemoyer, and Marco Tulio\nRibeiro. 2023. ART: Automatic multi-step reasoning and tool-use for large language models. arXiv preprint\narXiv:2303.09014 (2023).\nJoon Sung Park, Joseph C. O\u2019Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S.\nBernstein. 2023. Generative Agents: Interactive Simulacra of Human Behavior. In In the 36th Annual\nACM Symposium on User Interface Software and Technology (UIST \u201923) (San Francisco, CA, USA) (UIST \u201923).\nAssociation for Computing Machinery, New York, NY, USA.\nJoon Sung Park, Lindsay Popowski, Carrie Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein.\n2022. Social simulacra: Creating populated prototypes for social computing systems. In Proceedings of the\n35th Annual ACM Symposium on User Interface Software and Technology. 1\u201318.\nShishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez. 2023. Gorilla: Large language model\nconnected with massive apis. arXiv preprint arXiv:2305.15334 (2023).\nDebjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges, Antoine Bosselut, Robert West, and Boi\nFaltings. 2023. Refiner: Reasoning feedback on intermediate representations. arXiv preprint arXiv:2304.01904\n(2023).\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. 2022. Measuring\nand narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350 (2022).\nShuofei Qiao, Honghao Gui, Huajun Chen, and Ningyu Zhang. 2023. Making Language Models Better Tool\nLearners with Execution Feedback. arXiv preprint arXiv:2305.13068 (2023).\nWilliam Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. 2022.\nSelf-critiquing models for assisting human evaluators. arXiv preprint arXiv:2206.05802 (2022).\n18\n\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessi, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023a. Toolformer: Language Models Can Teach Themselves to Use\nTools.\nTimo Schick, Jane A. Yu, Zhengbao Jiang, Fabio Petroni, Patrick Lewis, Gautier Izacard, Qingfei You, Christo-\nforos Nalmpantis, Edouard Grave, and Sebastian Riedel. 2023b. PEER: A Collaborative Language Model.\nIn The Eleventh International Conference on Learning Representations. https://openreview.net/forum?id=\nKbYevcLjnc\nTal Schuster, Ashwin Kalyan, Alex Polozov, and Adam Tauman Kalai. 2021. Programming Puzzles. In\nThirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track.\nhttps://\narxiv.org/abs/2106.05784\nFreda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won\nChung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. 2023. Language models are\nmultilingual chain-of-thought reasoners. In The Eleventh International Conference on Learning Representations.\nhttps://openreview.net/forum?id=fR3wGCk-IXp\nNoah Shinn, Beck Labash, and Ashwin Gopinath. 2023. Reflexion: an autonomous agent with dynamic\nmemory and self-reflection. arXiv preprint arXiv:2303.11366 (2023).\nHaotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, and Chao Zhang. 2023. AdaPlanner: Adaptive Planning\nfrom Feedback with Language Models. arXiv preprint arXiv:2305.16653 (2023).\nMirac Suzgun, Luke Melas-Kyriazi, and Dan Jurafsky. 2023a. Follow the Wisdom of the Crowd: Effective Text\nGeneration via Minimum Bayes Risk Decoding. In Findings of the Association for Computational Linguistics:\nACL 2023, Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational\nLinguistics, Toronto, Canada, 4265\u20134293. https://doi.org/10.18653/v1/2023.findings-acl.262\nMirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha\nChowdhery, Quoc Le, Ed Chi, Denny Zhou, and Jason Wei. 2023b. Challenging BIG-Bench Tasks and\nWhether Chain-of-Thought Can Solve Them. In Findings of the Association for Computational Linguistics: ACL\n2023. Association for Computational Linguistics, Toronto, Canada, 13003\u201313051. https://doi.org/10.\n18653/v1/2023.findings-acl.824\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bash-\nlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and\nfine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023).\nZhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji. 2023. Unleashing Cogni-\ntive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration.\narXiv preprint arXiv:2307.05300 (2023).\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten\nBosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff\nDean, and William Fedus. 2022a. Emergent Abilities of Large Language Models. Transactions on Machine\nLearning Research (2022). https://openreview.net/forum?id=yzkSU5zdwD Survey Certification.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,\nand Denny Zhou. 2022b. Chain of Thought Prompting Elicits Reasoning in Large Language Models. In\nAdvances in Neural Information Processing Systems, Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and\nKyunghyun Cho (Eds.). https://openreview.net/forum?id=_VjQlMeSB_J\nSean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao Shen, Daniel Khashabi, and Yejin Choi. 2023.\nGenerating Sequences by Learning to Self-Correct. In The Eleventh International Conference on Learning\nRepresentations. https://openreview.net/forum?id=hH36JeQZDaO\n19\n\nQingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang,\nXiaoyun Zhang, and Chi Wang. 2023. AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent\nConversation Framework. arXiv:2308.08155 [cs.AI]\nKai Xiong, Xiao Ding, Yixin Cao, Ting Liu, and Bing Qin. 2023. Diving into the Inter-Consistency of Large\nLanguage Models: An Insightful Analysis through Debate. arXiv preprint arXiv:2305.11595 (2023).\nBenfeng Xu, An Yang, Junyang Lin, Quan Wang, Chang Zhou, Yongdong Zhang, and Zhendong Mao.\n2023. ExpertPrompting: Instructing Large Language Models to be Distinguished Experts. arXiv preprint\narXiv:2305.14688 (2023).\nKevin Yang, Yuandong Tian, Nanyun Peng, and Dan Klein. 2022. Re3: Generating Longer Stories With\nRecursive Reprompting and Revision. In Proceedings of the 2022 Conference on Empirical Methods in Natural\nLanguage Processing, Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computa-\ntional Linguistics, Abu Dhabi, United Arab Emirates, 4393\u20134479. https://doi.org/10.18653/v1/2022.\nemnlp-main.296\nRui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, and Ying Shan. 2023. Gpt4tools: Teaching large\nlanguage model to use tools via self-instruction. arXiv preprint arXiv:2305.18752 (2023).\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan.\n2023a.\nTree of thoughts: Deliberate problem solving with large language models.\narXiv preprint\narXiv:2305.10601 (2023).\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023b.\nReAct: Synergizing Reasoning and Acting in Language Models. In International Conference on Learning\nRepresentations (ICLR).\nLifan Yuan, Yangyi Chen, Xingyao Wang, Yi R Fung, Hao Peng, and Heng Ji. 2023. Craft: Customizing llms\nby creating and retrieving from specialized toolsets. arXiv preprint arXiv:2309.17428 (2023).\nEric Zelikman, Eliana Lorch, Lester Mackey, and Adam Tauman Kalai. 2023. Self-Taught Optimizer (STOP):\nRecursively Self-Improving Code Generation. arXiv preprint arXiv:2310.02304 (2023).\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. 2022. STaR: Bootstrapping Reasoning With\nReasoning. In Advances in Neural Information Processing Systems, Alice H. Oh, Alekh Agarwal, Danielle\nBelgrave, and Kyunghyun Cho (Eds.). https://openreview.net/forum?id=_3ELRdg2sgI\nKechi Zhang, Zhuo Li, Jia Li, Ge Li, and Zhi Jin. 2023a. Self-Edit: Fault-Aware Code Editor for Code\nGeneration. arXiv preprint arXiv:2305.04087 (2023).\nMuru Zhang, Ofir Press, Will Merrill, Alisa Liu, and Noah A. Smith. 2023b. How Language Model Hallu-\ncinations Can Snowball. ArXiv abs/2305.13534 (2023). https://api.semanticscholar.org/CorpusID:\n258841857\nYifan Zhang, Jingqin Yang, Yang Yuan, and Andrew Chi-Chih Yao. 2023c. Cumulative reasoning with large\nlanguage models. arXiv preprint arXiv:2308.04371 (2023).\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2023d. Automatic Chain of Thought Prompting\nin Large Language Models. In The Eleventh International Conference on Learning Representations.\nhttps:\n//openreview.net/forum?id=5NTt8GFjUHkr\nDenny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire\nCui, Olivier Bousquet, Quoc V Le, and Ed H. Chi. 2023. Least-to-Most Prompting Enables Complex\nReasoning in Large Language Models. In The Eleventh International Conference on Learning Representations.\nhttps://openreview.net/forum?id=WZH7099tgfM\nMingchen Zhuge, Haozhe Liu, Francesco Faccio, Dylan R Ashley, R\u00f3bert Csord\u00e1s, Anand Gopalakrishnan,\nAbdullah Hamdi, Hasan Abed Al Kader Hammoud, Vincent Herrmann, Kazuki Irie, et al. 2023. Mindstorms\nin Natural Language-Based Societies of Mind. arXiv preprint arXiv:2305.17066 (2023).\n20\n",
    " \n \nMeta-Learning and Knowledge Discovery based Physics-Informed \nNeural Network for Remaining Useful Life Prediction  \nYu Wang, Shujie Liu*, Shuai Lv, Gengshuo Liu \nState Key Laboratory of High-performance Precision Manufacturing, Dalian University of \nTechnology, Dalian 116024, China \nAbstract: Accurately predicting the Remaining Useful Life (RUL) of critical machinery is \nessential for enhancing the reliability and operational efficiency of modern manufacturing systems. \nHowever, existing machine learning-based methods often face two major challenges in practical \napplications: scarcity of target domain samples and a lack of explicit dynamic equation descriptions \nfor the equipment degradation process, making it difficult for models to effectively incorporate \nphysical laws for prediction. To address these challenges, this paper proposes a Meta-Learning and \nKnowledge Discovery based Physics-Informed Neural Network (MKDPINN) for RUL prediction \nof machinery. The method first maps high-dimensional, noisy sensor data to a smooth, low-\ndimensional hidden state space characterizing equipment degradation through a Hidden State \nMapper (HSM). Subsequently, a Physics-Guided Regulator (PGR) adaptively learns the unknown \nnon-linear partial differential equation (PDE) governing its evolution based on this hidden state, \nrealizing the discovery of potential physical laws from the data. This discovered PDE is embedded \nas a physical constraint into the PINN framework, guiding RUL prediction through a regularized \nloss function, and deeply integrating data-driven and physics knowledge. This framework is further \nembedded into a first-order meta-learning paradigm, and by performing inner and outer loop \noptimization on multiple source domain meta-tasks, the gradient of the expected loss function of all \nmeta-tasks is implicitly approximated, aligning the model parameters with the aggregated optimal \nparameters from each meta-task, requiring only a few target domain samples to quickly adjust the \nmodel and adapt to new RUL prediction tasks. Experimental validation using data from industrial \nslurry pumps operating in a large-scale processing system and the public C-MAPSS benchmark \nconfirms the superior performance and generalization of the proposed MKDPINN compared to \nbaseline models. Source code of the proposed model is publicly available at \nhttps://github.com/Sephiroth66616/MKDPINN \n \nKeywords: Remaining Useful Life, Meta learning, Physics-Informed Neural Networks \n1. Introduction \nModern manufacturing systems, increasingly reliant on automation and complex equipment \nintegration, depend heavily on the reliable operation of core components like rotating machinery. \n[1]. These devices operate under high loads and complex working conditions for extended periods, \ninevitably leading to various forms of degradation and failure. Within the context of interconnected \nmanufacturing environments, unscheduled downtime of such critical machinery not only results in \nsubstantial economic losses due to production halts but can also cascade through the system, \npotentially triggering safety incidents. Therefore, accurately predicting the Remaining Useful Life \n(RUL) of rotating machinery is crucial for enabling data-driven maintenance strategies, ensuring \nthe overall reliability and resilience of the manufacturing system, optimizing operational schedules, \nand reducing lifecycle costs [2, 3]. \nIn recent years, with the rapid development of prognostics and health management (PHM) \n\n \n \ntechnology, RUL prediction methods based on machine learning (ML) have gradually become a \nresearch hotspot and have demonstrated great application potential[4]. These methods do not rely \non an in-depth understanding of the degradation mechanisms of the equipment, but instead learn \ndegradation patterns directly from historical operating data [5] and predict the RUL of the equipment \naccordingly [6]. Compared to traditional methods based on physical models and statistics-based \nmodels, this approach exhibits strong nonlinear fitting capabilities, adaptability, and the potential \nfor modeling complex mechanical systems [7]. For example: Shi et al.[8] proposed a lightweight \ndual-attention mechanism and long short-term memory network model (DA-LSTM), combining \nLSTM's ability to model sequential degradation features with the dual-attention mechanism's ability \nto aggregate features, achieving accurate RUL prediction; Zhong et al.[9] proposed a bearing RUL \nprediction model based on a multi-region hypergraph self-attention network (M-HGSAN), \neffectively extracting high-order correlations and key features from bearing monitoring data by \nconstructing multi-region receptive fields and combining hypergraph neural networks and self-\nattention mechanisms; Wu et al.[10] proposed a dual-path prediction architecture (DP-TSDDC) \nbased on time series decomposition and degradation correction, which achieves effective mining \nand fusion of fine-grained information in the data by decomposing monitoring data into trend and \nnon-stationary components, and using temporal convolutional networks and Patch-integrated LSTM \nwith a multi-head attention mechanism to process them separately. However, the application of ML-\nbased methods in real-world industrial scenarios still faces many challenges. \nOne of the challenges is the scarcity of target domain samples[11]. In real-world industrial \nenvironments, equipment typically operates under complex and variable working conditions, and \nfor safety and economic reasons, obtaining large amounts of high-quality degradation data \n(especially full life-cycle data from healthy state to complete failure) is often extremely challenging \n[12, 13]. Although sufficient training data may exist in other related tasks or equipment (i.e., the \nsource domain) [14], directly applying models trained on the source domain to the target domain \n(e.g., new model equipment, new working conditions, etc.) often leads to domain shift problems \n[15], resulting in decreased prediction performance and limiting the reliability of the model in \npractical applications [16]. To address this challenge, researchers have attempted to introduce \nTransfer Learning (TL) [17] and Meta-Learning [18] technologies. TL uses Domain Adaptation (DA) \n[19] methods to transfer a pre-trained model from a source domain (typically with sufficient data) \nto a target domain, to achieve cross-domain reuse of knowledge [20, 21]. For instance, Xiang et al. \n[22] proposed a micro-transfer learning mechanism for multiple differentiated distributions, using \nmulti-unit LSTMs to obtain multiple differentiated distributions of monitoring data at different time \npoints, and using a domain adversarial mechanism to achieve knowledge transfer at the unit level, \nsolving the unreasonable assumption of using a single distribution to represent time-varying life \ncycle signals in traditional transfer learning; Li et al. [23] proposed a segmented adaptive RUL \nprediction method based on multi-feature space cross-adaptation transfer learning, improving the \nRUL prediction accuracy under unseen degradation data; Lyu et al.[24] proposed a TL method based \non deep degradation feature adaptive alignment, which solves the problem of traditional domain \nadaptation methods ignoring the influence of local features and improves the performance of RUL \nprediction under different working conditions. However, the effectiveness of TL is highly dependent \non the selection of the transfer strategy and the similarity between the source and target domains \n[25]. When the inter-domain differences are significant, simple transfer may lead to Negative \nTransfer, which can reduce the performance of the model in the target domain. In addition, \n\n \n \ntraditional TL methods usually require redesigning the transfer strategy for each new target domain, \nlacking flexibility and generality. In contrast, Meta-Learning, also known as \"Learning to Learn,\" \nprovides a more flexible solution [26]. By learning a general initialization model or learning strategy \non multiple related tasks, the model/strategy can quickly adapt to new tasks (target domain tasks) \nwith only a few samples [27]. This method can not only effectively utilize the knowledge of the \nsource domain, but also achieve rapid learning and generalization in the case of scarce target domain \nsamples [28]. Currently, algorithms based on Model-Agnostic Meta-Learning (MAML) [29] have \nshown promising application prospects in few-shot RUL prediction [30, 31]. For example, Ding et \nal. [32] explored the application of prediction methods based on graph neural networks (GNNs) \nunder partial monitoring data, established a graph-structured few-shot prediction framework, and \non this basis proposed a graph prediction algorithm based on embedded meta-learning training \nmodels, realizing the processing of spatio-temporal graph data and cross-domain generalization \nability in a few-shot training mode; Rai et al.[33] proposed a multi-stage, feature-adaptive meta-\nlearning model and combined bidirectional long short-term memory networks (BiLSTM) and \nvariational autoencoders (VAE) to capture statistical changes and time dependencies, achieving \naccurate prediction of RUL; Chang et al. [34] proposed a Bayesian Model-Agnostic Meta-Learning \nwith Prediction Uncertainty Calibration (BMLPUC) few-shot learning method, which achieves \naccurate quantification and calibration of prediction uncertainty in data-scarce scenarios by \nintroducing an uncertainty calibration term in the objective function of the model training. However, \nMAML-based meta-learning RUL prediction algorithms involve the computation of second-order \nderivatives during training, which brings huge computational burden and optimization instability \n[35], especially when dealing with high-dimensional data and complex models. Therefore, there is \nan urgent need to develop more concise and efficient meta-learning algorithms to address the \nchallenges of rotating machinery RUL prediction in target domain sample scarcity scenarios. \nAnother major challenge lies in the lack of physics-informed information during the model \ntraining process [36]. Most existing methods mainly rely on statistical patterns in the data, while \nignoring the rich physical knowledge contained in the equipment operation process [37, 38]. Some \nstudies have attempted to incorporate physics-informed information into RUL prediction models \n[39, 40]. For example, Yang et al.[41] proposed a physics-informed driven bearing RUL prediction \nmethod, which realizes the effective extraction and fusion of sequential periodic features and \nmedium-to-long-term trend features by constructing a physics-informed driven dynamic adaptive \ninverse discrete Fourier transform (IDFT) frequency domain module and a residual self-attention \nmulti-state gated control unit; Wang et al.[42] proposed a general degradation physics-informed \ndriven self-data-driven mechanical prediction method called Phyformer, which realizes the \ncomplementary advantages of physical models and data-driven models by combining a deep \nlearning model based on autocorrelation and Transformer architecture with multiple local physical \nmodels constructed within a sliding time window; de Beaulieu et al.[43] incorporated prior system \nknowledge and failure physics into training data through a hybrid data augmentation procedure and \ndeveloped an unsupervised health indicator (HI) extraction method, which solves the dependence \nof current prediction methods on labeled target data; Xiong et al.[44] proposed a hybrid framework \ncombining a controllable physics-informed driven data generation method and a deep learning \nprediction model, generating physically interpretable and diverse synthetic degradation trajectories, \nwhich solves the problem that deep learning methods are limited in performance or cannot be \napplied in practical applications due to limited representative failure time (TTF) trajectory data. \n\n \n \nHowever, these methods mostly remain at a shallow level of integration between physics-informed \ninformation and ML, that is, simple concatenation or weighting is mainly performed at the data level \nor model structure level. The reason is that for the complex degradation process of equipment, the \nderivation of first principles (such as establishing accurate partial differential equations) is difficult, \nand even in most scenarios, it is impossible to derive effective dynamic equations between sensor \nmonitoring data and equipment degradation states. The lack of utilization of potential physical \nprocesses can lead to the model over-relying on observation data [44, 45], and the prediction \nrobustness is insufficient when facing noise interference, changing working conditions, and other \nsituations [46, 47]. \nBased on the above discussion, to solve the problems of insufficient target domain samples and \nthe lack of physics-informed information during the training process in RUL prediction tasks in real \nindustrial scenarios, we propose a Meta-Learning and Knowledge Discovery based Physics-\nInformed Neural Network (MKDPINN). Based on the idea of Deep Hidden Physics Models \n(DeepHPM) [48, 49], we construct a hierarchical learning framework composed of a Hidden State \nMapper (HSM) and a Physics-Guided Regulator (PGR), where the HSM can map high-dimensional, \nnoisy, multi-source sensor time series to a continuous latent state space, achieving effective \nrepresentation of the equipment degradation state and avoiding the instability of traditional \nnumerical differentiation methods in noisy environments. Based on the hidden state output by the \nHSM, the PGR adaptively learns the non-linear partial differential equation (PDE) followed by the \nevolution of the hidden state, which serves as the physical law constraint of equipment degradation. \nThe PGR is embedded as a PDE into the PINN framework to guide the training process of the \nproposed method, thereby realizing the deep integration of data-driven and physics knowledge \ndiscovery. On this basis, aiming at the target domain data scarcity scenario, the proposed method is \nfurther extended to a first-order meta-learning framework [50]. By performing multi-step inner-loop \ngradient updates and outer-loop meta-parameter updates on multiple meta-tasks (each meta-task has \nonly a few samples), the gradient of the expected loss function of all meta-tasks is implicitly \napproximated, so that the model parameters are aligned with the aggregated optimal parameters \nfrom each meta-task, enabling the proposed method to quickly adjust parameters with only a small \namount of data from new equipment/working conditions to adapt to new RUL prediction tasks, \nwhile avoiding the high-order derivative calculations in MAML. \nThe main contributions of this paper are as follows: \n1. We propose a hierarchical learning framework consisting of an HSM and a PGR, where the \nHSM is responsible for extracting smooth hidden states from raw sensor data, and the PGR can \nadaptively learn the non-linear PDE dynamics equation followed by the latent states. \n2. We embed the PGR as a physical constraint into the PINN framework, guiding the training \nof the neural network through a regularized loss function, realizing the collaborative optimization \nof data-driven and physics knowledge, and improving the generalization ability of the model and its \ncompliance with physical rules. \n3. The model is further extended into a meta-learning framework, enabling it to quickly adjust \nmodel parameters with only a small amount of data from new equipment or new working conditions, \nachieving accurate prediction of new tasks, and avoiding the calculation of second-order derivatives \nbased on MAML. \n4. Comprehensive experimental validation is performed on real industrial scenarios and the \npublicly available CMAPSS dataset. The results show that MKDPINN exhibits superior \n\n \n \nperformance in predicting RUL of the same equipment in different life cycles and across different \nequipment in the scenario of scarce target domain samples, verifying the effectiveness and \nrobustness of the proposed method. \nThe subsequent organization of this paper is as follows: Section 2 reviews the relevant \nbackground; Section 3 elaborates on the theoretical framework and implementation details of the \nproposed MKDPINN method; Section 4 verifies and analyzes the performance of MKDPINN in \ndifferent scenarios through experiments; Section 5 summarizes the full paper and looks forward to \nfuture research directions. \n2. Problem Formulation \n2.1 Physics-Informed Neural Networks \nPINNs are a deep learning framework that embeds prior physical knowledge into neural \nnetworks, providing a promising approach to address the problem of missing physics-informed \ninformation during model training[38]. The core idea of PINNs is to incorporate PDEs that govern \nthe behavior of control systems into the loss function of the neural network as regularization terms \nor soft constraints, thereby guiding the network to learn solutions that satisfy physical laws. \nAssume that the degradation process of rotating machinery equipment can be described by the \nfollowing partial differential equation: \n\ud835\udf15\ud835\udf15\ud835\udf15\ud835\udf15(\ud835\udc89\ud835\udc89, \ud835\udc61\ud835\udc61)\n\ud835\udf15\ud835\udf15\ud835\udf15\ud835\udf15\n= \ud835\udca9\ud835\udca9[\ud835\udc62\ud835\udc62(\ud835\udc89\ud835\udc89, \ud835\udc61\ud835\udc61)], \n(\ud835\udc89\ud835\udc89, \ud835\udc61\ud835\udc61) \u2208\u03a9 \u00d7 [0, \ud835\udc47\ud835\udc47]\n\u0d6b1\u0d6f \nWhere \ud835\udc62\ud835\udc62(\ud835\udc89\ud835\udc89,\ud835\udc61\ud835\udc61) \u2208\ud835\udc45\ud835\udc45\ud835\udc51\ud835\udc51 represents the state vector related to equipment degradation, which is a \nfunction of the hidden state \ud835\udc89\ud835\udc89\u2208\u03a9 \u2282\ud835\udc45\ud835\udc45\ud835\udc51\ud835\udc51\u210e and time \ud835\udc61\ud835\udc61\u2208[0, \ud835\udc47\ud835\udc47], where \ud835\udc51\ud835\udc51 represents the dimension \nof the state vector. Here, the hidden state \ud835\udc89\ud835\udc89 represents those unobservable, but decisive, intrinsic \nstate variables that drive the overall degradation behavior of the equipment (related to factors such \nas changes in the microstructure of the material, the degree of damage accumulation, and the \nevolution of internal defects). \ud835\udca9\ud835\udca9[\u22c5] represents a non-linear differential operator, which describes \nthe physical process controlling equipment degradation (e.g., Navier-Stokes equations, heat \nconduction equations, etc.). \ud835\udefa\ud835\udefa represents the hidden state space. \nPINNs approximate the solution \ud835\udc62\ud835\udc62(\ud835\udc89\ud835\udc89,\ud835\udc61\ud835\udc61)  of the PDE by constructing a neural network \n\ud835\udc62\ud835\udc62\ud835\udec9\ud835\udec9(\ud835\udc89\ud835\udc89, \ud835\udc61\ud835\udc61), where \ud835\udf3d\ud835\udf3d represents the parameters of the neural network (weights and biases). The input \nof the network is the hidden state \ud835\udc89\ud835\udc89  and time \ud835\udc61\ud835\udc61 , and the output is the equipment state vector \n\ud835\udc62\ud835\udc62\u03b8(\ud835\udc89\ud835\udc89, \ud835\udc61\ud835\udc61). The loss function of PINNs is usually a weighted combination of a data loss term and a \nphysics loss term: \n\u2112(\ud835\udf03\ud835\udf03) = \ud835\udc64\ud835\udc64\ud835\udc51\ud835\udc51\u2112\ud835\udc51\ud835\udc51\ud835\udc51\ud835\udc51\ud835\udc51\ud835\udc51\ud835\udc51\ud835\udc51(\ud835\udf3d\ud835\udf3d) + \ud835\udc64\ud835\udc64\ud835\udc5d\ud835\udc5d\u2112\ud835\udc5d\ud835\udc5d\u210e\ud835\udc66\ud835\udc66\ud835\udc66\ud835\udc66\ud835\udc66\ud835\udc66\ud835\udc66\ud835\udc66\ud835\udc66\ud835\udc66(\ud835\udf3d\ud835\udf3d)\n\u0d6b2\u0d6f \nWhere, \ud835\udc64\ud835\udc64\ud835\udc51\ud835\udc51  and \ud835\udc64\ud835\udc64\ud835\udc5d\ud835\udc5d  are the weighting coefficients of the data loss and the physics loss, \nrespectively. \nThe data loss term \u2112\ud835\udc51\ud835\udc51\ud835\udc51\ud835\udc51\ud835\udc51\ud835\udc51\ud835\udc51\ud835\udc51(\u03b8) measures the difference between the neural network prediction \nvalue and the observed data. For prediction-related tasks involving the equipment degradation \nprocess, the Mean Squared Error (MSE) is usually used as the loss function: \n\u2112\ud835\udc51\ud835\udc51\ud835\udc51\ud835\udc51\ud835\udc51\ud835\udc51\ud835\udc51\ud835\udc51(\ud835\udf3d\ud835\udf3d) = 1\n\ud835\udc41\ud835\udc41\u0dcd\u0e2b|\ud835\udc62\ud835\udc62\ud835\udf3d\ud835\udf3d(\ud835\udc89\ud835\udc89\ud835\udc8a\ud835\udc8a, \ud835\udc61\ud835\udc61\ud835\udc56\ud835\udc56) \u2212\ud835\udc62\ud835\udc62\ud835\udc56\ud835\udc56|\u0e2b2\n2\n\ud835\udc41\ud835\udc41\n\ud835\udc56\ud835\udc56=1\n\u0d6b3\u0d6f \nWhere, (\ud835\udc89\ud835\udc89\ud835\udc8a\ud835\udc8a, \ud835\udc61\ud835\udc61\ud835\udc56\ud835\udc56) represents the hidden state and time point corresponding to the observed data, \n\ud835\udc62\ud835\udc62\ud835\udc56\ud835\udc56 represents the degradation state of the equipment at that space-time point, \ud835\udc41\ud835\udc41 is the number of \n\n \n \nobserved data, and || \u22c5||2 represents the L2 norm of the vector. \nThe physics loss term \u2112\ud835\udc5d\ud835\udc5d\u210e\ud835\udc66\ud835\udc66\ud835\udc66\ud835\udc66\ud835\udc66\ud835\udc66\ud835\udc66\ud835\udc66\ud835\udc66\ud835\udc66(\ud835\udf3d\ud835\udf3d)  measures the deviation between the neural network \nprediction value and the PDE constraint. Using Automatic Differentiation technology, the partial \nderivatives of the neural network output \ud835\udc62\ud835\udc62\ud835\udf3d\ud835\udf3d(\ud835\udc89\ud835\udc89, \ud835\udc61\ud835\udc61)  with respect to the input \ud835\udc89\ud835\udc89  and \ud835\udc61\ud835\udc61  can be \ncalculated, thereby constructing the PDE residual: \n\ud835\udc5f\ud835\udc5f(\ud835\udc89\ud835\udc89, \ud835\udc61\ud835\udc61;\ud835\udf3d\ud835\udf3d) = \u2202\ud835\udc62\ud835\udc62\ud835\udec9\ud835\udec9(\ud835\udc89\ud835\udc89,\ud835\udc61\ud835\udc61)\n\u2202\ud835\udc61\ud835\udc61\n\u2212\ud835\udca9\ud835\udca9[\ud835\udc62\ud835\udc62\ud835\udf3d\ud835\udf3d(\ud835\udc89\ud835\udc89, \ud835\udc61\ud835\udc61)]\n\u0d6b4\u0d6f \nThe physics loss is usually defined as the L2 norm of the PDE residual: \n\u2112\ud835\udc5d\ud835\udc5d\u210e\ud835\udc66\ud835\udc66\ud835\udc66\ud835\udc66\ud835\udc66\ud835\udc66\ud835\udc66\ud835\udc66\ud835\udc66\ud835\udc66(\ud835\udf3d\ud835\udf3d) = 1\n\ud835\udc40\ud835\udc40\u0dcd\u125a\u0e2b\ud835\udc5f\ud835\udc5f\u0d6b\ud835\udc89\ud835\udc89\ud835\udc8b\ud835\udc8b,\ud835\udc61\ud835\udc61\ud835\udc57\ud835\udc57; \ud835\udf3d\ud835\udf3d\u0d6f\u0e2b\u125a\n2\n2\n\ud835\udc40\ud835\udc40\n\ud835\udc57\ud835\udc57=1\n\u0d6b5\u0d6f \nWhere, \u0d6b\ud835\udc89\ud835\udc89\ud835\udc8b\ud835\udc8b, \ud835\udc61\ud835\udc61\ud835\udc57\ud835\udc57\u0d6f is a set of collocation points within the hidden state space and time domain \n\u03a9 \u00d7 [0, \ud835\udc47\ud835\udc47], and M is the number of collocation points. \nBy minimizing the total loss function \u2112(\ud835\udf3d\ud835\udf3d) , PINNs can balance between data-driven and \nphysical constraints, thereby learning a solution that both conforms to the observed data and satisfies \nphysical laws. This optimization process is usually implemented using gradient-based optimization \nalgorithms (such as Adam, L-BFGS, etc.). \n2.2 Meta-learning \nIn the RUL prediction task of rotating machinery equipment, traditional Machine Learning \nmethods usually assume that the training data and the test data come from the same distribution. \nHowever, in real industrial scenarios, this assumption is often difficult to satisfy due to the diversity \nof equipment operating conditions, the complexity of degradation patterns, and the limitations of \ndata acquisition. Degradation trajectories of different equipment or the same equipment under \ndifferent operating conditions may have significant differences, leading to a decline in the \ngeneralization ability of the model under new, unseen working conditions. \nMeta-learning, also known as \"Learning to Learn,\" improves the model's ability to quickly \nadapt to new tasks by learning knowledge across multiple tasks. In the context of rotating machinery \nRUL prediction, RUL prediction under different equipment or different working conditions can be \nregarded as different tasks. \nAssume that there is a set of RUL prediction tasks {\ud835\udcaf\ud835\udcaf\ud835\udc5d\ud835\udc5d}\ud835\udc5d\ud835\udc5d=1\n\ud835\udc43\ud835\udc43\n, where each task corresponds to \ndifferent equipment or working conditions. Each task \ud835\udcaf\ud835\udcaf\ud835\udc5d\ud835\udc5d  has an associated dataset \ud835\udc9f\ud835\udc9f\ud835\udc5d\ud835\udc5d=\n{\u1240\ud835\udc99\ud835\udc99\ud835\udc8a\ud835\udc8a\n(\ud835\udc91\ud835\udc91),\ud835\udc66\ud835\udc66\ud835\udc56\ud835\udc56\n(\ud835\udc5d\ud835\udc5d)\u1241}\ud835\udc56\ud835\udc56=1\n\ud835\udc41\ud835\udc41\ud835\udc5d\ud835\udc5d, where \ud835\udc99\ud835\udc99\ud835\udc8a\ud835\udc8a\n(\ud835\udc91\ud835\udc91) represents the observed data of the \ud835\udc56\ud835\udc56-th sample in the \ud835\udc5d\ud835\udc5d-th task (e.g., \nvibration signals, temperature, pressure, etc. collected by the sensor), \ud835\udc66\ud835\udc66\ud835\udc56\ud835\udc56\n(\ud835\udc5d\ud835\udc5d)  is the corresponding \nRUL ground truth, and \ud835\udc41\ud835\udc41\ud835\udc5d\ud835\udc5d  is the number of samples in the \ud835\udc5d\ud835\udc5d -th task. Due to differences in \nequipment or working conditions, the data distribution \ud835\udc43\ud835\udc43\ud835\udc5d\ud835\udc5d(\ud835\udc99\ud835\udc99, \ud835\udc66\ud835\udc66) of different tasks may be different. \nThe goal of meta-learning is to learn a meta-model \ud835\udc40\ud835\udc40\ud835\udf31\ud835\udf31 from this set of RUL prediction tasks \n{\ud835\udcaf\ud835\udcaf\ud835\udc5d\ud835\udc5d}\ud835\udc5d\ud835\udc5d=1\n\ud835\udc43\ud835\udc43\n, where the meta-model is defined by parameters \ud835\udf31\ud835\udf31. \ud835\udc40\ud835\udc40\ud835\udf31\ud835\udf31 should be able to quickly adapt \nand make accurate RUL predictions on a new task \ud835\udcaf\ud835\udcaf\ud835\udc5b\ud835\udc5b\ud835\udc5b\ud835\udc5b\ud835\udc5b\ud835\udc5b (drawn from a possibly different, held-out, \ntest task distribution, \ud835\udc43\ud835\udc43\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61(\ud835\udcaf\ud835\udcaf)), using only a small amount of data from that new task.  \nMore formally, the meta-learning goal is to find the optimal meta-model parameters \ud835\udf31\ud835\udf31\u2217, such \nthat: \n\ud835\udf31\ud835\udf31\u2217= arg min\n\ud835\udf31\ud835\udf31\ud835\udc38\ud835\udc38\ud835\udcaf\ud835\udcaf\ud835\udc5d\ud835\udc5d\u223c\ud835\udc43\ud835\udc43\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61(\ud835\udcaf\ud835\udcaf) \u1242\u2112\u1240\ud835\udc53\ud835\udc53\ud835\udf3d\ud835\udf3d\ud835\udc91\ud835\udc91; \ud835\udc9f\ud835\udc9f\ud835\udc5d\ud835\udc5d\n\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\u1241\u1243\n\u0d6b6\u0d6f \nWhere \ud835\udc43\ud835\udc43\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61(\ud835\udcaf\ud835\udcaf)  represents the distribution of training tasks; \ud835\udcaf\ud835\udcaf\ud835\udc5d\ud835\udc5d\u223c\ud835\udc43\ud835\udc43\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61(\ud835\udcaf\ud835\udcaf)  represents \n\n \n \nsampling a task from the training task distribution; \ud835\udc53\ud835\udc53\ud835\udf3d\ud835\udf3d\ud835\udc91\ud835\udc91  is the model adapted to task \ud835\udcaf\ud835\udcaf\ud835\udc5d\ud835\udc5d , with \nparameters \ud835\udf3d\ud835\udf3d\ud835\udc91\ud835\udc91 . These parameters are obtained by adapting the meta-model \ud835\udc40\ud835\udc40\ud835\udf31\ud835\udf31  using a small \namount of support data \ud835\udc9f\ud835\udc9f\ud835\udc5d\ud835\udc5d\n\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60 from task \ud835\udcaf\ud835\udcaf\ud835\udc5d\ud835\udc5d: \n\ud835\udf3d\ud835\udf3d\ud835\udc91\ud835\udc91= \ud835\udc3a\ud835\udc3a\u0d6b\ud835\udc40\ud835\udc40\ud835\udf31\ud835\udf31, \ud835\udc9f\ud835\udc9f\ud835\udc5d\ud835\udc5d\n\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\u0d6f\n\u0d6b7\u0d6f \n Where \ud835\udc3a\ud835\udc3a  represents the rapid adaptation process (e.g., a few steps of gradient descent).  \n\ud835\udc9f\ud835\udc9f\ud835\udc5d\ud835\udc5d\n\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60= {\u1240\ud835\udc99\ud835\udc99\ud835\udc8a\ud835\udc8a\n(\ud835\udc91\ud835\udc91),\ud835\udc66\ud835\udc66\ud835\udc56\ud835\udc56\n(\ud835\udc5d\ud835\udc5d)\u1241}\ud835\udc56\ud835\udc56=1\n\ud835\udc41\ud835\udc41\ud835\udc5d\ud835\udc5d\n\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\n is a small subset of \ud835\udc9f\ud835\udc9f\ud835\udc5d\ud835\udc5d. \n2.3 Few-shot learning \nFew-shot Learning (FSL) refers to a learning paradigm in which the model can quickly adapt \nto new tasks and make accurate predictions with only a very small number of labeled samples. In \nthe rotating machinery RUL prediction task, FSL can be formally defined as follows: \nGiven a new RUL prediction task \ud835\udcaf\ud835\udcaf\ud835\udc5b\ud835\udc5b\ud835\udc5b\ud835\udc5b\ud835\udc5b\ud835\udc5b , its corresponding support set \ud835\udc9f\ud835\udc9f\ud835\udc5b\ud835\udc5b\ud835\udc5b\ud835\udc5b\ud835\udc5b\ud835\udc5b\n\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60=\n{(\ud835\udc99\ud835\udc99\ud835\udc8a\ud835\udc8a\n\ud835\udc8f\ud835\udc8f\ud835\udc8f\ud835\udc8f\ud835\udc8f\ud835\udc8f,\ud835\udc66\ud835\udc66\ud835\udc56\ud835\udc56\n\ud835\udc5b\ud835\udc5b\ud835\udc5b\ud835\udc5b\ud835\udc5b\ud835\udc5b)}\ud835\udc56\ud835\udc56=1\n\ud835\udc3e\ud835\udc3e contains only \ud835\udc3e\ud835\udc3e samples, where \ud835\udc3e\ud835\udc3e is very small (usually \ud835\udc3e\ud835\udc3e is between 1-15). \n\ud835\udc99\ud835\udc99\ud835\udc8a\ud835\udc8a\n\ud835\udc8f\ud835\udc8f\ud835\udc8f\ud835\udc8f\ud835\udc8f\ud835\udc8f  represents the observed data of the \ud835\udc56\ud835\udc56 -th sample in the new task, and \ud835\udc66\ud835\udc66\ud835\udc56\ud835\udc56\n\ud835\udc5b\ud835\udc5b\ud835\udc5b\ud835\udc5b\ud835\udc5b\ud835\udc5b is the \ncorresponding RUL ground truth. This setting is called \ud835\udc3e\ud835\udc3e-shot learning. \nIn this paper, the goal of FSL is to use the meta-model \ud835\udc40\ud835\udc40\ud835\udf53\ud835\udf53 obtained based on meta-learning \nto quickly adapt to the new task \ud835\udcaf\ud835\udcaf\ud835\udc5b\ud835\udc5b\ud835\udc5b\ud835\udc5b\ud835\udc5b\ud835\udc5b, and obtain a parameterized model \ud835\udc53\ud835\udc53\ud835\udf3d\ud835\udf3d\ud835\udc8f\ud835\udc8f\ud835\udc8f\ud835\udc8f\ud835\udc8f\ud835\udc8f for this task: \n\ud835\udf3d\ud835\udf3d\ud835\udc8f\ud835\udc8f\ud835\udc8f\ud835\udc8f\ud835\udc8f\ud835\udc8f= \ud835\udc54\ud835\udc54\u0d6b\ud835\udc40\ud835\udc40\ud835\udf53\ud835\udf53, \ud835\udc9f\ud835\udc9f\ud835\udc5b\ud835\udc5b\ud835\udc5b\ud835\udc5b\ud835\udc5b\ud835\udc5b\n\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\u0d6f\n\u0d6b8\u0d6f \nThis model \ud835\udc53\ud835\udc53\ud835\udf3d\ud835\udf3d\ud835\udc8f\ud835\udc8f\ud835\udc8f\ud835\udc8f\ud835\udc8f\ud835\udc8f should be able to obtain a low prediction error on the query set \ud835\udc9f\ud835\udc9f\ud835\udc5b\ud835\udc5b\ud835\udc5b\ud835\udc5b\ud835\udc5b\ud835\udc5b\n\ud835\udc5e\ud835\udc5e\ud835\udc5e\ud835\udc5e\ud835\udc5e\ud835\udc5e\ud835\udc5e\ud835\udc5e\ud835\udc5e\ud835\udc5e=\n{\u1240\ud835\udc99\ud835\udc99\ud835\udc8b\ud835\udc8b\n\ud835\udc8f\ud835\udc8f\ud835\udc8f\ud835\udc8f\ud835\udc8f\ud835\udc8f,\ud835\udc66\ud835\udc66\ud835\udc57\ud835\udc57\n(\ud835\udc5b\ud835\udc5b\ud835\udc5b\ud835\udc5b\ud835\udc5b\ud835\udc5b)\u1241}\ud835\udc57\ud835\udc57=1\n\ud835\udc40\ud835\udc40 of the new task: \n\u2112\u0d6b\ud835\udc53\ud835\udc53\ud835\udf3d\ud835\udf3d\ud835\udc8f\ud835\udc8f\ud835\udc8f\ud835\udc8f\ud835\udc8f\ud835\udc8f, \ud835\udc9f\ud835\udc9f\ud835\udc5b\ud835\udc5b\ud835\udc5b\ud835\udc5b\ud835\udc5b\ud835\udc5b\n\ud835\udc5e\ud835\udc5e\ud835\udc5e\ud835\udc5e\ud835\udc5e\ud835\udc5e\ud835\udc5e\ud835\udc5e\ud835\udc5e\ud835\udc5e\u0d6f= 1\n\ud835\udc40\ud835\udc40\u0dcd\u2112\u1240\ud835\udc53\ud835\udc53\ud835\udf3d\ud835\udf3d\ud835\udc8f\ud835\udc8f\ud835\udc8f\ud835\udc8f\ud835\udc8f\ud835\udc8f\u1240\ud835\udc99\ud835\udc99\ud835\udc8b\ud835\udc8b\n(\ud835\udc8f\ud835\udc8f\ud835\udc8f\ud835\udc8f\ud835\udc8f\ud835\udc8f)\u1241, \ud835\udc66\ud835\udc66\ud835\udc57\ud835\udc57\n(\ud835\udc5b\ud835\udc5b\ud835\udc5b\ud835\udc5b\ud835\udc5b\ud835\udc5b)\u1241\n\ud835\udc40\ud835\udc40\n\ud835\udc57\ud835\udc57=1\n\u0d6b9\u0d6f \nIn practical applications, FSL can effectively solve the problem of insufficient samples for RUL \nFig. 1. The Overall Process of the Proposed MKDPINN \n\n \n \nprediction under new equipment or new working conditions. \n3. Proposed method \n3.1 Overall framework of the proposed method \nThe overall workflow of the proposed MKDPINN is shown in Figure 1. First, the monitoring \ndata during the equipment operation process is preprocessed, and a set of meta-tasks is constructed: \nthe complete life-cycle data of each equipment is divided into multiple subsequences, and each \nsubsequence constitutes an independent meta-task. Subsequently, the data flows through the three \ncore modules of MKDPINN in sequence: the HSM, a self-attention-based neural network, maps the \nhigh-dimensional, noisy raw sensor data to a continuous hidden state space, extracting hidden state \nvector sequences that represent the equipment degradation dynamics; the PGR takes the hidden state \nsequence output by the HSM as input, and approximates the PDE followed by the hidden state \nevolution through a neural network; the RUL predictor combines the hidden state and running time \ninformation to predict the RUL of the equipment. The PDE learned by the PGR serves as a physical \nconstraint, and together with the output of the RUL predictor, constitutes a loss function that \nincludes a data-driven loss and a physical residual loss, thereby realizing the deep integration of the \ndata-driven model and physics knowledge. The entire framework adopts a meta-learning strategy \nbased on first-order gradient optimization for meta-level parameter updates: in the meta-training \nstage, multiple source domain tasks are sampled from the meta-task set for iterative updating, \noptimizing the global initial parameters; in the meta-testing stage, a small amount of target domain \ndata is used for rapid adaptation, which can achieve rapid and accurate prediction of RUL under \nnew equipment or new working conditions. \n3.2 Knowledge Discovery based Physics-Informed Neural Network Framework \nAs shown in Figure 2, our proposed knowledge discovery physics-informed neural network \nframework consists of three key components: HSM, PGR, and RUL predictor, to achieve the fusion \nAWC\n.\nRUL Predictor\nAttention-based Feature Extractor\nK\nEmbedding\nQ\nV\nSoftmax\nLN\nLinear\nReLu\n&Linear\nResidual Connection\nResidual Connection\nLN\nInput Features\nOutput Features\nInput samples\nLinear\n\u00d7 N\nHidden state \nOperating time\nHidden State Mapper\n\u2026\nK\nEmbedding\nQ\nV\nSoftmax\nLN\n&Linear\nReLu\n&Linear\n\ud835\udc3f\ud835\udc3f\ud835\udc5c\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc5d\ud835\udc5d\u210e\ud835\udc66\ud835\udc66\nPhysics-Guided Regulator\nFig. 2. Structure of the Knowledge Discovery based Physics-Informed Neural Network Framework \n\n \n \nof data-driven and physical law constraints, and as a meta-learner in the subsequent meta-learning \nparadigm, it has the ability to quickly adapt to new RUL prediction tasks.  \n3.2.1 Hidden State Mapper (HSM) \nIn real industrial environments, the monitoring data of rotating machinery is usually high-\ndimensional, multi-source, and contains noise. Directly using raw sensor data for RUL prediction \nand physical law discovery faces many challenges, including the ill-posedness of numerical \ndifferentiation, and limited model expression ability [48]. To overcome these challenges, we \nintroduce the HSM. The goal of the HSM is to map high-dimensional, noisy raw sensor time series \ndata to a low-dimensional, smooth hidden state space, thereby effectively representing the \ndegradation state of the equipment, and providing reliable input for subsequent physical rule \nlearning and RUL prediction.  \nThe HSM uses a self-attention-based neural network for implementation. As shown in Figure \n2, assume that the input raw sample data is \ud835\udc7f\ud835\udc7f= [\ud835\udc99\ud835\udc99\ud835\udfcf\ud835\udfcf,\ud835\udc99\ud835\udc99\ud835\udfd0\ud835\udfd0,\u2026 , \ud835\udc99\ud835\udc99\ud835\udc7b\ud835\udc7b] \u2208\ud835\udc45\ud835\udc45\ud835\udc47\ud835\udc47\u00d7\ud835\udc51\ud835\udc51\ud835\udc65\ud835\udc65, where \ud835\udc47\ud835\udc47 is the length of \nthe time series, and \ud835\udc51\ud835\udc51\ud835\udc65\ud835\udc65 is the dimension of the original input features. \nFirst, the original input feature \ud835\udc99\ud835\udc99\ud835\udc95\ud835\udc95 is mapped to an embedding space through a linear layer to \nobtain the embedding vector \ud835\udc86\ud835\udc86\ud835\udc95\ud835\udc95: \n\ud835\udc86\ud835\udc86\ud835\udc95\ud835\udc95= \ud835\udc7e\ud835\udc7e(\ud835\udc52\ud835\udc52)\ud835\udc99\ud835\udc99\ud835\udc95\ud835\udc95+ \ud835\udc83\ud835\udc83(\ud835\udc52\ud835\udc52)\n\u0d6b10\u0d6f \nWhere, \ud835\udc7e\ud835\udc7e(\ud835\udc52\ud835\udc52) and \ud835\udc83\ud835\udc83(\ud835\udc52\ud835\udc52)are the weight matrix and bias vector of the linear embedding layer, \nrespectively. \nFor the embedding sequence \ud835\udc6c\ud835\udc6c= [\ud835\udc86\ud835\udc86\ud835\udfcf\ud835\udfcf,\ud835\udc86\ud835\udc86\ud835\udfd0\ud835\udfd0, \u2026 ,\ud835\udc86\ud835\udc86\ud835\udc7b\ud835\udc7b], the model calculates self-attention weights \nin the feature dimension. For the embedded feature vector \ud835\udc86\ud835\udc86\ud835\udc8b\ud835\udc8b\u2208\ud835\udc45\ud835\udc45\ud835\udc47\ud835\udc47 of each feature dimension \ud835\udc57\ud835\udc57, \ndefine the query \ud835\udc78\ud835\udc78\ud835\udc8b\ud835\udc8b, key \ud835\udc72\ud835\udc72\ud835\udc8b\ud835\udc8b and value \ud835\udc7d\ud835\udc7d\ud835\udc8b\ud835\udc8b: \n\ud835\udc78\ud835\udc78\ud835\udc8b\ud835\udc8b= \ud835\udc7e\ud835\udc7e(\ud835\udc92\ud835\udc92)\ud835\udc86\ud835\udc86\ud835\udc8b\ud835\udc8b,          \ud835\udc72\ud835\udc72\ud835\udc8b\ud835\udc8b= \ud835\udc7e\ud835\udc7e(\ud835\udc8c\ud835\udc8c)\ud835\udc86\ud835\udc86\ud835\udc8b\ud835\udc8b,\n\ud835\udc7d\ud835\udc7d\ud835\udc8b\ud835\udc8b= \ud835\udc7e\ud835\udc7e(\ud835\udc97\ud835\udc97)\ud835\udc86\ud835\udc86\ud835\udc8b\ud835\udc8b\n\u0d6b11\u0d6f \nWhere, \ud835\udc7e\ud835\udc7e(\ud835\udc92\ud835\udc92), \ud835\udc7e\ud835\udc7e(\ud835\udc8c\ud835\udc8c),\ud835\udc7e\ud835\udc7e(\ud835\udc97\ud835\udc97) are the weight matrices of the query, key, and value, respectively. \nNext, calculate the dot product of the query vector and the key vectors of other feature \ndimensions for each feature dimension, and normalize to obtain the attention weights: \n\ud835\udefc\ud835\udefc\ud835\udc57\ud835\udc57,\ud835\udc5a\ud835\udc5a= softmax \u1246\n\ud835\udc78\ud835\udc78\ud835\udc8b\ud835\udc8b\n\ud835\udc7b\ud835\udc7b\ud835\udc8c\ud835\udc8c\ud835\udc8e\ud835\udc8e\n\u0da5\ud835\udc51\ud835\udc51\ud835\udc58\ud835\udc58\n\u1247=\nexp\u0d6b\ud835\udc78\ud835\udc78\ud835\udc8b\ud835\udc8b\n\ud835\udc7b\ud835\udc7b\ud835\udc72\ud835\udc72\ud835\udc8e\ud835\udc8e/\u0da5\ud835\udc51\ud835\udc51\ud835\udc58\ud835\udc58\u0d6f\n\u2211\nexp\u0d6b\ud835\udc78\ud835\udc78\ud835\udc8b\ud835\udc8b\n\ud835\udc7b\ud835\udc7b\ud835\udc72\ud835\udc72\ud835\udc8f\ud835\udc8f/\u0da5\ud835\udc51\ud835\udc51\ud835\udc58\ud835\udc58\u0d6f\n\ud835\udc51\ud835\udc51\ud835\udc52\ud835\udc52\n\ud835\udc5b\ud835\udc5b=1\n\u0d6b12\u0d6f \nUse the attention weights to weight and average the value vectors to get the output \ud835\udc9b\ud835\udc9b\ud835\udc8b\ud835\udc8b\u2208\ud835\udc45\ud835\udc45\ud835\udc47\ud835\udc47 \nof the feature dimension \ud835\udc57\ud835\udc57: \n\ud835\udc9b\ud835\udc9b\ud835\udc8b\ud835\udc8b= \u0dcd\u03b1\ud835\udc57\ud835\udc57,\ud835\udc5a\ud835\udc5a\ud835\udc7d\ud835\udc7d\ud835\udc8e\ud835\udc8e\n\ud835\udc51\ud835\udc51\ud835\udc52\ud835\udc52\n\ud835\udc5a\ud835\udc5a=1\n\u0d6b13\u0d6f \nThe outputs of all feature dimensions are concatenated into \ud835\udc81\ud835\udc81= \u0d63\ud835\udc9b\ud835\udc9b\ud835\udfcf\ud835\udfcf,\ud835\udc9b\ud835\udc9b\ud835\udfd0\ud835\udfd0, \u2026 , \ud835\udc9b\ud835\udc9b\ud835\udc85\ud835\udc85\ud835\udc86\ud835\udc86\u0d67. \nThe output \ud835\udc81\ud835\udc81= \u0d63\ud835\udc9b\ud835\udc9b1,\ud835\udc9b\ud835\udc9b2, \u2026, \ud835\udc9b\ud835\udc9b\ud835\udc85\ud835\udc85\ud835\udc86\ud835\udc86\u0d67 of the self-attention layer is subjected to a non-linear \ntransformation through a feedforward neural network (FFN) to further enhance the expressive \nability of the model. The FFN contains two linear layers and an activation function (ReLU): \n\ud835\udc89\ud835\udc89\u2032 = FFN(\ud835\udc9b\ud835\udc9b\ud835\udc95\ud835\udc95) = \ud835\udc7e\ud835\udc7e(\ud835\udc53\ud835\udc532)ReLu\u0d6b\ud835\udc7e\ud835\udc7e(\ud835\udc53\ud835\udc531)\ud835\udc9b\ud835\udc9b\ud835\udc95\ud835\udc95+ \ud835\udc83\ud835\udc83(\ud835\udc53\ud835\udc531)\u0d6f+ \ud835\udc83\ud835\udc83(\ud835\udc53\ud835\udc532)\n\u0d6b14\u0d6f \nWhere, \ud835\udc7e\ud835\udc7e(\ud835\udc53\ud835\udc531), \ud835\udc7e\ud835\udc7e(\ud835\udc53\ud835\udc532),\ud835\udc83\ud835\udc83(\ud835\udc53\ud835\udc531),\ud835\udc83\ud835\udc83(\ud835\udc53\ud835\udc532)  are the weight matrix and bias vector of the two linear \nlayers in the FFN, respectively.  \nTo alleviate the gradient vanishing problem and accelerate the training process, residual \nconnections and layer normalization (LN) are added around the self-attention layer and the FFN \nlayer. Finally, the output of the HSM is a hidden state vector sequence \ud835\udc89\ud835\udc89\u2208\ud835\udc45\ud835\udc45\ud835\udc51\ud835\udc51\u210e. \n\n \n \nThrough the above steps, the HSM converts the original, high-dimensional, noisy data \ud835\udc7f\ud835\udc7f into \na smooth, continuous hidden state sequence \ud835\udc89\ud835\udc89. \n3.2.2 RUL predictor \nRUL prediction is the ultimate goal of the model. After the HSM extracts the smooth \ncontinuous hidden state, the RUL predictor will fuse these hidden states with the corresponding time \ninformation and transforming them into accurate RUL estimates. The RUL predictor adopts a \nstructural design that combines deep neural networks and an adaptive weighting mechanism. \nThe input of the RUL predictor is the concatenation of the hidden state vector \ud835\udc89\ud835\udc89 output by the \nHSM and the corresponding equipment running time \ud835\udc61\ud835\udc61, which can be expressed as: \n\ud835\udc84\ud835\udc84= [\ud835\udc89\ud835\udc89, \ud835\udc61\ud835\udc61]\n\u0d6b15\u0d6f \nWhere, [\u22c5,\u22c5] represents the vector concatenation operation.  \nThe concatenated feature vector \ud835\udc84\ud835\udc84 passes through a deep neural network (DNN) composed of \nfour fully connected layers, with the Tanh used as the activation function between each layer: \n\ud835\udc9b\ud835\udc9b(\ud835\udfcf\ud835\udfcf) = \ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\u210e\u0d6b\ud835\udc7e\ud835\udc7e(\ud835\udc5d\ud835\udc5d1)\ud835\udc84\ud835\udc84+ \ud835\udc83\ud835\udc83(\ud835\udc5d\ud835\udc5d1)\u0d6f\n\u0d6b16\u0d6f \n\ud835\udc9b\ud835\udc9b(\ud835\udfd0\ud835\udfd0) = \ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\u210e\u0d6b\ud835\udc7e\ud835\udc7e(\ud835\udc5d\ud835\udc5d2)\ud835\udc9b\ud835\udc9b(1) + \ud835\udc83\ud835\udc83(\ud835\udc5d\ud835\udc5d2)\u0d6f\n\u0d6b17\u0d6f \n\ud835\udc9b\ud835\udc9b(\ud835\udfd1\ud835\udfd1) = \ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\u210e\u0d6b\ud835\udc7e\ud835\udc7e(\ud835\udc5d\ud835\udc5d3)\ud835\udc9b\ud835\udc9b(2) + \ud835\udc83\ud835\udc83(\ud835\udc5d\ud835\udc5d3)\u0d6f\n\u0d6b18\u0d6f \n\ud835\udc90\ud835\udc90= \ud835\udc7e\ud835\udc7e(\ud835\udc5d\ud835\udc5d4)\ud835\udc9b\ud835\udc9b(3) + \ud835\udc83\ud835\udc83(\ud835\udc5d\ud835\udc5d4)\n\u0d6b19\u0d6f \nThe output \ud835\udc90\ud835\udc90 of the DNN is multiplied element-wise (Hadamard product) with the adaptive \nweighting coefficient \ud835\udf46\ud835\udf46, to achieve adaptive weighting of each prediction component: \n\ud835\udc9a\ud835\udc9a= \ud835\udc90\ud835\udc90\u2299\ud835\udf46\ud835\udf46\n\u0d6b20\u0d6f \nWhere, \u2299 represents the element-wise multiplication operation. \nFinally, the RUL prediction value is obtained by summing the weighted components. We use \n\ud835\udc62\ud835\udc62\u0ddc(\ud835\udc89\ud835\udc89, \ud835\udc61\ud835\udc61)  to represent the RUL prediction value predicted by the model at the running time \ud835\udc61\ud835\udc61 , \ncorresponding to the hidden state \ud835\udc89\ud835\udc89: \n\ud835\udc62\ud835\udc62\u0ddc(\ud835\udc89\ud835\udc89, \ud835\udc61\ud835\udc61) = \u0dcd\ud835\udc66\ud835\udc66\ud835\udc56\ud835\udc56\n\ud835\udc5b\ud835\udc5b\n\ud835\udc56\ud835\udc56=1\n\u0d6b21\u0d6f \n3.2.3 Physics-Guided Regulator (PGR) \n The core goal of the PGR is to discover and describe the physical laws governing the change \nof RUL with the hidden state. Given the hidden state vector \ud835\udc89\ud835\udc89 extracted by the hidden state mapper \nHSM and the current running time \ud835\udc61\ud835\udc61, we take the RUL of the equipment as the state variable \ud835\udc62\ud835\udc62(\ud835\udc89\ud835\udc89, \ud835\udc61\ud835\udc61) \nrelated to the description of the equipment degradation, and model its evolution law with time and \nhidden state. From formula (1), it can be known that the dynamic evolution of the equipment \ndegradation process can be expressed as the following PDE form: \n\ud835\udf15\ud835\udf15\ud835\udf15\ud835\udf15(\ud835\udc89\ud835\udc89, \ud835\udc61\ud835\udc61)\n\ud835\udf15\ud835\udf15\ud835\udf15\ud835\udf15\n\u2212\ud835\udca9\ud835\udca9\u0d64\ud835\udc62\ud835\udc62, \ud835\udf15\ud835\udf15\ud835\udf15\ud835\udf15\n\ud835\udf15\ud835\udf15\ud835\udf15\ud835\udf15, \u2207\u210e\ud835\udc62\ud835\udc62, \u2207\u210e\n2\ud835\udc62\ud835\udc62,\u2026 , \u2207\u210e\n\ud835\udc58\ud835\udc58\ud835\udc62\ud835\udc62\u0d68= 0,  \ud835\udc89\ud835\udc89\u2208\ud835\udefa\ud835\udefa, \ud835\udc61\ud835\udc61\u2208[0, \ud835\udc47\ud835\udc47]\n\u0d6b22\u0d6f \nWhere, \u2207\ud835\udc89\ud835\udc89\ud835\udc62\ud835\udc62 represents the gradient of \ud835\udc62\ud835\udc62 with respect to the hidden state \ud835\udc89\ud835\udc89: \n\u2207\ud835\udc89\ud835\udc89\ud835\udc62\ud835\udc62= \u1246\u2202\ud835\udc62\ud835\udc62\n\u2202\u210e1\n, \u2202\ud835\udc62\ud835\udc62\n\u2202\u210e2\n, \u2026, \u2202\ud835\udc62\ud835\udc62\n\u2202\u210e\ud835\udc51\ud835\udc51\u210e\n\u1247\n\u0d6b23\u0d6f \n\u2207\ud835\udc89\ud835\udc89\n\ud835\udc58\ud835\udc58\ud835\udc62\ud835\udc62 represents the \ud835\udc58\ud835\udc58-th order spatial derivative of u with respect to \ud835\udc89\ud835\udc89; \ud835\udca9\ud835\udca9[\u22c5] is a non-linear \ndifferential operator, describing the complex relationship between the state variable and its time and \nspace derivatives; \u03a9 represents the hidden state space. \nSince the physical laws of the degradation process are usually difficult to express explicitly \nthrough analytical methods, we designed a PGR (represented by \ud835\udcab\ud835\udcab\ud835\udf3d\ud835\udf3d) so that it can adaptively learn \n\n \n \nand approximate the non-linear operator \ud835\udca9\ud835\udca9[\u22c5], and transform the equation into: \n\u2202\ud835\udc62\ud835\udc62(\ud835\udc89\ud835\udc89,\ud835\udc61\ud835\udc61)\n\u2202\ud835\udc61\ud835\udc61\n\u2212\ud835\udcab\ud835\udcab\ud835\udf3d\ud835\udf3d\u0d6b\ud835\udc62\ud835\udc62\u0ddc,\u2207\ud835\udc89\ud835\udc89\ud835\udc62\ud835\udc62\u0ddc,\u2207\ud835\udc89\ud835\udc89\n2\ud835\udc62\ud835\udc62\u0ddc, \u2026, \u2207\ud835\udc89\ud835\udc89\n\ud835\udc58\ud835\udc58\ud835\udc62\ud835\udc62\u0ddc\u0d6f= 0,  \ud835\udc89\ud835\udc89\u2208\ud835\udefa\ud835\udefa, \ud835\udc61\ud835\udc61\u2208[0, \ud835\udc47\ud835\udc47]\n\u0d6b24\u0d6f \nHere \ud835\udcab\ud835\udcab\ud835\udf03\ud835\udf03 is a parameterized neural network whose input contains the predicted state variable \n\ud835\udc62\ud835\udc62\u0ddc and its spatial derivatives of various orders, and the output is the rate of change of the state \nvariable with respect to time \u2202\ud835\udc62\ud835\udc62\u0ddc(\ud835\udc89\ud835\udc89,\ud835\udc61\ud835\udc61)/ \u2202\ud835\udc61\ud835\udc61. \nTo train the PGR, we first use Pytorch's automatic differentiation technology to calculate the \nderivatives of each order of the training samples, and construct a feature data set: \n\ud835\udc9f\ud835\udc9f= \u0d5c\u0d6c\ud835\udc62\ud835\udc62\u0ddc\ud835\udc56\ud835\udc56, \u2202\ud835\udc62\ud835\udc62\u0ddc\ud835\udc56\ud835\udc56\n\u2202\ud835\udc61\ud835\udc61, \u2207\ud835\udc89\ud835\udc89\ud835\udc62\ud835\udc62\u0ddc\ud835\udc56\ud835\udc56, \u2207\ud835\udc89\ud835\udc89\n2\ud835\udc62\ud835\udc62\u0ddc\ud835\udc56\ud835\udc56, \u2026 , \u2207\ud835\udc89\ud835\udc89\n\ud835\udc58\ud835\udc58\ud835\udc62\ud835\udc62\u0ddc\ud835\udc56\ud835\udc56\u0d70\u0d60\n\ud835\udc56\ud835\udc56=1\n\ud835\udc40\ud835\udc40\n\u0d6b25\u0d6f \nFor each sample, the input feature vector of the PGR is: \n\u0d6b\ud835\udc62\ud835\udc62\u0ddc\ud835\udc56\ud835\udc56, \u2207\ud835\udc61\ud835\udc61\ud835\udc62\ud835\udc62\u0ddc\ud835\udc56\ud835\udc56, \u2207\ud835\udc89\ud835\udc89\ud835\udc62\ud835\udc62\u0ddc\ud835\udc56\ud835\udc56, \u2207\ud835\udc89\ud835\udc89\n2\ud835\udc62\ud835\udc62\u0ddc\ud835\udc56\ud835\udc56, \u2026 , \u2207\ud835\udc89\ud835\udc89\n\ud835\udc58\ud835\udc58\ud835\udc62\ud835\udc62\u0ddc\ud835\udc56\ud835\udc56\u0d6f\u2208\ud835\udc45\ud835\udc45\ud835\udc51\ud835\udc51\n\u0d6b26\u0d6f \nWhere \ud835\udc51\ud835\udc51 is the total dimension of the input feature. \nWe use the MSE to define the physical loss between the PGR prediction output and the real-\ntime derivative: \n\u2112\ud835\udc5d\ud835\udc5d\u210e\ud835\udc66\ud835\udc66(\ud835\udf3d\ud835\udf3d) = 1\n\ud835\udc40\ud835\udc40\u0dcd\u0d6c\ud835\udcab\ud835\udcab\ud835\udf3d\ud835\udf3d\u0d6c\ud835\udc62\ud835\udc62\u0ddc\ud835\udc56\ud835\udc56, \u2202\ud835\udc62\ud835\udc62\u0ddc\ud835\udc56\ud835\udc56\n\u2202\ud835\udc61\ud835\udc61, \u2207\ud835\udc89\ud835\udc89\ud835\udc62\ud835\udc62\u0ddc\ud835\udc56\ud835\udc56, \u2207\ud835\udc89\ud835\udc89\n2\ud835\udc62\ud835\udc62\u0ddc\ud835\udc56\ud835\udc56, \u2026 , \u2207\ud835\udc89\ud835\udc89\n\ud835\udc58\ud835\udc58\ud835\udc62\ud835\udc62\u0ddc\ud835\udc56\ud835\udc56\u0d70\u2212\ud835\udf15\ud835\udf15\ud835\udc62\ud835\udc62\ud835\udc56\ud835\udc56\n\ud835\udf15\ud835\udf15\ud835\udf15\ud835\udf15\u0d70\n2\n\ud835\udc40\ud835\udc40\n\ud835\udc56\ud835\udc56=1\n\u0d6b27\u0d6f \nConstruct the data loss function: \n\u2112\ud835\udc51\ud835\udc51\ud835\udc51\ud835\udc51\ud835\udc51\ud835\udc51\ud835\udc51\ud835\udc51(\ud835\udf3d\ud835\udf3d) = 1\n\ud835\udc40\ud835\udc40\u0dcd(\ud835\udc62\ud835\udc62\ud835\udc56\ud835\udc56\u2212\ud835\udc62\ud835\udc62\u0ddc\ud835\udc56\ud835\udc56)2\n\ud835\udc40\ud835\udc40\n\ud835\udc56\ud835\udc56=1\n\u0d6b28\u0d6f \nGet the total loss function of the final model: \n\u2112= \u2112\ud835\udc5d\ud835\udc5d\u210e\ud835\udc66\ud835\udc66(\ud835\udf3d\ud835\udf3d) + \u2112\ud835\udc51\ud835\udc51\ud835\udc51\ud835\udc51\ud835\udc51\ud835\udc51\ud835\udc51\ud835\udc51(\ud835\udf3d\ud835\udf3d)\n\u0d6b29\u0d6f \nAfter training, \ud835\udcab\ud835\udcab\ud835\udf3d\ud835\udf3d\u2217 can approximately describe the dynamic evolution law of the equipment \ndegradation process: \n\ud835\udf15\ud835\udf15\ud835\udf15\ud835\udf15(\ud835\udc89\ud835\udc89, \ud835\udc61\ud835\udc61)\n\ud835\udf15\ud835\udf15\ud835\udf15\ud835\udf15\n\u2248\ud835\udcab\ud835\udcab\ud835\udf3d\ud835\udf3d\u2217\u0d6b\ud835\udc62\ud835\udc62\u0ddc\ud835\udc56\ud835\udc56, \u2207\ud835\udc61\ud835\udc61\ud835\udc62\ud835\udc62\u0ddc\ud835\udc56\ud835\udc56, \u2207\ud835\udc89\ud835\udc89\ud835\udc62\ud835\udc62\u0ddc\ud835\udc56\ud835\udc56, \u2207\ud835\udc89\ud835\udc89\n2\ud835\udc62\ud835\udc62\u0ddc\ud835\udc56\ud835\udc56,\u2026 , \u2207\ud835\udc89\ud835\udc89\n\ud835\udc58\ud835\udc58\ud835\udc62\ud835\udc62\u0ddc\ud835\udc56\ud835\udc56\u0d6f\n\u0d6b30\u0d6f \n3.3 Meta Parameters Update Framework \nTo achieve the goal that the proposed MKDPINN framework (whose all trainable parameters \nare represented by meta-parameters \ud835\udf31\ud835\udf31 at the meta-learning level) can quickly adapt with only a \nsmall number of samples when facing new tasks, we designed a first-order optimization-based meta-\nlearning framework. This framework aims to learn a set of optimized meta-parameters \ud835\udf31\ud835\udf31\u2217, making \nthem an efficient starting point for adapting to new RUL prediction tasks. \n3.3.1 Objective Function \nThe ultimate goal of the proposed method is to find a set of meta-parameters \ud835\udf31\ud835\udf31\u2217 , so that \nstarting from this point, the model can reach the lowest expected loss on the corresponding \nvalidation data \ud835\udc9f\ud835\udc9f\ud835\udc5d\ud835\udc5d\n\ud835\udc63\ud835\udc63\ud835\udc63\ud835\udc63\ud835\udc63\ud835\udc63 after performing \ud835\udc58\ud835\udc58 steps of optimization on a small amount of training data \n\ud835\udc9f\ud835\udc9f\ud835\udc5d\ud835\udc5d\n\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61 of task \ud835\udcaf\ud835\udcaf\ud835\udc5d\ud835\udc5d(parameters are represented as \u03b8, using an inner-loop learning rate \u03b1 and the Adam \noptimizer, starting from \ud835\udf3d\ud835\udf3d\ud835\udc5d\ud835\udc5d\n(0) = \ud835\udf31\ud835\udf31 ). Let Adapt\ud835\udc58\ud835\udc58\n\ud835\udc34\ud835\udc34\ud835\udc34\ud835\udc34\ud835\udc34\ud835\udc34\ud835\udc34\ud835\udc34\u0d6b\ud835\udf31\ud835\udf31, \ud835\udc9f\ud835\udc9f\ud835\udc5d\ud835\udc5d\n\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61, \ud835\udefc\ud835\udefc\u0d6f  represent this \ud835\udc58\ud835\udc58 -step Adam \nadaptation process, which outputs the adapted parameters \ud835\udf3d\ud835\udf3d\ud835\udc5d\ud835\udc5d\n(\ud835\udc58\ud835\udc58) . From formula (6), the meta-\noptimization problem can be written as: \n\ud835\udf31\ud835\udf31\u2217= arg min\n\ud835\udf31\ud835\udf31\ud835\udc38\ud835\udc38\ud835\udcaf\ud835\udcaf\ud835\udc5d\ud835\udc5d\u223c\ud835\udc43\ud835\udc43(\ud835\udcaf\ud835\udcaf) \u0d63\ud835\udc39\ud835\udc39\ud835\udc5d\ud835\udc5d(\ud835\udf31\ud835\udf31)\u0d67 where \ud835\udc39\ud835\udc39\ud835\udc5d\ud835\udc5d(\ud835\udf31\ud835\udf31) = \u2112\ud835\udc5d\ud835\udc5d\u0d6bAdapt\ud835\udc58\ud835\udc58\n\ud835\udc34\ud835\udc34\ud835\udc34\ud835\udc34\ud835\udc34\ud835\udc34\ud835\udc34\ud835\udc34\u0d6b\ud835\udf31\ud835\udf31, \ud835\udc9f\ud835\udc9f\ud835\udc5d\ud835\udc5d\n\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61, \u03b1\u0d6f; \ud835\udc9f\ud835\udc9f\ud835\udc5d\ud835\udc5d\n\ud835\udc63\ud835\udc63\ud835\udc63\ud835\udc63\ud835\udc63\ud835\udc63\u0d6f\n\u0d6b31\u0d6f \n\n \n \nWhere \u2112\ud835\udc5d\ud835\udc5d is the total loss of our MKDPINN model on task \ud835\udcaf\ud835\udcaf\ud835\udc5d\ud835\udc5d (data loss + physics loss, defined \nby equation (29)). \nDirectly optimizing equation (31) usually requires calculating the gradient propagated through \nthe adaptation process. For MAML [29], this involves second-order derivatives (Hessian vector \nproduct), which is computationally expensive [35]. To circumvent this problem, we adopt an \nefficient first-order meta-learning algorithm, whose update mechanism evolves from Reptile [50]. \nThis method does not directly calculate the gradient of \ud835\udc39\ud835\udc39\ud835\udc5d\ud835\udc5d(\ud835\udf31\ud835\udf31), but implicitly optimizes the objective \nthrough a clever update rule. \n3.3.2 First-Order Meta-Update Algorithm with Task Batch \nWe adopt the following first-order meta-learning algorithm to update meta-parameters by batch \nprocessing tasks: \nMeta-Training stage: Iterate through the following steps: \n1.  Task Batch Sampling: Randomly sample a mini-batch containing \ud835\udc35\ud835\udc35 tasks from the source \ndomain task distribution \ud835\udc43\ud835\udc43(\ud835\udcaf\ud835\udcaf):{\ud835\udcaf\ud835\udcaf\ud835\udc5d\ud835\udc5d}\ud835\udc5d\ud835\udc5d=1\n\ud835\udc35\ud835\udc35\n. \n2.  Parallel Inner Loop Adaptation: For each task \ud835\udcaf\ud835\udcaf\ud835\udc5d\ud835\udc5d(\ud835\udc5d\ud835\udc5d= 1, \u2026 , \ud835\udc35\ud835\udc35)  in the batch, \nindependently perform the following operations: \n    (a.  Initialize task parameters: \ud835\udf3d\ud835\udf3d\ud835\udc91\ud835\udc91\n(\ud835\udfce\ud835\udfce) = \ud835\udf31\ud835\udf31 (using the latest meta-parameters). \n    (b.  Use the Adam optimizer (with a learning rate of \u03b1) to perform \ud835\udc58\ud835\udc58 steps of updates \non the training data \ud835\udc9f\ud835\udc9f\ud835\udc5d\ud835\udc5d\n\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61  of task \ud835\udcaf\ud835\udcaf\ud835\udc5d\ud835\udc5d  to minimize the loss \u2112\ud835\udc5d\ud835\udc5d\u0d6b\ud835\udf3d\ud835\udf3d; \ud835\udc9f\ud835\udc9f\ud835\udc5d\ud835\udc5d\n\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\u0d6f . Let \ud835\udc54\ud835\udc54\ud835\udc5d\ud835\udc5d\n(\ud835\udc56\ud835\udc56) =\n\u2207\ud835\udf3d\ud835\udf3d\u2112\ud835\udc5d\ud835\udc5d\u1240\ud835\udf3d\ud835\udf3d\ud835\udc91\ud835\udc91\n(\ud835\udc8a\ud835\udc8a\u2212\ud835\udfcf\ud835\udfcf);\ud835\udc9f\ud835\udc9f\ud835\udc5d\ud835\udc5d\n\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\u1241 be the gradient of the \ud835\udc56\ud835\udc56-th step with respect to the task parameter \ud835\udf3d\ud835\udf3d. The Adam \nupdate involves the exponential moving average estimation of the first-order moment \ud835\udc5a\ud835\udc5a and the \nsecond-order moment \ud835\udc63\ud835\udc63 of the gradient: \n\ud835\udc5a\ud835\udc5a\ud835\udc5d\ud835\udc5d\n(\ud835\udc56\ud835\udc56) = \ud835\udefd\ud835\udefd1\ud835\udc5a\ud835\udc5a\ud835\udc5d\ud835\udc5d\n(\ud835\udc56\ud835\udc56\u22121) + (1 \u2212\ud835\udefd\ud835\udefd1)\ud835\udc54\ud835\udc54\ud835\udc5d\ud835\udc5d\n(\ud835\udc56\ud835\udc56) \n\ud835\udc63\ud835\udc63\ud835\udc5d\ud835\udc5d\n(\ud835\udc56\ud835\udc56) = \ud835\udefd\ud835\udefd2\ud835\udc63\ud835\udc63\ud835\udc5d\ud835\udc5d\n(\ud835\udc56\ud835\udc56\u22121) + (1 \u2212\ud835\udefd\ud835\udefd2) \u1240\ud835\udc54\ud835\udc54\ud835\udc5d\ud835\udc5d\n(\ud835\udc56\ud835\udc56)\u1241\n2\n \n\ud835\udc5a\ud835\udc5a\u0ddd\ud835\udc5d\ud835\udc5d\n(\ud835\udc56\ud835\udc56) = \ud835\udc5a\ud835\udc5a\ud835\udc5d\ud835\udc5d\n(\ud835\udc56\ud835\udc56)/\u0d6b1 \u2212\ud835\udefd\ud835\udefd1\n\ud835\udc56\ud835\udc56\u0d6f \n\ud835\udc63\ud835\udc63\u0ddc\ud835\udc5d\ud835\udc5d\n(\ud835\udc56\ud835\udc56) = \ud835\udc63\ud835\udc63\ud835\udc5d\ud835\udc5d\n(\ud835\udc56\ud835\udc56)/\u0d6b1 \u2212\ud835\udefd\ud835\udefd2\n\ud835\udc56\ud835\udc56\u0d6f \n\ud835\udf3d\ud835\udf3d\ud835\udc91\ud835\udc91\n(\ud835\udc8a\ud835\udc8a) = \ud835\udf3d\ud835\udf3d\ud835\udc91\ud835\udc91\n(\ud835\udc8a\ud835\udc8a\u2212\ud835\udfcf\ud835\udfcf) \u2212\ud835\udefc\ud835\udefc\n\ud835\udc5a\ud835\udc5a\u0ddd\ud835\udc5d\ud835\udc5d\n(\ud835\udc56\ud835\udc56)\n\u0da7\ud835\udc63\ud835\udc63\u0ddc\ud835\udc5d\ud835\udc5d\n(\ud835\udc56\ud835\udc56) + \ud835\udf16\ud835\udf16\n for \ud835\udc56\ud835\udc56= 1, \u2026, \ud835\udc58\ud835\udc58 \n\u0d6b32\u0d6f \nWhere \ud835\udefd\ud835\udefd1, \ud835\udefd\ud835\udefd2 \u2208[0,1) are the exponential decay rates used to calculate the moment estimates, \nand \u03f5  is a small constant added for numerical stability. \ud835\udc5a\ud835\udc5a\u0ddd\ud835\udc5d\ud835\udc5d\n(\ud835\udc56\ud835\udc56)  and \ud835\udc63\ud835\udc63\u0ddc\ud835\udc5d\ud835\udc5d\n(\ud835\udc56\ud835\udc56)  are bias corrections for \n\ud835\udc5a\ud835\udc5a\ud835\udc5d\ud835\udc5d\n(\ud835\udc56\ud835\udc56) and \ud835\udc63\ud835\udc63\ud835\udc5d\ud835\udc5d\n(\ud835\udc56\ud835\udc56). Finally, the parameter \ud835\udf3d\ud835\udf3d\ud835\udc91\ud835\udc91\n(\ud835\udc8c\ud835\udc8c) after adapting \ud835\udc58\ud835\udc58 steps for task \ud835\udc5d\ud835\udc5d is obtained. \n3.  Outer Loop Meta-Parameter Update: Based on the adaptation results of all tasks in the batch, \ncalculate the update amount of the meta-parameter \ud835\udf31\ud835\udf31. The update rule is: \n\ud835\udf31\ud835\udf31\ud835\udc8f\ud835\udc8f\ud835\udc8f\ud835\udc8f\ud835\udc8f\ud835\udc8f= \ud835\udf31\ud835\udf31\ud835\udc90\ud835\udc90\ud835\udc90\ud835\udc90\ud835\udc90\ud835\udc90+ \ud835\udf02\ud835\udf021\n\ud835\udc35\ud835\udc35\u0dcd\u1240\ud835\udf3d\ud835\udf3d\ud835\udc91\ud835\udc91\n(\ud835\udc8c\ud835\udc8c) \u2212\ud835\udf31\ud835\udf31\ud835\udc90\ud835\udc90\ud835\udc90\ud835\udc90\ud835\udc90\ud835\udc90\u1241\n\ud835\udc35\ud835\udc35\n\ud835\udc5d\ud835\udc5d=1\n\u11e3\u11e7\u11e7\u11e7\u11e7\u11e7\u11e4\u11e7\u11e7\u11e7\u11e7\u11e7\u11e5\nAverage update direction \u0394\ud835\udf31\ud835\udf31\n \n\u0d6b33\u0d6f \n\n \n \nWhere \ud835\udf02\ud835\udf02 is the outer loop (meta) learning rate. This step moves the meta-parameter \ud835\udf31\ud835\udf31 \nalong the average vector direction from the initial point \ud835\udf31\ud835\udf31\ud835\udc90\ud835\udc90\ud835\udc90\ud835\udc90\ud835\udc90\ud835\udc90 to the respective adapted \nparameters \ud835\udf3d\ud835\udf3d\ud835\udc91\ud835\udc91\n(\ud835\udc8c\ud835\udc8c) of all tasks in the batch. \nFigure 3 intuitively depicts the update process of the first-order optimization-based meta-\nparameters \ud835\udf31\ud835\udf31  in the proposed MKDPINN framework, which includes two core stages: meta-\ntraining and rapid adaptation. In the meta-training stage, the goal is to learn a set of optimized meta-\nparameters \ud835\udf31\ud835\udf31\u2217 from a large number of source domain tasks, which possess good generalization \nability and rapid adaptation potential. Specifically, in the \ud835\udc56\ud835\udc56-th iteration, starting from the current \nmeta-parameters \ud835\udf31\ud835\udf31\ud835\udc8a\ud835\udc8a, the model samples a meta batch (Batch Size \ud835\udc35\ud835\udc35, \ud835\udc35\ud835\udc35=2 in the example figure) \nof meta-tasks (\ud835\udc5a\ud835\udc5a\ud835\udc56\ud835\udc56 and \ud835\udc5b\ud835\udc5b\ud835\udc56\ud835\udc56). For each task \ud835\udc5d\ud835\udc5d in the batch, inner loop optimization is performed: \nusing \ud835\udf31\ud835\udf31\ud835\udc8a\ud835\udc8a as the initial parameters, using a small amount of training data \ud835\udc9f\ud835\udc9f\ud835\udc5d\ud835\udc5d\n\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61 from the task itself, \nthe model parameters are updated independently through \ud835\udc58\ud835\udc58 steps of gradient descent based on the \nAdam optimizer (as described in equation (32)), and finally, the parameters \ud835\udf3d\ud835\udf3d\u0de1\ud835\udc91\ud835\udc91(\ud835\udc8c\ud835\udc8c) adapted to the \nspecific task are obtained (as shown by the endpoint \ud835\udf3d\ud835\udf3d\u0de1\ud835\udc8e\ud835\udc8e\ud835\udc8a\ud835\udc8a and \ud835\udf3d\ud835\udf3d\u0de1\ud835\udc8f\ud835\udc8f\ud835\udc8a\ud835\udc8a in the figure). After completing \nthe inner loop adaptation of all meta tasks, the meta-parameters are updated in the outer loop: the \nvectors pointing from the current meta-parameters \ud835\udf31\ud835\udf31\ud835\udc8a\ud835\udc8a to the adapted parameters \ud835\udf3d\ud835\udf3d\u0de1\ud835\udc91\ud835\udc91(\ud835\udc8c\ud835\udc8c) of all tasks \nin the batch are calculated (i.e., the \"task update vector\"  \ud835\udf3d\ud835\udf3d\u0de1\ud835\udc91\ud835\udc91(\ud835\udc8c\ud835\udc8c) \u2212\ud835\udf31\ud835\udf31\ud835\udc8a\ud835\udc8a, as shown by the dashed lines \nin the figure), and the average of these vectors is calculated. The current meta-parameters \ud835\udf31\ud835\udf31\ud835\udc8a\ud835\udc8a are \nthen updated along the direction of this average vector, with the step size controlled by the meta-\nlearning rate \u03b7 , so as to obtain the new meta-parameters \ud835\udf31\ud835\udf31\ud835\udc8a\ud835\udc8a+\ud835\udfcf\ud835\udfcf , whose update rule follows the \nequation \ud835\udf31\ud835\udf31\ud835\udc8a\ud835\udc8a+\ud835\udfcf\ud835\udfcf= \ud835\udf31\ud835\udf31\ud835\udc8a\ud835\udc8a + \ud835\udf02\ud835\udf02\u22c5(1/\ud835\udc35\ud835\udc35) \u2211\n\u0d6b\ud835\udf3d\ud835\udf3d\u0de1\ud835\udc91\ud835\udc91(\ud835\udc8c\ud835\udc8c) \u2212\ud835\udf31\ud835\udf31\ud835\udc8a\ud835\udc8a\u0d6f\n\ud835\udc35\ud835\udc35\n\ud835\udc5d\ud835\udc5d=1\n  (corresponding to equation (33)). By \nrepeatedly iterating through this inner and outer loop process on many different meta task batches, \nthe meta-parameters \ud835\udf31\ud835\udf31 are driven to converge towards an \"optimal initial point\" \ud835\udf31\ud835\udf31\u2217 that enables \nthe model to perform well on various tasks with slight adjustments. Subsequently, when facing a \nnew target domain task for the rapid adaptation stage, the final meta-parameters \ud835\udf31\ud835\udf31\u2217 obtained from \nmeta-training are used as the starting point for the new task model. Using the extremely small \nnumber of support samples \ud835\udc9f\ud835\udc9f\ud835\udc5b\ud835\udc5b\ud835\udc5b\ud835\udc5b\ud835\udc5b\ud835\udc5b\n\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60\ud835\udc60 provided by the new task, starting from \ud835\udf31\ud835\udf31\u2217, perform \ud835\udc58\ud835\udc58\u2032steps \nof gradient update as the meta-training inner loop, and the model parameters \ud835\udf3d\ud835\udf3d\u0de1\ud835\udc95\ud835\udc95\ud835\udc95\ud835\udc95\ud835\udc95\ud835\udc95\ud835\udc95\ud835\udc95 optimized for \nthe characteristics of the new task can be quickly obtained for subsequent RUL prediction. \n3.3.3 Theoretical Analysis of the Meta-Update Direction \nThe core of equation (33) lies in its update direction \u0394\ud835\udf31\ud835\udf31= 1/\ud835\udc35\ud835\udc35\u2211\n\u1240\ud835\udf3d\ud835\udf3d\ud835\udc91\ud835\udc91\n(\ud835\udc8c\ud835\udc8c) \u2212\ud835\udf31\ud835\udf31\ud835\udc90\ud835\udc90\ud835\udc90\ud835\udc90\ud835\udc90\ud835\udc90\u1241\n\ud835\udc35\ud835\udc35\n\ud835\udc5d\ud835\udc5d=1\n . \nFig. 3. Meta-Parameter Update of MKDPINN \n\n \n \nAlthough the explicit meta-gradient \u2207\ud835\udf31\ud835\udf31\ud835\udc39\ud835\udc39\ud835\udc5d\ud835\udc5d(\ud835\udf31\ud835\udf31) is not used, this direction implicitly contains the \ninformation needed to optimize the meta-objective (31). We can understand this by analyzing the \nexpected properties of the update vector \ud835\udc85\ud835\udc85\ud835\udc91\ud835\udc91= \ud835\udf3d\ud835\udf3d\ud835\udc91\ud835\udc91\n(\ud835\udc8c\ud835\udc8c) \u2212\ud835\udf31\ud835\udf31\ud835\udc90\ud835\udc90\ud835\udc90\ud835\udc90\ud835\udc90\ud835\udc90 for a single task. \nConsider the \ud835\udc58\ud835\udc58 -step inner-loop optimization process. Even when using Adam, \ud835\udf3d\ud835\udf3d\ud835\udc91\ud835\udc91\n(\ud835\udc8c\ud835\udc8c)  is \nobtained from \ud835\udf31\ud835\udf31, by iterating \ud835\udc58\ud835\udc58 times depending on the gradient of the task \ud835\udc5d\ud835\udc5d loss function \u2112\ud835\udc5d\ud835\udc5d \nwith respect to the task parameter \ud835\udf3d\ud835\udf3d  (calculated at the iteration point \ud835\udf3d\ud835\udf3d\ud835\udc91\ud835\udc91\n(\ud835\udc8a\ud835\udc8a\u2212\ud835\udfcf\ud835\udfcf) ). The vector \ud835\udc85\ud835\udc85\ud835\udc91\ud835\udc91 \nrepresents the total displacement of the task parameters starting from \ud835\udf31\ud835\udf31  and moving \ud835\udc58\ud835\udc58  steps \nunder the loss-driven of task \ud835\udc5d\ud835\udc5d. \nWe can draw on the idea of Taylor expansion to analyze the relationship between \ud835\udc85\ud835\udc85\ud835\udc91\ud835\udc91 and the \nproperties of \u2112\ud835\udc5d\ud835\udc5d at the initial meta-parameter point \ud835\udf31\ud835\udf31. Let \ud835\udc54\ud835\udc54\ud835\udc5d\ud835\udc5d,\ud835\udc56\ud835\udc56= \u2207\ud835\udf3d\ud835\udf3d\u2112\ud835\udc5d\ud835\udc5d\u1240\ud835\udf3d\ud835\udf3d\ud835\udc91\ud835\udc91\n(\ud835\udc8a\ud835\udc8a\u2212\ud835\udfcf\ud835\udfcf); \ud835\udc9f\ud835\udc9f\ud835\udc5d\ud835\udc5d,\ud835\udc56\ud835\udc56\n\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\u1241 denote the \n(mini-batch) gradient used in the \ud835\udc56\ud835\udc56-th step, \ud835\udc54\ud835\udc54\ud835\udc5d\ud835\udc5d,\ud835\udc56\ud835\udc56= \u2207\ud835\udf3d\ud835\udf3d\u2112\ud835\udc5d\ud835\udc5d\u0d6b\ud835\udf31\ud835\udf31; \ud835\udc9f\ud835\udc9f\ud835\udc5d\ud835\udc5d,\ud835\udc56\ud835\udc56\n\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\u0d6f denote the gradient with respect \nto the task parameter \ud835\udf3d\ud835\udf3d at the initial meta-parameter point \ud835\udf31\ud835\udf31, and \ud835\udc3b\ud835\udc3b\ud835\udc5d\ud835\udc5d,\ud835\udc56\ud835\udc56= \u2207\ud835\udf3d\ud835\udf3d\n2\u2112\ud835\udc5d\ud835\udc5d\u0d6b\ud835\udf31\ud835\udf31; \ud835\udc9f\ud835\udc9f\ud835\udc5d\ud835\udc5d,\ud835\udc56\ud835\udc56\n\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\u0d6f denote \nthe corresponding Hessian matrix with respect to the task parameter \ud835\udf3d\ud835\udf3d (also evaluated at \ud835\udf31\ud835\udf31). \nFor simple SGD (\ud835\udc58\ud835\udc58=1), \ud835\udc51\ud835\udc51\ud835\udc5d\ud835\udc5d= \ud835\udf3d\ud835\udf3d\ud835\udc91\ud835\udc91\n(\ud835\udfcf\ud835\udfcf) \u2212\ud835\udf31\ud835\udf31= \u2212\ud835\udefc\ud835\udefc\ud835\udc54\ud835\udc54\ud835\udc5d\ud835\udc5d,1 \u2248\u2212\ud835\udefc\ud835\udefc\ud835\udc54\ud835\udc54\ud835\udc5d\ud835\udc5d,1. \nFor SGD (\ud835\udc58\ud835\udc58>1), from the formula (40) in reference [50], we can know that: \n\ud835\udc85\ud835\udc85\ud835\udc91\ud835\udc91= \u0dcd\u1240\ud835\udf3d\ud835\udf3d\ud835\udc91\ud835\udc91\n(\ud835\udc8a\ud835\udc8a) \u2212\ud835\udf3d\ud835\udf3d\ud835\udc91\ud835\udc91\n(\ud835\udc8a\ud835\udc8a\u2212\ud835\udfcf\ud835\udfcf)\u1241\n\ud835\udc58\ud835\udc58\n\ud835\udc56\ud835\udc56=1\n= \u2212\ud835\udefc\ud835\udefc\u0dcd\ud835\udc54\ud835\udc54\ud835\udc5d\ud835\udc5d,\ud835\udc56\ud835\udc56\n\ud835\udc58\ud835\udc58\n\ud835\udc56\ud835\udc56=1\n\u0d6b34\u0d6f \nUsing the Taylor expansion \ud835\udc54\ud835\udc54\ud835\udc5d\ud835\udc5d,\ud835\udc56\ud835\udc56\u2248\ud835\udc54\ud835\udc54\ud835\udc5d\ud835\udc5d,\ud835\udc56\ud835\udc56+ \ud835\udc3b\ud835\udc3b\ud835\udc5d\ud835\udc5d,\ud835\udc56\ud835\udc56\u1240\ud835\udf3d\ud835\udf3d\ud835\udc91\ud835\udc91\n(\ud835\udc8a\ud835\udc8a\u2212\ud835\udfcf\ud835\udfcf) \u2212\ud835\udf31\ud835\udf31\u1241\u2248\ud835\udc54\ud835\udc54\ud835\udc5d\ud835\udc5d,\ud835\udc56\ud835\udc56\u2212\ud835\udefc\ud835\udefc\ud835\udc3b\ud835\udc3b\ud835\udc5d\ud835\udc5d,\ud835\udc56\ud835\udc56\u2211\n\ud835\udc54\ud835\udc54\ud835\udc5d\ud835\udc5d,\ud835\udc57\ud835\udc57\n\ud835\udc56\ud835\udc56\u22121\n\ud835\udc57\ud835\udc57=1\n+\n\ud835\udc42\ud835\udc42(\ud835\udefc\ud835\udefc2), and substituting it into (34), we can obtain (ignoring the \ud835\udc42\ud835\udc42(\ud835\udefc\ud835\udefc2) term): \n\ud835\udc85\ud835\udc85\ud835\udc91\ud835\udc91\u2248\u2212\ud835\udefc\ud835\udefc\u0dcd\u124c\ud835\udc54\ud835\udc54\ud835\udc5d\ud835\udc5d,\ud835\udc56\ud835\udc56\u2212\ud835\udefc\ud835\udefc\ud835\udc3b\ud835\udc3b\ud835\udc5d\ud835\udc5d,\ud835\udc56\ud835\udc56\u0dcd\ud835\udc54\ud835\udc54\ud835\udc5d\ud835\udc5d,\ud835\udc57\ud835\udc57\n\ud835\udc56\ud835\udc56\u22121\n\ud835\udc57\ud835\udc57=1\n\u124d\n\ud835\udc58\ud835\udc58\n\ud835\udc56\ud835\udc56=1\n= \u2212\ud835\udefc\ud835\udefc\u0dcd\ud835\udc54\ud835\udc54\ud835\udc5d\ud835\udc5d,\ud835\udc56\ud835\udc56\n\ud835\udc58\ud835\udc58\n\ud835\udc56\ud835\udc56=1\n+ \ud835\udefc\ud835\udefc2 \u0dcd\u0dcd\ud835\udc3b\ud835\udc3b\ud835\udc5d\ud835\udc5d,\ud835\udc56\ud835\udc56\ud835\udc54\ud835\udc54\ud835\udc5d\ud835\udc5d,\ud835\udc57\ud835\udc57\n\ud835\udc56\ud835\udc56\u22121\n\ud835\udc57\ud835\udc57=1\n\ud835\udc58\ud835\udc58\n\ud835\udc56\ud835\udc56=1\n \n\u0d6b35\u0d6f \nAlthough the Adam update is more complex than SGD, making the precise Taylor expansion \nmore difficult (because the update step size depends on the moment estimation of historical \ngradients), its update is essentially still an iteration process based on gradients. Therefore, it can be \nexpected that \ud835\udc85\ud835\udc85\ud835\udc91\ud835\udc91= \ud835\udf3d\ud835\udf3d\ud835\udc91\ud835\udc91\n(\ud835\udc8c\ud835\udc8c) \u2212\ud835\udf31\ud835\udf31 will still approximately capture a structure similar to equation (35), \nthat is, it not only depends on the initial gradient \ud835\udc54\ud835\udc54\ud835\udc5d\ud835\udc5d,\ud835\udc56\ud835\udc56 (evaluated at \ud835\udf31\ud835\udf31), but is also affected by the \nHessian matrix \ud835\udc3b\ud835\udc3b\ud835\udc5d\ud835\udc5d,\ud835\udc56\ud835\udc56 (evaluated at \ud835\udf31\ud835\udf31) and the interaction of previous gradients, especially when \n\ud835\udc58\ud835\udc58>1. \nNow consider the expectation of the outer loop update direction \ud835\udc38\ud835\udc38\ud835\udcaf\ud835\udcaf\ud835\udc5d\ud835\udc5d\u223c\ud835\udc43\ud835\udc43(\ud835\udcaf\ud835\udcaf),{\ud835\udc9f\ud835\udc9f\ud835\udc5d\ud835\udc5d,\ud835\udc56\ud835\udc56\n\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61}\u0d63\ud835\udc85\ud835\udc85\ud835\udc91\ud835\udc91\u0d67. Assume \nthat each step \ud835\udc56\ud835\udc56 of the inner loop uses different random mini-batch data \ud835\udc9f\ud835\udc9f\ud835\udc5d\ud835\udc5d,\ud835\udc56\ud835\udc56\n\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61 from task \ud835\udc5d\ud835\udc5d. \nTake the expectation of equation (35): \n\ud835\udc38\ud835\udc38\u0d63\ud835\udc85\ud835\udc85\ud835\udc91\ud835\udc91\u0d67\u2248\u2212\u03b1\u0dcd\ud835\udc38\ud835\udc38\u1242\ud835\udc54\ud835\udc54\ud835\udc5d\ud835\udc5d,\ud835\udc56\ud835\udc56\u1243\n\ud835\udc58\ud835\udc58\n\ud835\udc56\ud835\udc56=1\n+ \u03b12 \u0dcd\u0dcd\ud835\udc38\ud835\udc38\u1242\ud835\udc3b\ud835\udc3b\ud835\udc5d\ud835\udc5d,\ud835\udc56\ud835\udc56\ud835\udc54\ud835\udc54\ud835\udc5d\ud835\udc5d,\ud835\udc57\ud835\udc57\u1243\n\ud835\udc56\ud835\udc56\u22121\n\ud835\udc57\ud835\udc57=1\n\ud835\udc58\ud835\udc58\n\ud835\udc56\ud835\udc56=1\n \n\u0d6b36\u0d6f \nDefine the Average Gradient AvgGrad(\ud835\udf31\ud835\udf31) = \ud835\udc38\ud835\udc38\ud835\udcaf\ud835\udcaf\ud835\udc5d\ud835\udc5d,\ud835\udc9f\ud835\udc9f\ud835\udc5d\ud835\udc5d,\ud835\udc56\ud835\udc56\n\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\u0d63\u2207\ud835\udf3d\ud835\udf3d\u2112\ud835\udc5d\ud835\udc5d\u0d6b\ud835\udf31\ud835\udf31; \ud835\udc9f\ud835\udc9f\ud835\udc5d\ud835\udc5d\n\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\u0d6f\u0d67= \ud835\udc38\ud835\udc38\u1242\ud835\udc54\ud835\udc54\ud835\udc5d\ud835\udc5d,\ud835\udc56\ud835\udc56\u1243  (assuming \nthat the expected gradients in different steps are the same). This term drives \ud835\udf31\ud835\udf31  towards the \nminimization direction of the average loss of all tasks (evaluated at the point \ud835\udf31\ud835\udf31) (similar to joint \ntraining). \nDefine the Average Gradient Inner-product term AvgGradInner(\ud835\udf31\ud835\udf31) = \ud835\udc38\ud835\udc38\ud835\udcaf\ud835\udcaf\ud835\udc5d\ud835\udc5d,\ud835\udc9f\ud835\udc9f\ud835\udc5d\ud835\udc5d,\ud835\udc56\ud835\udc56\n\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61,\ud835\udc9f\ud835\udc9f\ud835\udc5d\ud835\udc5d,\ud835\udc57\ud835\udc57\n\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\u1242\ud835\udc3b\ud835\udc3b\ud835\udc5d\ud835\udc5d,\ud835\udc56\ud835\udc56\ud835\udc54\ud835\udc54\ud835\udc5d\ud835\udc5d,\ud835\udc57\ud835\udc57\u1243 \n(where \ud835\udc56\ud835\udc56 \u2260\ud835\udc57\ud835\udc57, and \ud835\udc9f\ud835\udc9f\ud835\udc5d\ud835\udc5d,\ud835\udc56\ud835\udc56\n\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61, \ud835\udc9f\ud835\udc9f\ud835\udc5d\ud835\udc5d,\ud835\udc57\ud835\udc57\n\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61 come from the same task \ud835\udcaf\ud835\udcaf\ud835\udc5d\ud835\udc5d, \ud835\udc3b\ud835\udc3b\ud835\udc5d\ud835\udc5d,\ud835\udc56\ud835\udc56 and \ud835\udc54\ud835\udc54\ud835\udc5d\ud835\udc5d,\ud835\udc57\ud835\udc57 are both evaluated at \n\n \n \n\ud835\udf31\ud835\udf31). As analyzed in reference [50] (equations 29-32), this type of term is related to maximizing the \n(expected) inner product between gradients of different data batches within the task (evaluated at \n\ud835\udf31\ud835\udf31 ), that is, \u2207\ud835\udf31\ud835\udf31\ud835\udc38\ud835\udc38\ud835\udcaf\ud835\udcaf\ud835\udc5d\ud835\udc5d,\ud835\udc9f\ud835\udc9f\ud835\udc5d\ud835\udc5d,1,\ud835\udc9f\ud835\udc9f\ud835\udc5d\ud835\udc5d,2 \u1242\ud835\udc54\ud835\udc54\ud835\udc5d\ud835\udc5d,1 \u22c5\ud835\udc54\ud835\udc54\ud835\udc5d\ud835\udc5d,2\u1243  (note that the derivative here is with respect to \ud835\udf31\ud835\udf31 ). \nOptimizing this term helps to improve the model's ability to quickly adapt from \ud835\udf31\ud835\udf31  (in-task \ngeneralization). \nSubstitute these definitions into equation (36) to get: \n\ud835\udc38\ud835\udc38\u0d63\ud835\udc85\ud835\udc85\ud835\udc91\ud835\udc91\u0d67\u2248\u2212(\ud835\udc58\ud835\udc58\ud835\udc58\ud835\udc58)AvgGrad(\ud835\udf31\ud835\udf31) + \u1246\ud835\udc58\ud835\udc58(\ud835\udc58\ud835\udc58\u22121)\n2\n\ud835\udefc\ud835\udefc2\u1247AvgGradInner(\ud835\udf31\ud835\udf31) \n\u0d6b37\u0d6f \nTherefore, the average direction \u2206\ud835\udf31\ud835\udf31= (1/\ud835\udc35\ud835\udc35) \u2211\n\ud835\udc85\ud835\udc85\ud835\udc91\ud835\udc91\n\ud835\udc35\ud835\udc35\n\ud835\udc5d\ud835\udc5d=1\n used in our outer loop update step (33), \nhas an expectation of \ud835\udc38\ud835\udc38[\u2206\ud835\udf31\ud835\udf31] = \ud835\udc38\ud835\udc38\u0d63\ud835\udc85\ud835\udc85\ud835\udc91\ud835\udc91\u0d67. This means that our update step \ud835\udf31\ud835\udf31\ud835\udc8f\ud835\udc8f\ud835\udc8f\ud835\udc8f\ud835\udc8f\ud835\udc8f= \ud835\udf31\ud835\udf31\ud835\udc90\ud835\udc90\ud835\udc90\ud835\udc90\ud835\udc90\ud835\udc90+ \ud835\udf02\ud835\udf02\u2206\ud835\udf31\ud835\udf31 is \nessentially performing an approximate gradient ascent (if \ud835\udf02\ud835\udf02  > 0). The key is that the update \ndirection simultaneously contains components pointing towards \u2212AvgGrad  (minimizing the \naverage loss) and +AvgGradInner (maximizing the in-task gradient alignment to promote rapid \nadaptation). Their relative weights are implicitly determined by \ud835\udc58\ud835\udc58 and \ud835\udefc\ud835\udefc . \nIn this way, the proposed first-order meta-learning algorithm based on Adam and task batches, \nimplicitly optimizes the meta-learning objective (31) without calculating second-order derivatives. \nThe learned meta-parameter \ud835\udf31\ud835\udf31 can balance the average performance on all tasks and the potential \nfor rapid adaptation on new tasks. \n4. Experiment \n4.1 Case study 1: Real Industrial Scenarios \nTo validate the proposed RUL prediction methodology in a relevant industrial setting, \nexperiments were conducted on critical assets within a large-scale iron ore processing system (an \niron ore concentrator). The experimental objects are two 150ZJ-I-A65 type slurry pumps, which \nhave a rated power of 178kW, a designed head of 63.2m, a rated flow of 470m\u00b3/h, and a working \nspeed of 980rpm. Among them, pump No. 1 fully recorded two full life-cycle operating data \nincluding installation and commissioning, normal operation, degradation failure, and re-launch after \nmaintenance, and pump No. 2 provided complete degradation process data from initial operation to \nfailure. The on-site operation of the pump is shown in Figure 4. Vibration sensors are installed at \nthe drive end, non-drive end, and the drive end and free end of the motor to collect vibration signals \nwith a sampling frequency of 12.8kHz for each segment lasting 2.56 seconds, and periodic sampling \nis performed at intervals of 10 minutes. \nFigure 5 shows the wear condition of the pump casing after the first life cycle of pump No. 1. \nFig. 4. Operational Status and Vibration Data Acquisition of Slurry Pump \nin Industrial Site \n\n \n \nIt can be seen that the pump casing has been worn through. In addition to the perforations, erosion-\ncorrosion marks also appear in other areas on the inner wall of the pump casing. This shows the \nharsh operating environment of the slurry pump. Figure 6 shows the original vibration signal \nwaveform of slurry pump No. 1 in two complete life cycles. Compared with the continuous and \nstable vibration signals usually obtained in a controlled laboratory environment, these data from real \nindustrial scenarios show complexity. The non-regular idling (amplitude significantly reduced) and \nshutdown (amplitude close to zero) periods are clearly visible in the figure. Further analysis shows \nthat the longer idling periods are mainly concentrated in the initial stage of each life cycle \n(approximately the first 110 hours for cycle 1 and approximately the first 180 hours for cycle 2). \nThis phenomenon is related to the commissioning and trial operation process of the equipment in \nthe early stage, or the need for the final-stage pump of the four-stage pumping system to wait for \nthe upstream pump group to deliver the material. The intermittent shutdowns that occur in the \nsubsequent operating stage are related to factors such as planned maintenance, production schedule \nadjustments, or upstream process interruptions. The existence of these non-steady-state conditions \ninterferes with the continuous monitoring of the equipment degradation trend, thereby increasing \nthe difficulty of the RUL prediction task. \nFig. 5. Wear condition of the pump casing \nFig. 6. Vibration Signals of Pump #1 during Two Life Cycles: (a) Cycle 1, (b) Cycle 2 \n\n \n \n4.1.1 Data Preprocessing \nIn this study, vibration signals are sampled by sensors at a frequency of 12800 Hz, and each \nsampling lasts for 2.56 seconds, generating a sequence with a length of 32768. To unify the time \nscale of analysis to the hour level, we extract features from the 6 continuous vibration signal \nsequences collected within each hour and calculate their average value as the feature representation \nfor that hour. \nWe extracted a wide range of candidate features from the original signal, includes mean, std, \nrms, max, min, peak-to-peak, Kurt, skewness, crest factor, shape factor, clearance factor, impulse \nfactor, peak frequency, total power, spectral centroid, spectral kurt, spectral skew, STFT magnitude, \nand power of the \ud835\udc56\ud835\udc56-th IMF component of the vibration signal. By calculating the Pearson correlation \ncoefficient between each feature and the RUL, we screened out the 15 features with the strongest \ncorrelation. This screening process is based on the data analysis of the first complete life cycle of \nslurry pump No. 1 (the correlation ranking is shown in Figure 7). \nTo train the MKDPINN, we constructed training samples with a dimension of (15, 30). The \nfirst dimension of the sample represents the average value of the 15 selected features in the current \nhour, and the second dimension contains the historical sequence data of these features in the past 30 \nhours. In the allocation of sample labels, we use the actual remaining running hours of the equipment \nuntil failure as the RUL label. The label value of each sample represents the number of hours the \nequipment can effectively operate from the current moment to the failure moment. To accurately \nreflect the actual wear accumulation, we processed special working conditions: data during \nshutdown periods were identified and removed using the root mean square value of the signal (RMS \n< 0.1), and these shutdown times were not included in the calculation of RUL; for idling conditions \n(equipment running but without load), it is considered that there is no significant wear during the \nperiod, so its RUL label value remains the same as the previous effective running moment, that is, \nthe RUL does not decrease with the passage of idling time. \n4.1.2 Model Parameter Configuration \nIn this experiment, the model input dimension is set to 30\u00d715, corresponding to the time step \nand the number of selected sensor features, respectively. We used two Attention-based Feature \nExtractor modules and applied a dropout rate of 0.1 to prevent overfitting. For the inner and outer \nloop optimization of meta-learning, the Adam optimizer is used for the inner loop, the learning rate \n(\u03b1) is set to 0.001, the number of update steps \ud835\udc58\ud835\udc58 is 8, and the batch size is 64. The outer loop (meta-\nFig. 7. Visualization of correlation analysis results \n\n \n \nupdate) is based on a meta-batch (\ud835\udc35\ud835\udc35) containing 5 meta-tasks, and the outer loop learning rate \ud835\udf02\ud835\udf02 is \nset to 0.1. \nThe model is trained for a total of 50 epochs, of which 10% of the training data is divided into \na validation set. During the training process, we save the optimal model weights according to the \ndecrease in the validation set loss. The final model performance evaluation is performed in the 15-\nshot adaptation scenario. \nTo comprehensively evaluate the generalization ability of the model, we designed two test tasks: \n1.  Task 1 (Cross-Life Cycle RUL Prediction): The source domain data comes from the first \nlife cycle of pump No. 1, and the target domain data is the second life cycle of the pump. \n2.  Task 2 (Cross-Machine Few-Shot RUL Prediction): This task represents a more \nchallenging scenario and evaluates the model's RUL prediction performance for completely new \nequipment. The source domain data also comes from the first life cycle of pump No. 1, but the target \ndomain data is taken from pump No. 2. \nTo comprehensively evaluate the performance of the proposed MKDPINN model and verify \nthe effectiveness of its key components, we established three baseline models for comparative \nanalysis. These models also form the basis of ablation experiments to measure the contribution of \neach module to the overall performance: \n1.  Base Learner: Serves as the basic model and adopts the same core network architecture \nand parameter configuration as MKDPINN, but does not integrate the PINN framework or the meta-\nlearning strategy. Its performance represents the baseline level of relying solely on data-driven \nlearning. \n2.  KDPINN: This model introduces the knowledge discovery-based PINN framework on the \nbasis of the Base Learner. By comparing it with the Base Learner, the effect of PINN constraints on \nimproving prediction accuracy can be evaluated. \n3.  Meta Learner: This model applies our proposed meta-learning framework on the Base \nLearner architecture, but omits the PINN part. Comparing it with the Base Learner and MKDPINN \ncan reveal the contribution of the meta-learning strategy in improving the model's adaptability and \ngeneralization ability. \nWe use RMSE, MAE, R2 as the evaluation criteria of the model performance, which are defined \nas follows: \n \n\ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc45= \u0da8\u2211\n\u0d6b\ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc3f\ud835\udc3f\ud835\udc56\ud835\udc56\u2212\ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc45\n\u0de3\ud835\udc56\ud835\udc56\u0d6f\n2\n\ud835\udc5b\ud835\udc5b\n\ud835\udc56\ud835\udc56=1\n\ud835\udc5b\ud835\udc5b\n\u0d6b38\u0d6f \n\ud835\udc45\ud835\udc452 = 1 \u2212\n\u2211\n\u0d6b\ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc3f\ud835\udc3f\ud835\udc56\ud835\udc56\u2212\ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc45\n\u0de3\ud835\udc56\ud835\udc56\u0d6f\n2\n\ud835\udc5b\ud835\udc5b\n\ud835\udc56\ud835\udc56=1\n\u2211\n\u1240\ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc3f\ud835\udc3f\ud835\udc56\ud835\udc56\u22121\n\ud835\udc5b\ud835\udc5b\u2211\n\ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc3f\ud835\udc3f\ud835\udc56\ud835\udc56\n\ud835\udc5b\ud835\udc5b\n\ud835\udc56\ud835\udc56=1\n\u1241\n2\n\ud835\udc5b\ud835\udc5b\n\ud835\udc56\ud835\udc56=1\n\u0d6b39\u0d6f \n\ud835\udc40\ud835\udc40\ud835\udc40\ud835\udc40\ud835\udc40\ud835\udc40= 1\n\ud835\udc5b\ud835\udc5b\u0dcd\u0e2b\ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc3f\ud835\udc3f\ud835\udc56\ud835\udc56\u2212\ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc45\n\u0de3\ud835\udc56\ud835\udc56\u0e2b\n\ud835\udc5b\ud835\udc5b\n\ud835\udc56\ud835\udc56=1\n\u0d6b40\u0d6f \n4.1.3 Experimental Results and Discussion \nFigure 8 shows the prediction results of each model in Task 1, and Table 1 shows the \nperformance comparison of different models. The results indicate that: (a) The Base Learner model \nperformed poorly, with considerable deviations between its predicted RUL and the actual RUL, \n\n \n \nespecially with severe error accumulation in the later stages of equipment operation, and with \nsubstantial fluctuations in the prediction results. (b) Although the KDPINN model still had generally \nlarge prediction errors, thanks to the physical constraints introduced by the PINN framework, its \npredicted RUL showed a stronger monotonic decreasing trend compared to the Base Learner, \ndemonstrating better prediction stability and better adherence to physical degradation laws, even if  \nits performance was still insufficient. (c) The Meta Learner, by introducing the meta-learning \nframework, exhibited a clear advantage in adapting to target domain data, with a marked \nimprovement in prediction accuracy compared to the previous two models and a large reduction in \nthe magnitude of errors. (d) The proposed MKDPINN model combines the rapid adaptation \ncapability of meta-learning and the physical constraints of PINN, and its prediction results are the \nmost consistent with the actual RUL. Not only are the prediction results smooth and have the \nsmallest errors, but they also maintain a good downward trend, demonstrating its superior \nperformance and robustness in cross-life cycle RUL prediction tasks. \nFigure 9 visually shows the distribution of key performance indicators (RMSE, MAE, R\u00b2) for \nthe four models after 10 independent repeated experiments through violin plots. It can be clearly \nobserved from the figure that the proposed MKDPINN method has a considerable performance \nadvantage in the cross-life cycle RUL prediction task compared to the baseline model and ablation \nmodels. Not only is the average prediction accuracy higher, but it also exhibits enhanced robustness \nand consistency in multiple experiments. \nModel \nRMSE \nMAE \nR2 \nBase Learner \n149.39 \n124.45 \n0.34 \nKDPINN \n171.33 \n151.81 \n0.13 \nMeta Learner \n43.60 \n34.27 \n0.94 \nMKDPINN \n27.19 \n21.30 \n0.97 \n(a)\n(b)\n(c)\n(d)\nFig. 8. Task 1 RUL Prediction Results: (a) Base Learner, (b)KDPINN, (c) Meta Learner, (d) MKDPINN. \nTable 1 Performance Comparison of Models on Task 1 \n\n \n \nFigure 10 shows the wear condition of the pump casing of pump No. 2 used for Task 2. Its \nwear characteristics are manifested as local damage. A clear penetrating hole can be seen in the left \nimage (outer wall perspective). The right image (inner wall perspective) reveals a larger range of \nsurface damage inside the pump casing, showing typical erosion-abrasion features, including a large \narea of unevenness, pitting, and groove-like wear marks along the flow direction, reflecting the \ncontinuous erosion effect of the high-speed flow of the slurry on the material. Figure 11 presents \nthe vibration signal waveform of pump No. 2 throughout its entire life cycle. Similar to the signal \nof pump No. 1 shown in Figure 6, the signal of pump No. 2 also clearly demonstrates the \ncharacteristics of discontinuous operation in a real industrial environment, including a low-\namplitude idling period in the initial stage (approximately the first 110 hours) and high-amplitude \nvibrations in the subsequent working stage. However, compared to pump No. 1, the vibration signal \nof pump No. 2 shows more frequent and pronounced intermittent shutdown or idling periods after \nabout 110 hours. This difference in operating mode, i.e., more frequent starts and stops or load \nfluctuations, indicates that pump No. 2 experienced different operational scheduling or process \ndisturbances compared to pump No. 1 in actual working conditions, which also brings additional \nFig. 9. Task 1: RUL Prediction Metric Comparison \nFig. 10. Wear condition of Pump 2 \nFig. 11. Vibration Signals of Pump #2. \n\n \n \nchallenges to cross-machine RUL prediction. \nFigure 12 presents the prediction results of the four models in Task 2; Table 2 shows the \nPerformance Comparison of different models. It can be seen that the proposed MKDPINN model \nperforms best in this more challenging cross-machine task. Its prediction results are closest to the \nactual RUL, and the prediction errors are the smallest and most stable. This highlights the important \nrole of combining the rapid adaptation capability of meta-learning and the physical constraints of \nPINN in improving the RUL prediction accuracy and robustness of the model under few-shot, cross-\nmachine conditions. \nTable 2 Performance Comparison of Models on Task 2 \nModel \nRMSE \nMAE \nR2 \nBase Learner \n126.58 \n105.34 \n0.36 \nKDPINN \n140.39 \n112.61 \n0.21 \nMeta Learner \n44.11 \n33.88 \n0.91 \nMKDPINN \n25.62 \n20.04 \n0.97 \n \nFigure 13 visually shows the statistical distribution of key performance indicators (RMSE, \nMAE, R\u00b2) of each model after 10 independent experiments in Task 2 using violin plots. \nComprehensively, the statistical results of Figure 13 strongly prove that even in cross-machine Few-\nShot prediction tasks with data scarcity and obvious domain shift, the proposed MKDPINN method \ncan still maintain high accuracy and high stability, superior to the baseline and ablation models. \nFigure 14 explores the influence of the source of adaptation samples on the prediction \nperformance of the MKDPINN model in the 15-shot cross-machine prediction scenario (Task 2), \nespecially the performance of the model when it can only obtain health status information of the \n(a)\n(b)\n(c)\n(d)\nFig. 12. Task 2 RUL Prediction Results: (a) Base Learner, (b) KDPINN, (c) Meta Learner, (d) MKDPINN. \n\n \n \ntarget equipment (pump No. 2). As shown in Figure 14(a), when the 15 adaptation samples are \ncompletely selected from the initial operation stage of the equipment and do not contain any clear \ndegradation characteristics, the MKDPINN can still capture the basic trend of the RUL generally \ndecreasing over time, rather than outputting invalid predictions. This prediction ability under zero-\ndegradation sample adaptation highlights the intrinsic robustness of the model, which is mainly due \nto the general degradation prior knowledge and rapid adaptation ability given to the model by the \nPINN and meta-learning frameworks, and the early subtle state changes that the feature extractor \nmay capture, which together enable the model to perform meaningful degradation trend \nextrapolation even when information is extremely scarce. Figure 15 shows the performance of \nMKDPINN for RUL prediction in 15-shot scenario using samples from different degradation ranges; \nTable 3 shows the performance metrics of MKDPINN for 15-shot RUL prediction with samples \nFig. 13. Task 2: RUL Prediction Metric Comparison \n(a)\n(b)\n(c)\n(d)\n(e)\n(f)\nFig. 14. RUL Prediction Results of MKDPINN in 15-shot Scenario Using Samples from Different Degradation \nStages. (a) Using samples from normal operation (no degradation samples); (b) Using samples randomly selected \nfrom the first 20% of degradation samples; (c) Using samples randomly selected from the first 40% of degradation \nsamples; (d) Using samples randomly selected from the first 60% of degradation samples; (e) Using samples \nrandomly selected from the first 80% of degradation samples; (f) Using samples randomly selected from the entire \n(100%) degradation sample set.  \n\n \n \nfrom different degradation stages. It can be seen that as the adaptation samples gradually include \ndata from the later degradation stages, the prediction performance of the model is considerably \nimproved, and the error is greatly reduced. \nThis finding proves the industrial application value of the MKDPINN model. For equipment \nthat is newly put into operation or lacks historical failure data, the model can overcome the data \naccumulation period required by traditional methods and provide preliminary, trend-based RUL \npredictions based solely on initial healthy operating data, thereby supporting the early deployment \nof predictive maintenance strategies. This greatly reduces the reliance on complete \"run-to-failure\" \ndatasets and reduces the cost and time constraints of data acquisition, especially suitable for \nscenarios where full life-cycle data cannot be obtained due to planned maintenance. More \nimportantly, even if the accuracy of early predictions needs to be corrected by subsequent data, the \nRUL decline trend it provides itself constitutes an important early warning signal, which helps to \npay attention to potential risks and adjust maintenance plans in a timely manner, reflecting the \npracticality and forward-looking nature of the method in data-limited real industrial environments. \nAlthough, as shown in Figures 14(b) to (f), incorporating degradation samples can considerably \nimprove prediction accuracy, the basic prediction ability of the model under extreme information \nloss conditions has fully proven its superiority. \n \nTable 3 Performance Metrics of MKDPINN for 15-Shot RUL Prediction with Samples from Different \nDegradation Stages \nDegradation Stages \nRMSE \nMAE \nR2 \nNo Degradation  \n66.27 \n59.38 \n0.82 \nFirst 20% of Degradation \n53.18 \n42.87 \n0.88 \nFirst 40% of Degradation \n44.14 \n37.33 \n0.92 \nFirst 60% of Degradation \n38.47 \n28.43 \n0.94 \nFirst 80% of Degradation \n25.33 \n16.95 \n0.97 \n100% of Degradation \n22.80 \n18.66 \n0.97 \n \nWe also evaluated the effect of the number of adaptation samples (i.e., \"shot size\") on the \nperformance of the proposed MKDPINN model in the most challenging cross-machine RUL \nFig. 15. Performance of MKDPINN for RUL Prediction in 15-shot Scenario Using Samples from \nDifferent Degradation Ranges \n\n \n \nprediction task (Task 2). Figure 16 shows the RUL prediction results of MKDPINN when adapting \nthe model using different shot sizes (5, 10, 15, 20 respectively). From Figures (a) to (d), it can be \nvisually observed that as the number of samples used for adaptation increases, the predicted RUL \nbecomes closer and closer to the actual RUL, and the magnitude of the prediction error also decreas \nes, indicating that the prediction accuracy has a marked improvement. \nFigure 17 plots the curves of key performance indicators (RMSE, MAE, and R\u00b2) versus shot \nsize. This figure clearly reveals the quantified improvement in performance: as the shot size \nincreases from 5 to 20, RMSE and MAE show a downward trend, while R\u00b2 rises and approaches 1. \nThis quantitatively confirms that increasing the number of adaptation samples can effectively \nimprove the prediction accuracy and stability of the model. \nTable 4 specifically lists the detailed performance indicator values under different shot sizes. \nThe data shows that when the shot size is 5, the model's RMSE is 43.35, MAE is 36.50, and R\u00b2 is \n(a)\n(b)\n(c)\n(d)\nFig. 16. RUL Prediction Results of MKDPINN for Cross-Machine RUL Prediction Tasks with \nDifferent Shot Sizes. (a) 5 shot; (b) 10 shot; (c) 15 shot; (d) 20 shot. \nFig. 17. Performance Metrics of MKDPINN for Cross-Machine RUL Prediction Tasks with \nDifferent Shot Sizes \n\n \n \n0.92. As the shot size increases, all indicators are continuously optimized. When the shot size \nreaches 20, the model achieves the best performance, RMSE drops to 19.05, MAE drops to 16.35, \nand R\u00b2 is as high as 0.98. Combining the results of Figure 16, Figure 17, and Table 4, it is proven \nthat only a small amount of labeled samples from the target equipment needs to be increased to \nconsiderably improve the accuracy and robustness of the MKDPINN model in the cross-machine \nFew-Shot RUL prediction task. \nTable 4 RUL Prediction Performance of MKDPINN Under Different Test Scenarios \nTest Scenario \nRMSE \nMAE \nR2 \n5-Shot \n43.35 \n36.50 \n0.92 \n10-Shot \n33.81 \n28.94 \n0.95 \n15-Shot \n22.80 \n18.66 \n0.97 \n20-Shot \n19.05 \n16.35 \n0.98 \n4.2 Case Study 2: CMAPSS Dataset \n4.2.1 Dataset Description \nThe Commercial Modular Aero-Propulsion System Simulation (C-MAPSS) dataset [51] is a \nbenchmark dataset widely used in the PHM field. This dataset contains multiple sets of run-to-\nfailure data of turbofan engines generated by C-MAPSS software simulation. Each set of data \ncontains multivariate time series records of multiple engine units, including 21 sensor readings and \n3 operating setting values, reflecting the performance degradation process of the engine under \ndifferent flight conditions and fault modes. The C-MAPSS dataset is usually divided into four \nsubsets (FD001, FD002, FD003, FD004). The details of this dataset are shown in Table 5, and the \nDetailed description of sensors in the C-MAPSS dataset is shown in Table 6. \nTable 5 Description of the CMAPSS dataset \nSubset \nOperational Condition \nFault Mode \nTraining \nSet Units \nTest Set \nUnits \nFD001 \nSingle operating condition \nSingle fault \nmode \n100 \n100 \nFD002 \n6 operating conditions \nSingle fault \nmode \n260 \n259 \nFD003 \nSingle operating condition \n2 fault modes \n100 \n100 \nFD004 \n6 operating conditions \n2 fault modes \n249 \n248 \n \nTable 6 Detailed description of sensors in the C-MAPSS dataset \nNumber \nSymbol \nDescription \nUnits \nS1 \nT2 \nTemperature at the inlet of the fan \n\u00b0R \nS2 \nT24 \nTemperature at the LPC  \n\u00b0R \nS3 \nT30 \nTemperature at the HPC  \n\u00b0R \nS4 \nT50 \nTemperature at the LPT  \n\u00b0R \nS5 \nP2 \nFan inlet pressure \npsia \nS6 \nP15 \nPressure within the bypass duct \npsia \nS7 \nP30 \nPressure at the HPC outlet \npsia \nS8 \nNf \nFan's physical rotational speed \nrpm \nS9 \nNc \nCore's physical rotational speed \nrpm \nS10 \nepr \nEngine pressure  \n\u2013 \nS11 \nPs30 \nStatic pressure at HPC outlet \npsia \nS12 \nphi \nFuel flow to Ps30 ratio \npps/psi \n\n \n \nS13 \nNRf \nCorrected rotational speed of fan \nrpm \nS14 \nNRc \nCorrected rotational speed of core \nrpm \nS15 \nBPR \nRatio of bypass air to core air \n\u2013 \nS16 \nfarB \nFuel-to-air ratio in the burner \n\u2013 \nS17 \nhtBleed \nEnthalpy of the bleed air \n\u2013 \nS18 \nNf_dmd \nDesired fan rotational speed \nrpm \nS19 \nPCNfR_d \nDesired corrected fan rotational \nspeed \nrpm \nS20 \nW31 \nCoolant bleed from the HPT \nlbm/s \nS21 \nW32 \nCoolant bleed from the LPT \nlbm/s \n \n4.2.2 Data Preprocessing \nWhen constructing the samples for model training, we selected the following 14 sensor signals: \n' S 2', ' S 3', ' S 4', ' S 7', ' S 8', ' S 9', ' S 11', ' S 12', ' S 13', ' S 14', ' S 15', ' S 17', ' S 20', ' S 21'. \nConsidering that the C-MAPSS dataset contains a variety of different operating conditions, these \nchanges in operating conditions can have a marked influence on sensor readings, which may mask \nthe true equipment degradation trend. We adopted the Condition-Based Standardization (CS) \nmethod proposed in the literature [52] to process the selected sensor sequences. \nDifferent from the traditional Global Standardization (GS) method, the CS method first divides \nthe data into different operating condition groups according to operating parameters, and then \nperforms standardization calculations independently within each operating condition group. This \nstrategy can effectively filter out signal fluctuations caused by operating condition switching, and \nmore clearly reveal the intrinsic degradation patterns related to the equipment health status. Figure \n18 visually compares the standardized values of three representative sensors (P30, Nc, BPR) after \nbeing processed by GS (Figure 18a) and CS (Figure 18b) methods. It can be clearly seen that the \nsignal processed by CS (Figure 18b) presents smoother and more trend-oriented degradation \ncharacteristics, while in the signal processed by GS (Figure 18a), these degradation trends are \nlargely submerged by dramatic operating condition fluctuations. Therefore, the use of the CS \nmethod helps to extract purer features that can better reflect the true degradation process. \nAfter CS processing, we further applied the Exponentially Weighted Moving Average (EWMA) \nsmoothing technique to each standardized sensor time series to further suppress high-frequency \nnoise and random fluctuations in the signal, thereby more clearly highlighting the long-term \ndegradation trend that reflects the slow changes in the equipment health status. EWMA achieves \n(a)\n(b)\nFig. 18. Standardized Values of P30, Nc, and BPR Sensors: (a) Global Standardization, (b) Condition-Based \nStandardization \n\n \n \nsmoothing by assigning exponentially decreasing weights to historical data points. For a given \nsensor sequence \ud835\udc60\ud835\udc60\u2008 = \u2008[\ud835\udc60\ud835\udc601, \ud835\udc60\ud835\udc602, \u2026 , \ud835\udc60\ud835\udc60\ud835\udc5b\ud835\udc5b], the formula for calculating EWMA is as follows: \n\ud835\udc60\ud835\udc60\ud835\udc61\ud835\udc61\n\u2032 = \ud835\udf0c\ud835\udf0c\u2217\ud835\udc60\ud835\udc60\ud835\udc61\ud835\udc61+ (1 \u2212\ud835\udf0c\ud835\udf0c) \u2217\ud835\udc60\ud835\udc60\ud835\udc61\ud835\udc61\u22121\n\u2032\n\u0d6b41\u0d6f \nWhere, \ud835\udc60\ud835\udc60\ud835\udc61\ud835\udc61\u2032  is the smoothed value at time point t, \ud835\udc60\ud835\udc60\ud835\udc61\ud835\udc61 is the original sensor reading after CS at \ntime point t, and \ud835\udc60\ud835\udc60\ud835\udc61\ud835\udc61\u22121\u2032 is the smoothed value at the previous time point. \ud835\udf0c\ud835\udf0c is the smoothing factor \n(0\u2008 < \u2008\ud835\udf0c\ud835\udf0c\u22641). \nFinally, by applying the Sliding Window Method and setting the window length (i.e., the time \nstep) to 15, a fixed-size window of 15 is used to slide along the time series data of each engine unit \n(including 14 selected sensors) with a stride of 1. At each sliding position, the 14 sensor readings \non the 15 consecutive time points covered by the window are extracted to form a two-dimensional \nmatrix sample with a shape of (15, 14). Where, the first dimension (15) represents the time step or \nsequence length, and the second dimension (14) represents the number of features (sensors) at each \ntime step. \nWe followed the practice commonly used in the field to set the RUL labels. Specifically, we \ncapped the actual RUL values: any RUL value exceeding 125 operating cycles was set to 125. This \ntreatment helps the model focus on the degradation region of the equipment and maintains \nconsistency with numerous comparative studies using C-MAPSS. \nRegarding model evaluation, this experiment adopted a 0-shot testing scenario. This is because \nthe C-MAPSS dataset itself provides clearly divided training and test sets, where the test set contains \nengine data that runs to an unknown time point and then terminates. According to the standard \nevaluation process of this dataset, we first train the MKDPINN model on the complete training set, \nand then directly apply the trained model to the test set for RUL prediction without using any \nsamples from the test set (target domain) for any form of model fine-tuning or adaptation. Since the \nmodel has not been exposed to any target domain samples before evaluation, this process fully meets \nthe definition of 0-shot learning. Choosing this standard 0-shot evaluation method also ensures that \nour results can be compared fairly and directly with a large number of existing studies on this dataset. \nIn addition to RMSE, this dataset also uses SCORE as an evaluation criterion, which is defined \nas follows: \n\ud835\udc46\ud835\udc46\ud835\udc46\ud835\udc46\ud835\udc46\ud835\udc46\ud835\udc46\ud835\udc46\ud835\udc46\ud835\udc46=\n\u23a9\n\u23aa\n\u23a8\n\u23aa\n\u23a7\u0dcd(\ud835\udc52\ud835\udc52\u2212\ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc45\n\u0de3\ud835\udc56\ud835\udc56\u2212\ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc56\ud835\udc56\n13\n\u22121), \ud835\udc56\ud835\udc56\ud835\udc56\ud835\udc56  \ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc45\n\u0de3\ud835\udc56\ud835\udc56< \ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc56\ud835\udc56 \n\ud835\udc5b\ud835\udc5b\n\ud835\udc56\ud835\udc56=1\n\u0dcd(\ud835\udc52\ud835\udc52\n\ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc45\n\u0de3\ud835\udc56\ud835\udc56\u2212\ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc56\ud835\udc56\n10\n\u22121), \ud835\udc56\ud835\udc56\ud835\udc56\ud835\udc56  \ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc45\n\u0de3\ud835\udc56\ud835\udc56\u2265\ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc45\ud835\udc56\ud835\udc56 \n\ud835\udc5b\ud835\udc5b\n\ud835\udc56\ud835\udc56=1\n\u0d6b42\u0d6f \n4.2.3 Experimental Results and Analysis \nFigure 19 presents the overall results of the proposed MKDPINN model for RUL prediction \non the four subsets (FD001, FD002, FD003, FD004) of the C-MAPSS dataset. This figure shows \nthe comparison between the RUL prediction value and the corresponding actual RUL for all test \nengine units in each subset at the last time point of their available operating data. It can be seen from \nthe figure that on all four subsets, the prediction values of the MKDPINN model show a high degree \nof consistency with the actual values. The prediction errors of most engines are small, and fluctuate \naround the zero value. This initially indicates that the model can achieve accurate prediction of the \nfinal RUL of the engine under different operating condition complexities and fault mode \ncombinations, and has good generalization ability and robustness. \nFigure 20 further shows the RUL prediction results of the MKDPINN model on each subset of \n\n \n \nC-MAPSS. By randomly selecting a representative engine unit from each subset, the RUL \nprediction results for its entire operating cycle are plotted. Observation reveals that for these selected \nrepresentative samples, the predicted RUL of the MKDPINN model can effectively present a \ndownward trend. Especially after the engine enters the degradation stage, the predicted value and \nthe true value are very close. Although there may be some fluctuations in the early stages or when \noperating conditions change, the model has successfully captured the key degradation dynamics on \nthe whole, and can provide a relatively precise RUL estimate near the end of life, further verifying \nthe model's adaptability on different datasets. \nTable 7 compares the RUL prediction performance of the proposed MKDPINN model with a \nseries of recently published advanced models on the four subsets of the C-MAPSS dataset. The \nresults clearly show that the MKDPINN model exhibits excellent comprehensive performance. In \nterms of average performance, MKDPINN achieved the lowest average RMSE (12.71) and the \nlowest average SCORE (622.15) among all the compared models, which highlights its overall \n(a)\n(b)\n(c)\n(d)\nFig. 19. MKDPINN RUL Prediction Results on C-MAPSS Dataset: (a) FD001, (b) FD002, (c) FD003, (d) FD004 \n\n \n \nprediction accuracy and robustness under different complex working conditions and fault mode \ncombinations. Among them, MKDPINN obtained the lowest RMSE values (13.85, 11.01, 13.60 \nrespectively) on the three subsets of FD002, FD003, and FD004, and also obtained the lowest \nSCORE value (956.42) on the FD004 subset. This quantitative comparison result demonstrates the \neffectiveness and reliability of the MKDPINN model in RUL prediction tasks. \n \nTable 7 Performance Comparison of MKDPINN with Recently Published Models on C-MAPSS \nDataset \nModel \nFD001 \nFD002 \nFD003 \nFD004 \nAverage \nRMSE \nSCORE \nRMSE \nSCORE \nRMSE \nSCORE \nRMSE \nSCORE \nRMSE \nSCORE \nRVE[52] \n(2022) \n13.42 \n323.82 \n14.92 \n1379.17 \n12.51 \n256.36 \n16.37 \n1845.99 \n14.31 \n951.34 \nED-LSTM[53] \n(2023) \n9.14 \n53 \n18.17 \n1693 \n11.96 \n238 \n18.51 \n2160 \n14.45 \n1036.00 \nAttnPINN[49] \n(2023) \n16.89 \n523 \n16.32 \n1479 \n17.75 \n1194 \n18.37 \n2059 \n17.33 \n1313.75 \nMSTSDN[7] \n(2024) \n13.67 \n246.01 \n16.28 \n1342.59 \n13.66 \n258.92 \n17.33 \n1641.51 \n15.24 \n872.26 \nARR[54] \n(2024) \n11.36 \n192.22 \n18.97 \n2433.15 \n11.28 \n133.41 \n20.69 \n2842.44 \n15.58 \n1400.31 \nMeta-\nTransformer[35] \n(2024) \n12.28 \n/ \n14.49 \n/ \n12.86 \n/ \n15.90 \n/ \n13.88 \n/ \nMGCAL-\nUQ[55] \n(2025) \n11.63 \n180.61 \n17.16 \n1481.92 \n12.42 \n230.28 \n16.10 \n1278.56 \n14.33 \n792.84 \nDAM[56] \n(2025) \n13.03 \n217 \n15.41 \n796 \n12.21 \n189 \n16.43 \n1029 \n14.27 \n557.75 \nMKDPINN \n(proposed) \n12.36 \n273.19 \n13.85 \n1037.75 \n11.01 \n221.24 \n13.60 \n956.42 \n12.71 \n622.15 \nFig. 20. Engine RUL Predictions on C-MAPSS Sub-datasets \n\n \n \n \n5. Conclusion \nWe proposed MKDPINN to address two major challenges in practical industrial applications \nof rotating machinery RUL prediction in manufacturing systems: target domain sample scarcity and \nthe difficulty of effectively incorporating physical laws due to a lack of explicit dynamic equations. \nWe designed a knowledge discovery-based PINN framework, which learns the PDEs implicit in the \nequipment degradation process through the PGR and uses them to guide the training process of the \nmodel, improving the generalization ability of the model and its compliance with physical processes. \nAt the same time, we embedded the knowledge discovery-based PINN framework into a first-order \nmeta-learning strategy. Through learning on multiple meta-tasks, the meta-parameters of the model \nare aligned with the aggregated optimal parameters from each meta-task, enabling the model to \nquickly adapt to RUL prediction tasks under new equipment or new working conditions with \nextremely few target domain samples, substantially improving the model's practicality and \ndeployment efficiency. \nThrough cross-life cycle and cross-machine few-shot RUL prediction experiments on a real \nindustrial slurry pump dataset, and a comprehensive evaluation on the standard C-MAPSS \nbenchmark dataset, the effectiveness of the MKDPINN method has been fully verified. The \nexperimental results show that: 1. MKDPINN is substantially superior to baseline models that rely \nonly on data-driven, only contain physics-informed information, or only use meta-learning in few-\nshot scenarios. It achieves the best performance on indicators such as RMSE, MAE, and R\u00b2, \ndemonstrating the synergistic advantage of combining physical knowledge discovery and meta-\nlearning. 2. The method exhibits strong robustness when facing domain shift (such as cross-machine \nprediction). Even when only using the health status data of the target equipment for adaptation, it \ncan provide meaningful RUL predictions, and the accuracy steadily improves as the adaptation \nsamples cover more degradation stages or increase in number. 3. In the evaluation of the C-MAPSS \ndataset, MKDPINN achieved competitive results, exceeding the performance of existing advanced \nmethods on multiple subsets, further confirming its applicability and accuracy. \nIn future research, we plan to explore how to extract more explicit and interpretable physical \nparameters or structures from the PDE learned by the PGR, and even combine prior domain \nknowledge to guide the PGR to learn specific forms of (partially known) physical equations, to \nachieve a deeper level of physics-informed information fusion. \nAcknowledgements  \nThe work is supported by the National Natural Science Foundation of China (No. 51975100)  \nand the Key Research and Development Program of Dalian (2024YF16PT026). \nDeclarations  \nThe authors declare that they have no financial interests or personal relationships that could \nhave appeared to influence the work reported in this paper. \n \n \n\n \n \n \n \nReference: \n[1] P. Ma, G. Li, H. Zhang, C. Wang, and X. Li, \"Prediction of Remaining Useful Life of \nRolling Bearings Based on Multiscale Efficient Channel Attention CNN and Bidirectional GRU,\" \nIEEE Transactions on Instrumentation and Measurement, vol. 73, pp. 1-13, 2024, doi: \n10.1109/tim.2023.3347787. \n[2] Z. Zhou et al., \"Time-varying trajectory modeling via dynamic governing network for \nremaining \nuseful \nlife \nprediction,\" \nMech \nSyst \nSignal \nPr, \nvol. \n182, \n2023, \ndoi: \n10.1016/j.ymssp.2022.109610. \n[3] X. Liu et al., \"Optimized Online Remaining Useful Life Prediction for Nuclear \nCirculating Water Pump Considering Time-Varying Degradation Mechanism,\" IEEE Transactions \non Industrial Informatics, vol. 20, no. 9, pp. 11057-11068, 2024, doi: 10.1109/tii.2024.3399878. \n[4] D. Li, J. Chen, R. Huang, Z. Chen, and W. Li, \"Sensor-aware CapsNet: Towards \ntrustworthy multisensory fusion for remaining useful life prediction,\" 2025, vol. 72, pp. 26-37, 2024, \ndoi: 10.1016/j.jmsy.2023.11.009. \n[5] Q. Liu et al., \"A method for remaining useful life prediction of milling cutter using multi-\nscale spatial data feature visualization and domain separation prediction network,\" Mech Syst Signal \nPr, vol. 225, 2025, doi: 10.1016/j.ymssp.2024.112251. \n[6] W. Cheng et al., \"Interactive Hybrid Model for Remaining Useful Life Prediction With \nUncertainty Quantification of Bearing in Nuclear Circulating Water Pump,\" IEEE Transactions on \nIndustrial Informatics, vol. 20, no. 2, pp. 2154-2166, 2024, doi: 10.1109/tii.2023.3288225. \n[7] Z. Liu, X. Zheng, A. Xue, and M. Ge, \"Multi-Scale Temporal-Spatial Feature-Based \nHybrid Deep Neural Network for Remaining Useful Life Prediction of Aero-Engine,\" ACS Omega, \nvol. 9, no. 48, pp. 47410-47427, Dec 3 2024, doi: 10.1021/acsomega.4c03873. \n[8] J. Shi, J. Zhong, Y. Zhang, B. Xiao, L. Xiao, and Y. Zheng, \"A dual attention LSTM \nlightweight model based on exponential smoothing for remaining useful life prediction,\" Reliability \nEngineering & System Safety, vol. 243, 2024, doi: 10.1016/j.ress.2023.109821. \n[9] J. Zhong, H. Jiang, K. Gu, J. Zhong, and S. Zhong, \"Remaining useful life prediction of \nrolling bearing based on multi-region hypergraph self-attention network,\" Mech Syst Signal Pr, vol. \n225, 2025, doi: 10.1016/j.ymssp.2025.112331. \n[10] C. Wu, J. He, L. Wang, C. Ma, and Y. Liu, \"A dual-path architecture based on time series \ndecomposition and degradation correction for remaining useful life prediction of aero-engine,\" \nComputers & Industrial Engineering, vol. 203, 2025, doi: 10.1016/j.cie.2025.110964. \n[11] X. Pei, L. Gao, and X. Li, \"Remaining useful life prediction of machinery based on \nperformance evaluation and online cross-domain health indicator under unknown working \nconditions,\" Journal of Manufacturing Systems, vol. 75, pp. 213-227, 2024, doi: \n10.1016/j.jmsy.2024.06.005. \n[12] P. Ding, M. Jia, Y. Ding, Y. Cao, J. Zhuang, and X. Zhao, \"Machinery Probabilistic Few-\nShot Prognostics Considering Prediction Uncertainty,\" IEEE/ASME Transactions on Mechatronics, \nvol. 29, no. 1, pp. 106-118, 2024, doi: 10.1109/tmech.2023.3270901. \n[13] J.-Y. Wu, M. Wu, Z. Chen, X. Li, and R. Yan, \"A joint classification-regression method \nfor multi-stage remaining useful life prediction,\" Journal of Manufacturing Systems, vol. 58, pp. \n109-119, 2021, doi: 10.1016/j.jmsy.2020.11.016. \n\n \n \n[14] Y. Mo, L. Li, B. Huang, and X. Li, \"Few-shot RUL estimation based on model-agnostic \nmeta-learning,\" Journal of Intelligent Manufacturing, 2022, doi: 10.1007/s10845-022-01929-w. \n[15] Y. Wang, S. Liu, S. Lv, and G. Liu, \"Few-Shot Probabilistic RUL Prediction With \nUncertainty Quantification of Slurry Pumps,\" IEEE Sensors Journal, vol. 25, no. 4, pp. 6122-6132, \n2025, doi: 10.1109/jsen.2024.3523335. \n[16] C. He, H. Shi, X. Liu, and J. Li, \"Interpretable physics-informed domain adaptation \nparadigm for cross-machine transfer diagnosis,\" Knowledge-Based Systems, vol. 288, 2024, doi: \n10.1016/j.knosys.2024.111499. \n[17] J. Chen, R. Huang, Z. Chen, W. Mao, and W. Li, \"Transfer learning algorithms for bearing \nremaining useful life prediction: A comprehensive review from an industrial application \nperspective,\" Mech Syst Signal Pr, vol. 193, 2023, doi: 10.1016/j.ymssp.2023.110239. \n[18] Y. Feng, J. Chen, J. Xie, T. Zhang, H. Lv, and T. Pan, \"Meta-learning as a promising \napproach for few-shot cross-domain fault diagnosis: Algorithms, applications, and prospects,\" \nKnowledge-Based Systems, vol. 235, 2022, doi: 10.1016/j.knosys.2021.107646. \n[19] X. Lu, X. Yao, Q. Jiang, Y. Shen, F. Xu, and Q. Zhu, \"Remaining useful life prediction \nmodel of cross-domain rolling bearing via dynamic hybrid domain adaptation and attention \ncontrastive learning,\" Computers in Industry, vol. 164, 2025, doi: 10.1016/j.compind.2024.104172. \n[20] J. Lin et al., \"Cross-domain fault diagnosis of bearing using improved semi-supervised \nmeta-learning towards interference of out-of-distribution samples,\" Knowledge-Based Systems, vol. \n252, 2022, doi: 10.1016/j.knosys.2022.109493. \n[21] Z. He, H. Shao, P. Wang, J. Lin, J. Cheng, and Y. Yang, \"Deep transfer multi-wavelet auto-\nencoder for intelligent fault diagnosis of gearbox with few target training samples,\" Knowledge-\nBased Systems, vol. 191, 2020, doi: 10.1016/j.knosys.2019.105313. \n[22] S. Xiang, P. Li, J. Luo, and Y. Qin, \"Micro Transfer Learning Mechanism for Cross-\nDomain Equipment RUL Prediction,\" IEEE Transactions on Automation Science and Engineering, \nvol. 22, pp. 1460-1470, 2025, doi: 10.1109/tase.2024.3366288. \n[23] Z.-J. Li, D.-J. Cheng, H.-B. Zhang, K.-L. Zhou, and Y.-F. Wang, \"Multi-feature spaces \ncross adaption transfer learning-based bearings piece-wise remaining useful life prediction under \nunseen degradation data,\" Advanced Engineering Informatics, vol. 60, 2024, doi: \n10.1016/j.aei.2024.102413. \n[24] Y. Lyu, Z. Wen, and A. Chen, \"A novel transfer learning approach based on deep \ndegradation feature adaptive alignment for remaining useful life prediction with multi-condition \ndata,\" Journal of Intelligent Manufacturing, vol. 36, no. 1, pp. 619-637, 2023, doi: 10.1007/s10845-\n023-02264-4. \n[25] Y. Deng, D. Huang, S. Du, G. Li, C. Zhao, and J. Lv, \"A double-layer attention based \nadversarial network for partial transfer learning in machinery fault diagnosis,\" Computers in \nIndustry, vol. 127, 2021, doi: 10.1016/j.compind.2021.103399. \n[26] A. Vettoruzzo, M.-R. Bouguelia, J. Vanschoren, T. R\u00f6gnvaldsson, and K. J. a. p. a. Santosh, \n\"Advances and Challenges in Meta-Learning: A Technical Review,\" 2023. \n[27] T. Pan, J. Chen, and Z. Liu, \"A meta-weighted network equipped with uncertainty \nestimations for remaining useful life prediction of turbopump bearings,\" Expert Systems with \nApplications, vol. 252, 2024, doi: 10.1016/j.eswa.2024.124161. \n[28] B. M. Lake and M. Baroni, \"Human-like systematic generalization through a meta-\nlearning neural network,\" Nature, vol. 623, no. 7985, pp. 115-121, 2023, doi: 10.1038/s41586-023-\n\n \n \n06668-3. \n[29] C. Finn, P. Abbeel, and S. Levine, \"Model-agnostic meta-learning for fast adaptation of \ndeep networks,\" in International conference on machine learning, 2017: PMLR, pp. 1126-1135.  \n[30] B. Zhang, X. Liu, C. Yue, S. Y. Liang, and L. Wang, \"Meta-learning-based approach for \ntool condition monitoring in multi-condition small sample scenarios,\" Mech Syst Signal Pr, vol. 216, \n2024, doi: 10.1016/j.ymssp.2024.111444. \n[31] L. Cao, X. Wang, H. Zhang, Z. Meng, J. Li, and M. Liu, \"A Novel Cross-Scenario \nTransferable RUL Prediction Network With Multisource Domain Meta Transfer Learning for Wind \nTurbine Bearings,\" IEEE Transactions on Instrumentation and Measurement, vol. 74, pp. 1-9, 2025, \ndoi: 10.1109/tim.2025.3533626. \n[32] P. Ding, J. Xia, X. Zhao, and M. Jia, \"Graph structure few-shot prognostics for machinery \nremaining useful life prediction under variable operating conditions,\" Advanced Engineering \nInformatics, vol. 60, 2024, doi: 10.1016/j.aei.2024.102360. \n[33] A. Rai and J. Liu, \"A novel feature adaptive meta-model for efficient remaining useful \nlife prediction of lithium-ion batteries,\" Journal of Energy Storage, vol. 114, 2025, doi: \n10.1016/j.est.2025.115715. \n[34] L. Chang and Y.-H. Lin, \"Few-shot remaining useful life prediction based on Bayesian \nmeta-learning with predictive uncertainty calibration,\" Engineering Applications of Artificial \nIntelligence, vol. 142, 2025, doi: 10.1016/j.engappai.2024.109980. \n[35] Y. Wang, S. Liu, S. Lv, and G. Liu, \"Few-Shot Rotating Machinery RUL Prediction Based \non Reptile Framework and Transformer,\" presented at the 2024 Global Reliability and Prognostics \nand Health Management Conference (PHM-Beijing), 2024. \n[36] Y. He et al., \"A systematic method of remaining useful life estimation based on physics-\ninformed graph neural networks with multisensor data,\" Reliability Engineering & System Safety, \nvol. 237, 2023, doi: 10.1016/j.ress.2023.109333. \n[37] S. Zhang, Z. Liu, Y. Xu, and H. Su, \"A Physics-Informed Hybrid Multitask Learning for \nLithium-Ion Battery Full-Life Aging Estimation at Early Lifetime,\" IEEE Transactions on \nIndustrial Informatics, vol. 21, no. 1, pp. 415-424, 2025, doi: 10.1109/tii.2024.3452273. \n[38] H. Li, Z. Zhang, T. Li, and X. Si, \"A review on physics-informed data-driven remaining \nuseful life prediction: Challenges and opportunities,\" Mech Syst Signal Pr, vol. 209, 2024, doi: \n10.1016/j.ymssp.2024.111120. \n[39] F. Wang, Z. Zhai, Z. Zhao, Y. Di, and X. Chen, \"Physics-informed neural network for \nlithium-ion battery degradation stable modeling and prognosis,\" Nat Commun, vol. 15, no. 1, p. \n4332, May 21 2024, doi: 10.1038/s41467-024-48779-z. \n[40] Y. A. Yucesan and F. A. C. Viana, \"A Physics-informed Neural Network for Wind Turbine \nMain Bearing Fatigue,\" International Journal of Prognostics and Health Management, vol. 11, no. \n1, 2023, doi: 10.36001/ijphm.2020.v11i1.2594. \n[41] S. Yang, B. Tang, W. Wang, Q. Yang, and C. Hu, \"Physics-informed multi-state temporal \nfrequency network for RUL prediction of rolling bearings,\" Reliability Engineering & System Safety, \nvol. 242, 2024, doi: 10.1016/j.ress.2023.109716. \n[42] Y. Wang, M. Li, L. Zheng, M. Shi, Z. Zheng, and X. Pei, \"Phyformer: A degradation \nphysics-informed self-data driven approach to machinery prognostics,\" Advanced Engineering \nInformatics, vol. 62, 2024, doi: 10.1016/j.aei.2024.102772. \n[43] M. Herv\u00e9 de Beaulieu, M. S. Jha, H. Garnier, and F. Cerbah, \"Remaining Useful Life \n\n \n \nprediction based on physics-informed data augmentation,\" Reliability Engineering & System Safety, \nvol. 252, 2024, doi: 10.1016/j.ress.2024.110451. \n[44] J. Xiong, O. Fink, J. Zhou, and Y. Ma, \"Controlled physics-informed data generation for \ndeep learning-based remaining useful life prediction under unseen operation conditions,\" Mech Syst \nSignal Pr, vol. 197, 2023, doi: 10.1016/j.ymssp.2023.110359. \n[45] L. S\u00e1nchez, N. Costa, J. Otero, and I. Couso, \"Physics-informed learning under epistemic \nuncertainty with an application to system health modeling,\" International Journal of Approximate \nReasoning, vol. 161, 2023, doi: 10.1016/j.ijar.2023.108988. \n[46] J. Hua, Y. Li, C. Liu, P. Wan, and X. Liu, \"Physics-Informed Neural Networks With \nWeighted Losses by Uncertainty Evaluation for Accurate and Stable Prediction of Manufacturing \nSystems,\" \nIEEE \nTrans \nNeural \nNetw \nLearn \nSyst, \nvol. \nPP, \nMar \n7 \n2023, \ndoi: \n10.1109/TNNLS.2023.3247163. \n[47] C. He, H. Shi, J. Si, and J. Li, \"Physics-informed interpretable wavelet weight \ninitialization and balanced dynamic adaptive threshold for intelligent fault diagnosis of rolling \nbearings,\" \nJournal \nof \nManufacturing \nSystems, \nvol. \n70, \npp. \n579-592, \n2023, \ndoi: \n10.1016/j.jmsy.2023.08.014. \n[48] M. J. J. o. M. L. R. Raissi, \"Deep hidden physics models: Deep learning of nonlinear \npartial differential equations,\" vol. 19, no. 25, pp. 1-24, 2018. \n[49] X. Liao, S. Chen, P. Wen, and S. Zhao, \"Remaining useful life with self-attention assisted \nphysics-informed neural network,\" Advanced Engineering Informatics, vol. 58, 2023, doi: \n10.1016/j.aei.2023.102195. \n[50] A. Nichol, J. Achiam, and J. Schulman, \"On first-order meta-learning algorithms,\" arXiv \npreprint arXiv:.02999, 2018. \n[51] A. Saxena and K. Goebel, \"Turbofan engine degradation simulation data set,\" NASA ames \nprognostics data repository, vol. 18, pp. 878-887, 2008. \n[52] N. Costa and L. S\u00e1nchez, \"Variational encoding approach for interpretable assessment of \nremaining useful life estimation,\" Reliability Engineering & System Safety, vol. 222, 2022, doi: \n10.1016/j.ress.2022.108353. \n[53] Y. Zhang, C. Zhang, S. Wang, H. Dui, and R. Chen, \"Health indicators for remaining \nuseful life prediction of complex systems based on long short-term memory network and improved \nparticle \nfilter,\" \nReliability \nEngineering \n& \nSystem \nSafety, \nvol. \n241, \n2024, \ndoi: \n10.1016/j.ress.2023.109666. \n[54] G. Kim, J. G. Choi, and S. Lim, \"Using transformer and a reweighting technique to \ndevelop a remaining useful life estimation method for turbofan engines,\" Engineering Applications \nof Artificial Intelligence, vol. 133, 2024, doi: 10.1016/j.engappai.2024.108475. \n[55] S. Liu, C. Lv, F. Song, X. Liu, and D. Chen, \"Remaining useful life prediction integrating \nworking conditions and uncertainty quantification based on multilayer graph neural networks,\" \nJournal of the Brazilian Society of Mechanical Sciences and Engineering, vol. 47, no. 2, 2025, doi: \n10.1007/s40430-025-05400-8. \n[56] F. Wang, A. Liu, C. Qu, R. Xiong, and L. Chen, \"A Deep-Learning Method for Remaining \nUseful Life Prediction of Power Machinery via Dual-Attention Mechanism,\" Sensors (Basel), vol. \n25, no. 2, Jan 16 2025, doi: 10.3390/s25020497. \n \n",
    "1\nRepresentation Learning: A Review and New\nPerspectives\nYoshua Bengio\u2020, Aaron Courville, and Pascal Vincent\u2020\nDepartment of computer science and operations research, U. Montreal\n\u2020 also, Canadian Institute for Advanced Research (CIFAR)\n!\nAbstract\u2014\nThe success of machine learning algorithms generally depends on\ndata representation, and we hypothesize that this is because different\nrepresentations can entangle and hide more or less the different ex-\nplanatory factors of variation behind the data. Although speci\ufb01c domain\nknowledge can be used to help design representations, learning with\ngeneric priors can also be used, and the quest for AI is motivating\nthe design of more powerful representation-learning algorithms imple-\nmenting such priors. This paper reviews recent work in the area of\nunsupervised feature learning and deep learning, covering advances\nin probabilistic models, auto-encoders, manifold learning, and deep\nnetworks. This motivates longer-term unanswered questions about the\nappropriate objectives for learning good representations, for computing\nrepresentations (i.e., inference), and the geometrical connections be-\ntween representation learning, density estimation and manifold learning.\nIndex Terms\u2014Deep learning, representation learning, feature learning,\nunsupervised learning, Boltzmann Machine, autoencoder, neural nets\n1\nINTRODUCTION\nThe performance of machine learning methods is heavily\ndependent on the choice of data representation (or features)\non which they are applied. For that reason, much of the actual\neffort in deploying machine learning algorithms goes into the\ndesign of preprocessing pipelines and data transformations that\nresult in a representation of the data that can support effective\nmachine learning. Such feature engineering is important but\nlabor-intensive and highlights the weakness of current learning\nalgorithms: their inability to extract and organize the discrimi-\nnative information from the data. Feature engineering is a way\nto take advantage of human ingenuity and prior knowledge to\ncompensate for that weakness. In order to expand the scope\nand ease of applicability of machine learning, it would be\nhighly desirable to make learning algorithms less dependent\non feature engineering, so that novel applications could be\nconstructed faster, and more importantly, to make progress\ntowards Arti\ufb01cial Intelligence (AI). An AI must fundamentally\nunderstand the world around us, and we argue that this can\nonly be achieved if it can learn to identify and disentangle the\nunderlying explanatory factors hidden in the observed milieu\nof low-level sensory data.\nThis paper is about representation learning, i.e., learning\nrepresentations of the data that make it easier to extract useful\ninformation when building classi\ufb01ers or other predictors. In\nthe case of probabilistic models, a good representation is often\none that captures the posterior distribution of the underlying\nexplanatory factors for the observed input. A good representa-\ntion is also one that is useful as input to a supervised predictor.\nAmong the various ways of learning representations, this paper\nfocuses on deep learning methods: those that are formed by\nthe composition of multiple non-linear transformations, with\nthe goal of yielding more abstract \u2013 and ultimately more useful\n\u2013 representations. Here we survey this rapidly developing area\nwith special emphasis on recent progress. We consider some\nof the fundamental questions that have been driving research\nin this area. Speci\ufb01cally, what makes one representation better\nthan another? Given an example, how should we compute its\nrepresentation, i.e. perform feature extraction? Also, what are\nappropriate objectives for learning good representations?\n2\nWHY SHOULD WE CARE ABOUT LEARNING\nREPRESENTATIONS?\nRepresentation learning has become a \ufb01eld in itself in the\nmachine learning community, with regular workshops at the\nleading conferences such as NIPS and ICML, and a new\nconference dedicated to it, ICLR1, sometimes under the header\nof Deep Learning or Feature Learning. Although depth is an\nimportant part of the story, many other priors are interesting\nand can be conveniently captured when the problem is cast as\none of learning a representation, as discussed in the next sec-\ntion. The rapid increase in scienti\ufb01c activity on representation\nlearning has been accompanied and nourished by a remarkable\nstring of empirical successes both in academia and in industry.\nBelow, we brie\ufb02y highlight some of these high points.\nSpeech Recognition and Signal Processing\nSpeech was one of the early applications of neural networks,\nin particular convolutional (or time-delay) neural networks 2.\nThe recent revival of interest in neural networks, deep learning,\nand representation learning has had a strong impact in the\narea of speech recognition, with breakthrough results (Dahl\net al., 2010; Deng et al., 2010; Seide et al., 2011a; Mohamed\net al., 2012; Dahl et al., 2012; Hinton et al., 2012) obtained\nby several academics as well as researchers at industrial labs\nbringing these algorithms to a larger scale and into products.\nFor example, Microsoft has released in 2012 a new version\nof their MAVIS (Microsoft Audio Video Indexing Service)\n1. International Conference on Learning Representations\n2. See Bengio (1993) for a review of early work in this area.\narXiv:1206.5538v3  [cs.LG]  23 Apr 2014\n\n2\nspeech system based on deep learning (Seide et al., 2011a).\nThese authors managed to reduce the word error rate on\nfour major benchmarks by about 30% (e.g. from 27.4% to\n18.5% on RT03S) compared to state-of-the-art models based\non Gaussian mixtures for the acoustic modeling and trained on\nthe same amount of data (309 hours of speech). The relative\nimprovement in error rate obtained by Dahl et al. (2012) on a\nsmaller large-vocabulary speech recognition benchmark (Bing\nmobile business search dataset, with 40 hours of speech) is\nbetween 16% and 23%.\nRepresentation-learning algorithms have also been applied\nto music, substantially beating the state-of-the-art in poly-\nphonic transcription (Boulanger-Lewandowski et al., 2012),\nwith relative error improvement between 5% and 30% on a\nstandard benchmark of 4 datasets. Deep learning also helped\nto win MIREX (Music Information Retrieval) competitions,\ne.g. in 2011 on audio tagging (Hamel et al., 2011).\nObject Recognition\nThe beginnings of deep learning in 2006 have focused on\nthe MNIST digit image classi\ufb01cation problem (Hinton et al.,\n2006; Bengio et al., 2007), breaking the supremacy of SVMs\n(1.4% error) on this dataset3. The latest records are still held\nby deep networks: Ciresan et al. (2012) currently claims the\ntitle of state-of-the-art for the unconstrained version of the task\n(e.g., using a convolutional architecture), with 0.27% error,\nand Rifai et al. (2011c) is state-of-the-art for the knowledge-\nfree version of MNIST, with 0.81% error.\nIn the last few years, deep learning has moved from\ndigits to object recognition in natural images, and the latest\nbreakthrough has been achieved on the ImageNet dataset4\nbringing down the state-of-the-art error rate from 26.1% to\n15.3% (Krizhevsky et al., 2012).\nNatural Language Processing\nBesides speech recognition, there are many other Natural\nLanguage Processing (NLP) applications of representation\nlearning. Distributed representations for symbolic data were\nintroduced by\nHinton (1986), and \ufb01rst developed in the\ncontext of statistical language modeling by Bengio et al.\n(2003) in so-called neural net language models (Bengio,\n2008). They are all based on learning a distributed repre-\nsentation for each word, called a word embedding. Adding a\nconvolutional architecture, Collobert et al. (2011) developed\nthe SENNA system5 that shares representations across the\ntasks of language modeling, part-of-speech tagging, chunking,\nnamed entity recognition, semantic role labeling and syntactic\nparsing. SENNA approaches or surpasses the state-of-the-art\non these tasks but is simpler and much faster than traditional\npredictors. Learning word embeddings can be combined with\nlearning image representations in a way that allow to associate\ntext and images. This approach has been used successfully to\nbuild Google\u2019s image search, exploiting huge quantities of data\nto map images and queries in the same space (Weston et al.,\n3. for the knowledge-free version of the task, where no image-speci\ufb01c prior\nis used, such as image deformations or convolutions\n4. The 1000-class ImageNet benchmark, whose results are detailed here:\nhttp://www.image-net.org/challenges/LSVRC/2012/results.html\n5. downloadable from http://ml.nec-labs.com/senna/\n2010) and it has recently been extended to deeper multi-modal\nrepresentations (Srivastava and Salakhutdinov, 2012).\nThe neural net language model was also improved by\nadding recurrence to the hidden layers (Mikolov et al., 2011),\nallowing it to beat the state-of-the-art (smoothed n-gram\nmodels) not only in terms of perplexity (exponential of the\naverage negative log-likelihood of predicting the right next\nword, going down from 140 to 102) but also in terms of\nword error rate in speech recognition (since the language\nmodel is an important component of a speech recognition\nsystem), decreasing it from 17.2% (KN5 baseline) or 16.9%\n(discriminative language model) to 14.4% on the Wall Street\nJournal benchmark task. Similar models have been applied\nin statistical machine translation (Schwenk et al., 2012; Le\net al., 2013), improving perplexity and BLEU scores. Re-\ncursive auto-encoders (which generalize recurrent networks)\nhave also been used to beat the state-of-the-art in full sentence\nparaphrase detection (Socher et al., 2011a) almost doubling the\nF1 score for paraphrase detection. Representation learning can\nalso be used to perform word sense disambiguation (Bordes\net al., 2012), bringing up the accuracy from 67.8% to 70.2%\non the subset of Senseval-3 where the system could be applied\n(with subject-verb-object sentences). Finally, it has also been\nsuccessfully used to surpass the state-of-the-art in sentiment\nanalysis (Glorot et al., 2011b; Socher et al., 2011b).\nMulti-Task and Transfer Learning, Domain Adaptation\nTransfer learning is the ability of a learning algorithm to\nexploit commonalities between different learning tasks in order\nto share statistical strength, and transfer knowledge across\ntasks. As discussed below, we hypothesize that representation\nlearning algorithms have an advantage for such tasks because\nthey learn representations that capture underlying factors, a\nsubset of which may be relevant for each particular task, as\nillustrated in Figure 1. This hypothesis seems con\ufb01rmed by a\nnumber of empirical results showing the strengths of repre-\nsentation learning algorithms in transfer learning scenarios.\noutput y1 \noutput y3 \noutput y2 \nTask%A%\nTask%B%\nTask%C%\noutput%\n%input%\n%shared%\nsubsets%of%\nfactors%\nFig. 1.\nIllustration of representation-learning discovering ex-\nplanatory factors (middle hidden layer, in red), some explaining\nthe input (semi-supervised setting), and some explaining target\nfor each task. Because these subsets overlap, sharing of statis-\ntical strength helps generalization.\n.\nMost impressive are the two transfer learning challenges\nheld in 2011 and won by representation learning algorithms.\nFirst, the Transfer Learning Challenge, presented at an ICML\n2011 workshop of the same name, was won using unsuper-\nvised layer-wise pre-training (Bengio, 2011; Mesnil et al.,\n2011). A second Transfer Learning Challenge was held the\n\n3\nsame year and won by Goodfellow et al. (2011). Results\nwere presented at NIPS 2011\u2019s Challenges in Learning Hier-\narchical Models Workshop. In the related domain adaptation\nsetup, the target remains the same but the input distribution\nchanges (Glorot et al., 2011b; Chen et al., 2012). In the\nmulti-task learning setup, representation learning has also been\nfound advantageous Krizhevsky et al. (2012); Collobert et al.\n(2011), because of shared factors across tasks.\n3\nWHAT MAKES A REPRESENTATION GOOD?\n3.1\nPriors for Representation Learning in AI\nIn Bengio and LeCun (2007), one of us introduced the\nnotion of AI-tasks, which are challenging for current machine\nlearning algorithms, and involve complex but highly structured\ndependencies. One reason why explicitly dealing with repre-\nsentations is interesting is because they can be convenient to\nexpress many general priors about the world around us, i.e.,\npriors that are not task-speci\ufb01c but would be likely to be useful\nfor a learning machine to solve AI-tasks. Examples of such\ngeneral-purpose priors are the following:\n\u2022 Smoothness: assumes the function to be learned f is s.t.\nx \u2248y generally implies f(x) \u2248f(y). This most basic prior\nis present in most machine learning, but is insuf\ufb01cient to get\naround the curse of dimensionality, see Section 3.2.\n\u2022 Multiple explanatory factors: the data generating distribu-\ntion is generated by different underlying factors, and for the\nmost part what one learns about one factor generalizes in many\ncon\ufb01gurations of the other factors. The objective to recover\nor at least disentangle these underlying factors of variation is\ndiscussed in Section 3.5. This assumption is behind the idea of\ndistributed representations, discussed in Section 3.3 below.\n\u2022 A hierarchical organization of explanatory factors: the\nconcepts that are useful for describing the world around us\ncan be de\ufb01ned in terms of other concepts, in a hierarchy, with\nmore abstract concepts higher in the hierarchy, de\ufb01ned in\nterms of less abstract ones. This assumption is exploited with\ndeep representations, elaborated in Section 3.4 below.\n\u2022 Semi-supervised learning: with inputs X and target Y to\npredict, a subset of the factors explaining X\u2019s distribution\nexplain much of Y , given X. Hence representations that are\nuseful for P(X) tend to be useful when learning P(Y |X),\nallowing sharing of statistical strength between the unsuper-\nvised and supervised learning tasks, see Section 4.\n\u2022 Shared factors across tasks: with many Y \u2019s of interest or\nmany learning tasks in general, tasks (e.g., the corresponding\nP(Y |X, task)) are explained by factors that are shared with\nother tasks, allowing sharing of statistical strengths across\ntasks, as discussed in the previous section (Multi-Task and\nTransfer Learning, Domain Adaptation).\n\u2022 Manifolds: probability mass concentrates near regions that\nhave a much smaller dimensionality than the original space\nwhere the data lives. This is explicitly exploited in some\nof the auto-encoder algorithms and other manifold-inspired\nalgorithms described respectively in Sections 7.2 and 8.\n\u2022 Natural clustering: different values of categorical variables\nsuch as object classes are associated with separate manifolds.\nMore precisely, the local variations on the manifold tend to\npreserve the value of a category, and a linear interpolation\nbetween examples of different classes in general involves\ngoing through a low density region, i.e., P(X|Y = i) for\ndifferent i tend to be well separated and not overlap much. For\nexample, this is exploited in the Manifold Tangent Classi\ufb01er\ndiscussed in Section 8.3. This hypothesis is consistent with the\nidea that humans have named categories and classes because\nof such statistical structure (discovered by their brain and\npropagated by their culture), and machine learning tasks often\ninvolves predicting such categorical variables.\n\u2022 Temporal and spatial coherence: consecutive (from a se-\nquence) or spatially nearby observations tend to be associated\nwith the same value of relevant categorical concepts, or result\nin a small move on the surface of the high-density manifold.\nMore generally, different factors change at different temporal\nand spatial scales, and many categorical concepts of interest\nchange slowly. When attempting to capture such categorical\nvariables, this prior can be enforced by making the associated\nrepresentations slowly changing, i.e., penalizing changes in\nvalues over time or space. This prior was introduced in Becker\nand Hinton (1992) and is discussed in Section 11.3.\n\u2022 Sparsity: for any given observation x, only a small fraction\nof the possible factors are relevant. In terms of representation,\nthis could be represented by features that are often zero (as\ninitially proposed by Olshausen and Field (1996)), or by the\nfact that most of the extracted features are insensitive to small\nvariations of x. This can be achieved with certain forms of\npriors on latent variables (peaked at 0), or by using a non-\nlinearity whose value is often \ufb02at at 0 (i.e., 0 and with a\n0 derivative), or simply by penalizing the magnitude of the\nJacobian matrix (of derivatives) of the function mapping input\nto representation. This is discussed in Sections 6.1.1 and 7.2.\n\u2022 Simplicity of Factor Dependencies: in good high-level\nrepresentations, the factors are related to each other through\nsimple, typically linear dependencies. This can be seen in\nmany laws of physics, and is assumed when plugging a linear\npredictor on top of a learned representation.\nWe can view many of the above priors as ways to help the\nlearner discover and disentangle some of the underlying (and\na priori unknown) factors of variation that the data may reveal.\nThis idea is pursued further in Sections 3.5 and 11.4.\n3.2\nSmoothness and the Curse of Dimensionality\nFor AI-tasks, such as vision and NLP, it seems hopeless to\nrely only on simple parametric models (such as linear models)\nbecause they cannot capture enough of the complexity of in-\nterest unless provided with the appropriate feature space. Con-\nversely, machine learning researchers have sought \ufb02exibility in\nlocal6 non-parametric learners such as kernel machines with\na \ufb01xed generic local-response kernel (such as the Gaussian\nkernel). Unfortunately, as argued at length by Bengio and\nMonperrus (2005); Bengio et al. (2006a); Bengio and LeCun\n(2007); Bengio (2009); Bengio et al. (2010), most of these\nalgorithms only exploit the principle of local generalization,\ni.e., the assumption that the target function (to be learned)\nis smooth enough, so they rely on examples to explicitly\nmap out the wrinkles of the target function. Generalization\n6. local in the sense that the value of the learned function at x depends\nmostly on training examples x(t)\u2019s close to x\n\n4\nis mostly achieved by a form of local interpolation between\nneighboring training examples. Although smoothness can be\na useful assumption, it is insuf\ufb01cient to deal with the curse\nof dimensionality, because the number of such wrinkles (ups\nand downs of the target function) may grow exponentially\nwith the number of relevant interacting factors, when the data\nare represented in raw input space. We advocate learning\nalgorithms that are \ufb02exible and non-parametric7 but do not\nrely exclusively on the smoothness assumption. Instead, we\npropose to incorporate generic priors such as those enumerated\nabove into representation-learning algorithms. Smoothness-\nbased learners (such as kernel machines) and linear models\ncan still be useful on top of such learned representations. In\nfact, the combination of learning a representation and kernel\nmachine is equivalent to learning the kernel, i.e., the feature\nspace. Kernel machines are useful, but they depend on a prior\nde\ufb01nition of a suitable similarity metric, or a feature space\nin which naive similarity metrics suf\ufb01ce. We would like to\nuse the data, along with very generic priors, to discover those\nfeatures, or equivalently, a similarity function.\n3.3\nDistributed representations\nGood\nrepresentations\nare\nexpressive,\nmeaning\nthat\na\nreasonably-sized learned representation can capture a huge\nnumber of possible input con\ufb01gurations. A simple counting\nargument helps us to assess the expressiveness of a model pro-\nducing a representation: how many parameters does it require\ncompared to the number of input regions (or con\ufb01gurations) it\ncan distinguish? Learners of one-hot representations, such as\ntraditional clustering algorithms, Gaussian mixtures, nearest-\nneighbor algorithms, decision trees, or Gaussian SVMs all re-\nquire O(N) parameters (and/or O(N) examples) to distinguish\nO(N) input regions. One could naively believe that one cannot\ndo better. However, RBMs, sparse coding, auto-encoders or\nmulti-layer neural networks can all represent up to O(2k) input\nregions using only O(N) parameters (with k the number of\nnon-zero elements in a sparse representation, and k = N in\nnon-sparse RBMs and other dense representations). These are\nall distributed 8 or sparse9 representations. The generalization\nof clustering to distributed representations is multi-clustering,\nwhere either several clusterings take place in parallel or the\nsame clustering is applied on different parts of the input,\nsuch as in the very popular hierarchical feature extraction for\nobject recognition based on a histogram of cluster categories\ndetected in different patches of an image (Lazebnik et al.,\n2006; Coates and Ng, 2011a). The exponential gain from\ndistributed or sparse representations is discussed further in\nsection 3.2 (and Figure 3.2) of\nBengio (2009). It comes\nabout because each parameter (e.g. the parameters of one of\nthe units in a sparse code, or one of the units in a Restricted\n7. We understand non-parametric as including all learning algorithms\nwhose capacity can be increased appropriately as the amount of data and its\ncomplexity demands it, e.g. including mixture models and neural networks\nwhere the number of parameters is a data-selected hyper-parameter.\n8. Distributed representations: where k out of N representation elements\nor feature values can be independently varied, e.g., they are not mutually\nexclusive. Each concept is represented by having k features being turned on\nor active, while each feature is involved in representing many concepts.\n9. Sparse representations: distributed representations where only a few of\nthe elements can be varied at a time, i.e., k < N.\nBoltzmann Machine) can be re-used in many examples that are\nnot simply near neighbors of each other, whereas with local\ngeneralization, different regions in input space are basically\nassociated with their own private set of parameters, e.g., as\nin decision trees, nearest-neighbors, Gaussian SVMs, etc. In\na distributed representation, an exponentially large number of\npossible subsets of features or hidden units can be activated\nin response to a given input. In a single-layer model, each\nfeature is typically associated with a preferred input direction,\ncorresponding to a hyperplane in input space, and the code\nor representation associated with that input is precisely the\npattern of activation (which features respond to the input,\nand how much). This is in contrast with a non-distributed\nrepresentation such as the one learned by most clustering\nalgorithms, e.g., k-means, in which the representation of a\ngiven input vector is a one-hot code identifying which one of\na small number of cluster centroids best represents the input 10.\n3.4\nDepth and abstraction\nDepth is a key aspect to representation learning strategies we\nconsider in this paper. As we will discuss, deep architectures\nare often challenging to train effectively and this has been\nthe subject of much recent research and progress. However,\ndespite these challenges, they carry two signi\ufb01cant advantages\nthat motivate our long-term interest in discovering successful\ntraining strategies for deep architectures. These advantages\nare: (1) deep architectures promote the re-use of features, and\n(2) deep architectures can potentially lead to progressively\nmore abstract features at higher layers of representations\n(more removed from the data).\nFeature re-use. The notion of re-use, which explains the\npower of distributed representations, is also at the heart of the\ntheoretical advantages behind deep learning, i.e., constructing\nmultiple levels of representation or learning a hierarchy of\nfeatures. The depth of a circuit is the length of the longest\npath from an input node of the circuit to an output node of\nthe circuit. The crucial property of a deep circuit is that its\nnumber of paths, i.e., ways to re-use different parts, can grow\nexponentially with its depth. Formally, one can change the\ndepth of a given circuit by changing the de\ufb01nition of what\neach node can compute, but only by a constant factor. The\ntypical computations we allow in each node include: weighted\nsum, product, arti\ufb01cial neuron model (such as a monotone non-\nlinearity on top of an af\ufb01ne transformation), computation of a\nkernel, or logic gates. Theoretical results clearly show families\nof functions where a deep representation can be exponentially\nmore ef\ufb01cient than one that is insuf\ufb01ciently deep (H\u02daastad,\n1986; H\u02daastad and Goldmann, 1991; Bengio et al., 2006a;\nBengio and LeCun, 2007; Bengio and Delalleau, 2011). If\nthe same family of functions can be represented with fewer\n10. As discussed in (Bengio, 2009), things are only slightly better when\nallowing continuous-valued membership values, e.g., in ordinary mixture\nmodels (with separate parameters for each mixture component), but the\ndifference in representational power is still exponential (Montufar and Morton,\n2012). The situation may also seem better with a decision tree, where each\ngiven input is associated with a one-hot code over the tree leaves, which\ndeterministically selects associated ancestors (the path from root to node).\nUnfortunately, the number of different regions represented (equal to the\nnumber of leaves of the tree) still only grows linearly with the number of\nparameters used to specify it (Bengio and Delalleau, 2011).\n\n5\nparameters (or more precisely with a smaller VC-dimension),\nlearning theory would suggest that it can be learned with\nfewer examples, yielding improvements in both computational\nef\ufb01ciency (less nodes to visit) and statistical ef\ufb01ciency (less\nparameters to learn, and re-use of these parameters over many\ndifferent kinds of inputs).\nAbstraction and invariance. Deep architectures can lead\nto abstract representations because more abstract concepts can\noften be constructed in terms of less abstract ones. In some\ncases, such as in the convolutional neural network (LeCun\net al., 1998b), we build this abstraction in explicitly via a\npooling mechanism (see section 11.2). More abstract concepts\nare generally invariant to most local changes of the input. That\nmakes the representations that capture these concepts generally\nhighly non-linear functions of the raw input. This is obviously\ntrue of categorical concepts, where more abstract representa-\ntions detect categories that cover more varied phenomena (e.g.\nlarger manifolds with more wrinkles) and thus they potentially\nhave greater predictive power. Abstraction can also appear in\nhigh-level continuous-valued attributes that are only sensitive\nto some very speci\ufb01c types of changes in the input. Learning\nthese sorts of invariant features has been a long-standing goal\nin pattern recognition.\n3.5\nDisentangling Factors of Variation\nBeyond being distributed and invariant, we would like our rep-\nresentations to disentangle the factors of variation. Different\nexplanatory factors of the data tend to change independently\nof each other in the input distribution, and only a few at a time\ntend to change when one considers a sequence of consecutive\nreal-world inputs.\nComplex data arise from the rich interaction of many\nsources. These factors interact in a complex web that can\ncomplicate AI-related tasks such as object classi\ufb01cation. For\nexample, an image is composed of the interaction between one\nor more light sources, the object shapes and the material prop-\nerties of the various surfaces present in the image. Shadows\nfrom objects in the scene can fall on each other in complex\npatterns, creating the illusion of object boundaries where there\nare none and dramatically effect the perceived object shape.\nHow can we cope with these complex interactions? How can\nwe disentangle the objects and their shadows? Ultimately,\nwe believe the approach we adopt for overcoming these\nchallenges must leverage the data itself, using vast quantities\nof unlabeled examples, to learn representations that separate\nthe various explanatory sources. Doing so should give rise to\na representation signi\ufb01cantly more robust to the complex and\nrichly structured variations extant in natural data sources for\nAI-related tasks.\nIt is important to distinguish between the related but distinct\ngoals of learning invariant features and learning to disentangle\nexplanatory factors. The central difference is the preservation\nof information. Invariant features, by de\ufb01nition, have reduced\nsensitivity in the direction of invariance. This is the goal of\nbuilding features that are insensitive to variation in the data\nthat are uninformative to the task at hand. Unfortunately, it\nis often dif\ufb01cult to determine a priori which set of features\nand variations will ultimately be relevant to the task at hand.\nFurther, as is often the case in the context of deep learning\nmethods, the feature set being trained may be destined to\nbe used in multiple tasks that may have distinct subsets of\nrelevant features. Considerations such as these lead us to the\nconclusion that the most robust approach to feature learning\nis to disentangle as many factors as possible, discarding as\nlittle information about the data as is practical. If some form\nof dimensionality reduction is desirable, then we hypothesize\nthat the local directions of variation least represented in the\ntraining data should be \ufb01rst to be pruned out (as in PCA,\nfor example, which does it globally instead of around each\nexample).\n3.6\nGood criteria for learning representations?\nOne of the challenges of representation learning that distin-\nguishes it from other machine learning tasks such as classi-\n\ufb01cation is the dif\ufb01culty in establishing a clear objective, or\ntarget for training. In the case of classi\ufb01cation, the objective\nis (at least conceptually) obvious, we want to minimize the\nnumber of misclassi\ufb01cations on the training dataset. In the\ncase of representation learning, our objective is far-removed\nfrom the ultimate objective, which is typically learning a\nclassi\ufb01er or some other predictor. Our problem is reminiscent\nof the credit assignment problem encountered in reinforcement\nlearning. We have proposed that a good representation is one\nthat disentangles the underlying factors of variation, but how\ndo we translate that into appropriate training criteria? Is it even\nnecessary to do anything but maximize likelihood under a good\nmodel or can we introduce priors such as those enumerated\nabove (possibly data-dependent ones) that help the representa-\ntion better do this disentangling? This question remains clearly\nopen but is discussed in more detail in Sections 3.5 and 11.4.\n4\nBUILDING DEEP REPRESENTATIONS\nIn 2006, a breakthrough in feature learning and deep learning\nwas initiated by Geoff Hinton and quickly followed up in\nthe same year (Hinton et al., 2006; Bengio et al., 2007;\nRanzato et al., 2007), and soon after by Lee et al. (2008)\nand many more later. It has been extensively reviewed and\ndiscussed in Bengio (2009). A central idea, referred to as\ngreedy layerwise unsupervised pre-training, was to learn a\nhierarchy of features one level at a time, using unsupervised\nfeature learning to learn a new transformation at each level\nto be composed with the previously learned transformations;\nessentially, each iteration of unsupervised feature learning adds\none layer of weights to a deep neural network. Finally, the set\nof layers could be combined to initialize a deep supervised pre-\ndictor, such as a neural network classi\ufb01er, or a deep generative\nmodel, such as a Deep Boltzmann Machine (Salakhutdinov\nand Hinton, 2009).\nThis paper is mostly about feature learning algorithms\nthat can be used to form deep architectures. In particular, it\nwas empirically observed that layerwise stacking of feature\nextraction often yielded better representations, e.g., in terms\nof classi\ufb01cation error (Larochelle et al., 2009; Erhan et al.,\n2010b), quality of the samples generated by a probabilistic\nmodel (Salakhutdinov and Hinton, 2009) or in terms of the\ninvariance properties of the learned features (Goodfellow\n\n6\net al., 2009). Whereas this section focuses on the idea of\nstacking single-layer models, Section 10 follows up with a\ndiscussion on joint training of all the layers.\nAfter greedy layerwise unsuperivsed pre-training, the re-\nsulting deep features can be used either as input to a standard\nsupervised machine learning predictor (such as an SVM) or as\ninitialization for a deep supervised neural network (e.g., by ap-\npending a logistic regression layer or purely supervised layers\nof a multi-layer neural network). The layerwise procedure can\nalso be applied in a purely supervised setting, called the greedy\nlayerwise supervised pre-training (Bengio et al., 2007). For\nexample, after the \ufb01rst one-hidden-layer MLP is trained, its\noutput layer is discarded and another one-hidden-layer MLP\ncan be stacked on top of it, etc. Although results reported\nin Bengio et al. (2007) were not as good as for unsupervised\npre-training, they were nonetheless better than without pre-\ntraining at all. Alternatively, the outputs of the previous layer\ncan be fed as extra inputs for the next layer (in addition to the\nraw input), as successfully done in Yu et al. (2010). Another\nvariant (Seide et al., 2011b) pre-trains in a supervised way all\nthe previously added layers at each step of the iteration, and\nin their experiments this discriminant variant yielded better\nresults than unsupervised pre-training.\nWhereas combining single layers into a supervised model\nis straightforward, it is less clear how layers pre-trained by\nunsupervised learning should be combined to form a better\nunsupervised model. We cover here some of the approaches\nto do so, but no clear winner emerges and much work has to\nbe done to validate existing proposals or improve them.\nThe \ufb01rst proposal was to stack pre-trained RBMs into a\nDeep Belief Network (Hinton et al., 2006) or DBN, where\nthe top layer is interpreted as an RBM and the lower layers\nas a directed sigmoid belief network. However, it is not clear\nhow to approximate maximum likelihood training to further\noptimize this generative model. One option is the wake-sleep\nalgorithm (Hinton et al., 2006) but more work should be done\nto assess the ef\ufb01ciency of this procedure in terms of improving\nthe generative model.\nThe second approach that has been put forward is to\ncombine the RBM parameters into a Deep Boltzmann Machine\n(DBM), by basically halving the RBM weights to obtain\nthe DBM weights (Salakhutdinov and Hinton, 2009). The\nDBM can then be trained by approximate maximum likelihood\nas discussed in more details later (Section 10.2). This joint\ntraining has brought substantial improvements, both in terms\nof likelihood and in terms of classi\ufb01cation performance of\nthe resulting deep feature learner (Salakhutdinov and Hinton,\n2009).\nAnother early approach was to stack RBMs or auto-\nencoders into a deep auto-encoder (Hinton and Salakhutdi-\nnov, 2006). If we have a series of encoder-decoder pairs\n(f (i)(\u00b7), g(i)(\u00b7)), then the overall encoder is the composition of\nthe encoders, f (N)(. . . f (2)(f (1)(\u00b7))), and the overall decoder\nis its \u201ctranspose\u201d (often with transposed weight matrices as\nwell), g(1)(g(2)(. . . f (N)(\u00b7))). The deep auto-encoder (or its\nregularized version, as discussed in Section 7.2) can then\nbe jointly trained, with all the parameters optimized with\nrespect to a global reconstruction error criterion. More work\non this avenue clearly needs to be done, and it was probably\navoided by fear of the challenges in training deep feedfor-\nward networks, discussed in the Section 10 along with very\nencouraging recent results.\nYet another recently proposed approach to training deep\narchitectures (Ngiam et al., 2011) is to consider the iterative\nconstruction of a free energy function (i.e., with no explicit\nlatent variables, except possibly for a top-level layer of hidden\nunits) for a deep architecture as the composition of transforma-\ntions associated with lower layers, followed by top-level hid-\nden units. The question is then how to train a model de\ufb01ned by\nan arbitrary parametrized (free) energy function. Ngiam et al.\n(2011) have used Hybrid Monte Carlo (Neal, 1993), but other\noptions include contrastive divergence (Hinton, 1999; Hinton\net al., 2006), score matching (Hyv\u00a8arinen, 2005; Hyv\u00a8arinen,\n2008), denoising score matching (Kingma and LeCun, 2010;\nVincent, 2011), ratio-matching (Hyv\u00a8arinen, 2007) and noise-\ncontrastive estimation (Gutmann and Hyvarinen, 2010).\n5\nSINGLE-LAYER LEARNING MODULES\nWithin the community of researchers interested in representa-\ntion learning, there has developed two broad parallel lines of\ninquiry: one rooted in probabilistic graphical models and one\nrooted in neural networks. Fundamentally, the difference be-\ntween these two paradigms is whether the layered architecture\nof a deep learning model is to be interpreted as describing a\nprobabilistic graphical model or as describing a computation\ngraph. In short, are hidden units considered latent random\nvariables or as computational nodes?\nTo date, the dichotomy between these two paradigms has\nremained in the background, perhaps because they appear to\nhave more characteristics in common than separating them.\nWe suggest that this is likely a function of the fact that\nmuch recent progress in both of these areas has focused\non single-layer greedy learning modules and the similarities\nbetween the types of single-layer models that have been\nexplored: mainly, the restricted Boltzmann machine (RBM)\non the probabilistic side, and the auto-encoder variants on the\nneural network side. Indeed, as shown by one of us (Vincent,\n2011) and others (Swersky et al., 2011), in the case of\nthe restricted Boltzmann machine, training the model via\nan inductive principle known as score matching (Hyv\u00a8arinen,\n2005) (to be discussed in sec. 6.4.3) is essentially identical\nto applying a regularized reconstruction objective to an auto-\nencoder. Another strong link between pairs of models on\nboth sides of this divide is when the computational graph for\ncomputing representation in the neural network model corre-\nsponds exactly to the computational graph that corresponds to\ninference in the probabilistic model, and this happens to also\ncorrespond to the structure of graphical model itself (e.g., as\nin the RBM).\nThe connection between these two paradigms becomes more\ntenuous when we consider deeper models where, in the case\nof a probabilistic model, exact inference typically becomes\nintractable. In the case of deep models, the computational\ngraph diverges from the structure of the model. For example,\nin the case of a deep Boltzmann machine, unrolling variational\n(approximate) inference into a computational graph results in\n\n7\na recurrent graph structure. We have performed preliminary\nexploration (Savard, 2011) of deterministic variants of deep\nauto-encoders whose computational graph is similar to that of\na deep Boltzmann machine (in fact very close to the mean-\n\ufb01eld variational approximations associated with the Boltzmann\nmachine), and that is one interesting intermediate point to ex-\nplore (between the deterministic approaches and the graphical\nmodel approaches).\nIn the next few sections we will review the major de-\nvelopments in single-layer training modules used to support\nfeature learning and particularly deep learning. We divide these\nsections between (Section 6) the probabilistic models, with\ninference and training schemes that directly parametrize the\ngenerative \u2013 or decoding \u2013 pathway and (Section 7) the typ-\nically neural network-based models that directly parametrize\nthe encoding pathway. Interestingly, some models, like Pre-\ndictive Sparse Decomposition (PSD) (Kavukcuoglu et al.,\n2008) inherit both properties, and will also be discussed (Sec-\ntion 7.2.4). We then present a different view of representation\nlearning, based on the associated geometry and the manifold\nassumption, in Section 8.\nFirst, let us consider an unsupervised single-layer represen-\ntation learning algorithm spaning all three views: probabilistic,\nauto-encoder, and manifold learning.\nPrincipal Components Analysis\nWe will use probably the oldest feature extraction algorithm,\nprincipal components analysis (PCA), to illustrate the proba-\nbilistic, auto-encoder and manifold views of representation-\nlearning. PCA learns a linear transformation h = f(x) =\nW T x + b of input x \u2208Rdx, where the columns of dx \u00d7 dh\nmatrix W form an orthogonal basis for the dh orthogonal\ndirections of greatest variance in the training data. The result\nis dh features (the components of representation h) that\nare decorrelated. The three interpretations of PCA are the\nfollowing: a) it is related to probabilistic models (Section 6)\nsuch as probabilistic PCA, factor analysis and the traditional\nmultivariate Gaussian distribution (the leading eigenvectors of\nthe covariance matrix are the principal components); b) the\nrepresentation it learns is essentially the same as that learned\nby a basic linear auto-encoder (Section 7.2); and c) it can be\nviewed as a simple linear form of linear manifold learning\n(Section 8), i.e., characterizing a lower-dimensional region\nin input space near which the data density is peaked. Thus,\nPCA may be in the back of the reader\u2019s mind as a common\nthread relating these various viewpoints. Unfortunately the\nexpressive power of linear features is very limited: they cannot\nbe stacked to form deeper, more abstract representations since\nthe composition of linear operations yields another linear\noperation. Here, we focus on recent algorithms that have\nbeen developed to extract non-linear features, which can be\nstacked in the construction of deep networks, although some\nauthors simply insert a non-linearity between learned single-\nlayer linear projections (Le et al., 2011c; Chen et al., 2012).\nAnother rich family of feature extraction techniques that this\nreview does not cover in any detail due to space constraints is\nIndependent Component Analysis or ICA (Jutten and Herault,\n1991; Bell and Sejnowski, 1997). Instead, we refer the reader\nto Hyv\u00a8arinen et al. (2001a); Hyv\u00a8arinen et al. (2009). Note that,\nwhile in the simplest case (complete, noise-free) ICA yields\nlinear features, in the more general case it can be equated\nwith a linear generative model with non-Gaussian independent\nlatent variables, similar to sparse coding (section 6.1.1), which\nresult in non-linear features. Therefore, ICA and its variants\nlike Independent and Topographic ICA (Hyv\u00a8arinen et al.,\n2001b) can and have been used to build deep networks (Le\net al., 2010, 2011c): see section 11.2. The notion of obtaining\nindependent components also appears similar to our stated\ngoal of disentangling underlying explanatory factors through\ndeep networks. However, for complex real-world distributions,\nit is doubtful that the relationship between truly independent\nunderlying factors and the observed high-dimensional data can\nbe adequately characterized by a linear transformation.\n6\nPROBABILISTIC MODELS\nFrom the probabilistic modeling perspective, the question of\nfeature learning can be interpreted as an attempt to recover\na parsimonious set of latent random variables that describe\na distribution over the observed data. We can express as\np(x, h) a probabilistic model over the joint space of the latent\nvariables, h, and observed data or visible variables x. Feature\nvalues are conceived as the result of an inference process to\ndetermine the probability distribution of the latent variables\ngiven the data, i.e. p(h | x), often referred to as the posterior\nprobability. Learning is conceived in term of estimating a set\nof model parameters that (locally) maximizes the regularized\nlikelihood of the training data. The probabilistic graphical\nmodel formalism gives us two possible modeling paradigms\nin which we can consider the question of inferring latent\nvariables, directed and undirected graphical models, which\ndiffer in their parametrization of the joint distribution p(x, h),\nyielding major impact on the nature and computational costs\nof both inference and learning.\n6.1\nDirected Graphical Models\nDirected latent factor models separately parametrize the con-\nditional likelihood p(x | h) and the prior p(h) to construct\nthe joint distribution, p(x, h) = p(x | h)p(h). Examples of\nthis decomposition include: Principal Components Analysis\n(PCA) (Roweis, 1997; Tipping and Bishop, 1999), sparse\ncoding (Olshausen and Field, 1996), sigmoid belief net-\nworks (Neal, 1992) and the newly introduced spike-and-slab\nsparse coding model (Goodfellow et al., 2011).\n6.1.1\nExplaining Away\nDirected models often leads to one important property: ex-\nplaining away, i.e., a priori independent causes of an event can\nbecome non-independent given the observation of the event.\nLatent factor models can generally be interpreted as latent\ncause models, where the h activations cause the observed x.\nThis renders the a priori independent h to be non-independent.\nAs a consequence, recovering the posterior distribution of h,\np(h | x) (which we use as a basis for feature representation),\nis often computationally challenging and can be entirely\nintractable, especially when h is discrete.\nA classic example that illustrates the phenomenon is to\nimagine you are on vacation away from home and you receive\na phone call from the security system company, telling you that\n\n8\nthe alarm has been activated. You begin worrying your home\nhas been burglarized, but then you hear on the radio that a\nminor earthquake has been reported in the area of your home.\nIf you happen to know from prior experience that earthquakes\nsometimes cause your home alarm system to activate, then\nsuddenly you relax, con\ufb01dent that your home has very likely\nnot been burglarized.\nThe example illustrates how the alarm activation rendered\ntwo otherwise entirely independent causes, burglarized and\nearthquake, to become dependent \u2013 in this case, the depen-\ndency is one of mutual exclusivity. Since both burglarized\nand earthquake are very rare events and both can cause\nalarm activation, the observation of one explains away the\nother. Despite the computational obstacles we face when\nattempting to recover the posterior over h, explaining away\npromises to provide a parsimonious p(h | x), which can\nbe an extremely useful characteristic of a feature encoding\nscheme. If one thinks of a representation as being composed\nof various feature detectors and estimated attributes of the\nobserved input, it is useful to allow the different features to\ncompete and collaborate with each other to explain the input.\nThis is naturally achieved with directed graphical models, but\ncan also be achieved with undirected models (see Section 6.2)\nsuch as Boltzmann machines if there are lateral connections\nbetween the corresponding units or corresponding interaction\nterms in the energy function that de\ufb01nes the probability model.\nProbabilistic Interpretation of PCA. PCA can be given\na natural probabilistic interpretation (Roweis, 1997; Tipping\nand Bishop, 1999) as factor analysis:\np(h)\n=\nN(h; 0, \u03c32\nhI)\np(x | h)\n=\nN(x; Wh + \u00b5x, \u03c32\nxI),\n(1)\nwhere x \u2208Rdx, h \u2208Rdh, N(v; \u00b5, \u03a3) is the multivariate\nnormal density of v with mean \u00b5 and covariance \u03a3, and\ncolumns of W span the same space as leading dh principal\ncomponents, but are not constrained to be orthonormal.\nSparse Coding. Like PCA, sparse coding has both a proba-\nbilistic and non-probabilistic interpretation. Sparse coding also\nrelates a latent representation h (either a vector of random\nvariables or a feature vector, depending on the interpretation)\nto the data x through a linear mapping W, which we refer\nto as the dictionary. The difference between sparse coding\nand PCA is that sparse coding includes a penalty to ensure a\nsparse activation of h is used to encode each input x. From\na non-probabilistic perspective, sparse coding can be seen as\nrecovering the code or feature vector associated with a new\ninput x via:\nh\u2217= f(x) = argmin\nh\n\u2225x \u2212Wh\u22252\n2 + \u03bb\u2225h\u22251,\n(2)\nLearning the dictionary W can be accomplished by optimizing\nthe following training criterion with respect to W:\nJSC =\nX\nt\n\u2225x(t) \u2212Wh\u2217(t)\u22252\n2,\n(3)\nwhere x(t) is the t-th example and h\u2217(t) is the corresponding\nsparse code determined by Eq. 2. W is usually constrained to\nhave unit-norm columns (because one can arbitrarily exchange\nscaling of column i with scaling of h(t)\ni , such a constraint is\nnecessary for the L1 penalty to have any effect).\nThe probabilistic interpretation of sparse coding differs from\nthat of PCA, in that instead of a Gaussian prior on the latent\nrandom variable h, we use a sparsity inducing Laplace prior\n(corresponding to an L1 penalty):\np(h)\n=\ndh\nY\ni\n\u03bb\n2 exp(\u2212\u03bb|hi|)\np(x | h)\n=\nN(x; Wh + \u00b5x, \u03c32\nxI).\n(4)\nIn the case of sparse coding, because we will seek a sparse\nrepresentation (i.e., one with many features set to exactly zero),\nwe will be interested in recovering the MAP (maximum a\nposteriori value of h: i.e. h\u2217= argmaxh p(h | x) rather than\nits expected value E [h|x]. Under this interpretation, dictionary\nlearning proceeds as maximizing the likelihood of the data\ngiven these MAP values of h\u2217: argmaxW\nQ\nt p(x(t) | h\u2217(t))\nsubject to the norm constraint on W. Note that this pa-\nrameter learning scheme, subject to the MAP values of\nthe latent h, is not standard practice in the probabilistic\ngraphical model literature. Typically the likelihood of the\ndata p(x) = P\nh p(x | h)p(h) is maximized directly. In the\npresence of latent variables, expectation maximization is em-\nployed where the parameters are optimized with respect to\nthe marginal likelihood, i.e., summing or integrating the joint\nlog-likelihood over the all values of the latent variables under\ntheir posterior P(h | x), rather than considering only the\nsingle MAP value of h. The theoretical properties of this\nform of parameter learning are not yet well understood but\nseem to work well in practice (e.g. k-Means vs Gaussian\nmixture models and Viterbi training for HMMs). Note also\nthat the interpretation of sparse coding as a MAP estimation\ncan be questioned (Gribonval, 2011), because even though the\ninterpretation of the L1 penalty as a log-prior is a possible\ninterpretation, there can be other Bayesian interpretations\ncompatible with the training criterion.\nSparse coding is an excellent example of the power of\nexplaining away. Even with a very overcomplete dictionary11,\nthe MAP inference process used in sparse coding to \ufb01nd h\u2217\ncan pick out the most appropriate bases and zero the others,\ndespite them having a high degree of correlation with the input.\nThis property arises naturally in directed graphical models\nsuch as sparse coding and is entirely owing to the explaining\naway effect. It is not seen in commonly used undirected prob-\nabilistic models such as the RBM, nor is it seen in parametric\nfeature encoding methods such as auto-encoders. The trade-\noff is that, compared to methods such as RBMs and auto-\nencoders, inference in sparse coding involves an extra inner-\nloop of optimization to \ufb01nd h\u2217with a corresponding increase\nin the computational cost of feature extraction. Compared\nto auto-encoders and RBMs, the code in sparse coding is a\nfree variable for each example, and in that sense the implicit\nencoder is non-parametric.\nOne might expect that the parsimony of the sparse coding\nrepresentation and its explaining away effect would be advan-\ntageous and indeed it seems to be the case. Coates and Ng\n(2011a) demonstrated on the CIFAR-10 object classi\ufb01cation\ntask (Krizhevsky and Hinton, 2009) with a patch-base feature\nextraction pipeline, that in the regime with few (< 1000)\n11. Overcomplete: with more dimensions of h than dimensions of x.\n\n9\nlabeled training examples per class, the sparse coding repre-\nsentation signi\ufb01cantly outperformed other highly competitive\nencoding schemes. Possibly because of these properties, and\nbecause of the very computationally ef\ufb01cient algorithms that\nhave been proposed for it (in comparison with the general\ncase of inference in the presence of explaining away), sparse\ncoding enjoys considerable popularity as a feature learning and\nencoding paradigm. There are numerous examples of its suc-\ncessful application as a feature representation scheme, includ-\ning natural image modeling (Raina et al., 2007; Kavukcuoglu\net al., 2008; Coates and Ng, 2011a; Yu et al., 2011), audio\nclassi\ufb01cation (Grosse et al., 2007), NLP (Bagnell and Bradley,\n2009), as well as being a very successful model of the early\nvisual cortex (Olshausen and Field, 1996). Sparsity criteria can\nalso be generalized successfully to yield groups of features that\nprefer to all be zero, but if one or a few of them are active then\nthe penalty for activating others in the group is small. Different\ngroup sparsity patterns can incorporate different forms of prior\nknowledge (Kavukcuoglu et al., 2009; Jenatton et al., 2009;\nBach et al., 2011; Gregor et al., 2011).\nSpike-and-Slab Sparse Coding. Spike-and-slab sparse cod-\ning (S3C) is one example of a promising variation on sparse\ncoding for feature learning (Goodfellow et al., 2012). The\nS3C model possesses a set of latent binary spike variables\ntogether with a a set of latent real-valued slab variables. The\nactivation of the spike variables dictates the sparsity pattern.\nS3C has been applied to the CIFAR-10 and CIFAR-100 object\nclassi\ufb01cation tasks (Krizhevsky and Hinton, 2009), and shows\nthe same pattern as sparse coding of superior performance in\nthe regime of relatively few (< 1000) labeled examples per\nclass (Goodfellow et al., 2012). In fact, in both the CIFAR-\n100 dataset (with 500 examples per class) and the CIFAR-\n10 dataset (when the number of examples is reduced to a\nsimilar range), the S3C representation actually outperforms\nsparse coding representations. This advantage was revealed\nclearly with S3C winning the NIPS\u20192011 Transfer Learning\nChallenge (Goodfellow et al., 2011).\n6.2\nUndirected Graphical Models\nUndirected graphical models, also called Markov random\n\ufb01elds (MRFs), parametrize the joint p(x, h) through a product\nof unnormalized non-negative clique potentials:\np(x, h) = 1\nZ\u03b8\nY\ni\n\u03c8i(x)\nY\nj\n\u03b7j(h)\nY\nk\n\u03bdk(x, h)\n(5)\nwhere \u03c8i(x), \u03b7j(h) and \u03bdk(x, h) are the clique potentials de-\nscribing the interactions between the visible elements, between\nthe hidden variables, and those interaction between the visible\nand hidden variables respectively. The partition function Z\u03b8\nensures that the distribution is normalized. Within the context\nof unsupervised feature learning, we generally see a particular\nform of Markov random \ufb01eld called a Boltzmann distribution\nwith clique potentials constrained to be positive:\np(x, h) = 1\nZ\u03b8 exp (\u2212E\u03b8(x, h)) ,\n(6)\nwhere E\u03b8(x, h) is the energy function and contains the inter-\nactions described by the MRF clique potentials and \u03b8 are the\nmodel parameters that characterize these interactions.\nThe Boltzmann machine was originally de\ufb01ned as a network\nof symmetrically-coupled binary random variables or units.\nThese stochastic units can be divided into two groups: (1) the\nvisible units x \u2208{0, 1}dx that represent the data, and (2) the\nhidden or latent units h \u2208{0, 1}dh that mediate dependencies\nbetween the visible units through their mutual interactions. The\npattern of interaction is speci\ufb01ed through the energy function:\nEBM\n\u03b8\n(x, h) = \u22121\n2xT Ux \u22121\n2hT V h \u2212xT Wh \u2212bT x \u2212dT h,\n(7)\nwhere \u03b8 = {U, V, W, b, d} are the model parameters which\nrespectively encode the visible-to-visible interactions, the\nhidden-to-hidden interactions, the visible-to-hidden interac-\ntions, the visible self-connections, and the hidden self-\nconnections (called biases). To avoid over-parametrization, the\ndiagonals of U and V are set to zero.\nThe Boltzmann machine energy function speci\ufb01es the prob-\nability distribution over [x, h], via the Boltzmann distribution,\nEq. 6, with the partition function Z\u03b8 given by:\nZ\u03b8 =\nx1=1\nX\nx1=0\n\u00b7 \u00b7 \u00b7\nxdx =1\nX\nxdx =0\nh1=1\nX\nh1=0\n\u00b7 \u00b7 \u00b7\nhdh =1\nX\nhdh =0\nexp\n\u0010\n\u2212EBM\n\u03b8\n(x, h; \u03b8)\n\u0011\n.\n(8)\nThis joint probability distribution gives rise to the set of\nconditional distributions of the form:\nP(hi | x, h\\i) = sigmoid\n\uf8eb\n\uf8edX\nj\nWjixj +\nX\ni\u2032\u0338=i\nVii\u2032hi\u2032 + di\n\uf8f6\n\uf8f8\n(9)\nP(xj | h, x\\j) = sigmoid\n\uf8eb\n\uf8edX\ni\nWjixj +\nX\nj\u2032\u0338=j\nUjj\u2032xj\u2032 + bj\n\uf8f6\n\uf8f8.\n(10)\nIn general, inference in the Boltzmann machine is intractable.\nFor example, computing the conditional probability of hi given\nthe visibles, P(hi | x), requires marginalizing over the rest of\nthe hiddens, which implies evaluating a sum with 2dh\u22121 terms:\nP(hi | x) =\nh1=1\nX\nh1=0\n\u00b7 \u00b7 \u00b7\nhi\u22121=1\nX\nhi\u22121=0\nhi+1=1\nX\nhi+1=0\n\u00b7 \u00b7 \u00b7\nhdh =1\nX\nhdh =0\nP(h | x)\n(11)\nHowever with some judicious choices in the pattern of inter-\nactions between the visible and hidden units, more tractable\nsubsets of the model family are possible, as we discuss next.\nRestricted Boltzmann Machines (RBMs). The RBM\nis likely the most popular subclass of Boltzmann ma-\nchine (Smolensky, 1986). It is de\ufb01ned by restricting the\ninteractions in the Boltzmann energy function, in Eq. 7, to\nonly those between h and x, i.e. ERBM\n\u03b8\nis EBM\n\u03b8\nwith U = 0\nand V = 0. As such, the RBM can be said to form a bipartite\ngraph with the visibles and the hiddens forming two layers\nof vertices in the graph (and no connection between units of\nthe same layer). With this restriction, the RBM possesses the\nuseful property that the conditional distribution over the hidden\nunits factorizes given the visibles:\nP(h | x) =\nQ\ni P(hi | x)\nP(hi = 1 | x) =\nsigmoid\n\u0010P\nj Wjixj + di\n\u0011\n.\n(12)\nLikewise, the conditional distribution over the visible units\ngiven the hiddens also factorizes:\nP(x | h) =\nY\nj\nP(xj | h)\nP(xj = 1 | h) = sigmoid\n X\ni\nWjihi + bj\n!\n.\n(13)\n\n10\nThis makes inferences readily tractable in RBMs. For exam-\nple, the RBM feature representation is taken to be the set of\nposterior marginals P(hi | x), which, given the conditional\nindependence described in Eq. 12, are immediately available.\nNote that this is in stark contrast to the situation with popular\ndirected graphical models for unsupervised feature extraction,\nwhere computing the posterior probability is intractable.\nImportantly, the tractability of the RBM does not extend\nto its partition function, which still involves summing an\nexponential number of terms. It does imply however that we\ncan limit the number of terms to min{2dx, 2dh}. Usually this is\nstill an unmanageable number of terms and therefore we must\nresort to approximate methods to deal with its estimation.\nIt is dif\ufb01cult to overstate the impact the RBM has had to\nthe \ufb01elds of unsupervised feature learning and deep learning.\nIt has been used in a truly impressive variety of applica-\ntions, including fMRI image classi\ufb01cation (Schmah et al.,\n2009), motion and spatial transformations (Taylor and Hinton,\n2009; Memisevic and Hinton, 2010), collaborative \ufb01ltering\n(Salakhutdinov et al., 2007) and natural image modeling\n(Ranzato and Hinton, 2010; Courville et al., 2011b).\n6.3\nGeneralizations of the RBM to Real-valued data\nImportant progress has been made in the last few years in\nde\ufb01ning generalizations of the RBM that better capture real-\nvalued data, in particular real-valued image data, by better\nmodeling the conditional covariance of the input pixels. The\nstandard RBM, as discussed above, is de\ufb01ned with both binary\nvisible variables v \u2208{0, 1} and binary latent variables h \u2208\n{0, 1}. The tractability of inference and learning in the RBM\nhas inspired many authors to extend it, via modi\ufb01cations of its\nenergy function, to model other kinds of data distributions. In\nparticular, there has been multiple attempts to develop RBM-\ntype models of real-valued data, where x \u2208Rdx. The most\nstraightforward approach to modeling real-valued observations\nwithin the RBM framework is the so-called Gaussian RBM\n(GRBM) where the only change in the RBM energy function\nis to the visible units biases, by adding a bias term that is\nquadratic in the visible units x. While it probably remains\nthe most popular way to model real-valued data within the\nRBM framework, Ranzato and Hinton (2010) suggest that the\nGRBM has proved to be a somewhat unsatisfactory model of\nnatural images. The trained features typically do not represent\nsharp edges that occur at object boundaries and lead to latent\nrepresentations that are not particularly useful features for\nclassi\ufb01cation tasks. Ranzato and Hinton (2010) argue that\nthe failure of the GRBM to adequately capture the statistical\nstructure of natural images stems from the exclusive use of the\nmodel capacity to capture the conditional mean at the expense\nof the conditional covariance. Natural images, they argue, are\nchie\ufb02y characterized by the covariance of the pixel values,\nnot by their absolute values. This point is supported by the\ncommon use of preprocessing methods that standardize the\nglobal scaling of the pixel values across images in a dataset\nor across the pixel values within each image.\nThese kinds of concerns about the ability of the GRBM\nto model natural image data has lead to the development of\nalternative RBM-based models that each attempt to take on this\nobjective of better modeling non-diagonal conditional covari-\nances. (Ranzato and Hinton, 2010) introduced the mean and\ncovariance RBM (mcRBM). Like the GRBM, the mcRBM is a\n2-layer Boltzmann machine that explicitly models the visible\nunits as Gaussian distributed quantities. However unlike the\nGRBM, the mcRBM uses its hidden layer to independently\nparametrize both the mean and covariance of the data through\ntwo sets of hidden units. The mcRBM is a combination of the\ncovariance RBM (cRBM) (Ranzato et al., 2010a), that models\nthe conditional covariance, with the GRBM that captures the\nconditional mean. While the GRBM has shown considerable\npotential as the basis of a highly successful phoneme recogni-\ntion system (Dahl et al., 2010), it seems that due to dif\ufb01culties\nin training the mcRBM, the model has been largely superseded\nby the mPoT model. The mPoT model (mean-product of\nStudent\u2019s T-distributions model)\n(Ranzato et al., 2010b) is\na combination of the GRBM and the product of Student\u2019s T-\ndistributions model (Welling et al., 2003). It is an energy-based\nmodel where the conditional distribution over the visible units\nconditioned on the hidden variables is a multivariate Gaussian\n(non-diagonal covariance) and the complementary conditional\ndistribution over the hidden variables given the visibles are a\nset of independent Gamma distributions.\nThe PoT model has recently been generalized to the mPoT\nmodel (Ranzato et al., 2010b) to include nonzero Gaussian\nmeans by the addition of GRBM-like hidden units, similarly to\nhow the mcRBM generalizes the cRBM. The mPoT model has\nbeen used to synthesize large-scale natural images (Ranzato\net al., 2010b) that show large-scale features and shadowing\nstructure. It has been used to model natural textures (Kivinen\nand Williams, 2012) in a tiled-convolution con\ufb01guration (see\nsection 11.2).\nAnother recently introduced RBM-based model with the\nobjective of having the hidden units encode both the mean\nand covariance information is the spike-and-slab Restricted\nBoltzmann Machine (ssRBM) (Courville et al., 2011a,b).\nThe ssRBM is de\ufb01ned as having both a real-valued \u201cslab\u201d\nvariable and a binary \u201cspike\u201d variable associated with each\nunit in the hidden layer. The ssRBM has been demonstrated\nas a feature learning and extraction scheme in the context\nof CIFAR-10 object classi\ufb01cation (Krizhevsky and Hinton,\n2009) from natural images and has performed well in the\nrole (Courville et al., 2011a,b). When trained convolutionally\n(see Section 11.2) on full CIFAR-10 natural images, the model\ndemonstrated the ability to generate natural image samples\nthat seem to capture the broad statistical structure of natural\nimages better than previous parametric generative models, as\nillustrated with the samples of Figure 2.\nThe mcRBM, mPoT and ssRBM each set out to model\nreal-valued data such that the hidden units encode not only\nthe conditional mean of the data but also its conditional\ncovariance. Other than differences in the training schemes, the\nmost signi\ufb01cant difference between these models is how they\nencode their conditional covariance. While the mcRBM and\nthe mPoT use the activation of the hidden units to enforce con-\nstraints on the covariance of x, the ssRBM uses the hidden unit\nto pinch the precision matrix along the direction speci\ufb01ed by\nthe corresponding weight vector. These two ways of modeling\n\n11\nFig. 2.\n(Top) Samples from convolutionally trained \u00b5-ssRBM\nfrom Courville et al. (2011b). (Bottom) Images in CIFAR-10 train-\ning set closest (L2 distance with contrast normalized training\nimages) to corresponding model samples on top. The model\ndoes not appear to be over\ufb01tting particular training examples.\nconditional covariance diverge when the dimensionality of the\nhidden layer is signi\ufb01cantly different from that of the input.\nIn the overcomplete setting, sparse activation with the ssRBM\nparametrization permits variance only in the select directions\nof the sparsely activated hidden units. This is a property the\nssRBM shares with sparse coding models (Olshausen and\nField, 1996; Grosse et al., 2007). On the other hand, in\nthe case of the mPoT or mcRBM, an overcomplete set of\nconstraints on the covariance implies that capturing arbitrary\ncovariance along a particular direction of the input requires\ndecreasing potentially all constraints with positive projection\nin that direction. This perspective would suggest that the mPoT\nand mcRBM do not appear to be well suited to provide a sparse\nrepresentation in the overcomplete setting.\n6.4\nRBM parameter estimation\nMany of the RBM training methods we discuss here are ap-\nplicable to more general undirected graphical models, but are\nparticularly practical in the RBM setting. Freund and Haussler\n(1994) proposed a learning algorithm for harmoniums (RBMs)\nbased on projection pursuit. Contrastive Divergence (Hinton,\n1999; Hinton et al., 2006) has been used most often to train\nRBMs, and many recent papers use Stochastic Maximum\nLikelihood (Younes, 1999; Tieleman, 2008).\nAs discussed in Sec. 6.1, in training probabilistic models\nparameters are typically adapted in order to maximize the like-\nlihood of the training data (or equivalently the log-likelihood,\nor its penalized version, which adds a regularization term).\nWith T training examples, the log likelihood is given by:\nT\nX\nt=1\nlog P(x(t); \u03b8) =\nT\nX\nt=1\nlog\nX\nh\u2208{0,1}dh\nP(x(t), h; \u03b8).\n(14)\nGradient-based optimization requires its gradient, which for\nBoltzmann machines, is given by:\n\u2202\n\u2202\u03b8i\nT\nX\nt=1\nlog p(x(t))\n=\n\u2212\nT\nX\nt=1\nEp(h|x(t))\n\u0014 \u2202\n\u2202\u03b8i EBM\n\u03b8\n(x(t), h)\n\u0015\n+\nT\nX\nt=1\nEp(x,h)\n\u0014 \u2202\n\u2202\u03b8i EBM\n\u03b8\n(x, h)\n\u0015\n,\n(15)\nwhere we have the expectations with respect to p(h(t) | x(t))\nin the \u201cclamped\u201d condition (also called the positive phase),\nand over the full joint p(x, h) in the \u201cunclamped\u201d condition\n(also called the negative phase). Intuitively, the gradient acts\nto locally move the model distribution (the negative phase\ndistribution) toward the data distribution (positive phase dis-\ntribution), by pushing down the energy of (h, x(t)) pairs (for\nh \u223cP(h|x(t))) while pushing up the energy of (h, x) pairs\n(for (h, x) \u223cP(h, x)) until the two forces are in equilibrium,\nat which point the suf\ufb01cient statistics (gradient of the energy\nfunction) have equal expectations with x sampled from the\ntraining distribution or with x sampled from the model.\nThe RBM conditional independence properties imply that\nthe expectation in the positive phase of Eq. 15 is tractable.\nThe negative phase term \u2013 arising from the partition func-\ntion\u2019s contribution to the log-likelihood gradient \u2013 is more\nproblematic because the computation of the expectation over\nthe joint is not tractable. The various ways of dealing with the\npartition function\u2019s contribution to the gradient have brought\nabout a number of different training algorithms, many trying\nto approximate the log-likelihood gradient.\nTo approximate the expectation of the joint distribution in\nthe negative phase contribution to the gradient, it is natural to\nagain consider exploiting the conditional independence of the\nRBM in order to specify a Monte Carlo approximation of the\nexpectation over the joint:\nEp(x,h)\n\u0014 \u2202\n\u2202\u03b8i ERBM\n\u03b8\n(x, h)\n\u0015\n\u22481\nL\nL\nX\nl=1\n\u2202\n\u2202\u03b8i ERBM\n\u03b8\n(\u02dcx(l), \u02dch(l)),\n(16)\nwith the samples (\u02dcx(l), \u02dch(l)) drawn by a block Gibbs MCMC\n(Markov chain Monte Carlo) sampling procedure:\n\u02dcx(l)\n\u223c\nP(x | \u02dch(l\u22121))\n\u02dch(l)\n\u223c\nP(h | \u02dcx(l)).\nNaively, for each gradient update step, one would start a\nGibbs sampling chain, wait until the chain converges to the\nequilibrium distribution and then draw a suf\ufb01cient number of\nsamples to approximate the expected gradient with respect\nto the model (joint) distribution in Eq. 16. Then restart the\nprocess for the next step of approximate gradient ascent on\nthe log-likelihood. This procedure has the obvious \ufb02aw that\nwaiting for the Gibbs chain to \u201cburn-in\u201d and reach equilibrium\nanew for each gradient update cannot form the basis of a prac-\ntical training algorithm. Contrastive Divergence (Hinton, 1999;\nHinton et al., 2006), Stochastic Maximum Likelihood (Younes,\n1999; Tieleman, 2008) and fast-weights persistent contrastive\ndivergence or FPCD (Tieleman and Hinton, 2009) are all ways\nto avoid or reduce the need for burn-in.\n6.4.1\nContrastive Divergence\nContrastive divergence (CD) estimation (Hinton, 1999; Hinton\net al., 2006) estimates the negative phase expectation (Eq. 15)\nwith a very short Gibbs chain (often just one step) initialized\n\n12\nat the training data used in the positive phase. This reduces\nthe variance of the gradient estimator and still moves in a\ndirection that pulls the negative chain samples towards the as-\nsociated positive chain samples. Much has been written about\nthe properties and alternative interpretations of CD and its\nsimilarity to auto-encoder training, e.g. Carreira-Perpi\u02dcnan and\nHinton (2005); Yuille (2005); Bengio and Delalleau (2009);\nSutskever and Tieleman (2010).\n6.4.2\nStochastic Maximum Likelihood\nThe Stochastic Maximum Likelihood (SML) algorithm (also\nknown as persistent contrastive divergence or PCD) (Younes,\n1999; Tieleman, 2008) is an alternative way to sidestep an\nextended burn-in of the negative phase Gibbs sampler. At each\ngradient update, rather than initializing the Gibbs chain at the\npositive phase sample as in CD, SML initializes the chain at\nthe last state of the chain used for the previous update. In\nother words, SML uses a continually running Gibbs chain (or\noften a number of Gibbs chains run in parallel) from which\nsamples are drawn to estimate the negative phase expectation.\nDespite the model parameters changing between updates, these\nchanges should be small enough that only a few steps of Gibbs\n(in practice, often one step is used) are required to maintain\nsamples from the equilibrium distribution of the Gibbs chain,\ni.e. the model distribution.\nA troublesome aspect of SML is that it relies on the Gibbs\nchain to mix well (especially between modes) for learning to\nsucceed. Typically, as learning progresses and the weights of\nthe RBM grow, the ergodicity of the Gibbs sample begins to\nbreak down12. If the learning rate \u03f5 associated with gradient\nascent \u03b8 \u2190\u03b8 + \u03f5\u02c6g (with E[\u02c6g] \u2248\u2202log p\u03b8(x)\n\u2202\u03b8\n) is not reduced\nto compensate, then the Gibbs sampler will diverge from the\nmodel distribution and learning will fail. Desjardins et al.\n(2010); Cho et al. (2010); Salakhutdinov (2010b,a) have all\nconsidered various forms of tempered transitions to address\nthe failure of Gibbs chain mixing, and convincing solutions\nhave not yet been clearly demonstrated. A recently introduced\npromising avenue relies on depth itself, showing that mixing\nbetween modes is much easier on deeper layers (Bengio et al.,\n2013) (Sec.9.4).\nTieleman and Hinton (2009) have proposed quite a dif-\nferent approach to addressing potential mixing problems of\nSML with their fast-weights persistent contrastive divergence\n(FPCD), and it has also been exploited to train Deep Boltz-\nmann Machines (Salakhutdinov, 2010a) and construct a pure\nsampling algorithm for RBMs (Breuleux et al., 2011). FPCD\nbuilds on the surprising but robust tendency of Gibbs chains\nto mix better during SML learning than when the model\nparameters are \ufb01xed. The phenomenon is rooted in the form of\nthe likelihood gradient itself (Eq. 15). The samples drawn from\nthe SML Gibbs chain are used in the negative phase of the\ngradient, which implies that the learning update will slightly\nincrease the energy (decrease the probability) of those samples,\nmaking the region in the neighborhood of those samples\n12. When weights become large, the estimated distribution is more peaky,\nand the chain takes very long time to mix, to move from mode to mode, so\nthat practically the gradient estimator can be very poor. This is a serious\nchicken-and-egg problem because if sampling is not effective, nor is the\ntraining procedure, which may seem to stall, and yields even larger weights.\nless likely to be resampled and therefore making it more\nlikely that the samples will move somewhere else (typically\ngoing near another mode). Rather than drawing samples from\nthe distribution of the current model (with parameters \u03b8),\nFPCD exaggerates this effect by drawing samples from a local\nperturbation of the model with parameters \u03b8\u2217and an update\n\u03b8\u2217\nt+1 = (1 \u2212\u03b7)\u03b8t+1 + \u03b7\u03b8\u2217\nt + \u03f5\u2217\u2202\n\u2202\u03b8i\n T\nX\nt=1\nlog p(x(t))\n!\n,\n(17)\nwhere \u03f5\u2217is the relatively large fast-weight learning rate\n(\u03f5\u2217> \u03f5) and 0 < \u03b7 < 1 (but near 1) is a forgetting factor\nthat keeps the perturbed model close to the current model.\nUnlike tempering, FPCD does not converge to the model\ndistribution as \u03f5 and \u03f5\u2217go to 0, and further work is necessary\nto characterize the nature of its approximation to the model\ndistribution. Nevertheless, FPCD is a popular and apparently\neffective means of drawing approximate samples from the\nmodel distribution that faithfully represent its diversity, at the\nprice of sometimes generating spurious samples in between\ntwo modes (because the fast weights roughly correspond to a\nsmoothed view of the current model\u2019s energy function). It has\nbeen applied in a variety of applications (Tieleman and Hinton,\n2009; Ranzato et al., 2011; Kivinen and Williams, 2012) and\nit has been transformed into a sampling algorithm (Breuleux\net al., 2011) that also shares this fast mixing property with\nherding (Welling, 2009), for the same reason, i.e., introducing\nnegative correlations between consecutive samples of the\nchain in order to promote faster mixing.\n6.4.3\nPseudolikelihood, Ratio-matching and More\nWhile CD, SML and FPCD are by far the most popular meth-\nods for training RBMs and RBM-based models, all of these\nmethods are perhaps most naturally described as offering dif-\nferent approximations to maximum likelihood training. There\nexist other inductive principles that are alternatives to maxi-\nmum likelihood that can also be used to train RBMs. In partic-\nular, these include pseudo-likelihood (Besag, 1975) and ratio-\nmatching (Hyv\u00a8arinen, 2007). Both of these inductive principles\nattempt to avoid explicitly dealing with the partition function,\nand their asymptotic ef\ufb01ciency has been analyzed (Marlin and\nde Freitas, 2011). Pseudo-likelihood seeks to maximize the\nproduct of all one-dimensional conditional distributions of the\nform P(xd|x\\d), while ratio-matching can be interpreted as\nan extension of score matching (Hyv\u00a8arinen, 2005) to discrete\ndata types. Both methods amount to weighted differences of\nthe gradient of the RBM free energy13 evaluated at a data\npoint and at neighboring points. One potential drawback of\nthese methods is that depending on the parametrization of\nthe energy function, their computational requirements may\nscale up to O(nd) worse than CD, SML, FPCD, or denoising\nscore matching (Kingma and LeCun, 2010; Vincent, 2011),\ndiscussed below. Marlin et al. (2010) empirically compared all\nof these methods (except denoising score matching) on a range\nof classi\ufb01cation, reconstruction and density modeling tasks and\nfound that, in general, SML provided the best combination of\noverall performance and computational tractability. However,\nin a later study, the same authors (Swersky et al., 2011)\n13. The free energy F(x; \u03b8) is the energy associated with the data marginal\nprobability, F(x; \u03b8) = \u2212log P(x) \u2212log Z\u03b8 and is tractable for the RBM.\n\n13\nfound denoising score matching to be a competitive inductive\nprinciple both in terms of classi\ufb01cation performance (with\nrespect to SML) and in terms of computational ef\ufb01ciency (with\nrespect to analytically obtained score matching). Denoising\nscore matching is a special case of the denoising auto-encoder\ntraining criterion (Section 7.2.2) when the reconstruction error\nresidual equals a gradient, i.e., the score function associated\nwith an energy function, as shown in (Vincent, 2011).\nIn the spirit of the Boltzmann machine gradient (Eq. 15)\nseveral approaches have been proposed to train energy-based\nmodels. One is noise-contrastive estimation (Gutmann and Hy-\nvarinen, 2010), in which the training criterion is transformed\ninto a probabilistic classi\ufb01cation problem: distinguish between\n(positive) training examples and (negative) noise samples\ngenerated by a broad distribution (such as the Gaussian).\nAnother family of approaches, more in the spirit of Contrastive\nDivergence, relies on distinguishing positive examples (of\nthe training distribution) and negative examples obtained by\nperturbations of the positive examples (Collobert and Weston,\n2008; Bordes et al., 2012; Weston et al., 2010).\n7\nDIRECTLY LEARNING A PARAMETRIC MAP\nFROM INPUT TO REPRESENTATION\nWithin the framework of probabilistic models adopted in\nSection 6, the learned representation is always associated with\nlatent variables, speci\ufb01cally with their posterior distribution\ngiven an observed input x. Unfortunately, this posterior dis-\ntribution tends to become very complicated and intractable if\nthe model has more than a couple of interconnected layers,\nwhether in the directed or undirected graphical model frame-\nworks. It then becomes necessary to resort to sampling or\napproximate inference techniques, and to pay the associated\ncomputational and approximation error price. If the true pos-\nterior has a large number of modes that matter then current\ninference techniques may face an unsurmountable challenge or\nendure a potentially serious approximation. This is in addition\nto the dif\ufb01culties raised by the intractable partition function in\nundirected graphical models. Moreover a posterior distribution\nover latent variables is not yet a simple usable feature vector\nthat can for example be fed to a classi\ufb01er. So actual feature\nvalues are typically derived from that distribution, taking the\nlatent variable\u2019s expectation (as is typically done with RBMs),\ntheir marginal probability, or \ufb01nding their most likely value\n(as in sparse coding). If we are to extract stable deterministic\nnumerical feature values in the end anyway, an alternative\n(apparently) non-probabilistic feature learning paradigm that\nfocuses on carrying out this part of the computation, very ef\ufb01-\nciently, is that of auto-encoders and other directly parametrized\nfeature or representation functions. The commonality between\nthese methods is that they learn a direct encoding, i.e., a\nparametric map from inputs to their representation.\nRegularized auto-encoders, discussed next, also involve\nlearning a decoding function that maps back from represen-\ntation to input space. Sections 8.1 and 11.3 discuss direct\nencoding methods that do not require a decoder, such as semi-\nsupervised embedding (Weston et al., 2008) and slow feature\nanalysis (Wiskott and Sejnowski, 2002).\n7.1\nAuto-Encoders\nIn the auto-encoder framework (LeCun, 1987; Bourlard and\nKamp, 1988; Hinton and Zemel, 1994), one starts by ex-\nplicitly de\ufb01ning a feature-extracting function in a speci\ufb01c\nparametrized closed form. This function, that we will denote\nf\u03b8, is called the encoder and will allow the straightforward\nand ef\ufb01cient computation of a feature vector h = f\u03b8(x)\nfrom an input x. For each example x(t) from a data set\n{x(1), . . . , x(T )}, we de\ufb01ne\nh(t) = f\u03b8(x(t))\n(18)\nwhere h(t) is the feature-vector or representation or code com-\nputed from x(t). Another closed form parametrized function\ng\u03b8, called the decoder, maps from feature space back into\ninput space, producing a reconstruction r = g\u03b8(h). Whereas\nprobabilistic models are de\ufb01ned from an explicit probability\nfunction and are trained to maximize (often approximately) the\ndata likelihood (or a proxy), auto-encoders are parametrized\nthrough their encoder and decoder and are trained using a\ndifferent training principle. The set of parameters \u03b8 of the\nencoder and decoder are learned simultaneously on the task\nof reconstructing as well as possible the original input, i.e.\nattempting to incur the lowest possible reconstruction error\nL(x, r) \u2013 a measure of the discrepancy between x and its\nreconstruction r \u2013 over training examples. Good generalization\nmeans low reconstruction error at test examples, while having\nhigh reconstruction error for most other x con\ufb01gurations. To\ncapture the structure of the data-generating distribution, it\nis therefore important that something in the training crite-\nrion or the parametrization prevents the auto-encoder from\nlearning the identity function, which has zero reconstruction\nerror everywhere. This is achieved through various means in\nthe different forms of auto-encoders, as described below in\nmore detail, and we call these regularized auto-encoders. A\nparticular form of regularization consists in constraining the\ncode to have a low dimension, and this is what the classical\nauto-encoder or PCA do.\nIn summary, basic auto-encoder training consists in \ufb01nding\na value of parameter vector \u03b8 minimizing reconstruction error\nJAE(\u03b8)\n=\nX\nt\nL(x(t), g\u03b8(f\u03b8(x(t))))\n(19)\nwhere x(t) is a training example. This minimization is usually\ncarried out by stochastic gradient descent as in the training\nof Multi-Layer-Perceptrons (MLPs). Since auto-encoders were\nprimarily developed as MLPs predicting their input, the most\ncommonly used forms for the encoder and decoder are af\ufb01ne\nmappings, optionally followed by a non-linearity:\nf\u03b8(x)\n=\nsf(b + Wx)\n(20)\ng\u03b8(h)\n=\nsg(d + W \u2032h)\n(21)\nwhere sf and sg are the encoder and decoder activation\nfunctions (typically the element-wise sigmoid or hyperbolic\ntangent non-linearity, or the identity function if staying linear).\nThe set of parameters of such a model is \u03b8 = {W, b, W \u2032, d}\nwhere b and d are called encoder and decoder bias vectors,\nand W and W \u2032 are the encoder and decoder weight matrices.\nThe choice of sg and L depends largely on the input domain\nrange and nature, and are usually chosen so that L returns a\nnegative log-likelihood for the observed value of x. A natural\nchoice for an unbounded domain is a linear decoder with a\n\n14\nsquared reconstruction error, i.e. sg(a) = a and L(x, r) =\n\u2225x \u2212r\u22252. If inputs are bounded between 0 and 1 however,\nensuring a similarly-bounded reconstruction can be achieved\nby using sg = sigmoid. In addition if the inputs are of a binary\nnature, a binary cross-entropy loss14 is sometimes used.\nIf both encoder and decoder use a sigmoid non-linearity,\nthen f\u03b8(x) and g\u03b8(h) have the exact same form as the\nconditionals P(h | v) and P(v | h) of binary RBMs (see\nSection 6.2). This similarity motivated an initial study (Bengio\net al., 2007) of the possibility of replacing RBMs with auto-\nencoders as the basic pre-training strategy for building deep\nnetworks, as well as the comparative analysis of auto-encoder\nreconstruction error gradient and contrastive divergence up-\ndates (Bengio and Delalleau, 2009).\nOne notable difference in the parametrization is that RBMs\nuse a single weight matrix, which follows naturally from their\nenergy function, whereas the auto-encoder framework allows\nfor a different matrix in the encoder and decoder. In practice\nhowever, weight-tying in which one de\ufb01nes W \u2032 = W T may\nbe (and is most often) used, rendering the parametrizations\nidentical. The usual training procedures however differ greatly\nbetween the two approaches. A practical advantage of training\nauto-encoder variants is that they de\ufb01ne a simple tractable\noptimization objective that can be used to monitor progress.\nIn the case of a linear auto-encoder (linear encoder and\ndecoder) with squared reconstruction error, minimizing Eq. 19\nlearns the same subspace15 as PCA. This is also true when\nusing a sigmoid nonlinearity in the encoder (Bourlard and\nKamp, 1988), but not if the weights W and W \u2032 are tied\n(W \u2032 = W T ), because W cannot be forced into being small\nand W \u2032 large to achieve a linear encoder.\nSimilarly, Le et al. (2011b) recently showed that adding a\nregularization term of the form P\nt\nP\nj s3(Wjx(t)) to a linear\nauto-encoder with tied weights, where s3 is a nonlinear convex\nfunction, yields an ef\ufb01cient algorithm for learning linear ICA.\n7.2\nRegularized Auto-Encoders\nLike PCA, auto-encoders were originally seen as a dimen-\nsionality reduction technique and thus used a bottleneck, i.e.\ndh < dx. On the other hand, successful uses of sparse\ncoding and RBM approaches tend to favour overcomplete\nrepresentations, i.e. dh\n> dx. This can allow the auto-\nencoder to simply duplicate the input in the features, with\nperfect reconstruction without having extracted more mean-\ningful features. Recent research has demonstrated very suc-\ncessful alternative ways, called regulrized auto-encoders, to\n\u201cconstrain\u201d the representation, even when it is overcomplete.\nThe effect of a bottleneck or of this regularization is that the\nauto-encoder cannot reconstruct well everything, it is trained\nto reconstruct well the training examples and generalization\nmeans that reconstruction error is also small on test examples.\nAn interesting justi\ufb01cation (Ranzato et al., 2008) for the\nsparsity penalty (or any penalty that restricts in a soft way\n14. L(x, r) = \u2212Pdx\ni=1 xi log(ri) + (1 \u2212xi) log(1 \u2212ri)\n15. Contrary to traditional PCA loading factors, but similarly to the\nparameters learned by probabilistic PCA, the weight vectors learned by a\nlinear auto-encoder are not constrained to form an orthonormal basis, nor to\nhave a meaningful ordering. They will however span the same subspace.\nthe volume of hidden con\ufb01gurations easily accessible by the\nlearner) is that it acts in spirit like the partition function of\nRBMs, by making sure that only few input con\ufb01gurations can\nhave a low reconstruction error.\nAlternatively, one can view the objective of the regulariza-\ntion applied to an auto-encoder as making the representation\nas \u201cconstant\u201d (insensitive) as possible with respect to changes\nin input. This view immediately justi\ufb01es two variants of\nregularized auto-encoders described below: contractive auto-\nencoders reduce the number of effective degrees of freedom of\nthe representation (around each point) by making the encoder\ncontractive, i.e., making the derivative of the encoder small\n(thus making the hidden units saturate), while the denoising\nauto-encoder makes the whole mapping \u201crobust\u201d, i.e., insen-\nsitive to small random perturbations, or contractive, making\nsure that the reconstruction cannot stay good when moving in\nmost directions around a training example.\n7.2.1\nSparse Auto-Encoders\nThe earliest uses of single-layer auto-encoders for building\ndeep architectures by stacking them (Bengio et al., 2007)\nconsidered the idea of tying the encoder weights and decoder\nweights to restrict capacity as well as the idea of introducing a\nform of sparsity regularization (Ranzato et al., 2007). Sparsity\nin the representation can be achieved by penalizing the hidden\nunit biases (making these additive offset parameters more\nnegative) (Ranzato et al., 2007; Lee et al., 2008; Goodfellow\net al., 2009; Larochelle and Bengio, 2008) or by directly\npenalizing the output of the hidden unit activations (making\nthem closer to their saturating value at 0) (Ranzato et al.,\n2008; Le et al., 2011a; Zou et al., 2011). Penalizing the bias\nruns the danger that the weights could compensate for the\nbias, which could hurt numerical optimization. When directly\npenalizing the hidden unit outputs, several variants can be\nfound in the literature, but a clear comparative analysis is\nstill lacking. Although the L1 penalty (i.e., simply the sum\nof output elements hj in the case of sigmoid non-linearity)\nwould seem the most natural (because of its use in sparse cod-\ning), it is used in few papers involving sparse auto-encoders.\nA close cousin of the L1 penalty is the Student-t penalty\n(log(1+h2\nj)), originally proposed for sparse coding (Olshausen\nand Field, 1996). Several papers penalize the average output\n\u00afhj (e.g. over a minibatch), and instead of pushing it to 0,\nencourage it to approach a \ufb01xed target, either through a mean-\nsquare error penalty, or maybe more sensibly (because hj\nbehaves like a probability), a Kullback-Liebler divergence\nwith respect to the binomial distribution with probability \u03c1:\n\u2212\u03c1 log \u00afhj \u2212(1\u2212\u03c1) log(1\u2212\u00afhj)+constant, e.g., with \u03c1 = 0.05.\n7.2.2\nDenoising Auto-Encoders\nVincent et al. (2008, 2010) proposed altering the training ob-\njective in Eq. 19 from mere reconstruction to that of denoising\nan arti\ufb01cially corrupted input, i.e. learning to reconstruct the\nclean input from a corrupted version. Learning the identity\nis no longer enough: the learner must capture the structure\nof the input distribution in order to optimally undo the effect\nof the corruption process, with the reconstruction essentially\nbeing a nearby but higher density point than the corrupted\ninput. Figure 3 illustrates that the Denoising Auto-Encoder\n\n15\n(DAE) is learning a reconstruction function that corresponds\nto a vector \ufb01eld pointing towards high-density regions (the\nmanifold where examples concentrate).\nCorrupted input \nCorrupted input \nprior:&examples&concentrate&\nnear&a&lower&dimensional&\n\u201cmanifold\u201d&&\noriginal  \ninput \nFig. 3.\nWhen data concentrate near a lower-dimensional\nmanifold, the corruption vector is typically almost orthogonal to\nthe manifold, and the reconstruction function learns to denoise,\nmap from low-probability con\ufb01gurations (corrupted inputs) to\nhigh-probability ones (original inputs), creating a vector \ufb01eld\naligned with the score (derivative of the estimated density).\n.\nFormally, the objective optimized by a DAE is:\nJDAE\n=\nX\nt\nEq(\u02dcx|x(t))\nh\nL(x(t), g\u03b8(f\u03b8(\u02dcx)))\ni\n(22)\nwhere Eq(\u02dcx|x(t)) [\u00b7] averages over corrupted examples \u02dcx drawn\nfrom corruption process q(\u02dcx|x(t)). In practice this is optimized\nby stochastic gradient descent, where the stochastic gradient is\nestimated by drawing one or a few corrupted versions of x(t)\neach time x(t) is considered. Corruptions considered in Vin-\ncent et al. (2010) include additive isotropic Gaussian noise,\nsalt and pepper noise for gray-scale images, and masking\nnoise (salt or pepper only), e.g., setting some randomly chosen\ninputs to 0 (independently per example). Masking noise has\nbeen used in most of the simulations. Qualitatively better\nfeatures are reported with denoising, resulting in improved\nclassi\ufb01cation, and DAE features performed similarly or better\nthan RBM features. Chen et al. (2012) show that a simpler\nalternative with a closed form solution can be obtained when\nrestricting to a linear auto-encoder and have successfully\napplied it to domain adaptation.\nVincent (2011) relates DAEs to energy-based probabilistic\nmodels: DAEs basically learn in r(\u02dcx) \u2212\u02dcx a vector pointing\nin the direction of the estimated score \u2202log p(\u02dcx)\n\u2202\u02dcx\n(Figure 3). In\nthe special case of linear reconstruction and squared error,\nVincent (2011) shows that training an af\ufb01ne-sigmoid-af\ufb01ne\nDAE amounts to learning an energy-based model, whose\nenergy function is very close to that of a GRBM. Training uses\na regularized variant of the score matching parameter estima-\ntion technique (Hyv\u00a8arinen, 2005; Hyv\u00a8arinen, 2008; Kingma\nand LeCun, 2010) termed denoising score matching (Vincent,\n2011). Swersky (2010) had shown that training GRBMs with\nscore matching is equivalent to training a regular auto-encoder\nwith an additional regularization term, while, following up on\nthe theoretical results in Vincent (2011), Swersky et al. (2011)\nshowed the practical advantage of denoising to implement\nscore matching ef\ufb01ciently. Finally Alain and Bengio (2012)\ngeneralize Vincent (2011) and prove that DAEs of arbitrary\nparametrization with small Gaussian corruption noise are\ngeneral estimators of the score.\n7.2.3\nContractive Auto-Encoders\nContractive Auto-Encoders (CAE), proposed by Rifai et al.\n(2011a), follow up on Denoising Auto-Encoders (DAE) and\nshare a similar motivation of learning robust representations.\nCAEs achieve this by adding an analytic contractive penalty\nto Eq. 19: the Frobenius norm of the encoder\u2019s Jacobian,\nand results in penalizing the sensitivity of learned features to\nin\ufb01nitesimal input variations. Let J(x) = \u2202f\u03b8\n\u2202x (x) the Jacobian\nmatrix of the encoder at x. The CAE\u2019s training objective is\nJCAE\n=\nX\nt\nL(x(t), g\u03b8(f\u03b8(x(t)))) + \u03bb\n\r\r\rJ(x(t))\n\r\r\r\n2\nF\n(23)\nwhere \u03bb is a hyper-parameter controlling the strength of the\nregularization. For an af\ufb01ne sigmoid encoder, the contractive\npenalty term is easy to compute:\nJj(x)\n=\nf\u03b8(x)j(1 \u2212f\u03b8(x)j)Wj\n\u2225J(x)\u22252\nF\n=\nX\nj\n(f\u03b8(x)j(1 \u2212f\u03b8(x)j))2\u2225Wj\u22252\n(24)\nThere are at least three notable differences with DAEs, which\nmay be partly responsible for the better performance that CAE\nfeatures seem to empirically demonstrate: (a) the sensitivity of\nthe features is penalized16 rather than that of the reconstruc-\ntion; (b) the penalty is analytic rather than stochastic: an ef\ufb01-\nciently computable expression replaces what might otherwise\nrequire dx corrupted samples to size up (i.e. the sensitivity in\ndx directions); (c) a hyper-parameter \u03bb allows a \ufb01ne control of\nthe trade-off between reconstruction and robustness (while the\ntwo are mingled in a DAE). Note however that there is a tight\nconnection between the DAE and the CAE: as shown in (Alain\nand Bengio, 2012) a DAE with small corruption noise can be\nseen (through a Taylor expansion) as a type of contractive\nauto-encoder where the contractive penalty is on the whole\nreconstruction function rather than just on the encoder17.\nA potential disadvantage of the CAE\u2019s analytic penalty is\nthat it amounts to only encouraging robustness to in\ufb01nitesimal\ninput variations. This is remedied in Rifai et al. (2011b) with\nthe CAE+H, that penalizes all higher order derivatives, in an\nef\ufb01cient stochastic manner, by adding a term that encourages\nJ(x) and J(x + \u03f5) to be close:\nJCAE+H\n=\nX\nt\nL(x(t), g\u03b8(x(t))) + \u03bb\n\r\r\rJ(x(t))\n\r\r\r\n2\nF\n+\u03b3E\u03f5\n\u0002\n\u2225J(x) \u2212J(x + \u03f5)\u22252\nF\n\u0003\n(25)\nwhere \u03f5 \u223cN(0, \u03c32I), and \u03b3 is the associated regularization\nstrength hyper-parameter.\nThe DAE and CAE have been successfully used to win\nthe \ufb01nal phase of the Unsupervised and Transfer Learning\nChallenge (Mesnil et al., 2011). The representation learned\nby the CAE tends to be saturated rather than sparse, i.e.,\nmost hidden units are near the extremes of their range (e.g. 0\nor 1), and their derivative \u2202hi(x)\n\u2202x\nis near 0. The non-saturated\nunits are few and sensitive to the inputs, with their associated\n\ufb01lters (weight vectors) together forming a basis explaining\nthe local changes around x, as discussed in Section 8.2.\nAnother way to get saturated (nearly binary) units is semantic\nhashing (Salakhutdinov and Hinton, 2007).\n16. i.e., the robustness of the representation is encouraged.\n17. but note that in the CAE, the decoder weights are tied to the encoder\nweights, to avoid degenerate solutions, and this should also make the decoder\ncontractive.\n\n16\n7.2.4\nPredictive Sparse Decomposition\nSparse coding (Olshausen and Field, 1996) may be viewed as a\nkind of auto-encoder that uses a linear decoder with a squared\nreconstruction error, but whose non-parametric encoder f\u03b8\nperforms the comparatively non-trivial and relatively costly\niterative minimization of Eq. 2. A practically successful variant\nof sparse coding and auto-encoders, named Predictive Sparse\nDecomposition or PSD (Kavukcuoglu et al., 2008) replaces\nthat costly and highly non-linear encoding step by a fast\nnon-iterative approximation during recognition (computing the\nlearned features). PSD has been applied to object recognition\nin images and video (Kavukcuoglu et al., 2009, 2010; Jarrett\net al., 2009), but also to audio (Henaff et al., 2011), mostly\nwithin the framework of multi-stage convolutional deep archi-\ntectures (Section 11.2). The main idea can be summarized\nby the following equation for the training criterion, which\nis simultaneously optimized with respect to hidden codes\n(representation) h(t) and with respect to parameters (W, \u03b1):\nJPSD =\nX\nt\n\u03bb\u2225h(t)\u22251 + \u2225x(t) \u2212Wh(t)\u22252\n2 + \u2225h(t) \u2212f\u03b1(x(t))\u22252\n2 (26)\nwhere x(t) is the input vector for example t, h(t) is the\noptimized hidden code for that example, and f\u03b1(\u00b7) is the\nencoding function, the simplest variant being\nf\u03b1(x(t)) = tanh(b + W T x(t))\n(27)\nwhere encoding weights are the transpose of decoding\nweights. Many variants have been proposed, including the\nuse of a shrinkage operation instead of the hyperbolic tan-\ngent (Kavukcuoglu et al., 2010). Note how the L1 penalty on\nh tends to make them sparse, and how this is the same criterion\nas sparse coding with dictionary learning (Eq. 3) except for the\nadditional constraint that one should be able to approximate\nthe sparse codes h with a parametrized encoder f\u03b1(x). One can\nthus view PSD as an approximation to sparse coding, where we\nobtain a fast approximate encoder. Once PSD is trained, object\nrepresentations f\u03b1(x) are used to feed a classi\ufb01er. They are\ncomputed quickly and can be further \ufb01ne-tuned: the encoder\ncan be viewed as one stage or one layer of a trainable multi-\nstage system such as a feedforward neural network.\nPSD can also be seen as a kind of auto-encoder where\nthe codes h are given some freedom that can help to further\nimprove reconstruction. One can also view the encoding\npenalty added on top of sparse coding as a kind of regularizer\nthat forces the sparse codes to be nearly computable by a\nsmooth and ef\ufb01cient encoder. This is in contrast with the codes\nobtained by complete optimization of the sparse coding crite-\nrion, which are highly non-smooth or even non-differentiable,\na problem that motivated other approaches to smooth the\ninferred codes of sparse coding (Bagnell and Bradley, 2009),\nso a sparse coding stage could be jointly optimized along with\nfollowing stages of a deep architecture.\n8\nREPRESENTATION\nLEARNING\nAS\nMANI-\nFOLD LEARNING\nAnother important perspective on representation learning is\nbased on the geometric notion of manifold. Its premise is\nthe manifold hypothesis, according to which real-world data\npresented in high dimensional spaces are expected to con-\ncentrate in the vicinity of a manifold M of much lower\ndimensionality dM, embedded in high dimensional input space\nRdx. This prior seems particularly well suited for AI tasks\nsuch as those involving images, sounds or text, for which\nmost uniformly sampled input con\ufb01gurations are unlike natural\nstimuli. As soon as there is a notion of \u201crepresentation\u201d\nthen one can think of a manifold by considering the vari-\nations in input space, which are captured by or re\ufb02ected\n(by corresponding changes) in the learned representation.\nTo \ufb01rst approximation, some directions are well preserved\n(the tangent directions of the manifold) while others aren\u2019t\n(directions orthogonal to the manifolds). With this perspec-\ntive, the primary unsupervised learning task is then seen as\nmodeling the structure of the data-supporting manifold18. The\nassociated representation being learned can be associated with\nan intrinsic coordinate system on the embedded manifold. The\narchetypal manifold modeling algorithm is, not surprisingly,\nalso the archetypal low dimensional representation learning\nalgorithm: Principal Component Analysis, which models a\nlinear manifold. It was initially devised with the objective\nof \ufb01nding the closest linear manifold to a cloud of data\npoints. The principal components, i.e. the representation f\u03b8(x)\nthat PCA yields for an input point x, uniquely locates its\nprojection on that manifold: it corresponds to intrinsic co-\nordinates on the manifold. Data manifold for complex real\nworld domains are however expected to be strongly non-\nlinear. Their modeling is sometimes approached as patchworks\nof locally linear tangent spaces (Vincent and Bengio, 2003;\nBrand, 2003). The large majority of algorithms built on\nthis geometric perspective adopt a non-parametric approach,\nbased on a training set nearest neighbor graph (Sch\u00a8olkopf\net al., 1998; Roweis and Saul, 2000; Tenenbaum et al., 2000;\nBrand, 2003; Belkin and Niyogi, 2003; Donoho and Grimes,\n2003; Weinberger and Saul, 2004; Hinton and Roweis, 2003;\nvan der Maaten and Hinton, 2008). In these non-parametric\napproaches, each high-dimensional training point has its own\nset of free low-dimensional embedding coordinates, which are\noptimized so that certain properties of the neighborhood graph\ncomputed in original high dimensional input space are best\npreserved. These methods however do not directly learn a\nparametrized feature extraction function f\u03b8(x) applicable to\nnew test points19, which seriously limits their use as feature\nextractors, except in a transductive setting. Comparatively few\nnon-linear manifold learning methods have been proposed,\nthat learn a parametric map that can directly compute a\nrepresentation for new points; we will focus on these.\n8.1\nLearning a parametric mapping based on a\nneighborhood graph\nSome of the above non-parametric manifold learning al-\ngorithms can be modi\ufb01ed to learn a parametric mapping\nf\u03b8, i.e., applicable to new points: instead of having free\n18. Actually, data points need not strictly lie on the \u201cmanifold\u201d, but the\nprobability density is expected to fall off sharply as one moves away from it,\nand it may actually be constituted of several possibly disconnected manifolds\nwith different intrinsic dimensionality.\n19. For several of these techniques, representations for new points can\nbe computed using the Nystr\u00a8om approximation as has been proposed as\nan extension in (Bengio et al., 2004), but this remains cumbersome and\ncomputationally expensive.\n\n17\nlow-dimensional embedding coordinate \u201cparameters\u201d for each\ntraining point, these coordinates are obtained through an\nexplicitly parametrized function, as with the parametric vari-\nant (van der Maaten, 2009) of t-SNE (van der Maaten and\nHinton, 2008).\nInstead, Semi-Supervised Embedding (Weston et al., 2008)\nlearns a direct encoding while taking into account the manifold\nhypothesis through a neighborhood graph. A parametrized\nneural network architecture simultaneously learns a manifold\nembedding and a classi\ufb01er. The training criterion encourages\ntraining set neigbhors to have similar representations.\nThe reduced and tightly controlled number of free param-\neters in such parametric methods, compared to their pure\nnon-parametric counterparts, forces models to generalize the\nmanifold shape non-locally (Bengio et al., 2006b), which can\ntranslate into better features and \ufb01nal performance (van der\nMaaten and Hinton, 2008). However, basing the modeling of\nmanifolds on training set neighborhood relationships might\nbe risky statistically in high dimensional spaces (sparsely\npopulated due to the curse of dimensionality) as e.g. most\nEuclidean nearest neighbors risk having too little in common\nsemantically. The nearest neighbor graph is simply not enough\ndensely populated to map out satisfyingly the wrinkles of\nthe target manifold (Bengio and Monperrus, 2005; Bengio\net al., 2006b; Bengio and LeCun, 2007). It can also become\nproblematic computationally to consider all pairs of data\npoints20, which scales quadratically with training set size.\n8.2\nLearning to represent non-linear manifolds\nCan we learn a manifold without requiring nearest neighbor\nsearches? Yes, for example, with regularized auto-encoders or\nPCA. In PCA, the sensitivity of the extracted components (the\ncode) to input changes is the same regardless of position x.\nThe tangent space is the same everywhere along the linear\nmanifold. By contrast, for a non-linear manifold, the tangent\nof the manifold changes as we move on the manifold, as\nillustrated in Figure 6. In non-linear representation-learning\nalgorithms it is convenient to think about the local variations\nin the representation as the input x is varied on the manifold,\ni.e., as we move among high-probability con\ufb01gurations. As\nwe discuss below, the \ufb01rst derivative of the encoder therefore\nspeci\ufb01es the shape of the manifold (its tangent plane) around\nan example x lying on it. If the density was really concentrated\non the manifold, and the encoder had captured that, we\nwould \ufb01nd the encoder derivatives to be non-zero only in the\ndirections spanned by the tangent plane.\nLet us consider sparse coding in this light: parameter matrix\nW may be interpreted as a dictionary of input directions from\nwhich a different subset will be picked to model the local\ntangent space at an x on the manifold. That subset corresponds\nto the active, i.e. non-zero, features for input x. Non-zero\ncomponent hi will be sensitive to small changes of the input\nin the direction of the associated weight vector W:,i, whereas\ninactive features are more likely to be stuck at 0 until a\nsigni\ufb01cant displacement has taken place in input space.\n20. Even if pairs are picked stochastically, many must be considered before\nobtaining one that weighs signi\ufb01cantly on the optimization objective.\nThe Local Coordinate Coding (LCC) algorithm (Yu et al.,\n2009) is very similar to sparse coding, but is explicitly derived\nfrom a manifold perspective. Using the same notation as that\nof sparse coding in Eq. 2, LCC replaces regularization term\n\u2225h(t)\u22251 = P\nj |h(t)\nj | yielding objective\nJLCC =\nX\nt\n \n\u2225x(t) \u2212Wh(t)\u22252\n2 + \u03bb\nX\nj\n|h(t)\nj |\u2225W:,j \u2212x(t)\u22251+p\n!\n(28)\nThis is identical to sparse coding when p = \u22121, but with\nlarger p it encourages the active anchor points for x(t) (i.e.\nthe codebook vectors W:,j with non-negligible |h(t)\nj | that\nare combined to reconstruct x(t)) to be not too far from\nx(t), hence the local aspect of the algorithm. An important\ntheoretical contribution of Yu et al. (2009) is to show that\nthat any Lipschitz-smooth function \u03c6 : M \u2192R de\ufb01ned on a\nsmooth nonlinear manifold M embedded in Rdx can be well\napproximated by a globally linear function with respect to the\nresulting coding scheme (i.e. linear in h), where the accuracy\nof the approximation and required number dh of anchor points\ndepend on dM rather than dx. This result has been further\nextended with the use of local tangent directions (Yu and\nZhang, 2010), as well as to multiple layers (Lin et al., 2010).\nLet us now consider the ef\ufb01cient non-iterative \u201cfeed-\nforward\u201d encoders f\u03b8, used by PSD and the auto-encoders\nreviewed in Section 7.2, that are in the form of Eq. 20 or\n27.The computed representation for x will be only signi\ufb01-\ncantly sensitive to input space directions associated with non-\nsaturated hidden units (see e.g. Eq. 24 for the Jacobian of a\nsigmoid layer). These directions to which the representation\nis signi\ufb01cantly sensitive, like in the case of PCA or sparse\ncoding, may be viewed as spanning the tangent space of the\nmanifold at training point x.\nInput\"Point\"\nTangents\"\nFig. 4.\nThe tangent vectors to the high-density manifold as\nestimated by a Contractive Auto-Encoder (Rifai et al., 2011a).\nThe original input is shown on the top left. Each tangent vector\n(images on right side of \ufb01rst row) corresponds to a plausible\nadditive deformation of the original input, as illustrated on the\nsecond row, where a bit of the 3rd singular vector is added to\nthe original, to form a translated and deformed image. Unlike\nin PCA, the tangent vectors are different for different inputs,\nbecause the estimated manifold is highly non-linear.\n. Rifai et al. (2011a) empirically analyze in this light the\nsingular value spectrum of the Jacobian (derivatives of rep-\nresentation vector with respect to input vector) of a trained\nCAE. Here the SVD provides an ordered orthonormal basis of\nmost sensitive directions. The spectrum is sharply decreasing,\nindicating a relatively small number of signi\ufb01cantly sensi-\ntive directions. This is taken as empirical evidence that the\nCAE indeed modeled the tangent space of a low-dimensional\nmanifold. The leading singular vectors form a basis for the\ntangent plane of the estimated manifold, as illustrated in\nFigure 4. The CAE criterion is believed to achieve this thanks\nto its two opposing terms: the isotropic contractive penalty,\n\n18\nthat encourages the representation to be equally insensitive to\nchanges in any input directions, and the reconstruction term,\nthat pushes different training points (in particular neighbors) to\nhave a different representation (so they may be reconstructed\naccurately), thus counteracting the isotropic contractive pres-\nsure only in directions tangent to the manifold.\nAnalyzing learned representations through the lens of the\nspectrum of the Jacobian and relating it to the notion of tangent\nspace of a manifold is feasible, whenever the mapping is\ndifferentiable, and regardless of how it was learned, whether\nas direct encoding (as in auto-encoder variants), or derived\nfrom latent variable inference (as in sparse coding or RBMs).\nExact low dimensional manifold models (like PCA) would\nyield non-zero singular values associated to directions along\nthe manifold, and exact zeros for directions orthogonal to the\nmanifold. But in smooth models like the CAE or the RBM we\nwill instead have large versus relatively small singular values\n(as opposed to non-zero versus exactly zero).\n8.3\nLeveraging the modeled tangent spaces\nThe local tangent space, at a point along the manifold, can\nbe thought of capturing locally valid transformations that\nwere prominent in the training data. For example Rifai et al.\n(2011c) examine the tangent directions extracted with an SVD\nof the Jacobian of CAEs trained on digits, images, or text-\ndocument data: they appear to correspond to small transla-\ntions or rotations for images or digits, and to substitutions\nof words within a same theme for documents. Such very\nlocal transformations along a data manifold are not expected\nto change class identity. To build their Manifold Tangent\nClassi\ufb01er (MTC), Rifai et al. (2011c) then apply techniques\nsuch as tangent distance (Simard et al., 1993) and tangent\npropagation (Simard et al., 1992), that were initially developed\nto build classi\ufb01ers that are insensitive to input deformations\nprovided as prior domain knowledge. Now these techniques\nare applied using the local leading tangent directions extracted\nby a CAE, i.e. not using any prior domain knowledge (except\nthe broad prior about the existence of a manifold). This\napproach set a new record for MNIST digit classi\ufb01cation\namong prior-knowledge free approaches21.\n9\nCONNECTIONS\nBETWEEN PROBABILISTIC\nAND DIRECT ENCODING MODELS\nThe standard likelihood framework for probabilistic mod-\nels decomposes the training criterion for models with pa-\nrameters \u03b8 in two parts: the log-likelihood log P(x|\u03b8) (or\nlog P(x|h, \u03b8) with latent variables h), and the prior log P(\u03b8)\n(or log P(h|\u03b8) + log P(\u03b8) with latent variables).\n9.1\nPSD: a probabilistic interpretation\nIn the case of the PSD algorithm, a connection can be made\nbetween the above standard probabilistic view and the direct\nencoding computation graph. The probabilistic model of PSD\nis the same directed generative model P(x|h) of sparse coding\n(Section 6.1.1), which only accounts for the decoder. The\nencoder is viewed as an approximate inference mechanism to\n21. It yielded 0.81% error rate using the full MNIST training set, with no\nprior deformations, and no convolution.\nguess P(h|x) and initialize a MAP iterative inference (where\nthe sparse prior P(h) is taken into account). However, in\nPSD, the encoder is trained jointly with the decoder, rather\nthan simply taking the end result of iterative inference as a\ntarget to approximate. An interesting view22 to reconcile these\nfacts is that the encoder is a parametric approximation for\nthe MAP solution of a variational lower bound on the joint\nlog-likelihood. When MAP learning is viewed as a special\ncase of variational learning (where the approximation of the\njoint log-likelihood is with a dirac distribution located at the\nMAP solution), the variational recipe tells us to simultaneously\nimprove the likelihood (reduce reconstruction error) and im-\nprove the variational approximation (reduce the discrepancy\nbetween the encoder output and the latent variable value).\nHence PSD sits at the intersection of probabilistic models\n(with latent variables) and direct encoding methods (which\ndirectly parametrize the mapping from input to representation).\nRBMs also sit at the intersection because their particular\nparametrization includes an explicit mapping from input to\nrepresentation, thanks to the restricted connectivity between\nhidden units. However, this nice property does not extend\nto their natural deep generalizations, i.e., Deep Boltzmann\nMachines, discussed in Section 10.2.\n9.2\nRegularized\nAuto-Encoders\nCapture\nLocal\nStructure of the Density\nCan we also say something about the probabilistic interpreta-\ntion of regularized auto-encoders? Their training criterion does\nnot \ufb01t the standard likelihood framework because this would\ninvolve a data-dependent \u201cprior\u201d. An interesting hypothesis\nemerges to answer that question, out of recent theoretical\nresults (Vincent, 2011; Alain and Bengio, 2012): the training\ncriterion of regularized auto-encoders, instead of being a form\nof maximum likelihood, corresponds to a different inductive\nprinciple, such as score matching. The score matching con-\nnection is discussed in Section 7.2.2 and has been shown for\na particular parametrization of DAE and equivalent Gaussian\nRBM (Vincent, 2011). The work in Alain and Bengio (2012)\ngeneralizes this idea to a broader class of parametrizations (ar-\nbitrary encoders and decoders), and shows that by regularizing\nthe auto-encoder so that it be contractive, one obtains that the\nreconstruction function and its derivative estimate \ufb01rst and\nsecond derivatives of the underlying data-generative density.\nThis view can be exploited to successfully sample from auto-\nencoders, as shown in Rifai et al. (2012); Bengio et al. (2012).\nThe proposed sampling algorithms are MCMCs similar to\nLangevin MCMC, using not just the estimated \ufb01rst derivative\nof the density but also the estimated manifold tangents so as\nto stay close to manifolds of high density.\nThis interpretation connects well with the geometric per-\nspective introduced in Section 8. The regularization effects\n(e.g., due to a sparsity regularizer, a contractive regularizer,\nor the denoising criterion) asks the learned representation to\nbe as insensitive as possible to the input, while minimiz-\ning reconstruction error on the training examples forces the\nrepresentation to contain just enough information to distin-\n22. suggested by Ian Goodfellow, personal communication\n\n19\nx\"\nr(x)\"\nx1\"\nx2\"\nx3\"\nFig. 5.\nReconstruction function r(x) (green) learned by a\nhigh-capacity autoencoder on 1-dimensional input, minimizing\nreconstruction error at training examples x(t) (r(x(t)) in red)\nwhile trying to be as constant as possible otherwise. The dotted\nline is the identity reconstruction (which might be obtained\nwithout the regularizer). The blue arrows shows the vector \ufb01eld\nof r(x)\u2212x pointing towards high density peaks estimated by the\nmodel, and estimating the score (log-density derivative).\n.\nguish them. The solution is that variations along the high-\ndensity manifolds are preserved while other variations are\ncompressed: the reconstruction function should be as constant\nas possible while reproducing training examples, i.e., points\nnear a training example should be mapped to that training\nexample (Figure 5). The reconstruction function should map\nan input towards the nearest point manifold, i.e., the difference\nbetween reconstruction and input is a vector aligned with\nthe estimated score (the derivative of the log-density with\nrespect to the input). The score can be zero on the manifold\n(where reconstruction error is also zero), at local maxima of\nthe log-density, but it can also be zero at local minima. It\nmeans that we cannot equate low reconstruction error with\nhigh estimated probability. The second derivatives of the log-\ndensity corresponds to the \ufb01rst derivatives of the reconstruction\nfunction, and on the manifold (where the \ufb01rst derivative is 0),\nthey indicate the tangent directions of the manifold (where the\n\ufb01rst derivative remains near 0).\nFig. 6. Sampling from regularized auto-encoders (Rifai et al.,\n2012; Bengio et al., 2012): each MCMC step adds to current\nstate x the noise \u03b4 mostly in the directions of the estimated man-\nifold tangent plane H and projects back towards the manifold\n(high-density regions) by performing a reconstruction step.\n.\nAs illustrated in Figure 6, the basic idea of the auto-encoder\nsampling algorithms in Rifai et al. (2012); Bengio et al. (2012)\nis to make MCMC moves where one (a) moves toward the\nmanifold by following the density gradient (i.e., applying a\nreconstruction) and (b) adds noise in the directions of the\nleading singular vectors of the reconstruction (or encoder)\nJacobian, corresponding to those associated with smallest\nsecond derivative of the log-density.\n9.3\nLearning Approximate Inference\nLet us now consider from closer how a representation is\ncomputed in probabilistic models with latent variables, when\niterative inference is required. There is a computation graph\n(possibly with random number generation in some of the\nnodes, in the case of MCMC) that maps inputs to repre-\nsentation, and in the case of deterministic inference (e.g.,\nMAP inference or variational inference), that function could\nbe optimized directly. This is a way to generalize PSD that has\nbeen explored in recent work on probabilistic models at the\nintersection of inference and learning (Bagnell and Bradley,\n2009; Gregor and LeCun, 2010b; Grubb and Bagnell, 2010;\nSalakhutdinov and Larochelle, 2010; Stoyanov et al., 2011;\nEisner, 2012), where a central idea is that instead of using a\ngeneric inference mechanism, one can use one that is learned\nand is more ef\ufb01cient, taking advantage of the speci\ufb01cs of the\ntype of data on which it is applied.\n9.4\nSampling Challenges\nA troubling challenge with many probabilistic models with\nlatent variables like most Boltzmann machine variants is that\ngood MCMC sampling is required as part of the learning\nprocedure, but that sampling becomes extremely inef\ufb01cient (or\nunreliable) as training progresses because the modes of the\nlearned distribution become sharper, making mixing between\nmodes very slow. Whereas initially during training a learner as-\nsigns mass almost uniformly, as training progresses, its entropy\ndecreases, approaching the entropy of the target distribution as\nmore examples and more computation are provided. According\nto our Manifold and Natural Clustering priors of Section 3.1,\nthe target distribution has sharp modes (manifolds) separated\nby extremely low density areas. Mixing then becomes more\ndif\ufb01cult because MCMC methods, by their very nature, tend\nto make small steps to nearby high-probability con\ufb01gurations.\nThis is illustrated in Figure 7.\nFig. 7. Top: early during training, MCMC mixes easily between\nmodes because the estimated distribution has high entropy\nand puts enough mass everywhere for small-steps movements\n(MCMC) to go from mode to mode. Bottom: later on, training\nrelying on good mixing can stall because estimated modes are\nseparated by wide low-density deserts.\n.\nBengio et al. (2013) suggest that deep representations could\nhelp mixing between such well separated modes, based on\nboth theoretical arguments and on empirical evidence. The\nidea is that if higher-level representations disentangle better\n\n20\nthe underlying abstract factors, then small steps in this abstract\nspace (e.g., swapping from one category to another) can easily\nbe done by MCMC. The high-level representations can then\nbe mapped back to the input space in order to obtain input-\nlevel samples, as in the Deep Belief Networks (DBN) sampling\nalgorithm (Hinton et al., 2006). This has been demonstrated\nboth with DBNs and with the newly proposed algorithm for\nsampling from contracting and denoising auto-encoders (Rifai\net al., 2012; Bengio et al., 2012). This observation alone does\nnot suf\ufb01ce to solve the problem of training a DBN or a DBM,\nbut it may provide a crucial ingredient, and it makes it possible\nto consider successfully sampling from deep models trained\nby procedures that do not require an MCMC, like the stacked\nregularized auto-encoders used in Rifai et al. (2012).\n9.5\nEvaluating and Monitoring Performance\nIt is always possible to evaluate a feature learning algorithm\nin terms of its usefulness with respect to a particular task (e.g.\nobject classi\ufb01cation), with a predictor that is fed or initialized\nwith the learned features. In practice, we do this by saving\nthe features learned (e.g. at regular intervals during training,\nto perform early stopping) and training a cheap classi\ufb01er on\ntop (such as a linear classi\ufb01er). However, training the \ufb01nal\nclassi\ufb01er can be a substantial computational overhead (e.g.,\nsupervised \ufb01ne-tuning a deep neural network takes usually\nmore training iterations than the feature learning itself), so\nwe may want to avoid having to train a classi\ufb01er for ev-\nery training iteration of the unsupervised learner and every\nhyper-parameter setting. More importantly this may give an\nincomplete evaluation of the features (what would happen for\nother tasks?). All these issues motivate the use of methods to\nmonitor and evaluate purely unsupervised performance. This\nis rather easy with all the auto-encoder variants (with some\ncaution outlined below) and rather dif\ufb01cult with the undirected\ngraphical models such as the RBM and Boltzmann machines.\nFor auto-encoder and sparse coding variants, test set re-\nconstruction error can readily be computed, but by itself may\nbe misleading because larger capacity (e.g., more features,\nmore training time) tends to systematically lead to lower\nreconstruction error, even on the test set. Hence it cannot be\nused reliably for selecting most hyper-parameters. On the other\nhand, denoising reconstruction error is clearly immune to this\nproblem, so that solves the problem for DAEs. Based on the\nconnection between DAEs and CAEs uncovered in\nBengio\net al. (2012); Alain and Bengio (2012), this immunity can be\nextended to DAEs, but not to the hyper-parameter controlling\nthe amount of noise or of contraction.\nFor RBMs and some (not too deep) Boltzmann machines,\none option is the use of Annealed Importance Sampling (Mur-\nray and Salakhutdinov, 2009) in order to estimate the partition\nfunction (and thus the test log-likelihood). Note that this esti-\nmator can have high variance and that it becomes less reliable\n(variance becomes too large) as the model becomes more\ninteresting, with larger weights, more non-linearity, sharper\nmodes and a sharper probability density function (see our\nprevious discussion in Section 9.4). Another interesting and\nrecently proposed option for RBMs is to track the partition\nfunction during training (Desjardins et al., 2011), which could\nbe useful for early stopping and reducing the cost of ordinary\nAIS. For toy RBMs (e.g., 25 hidden units or less, or 25\ninputs or less), the exact log-likelihood can also be computed\nanalytically, and this can be a good way to debug and verify\nsome properties of interest.\n10\nGLOBAL TRAINING OF DEEP MODELS\nOne of the most interesting challenges raised by deep archi-\ntectures is: how should we jointly train all the levels? In the\nprevious section and in Section 4 we have only discussed\nhow single-layer models could be combined to form a deep\nmodel. Here we consider joint training of all the levels and\nthe dif\ufb01culties that may arise.\n10.1\nThe Challenge of Training Deep Architectures\nHigher-level abstraction means more non-linearity. It means\nthat two nearby input con\ufb01gurations may be interpreted very\ndifferently because a few surface details change the underlying\nsemantics, whereas most other changes in the surface details\nwould not change the underlying semantics. The representa-\ntions associated with input manifolds may be complex because\nthe mapping from input to representation may have to unfold\nand distort input manifolds that generally have complicated\nshapes into spaces where distributions are much simpler, where\nrelations between factors are simpler, maybe even linear or\ninvolving many (conditional) independencies. Our expectation\nis that modeling the joint distribution between high-level\nabstractions and concepts should be much easier in the sense\nof requiring much less data to learn. The hard part is learning a\ngood representation that does this unfolding and disentangling.\nThis may be at the price of a more dif\ufb01cult training problem,\npossibly involving ill-conditioning and local minima.\nIt is only since 2006 that researchers have seriously inves-\ntigated ways to train deep architectures, to the exception of\nthe convolutional networks (LeCun et al., 1998b). The \ufb01rst\nrealization (Section 4) was that unsupervised or supervised\nlayer-wise training was easier, and that this could be taken\nadvantage of by stacking single-layer models into deeper ones.\nIt is interesting to ask why does the layerwise unsuper-\nvised pre-training procedure sometimes help a supervised\nlearner (Erhan et al., 2010b). There seems to be a more\ngeneral principle at play 23 of guiding the training of inter-\nmediate representations, which may be easier than trying to\nlearn it all in one go. This is nicely related to the curriculum\nlearning idea (Bengio et al., 2009), that it may be much easier\nto learn simpler concepts \ufb01rst and then build higher-level\nones on top of simpler ones. This is also coherent with the\nsuccess of several deep learning algorithms that provide some\nsuch guidance for intermediate representations, like Semi-\nSupervised Embedding (Weston et al., 2008).\nThe question of why unsupervised pre-training could be\nhelpful was extensively studied (Erhan et al., 2010b), trying\nto dissect the answer into a regularization effect and an\noptimization effect. The regularization effect is clear from\nthe experiments where the stacked RBMs or denoising auto-\nencoders are used to initialize a supervised classi\ufb01cation neural\nnetwork (Erhan et al., 2010b). It may simply come from the\n23. First suggested to us by Leon Bottou\n\n21\nuse of unsupervised learning to bias the learning dynamics\nand initialize it in the basin of attraction of a \u201cgood\u201d local\nminimum (of the training criterion), where \u201cgood\u201d is in terms\nof generalization error. The underlying hypothesis exploited\nby this procedure is that some of the features or latent factors\nthat are good at capturing the leading variations in the input\ndistribution are also good at capturing the variations in the\ntarget output random variables of interest (e.g., classes). The\noptimization effect is more dif\ufb01cult to tease out because the\ntop two layers of a deep neural net can just over\ufb01t the training\nset whether the lower layers compute useful features or not,\nbut there are several indications that optimizing the lower\nlevels with respect to a supervised training criterion can be\nchallenging.\nOne such indication is that changing the numerical con-\nditions of the optimization procedure can have a profound\nimpact on the joint training of a deep architecture, for ex-\nample by changing the initialization range and changing the\ntype of non-linearity used (Glorot and Bengio, 2010), much\nmore so than with shallow architectures. One hypothesis to\nexplain some of the dif\ufb01culty in the optimization of deep\narchitectures is centered on the singular values of the Jacobian\nmatrix associated with the transformation from the features\nat one level into the features at the next level (Glorot and\nBengio, 2010). If these singular values are all small (less than\n1), then the mapping is contractive in every direction and\ngradients would vanish when propagated backwards through\nmany layers. This is a problem already discussed for recurrent\nneural networks (Bengio et al., 1994), which can be seen as\nvery deep networks with shared parameters at each layer, when\nunfolded in time. This optimization dif\ufb01culty has motivated\nthe exploration of second-order methods for deep architectures\nand recurrent networks, in particular Hessian-free second-\norder methods (Martens, 2010; Martens and Sutskever, 2011).\nUnsupervised pre-training has also been proposed to help\ntraining recurrent networks and temporal RBMs (Sutskever\net al., 2009), i.e., at each time step there is a local signal to\nguide the discovery of good features to capture in the state\nvariables: model with the current state (as hidden units) the\njoint distribution of the previous state and the current input.\nNatural gradient (Amari, 1998) methods that can be applied to\nnetworks with millions of parameters (i.e. with good scaling\nproperties) have also been proposed (Le Roux et al., 2008b;\nPascanu and Bengio, 2013). Cho et al. (2011) proposes to use\nadaptive learning rates for RBM training, along with a novel\nand interesting idea for a gradient estimator that takes into\naccount the invariance of the model to \ufb02ipping hidden unit bits\nand inverting signs of corresponding weight vectors. At least\none study indicates that the choice of initialization (to make\nthe Jacobian of each layer closer to 1 across all its singular\nvalues) could substantially reduce the training dif\ufb01culty of\ndeep networks (Glorot and Bengio, 2010) and this is coherent\nwith the success of the initialization procedure of Echo State\nNetworks (Jaeger, 2007), as recently studied by Sutskever\n(2012). There are also several experimental results (Glorot and\nBengio, 2010; Glorot et al., 2011a; Nair and Hinton, 2010)\nshowing that the choice of hidden units non-linearity could\nin\ufb02uence both training and generalization performance, with\nparticularly interesting results obtained with sparse rectifying\nunits (Jarrett et al., 2009; Nair and Hinton, 2010; Glorot\net al., 2011a; Krizhevsky et al., 2012). An old idea regarding\nthe ill-conditioning issue with neural networks is that of\nsymmetry breaking: part of the slowness of convergence may\nbe due to many units moving together (like sheep) and all\ntrying to reduce the output error for the same examples.\nBy initializing with sparse weights (Martens, 2010) or by\nusing often saturated non-linearities (such as recti\ufb01ers as max-\npooling units), gradients only \ufb02ow along a few paths, which\nmay help hidden units to specialize more quickly. Another\npromising idea to improve the conditioning of neural network\ntraining is to nullify the average value and slope of each\nhidden unit output (Raiko et al., 2012), and possibly locally\nnormalize magnitude as well (Jarrett et al., 2009). The debate\nstill rages between using online methods such as stochastic\ngradient descent and using second-order methods on large\nminibatches (of several thousand examples) (Martens, 2010;\nLe et al., 2011a), with a variant of stochastic gradient descent\nrecently winning an optimization challenge 24.\nFinally, several recent results exploiting large quantities\nof\nlabeled\ndata\nsuggest\nthat\nwith\nproper\ninitialization\nand choice of non-linearity, very deep purely supervised\nnetworks can be trained successfully without any layerwise\npre-training (Ciresan et al., 2010; Glorot et al., 2011a; Seide\net al., 2011a; Krizhevsky et al., 2012). Researchers report\nthan in such conditions, layerwise unsupervised pre-training\nbrought little or no improvement over pure supervised\nlearning from scratch when training for long enough. This\nreinforces\nthe\nhypothesis\nthat\nunsupervised\npre-training\nacts as a prior, which may be less necessary when very\nlarge quantities of labeled data are available, but begs the\nquestion of why this had not been discovered earlier. The\nlatest results reported in this respect (Krizhevsky et al.,\n2012) are particularly interesting because they allowed to\ndrastically reduce the error rate of object recognition on a\nbenchmark (the 1000-class ImageNet task) where many more\ntraditional computer vision approaches had been evaluated\n(http://www.image-net.org/challenges/LSVRC/2012/results.html).\nThe main techniques that allowed this success include the\nfollowing: ef\ufb01cient GPU training allowing one to train\nlonger (more than 100 million visits of examples), an aspect\n\ufb01rst reported by Lee et al. (2009a); Ciresan et al. (2010),\nlarge number of labeled examples, arti\ufb01cially transformed\nexamples (see Section 11.1), a large number of tasks (1000\nor 10000 classes for ImageNet), convolutional architecture\nwith max-pooling (see section 11 for these latter two\ntechniques),\nrectifying\nnon-linearities\n(discussed\nabove),\ncareful initialization (discussed above), careful parameter\nupdate and adaptive learning rate heuristics, layerwise\nfeature normalization (across features), and a new dropout\ntrick based on injecting strong binary multiplicative noise on\nhidden units. This trick is similar to the binary noise injection\nused at each layer of a stack of denoising auto-encoders.\nFuture work is hopefully going to help identify which of\nthese elements matter most, how to generalize them across\n24. https://sites.google.com/site/nips2011workshop/optimization-challenges\n\n22\na large variety of tasks and architectures, and in particular\ncontexts where most examples are unlabeled, i.e., including\nan unsupervised component in the training criterion.\n10.2\nJoint Training of Deep Boltzmann Machines\nWe now consider the problem of joint training of all layers of\na speci\ufb01c unsupervised model, the Deep Boltzmann Machine\n(DBM). Whereas much progress (albeit with many unan-\nswered questions) has been made on jointly training all the\nlayers of deep architectures using back-propagated gradients\n(i.e., mostly in the supervised setting), much less work has\nbeen done on their purely unsupervised counterpart, e.g. with\nDBMs25. Note however that one could hope that the successful\ntechniques described in the previous section could be applied\nto unsupervised learning algorithms.\nLike the RBM, the DBM is another particular subset of\nthe Boltzmann machine family of models where the units\nare again arranged in layers. However unlike the RBM, the\nDBM possesses multiple layers of hidden units, with units in\nodd-numbered layers being conditionally independent given\neven-numbered layers, and vice-versa. With respect to the\nBoltzmann energy function of Eq. 7, the DBM corresponds\nto setting U = 0 and a sparse connectivity structure in both V\nand W. We can make the structure of the DBM more explicit\nby specifying its energy function. For the model with two\nhidden layers it is given as:\nEDBM\n\u03b8\n(v, h(1), h(2); \u03b8) = \u2212vT Wh(1) \u2212h(1) T V h(2)\u2212\nd(1) T h(1) \u2212d(2) T h(2) \u2212bT v,\n(29)\nwith \u03b8 = {W, V, d(1), d(2), b}. The DBM can also be char-\nacterized as a bipartite graph between two sets of vertices,\nformed by odd and even-numbered layers (with v := h(0)).\n10.2.1\nMean-\ufb01eld approximate inference\nA key point of departure from the RBM is that the pos-\nterior distribution over the hidden units (given the visibles)\nis no longer tractable, due to the interactions between the\nhidden units. Salakhutdinov and Hinton (2009) resort to a\nmean-\ufb01eld approximation to the posterior. Speci\ufb01cally, in\nthe case of a model with two hidden layers, we wish to\napproximate P\n\u0000h(1), h(2) | v\n\u0001\nwith the factored distribution\nQv(h(1), h(2)) = QN1\nj=1 Qv\n\u0010\nh(1)\nj\n\u0011 QN2\ni=1 Qv\n\u0010\nh(2)\ni\n\u0011\n, such\nthat the KL divergence KL\n\u0000P\n\u0000h(1), h(2) | v\n\u0001\n\u2225Qv(h1, h2)\n\u0001\nis minimized or equivalently, that a lower bound to the log\nlikelihood is maximized:\nlog P(v) > L(Qv) \u2261\nX\nh(1)\nX\nh(2)\nQv(h(1), h(2)) log\n\u0012P(v, h(1), h(2))\nQv(h(1), h(2))\n\u0013\n(30)\nMaximizing this lower-bound with respect to the mean-\ufb01eld\ndistribution Qv(h1, h2) (by setting derivatives to zero) yields\nthe following mean \ufb01eld update equations:\n\u02c6h(1)\ni\n\u2190sigmoid\n X\nj\nWjivj +\nX\nk\nVik\u02c6h(2)\nk\n+ d(1)\ni\n!\n(31)\n\u02c6h(2)\nk\n\u2190sigmoid\n X\ni\nVik\u02c6h(1)\ni\n+ d(2)\nk\n!\n(32)\n25. Joint training of all the layers of a Deep Belief Net is much more\nchallenging because of the much harder inference problem involved.\nNote how the above equations ostensibly look like a \ufb01xed\npoint recurrent neural network, i.e., with constant input. In\nthe same way that an RBM can be associated with a simple\nauto-encoder, the above mean-\ufb01eld update equations for the\nDBM can be associated with a recurrent auto-encoder. In that\ncase the training criterion involves the reconstruction error at\nthe last or at consecutive time steps. This type of model has\nbeen explored by Savard (2011) and Seung (1998) and shown\nto do a better job at denoising than ordinary auto-encoders.\nIterating Eq. (31-32) until convergence yields the Q param-\neters of the \u201cvariational positive phase\u201d of Eq. 33:\nL(Qv) =EQv\nh\nlog P(v, h(1), h(2)) \u2212log Qv(h(1), h(2))\ni\n=EQv\nh\n\u2212EDBM\n\u03b8\n(v, h(1), h(2)) \u2212log Qv(h(1), h(2))\ni\n\u2212log Z\u03b8\n\u2202L(Qv)\n\u2202\u03b8\n= \u2212EQv\n\u0014\u2202EDBM\n\u03b8\n(v, h(1), h(2))\n\u2202\u03b8\n\u0015\n+ EP\n\u0014\u2202EDBM\n\u03b8\n(v, h(1), h(2))\n\u2202\u03b8\n\u0015\n(33)\nThis variational learning procedure leaves the \u201cnegative\nphase\u201d untouched, which can thus be estimated through SML\nor Contrastive Divergence (Hinton, 2000) as in the RBM case.\n10.2.2\nTraining Deep Boltzmann Machines\nThe major difference between training a DBM and an RBM\nis that instead of maximizing the likelihood directly, we\ninstead choose parameters to maximize the lower-bound on\nthe likelihood given in Eq. 30. The SML-based algorithm for\nmaximizing this lower-bound is as follows:\n1) Clamp the visible units to a training example.\n2) Iterate over Eq. (31-32) until convergence.\n3) Generate negative phase samples v\u2212, h(1)\u2212and h(2)\u2212\nthrough SML.\n4) Compute \u2202L(Qv) /\u2202\u03b8 using the values obtained in steps\n2-3.\n5) Finally, update the model parameters with a step of\napproximate stochastic gradient ascent.\nWhile the above procedure appears to be a simple extension\nof the highly effective SML scheme for training RBMs, as we\ndemonstrate in Desjardins et al. (2012), this procedure seems\nvulnerable to falling in poor local minima which leave many\nhidden units effectively dead (not signi\ufb01cantly different from\nits random initialization with small norm).\nThe failure of the SML joint training strategy was noted\nby Salakhutdinov and Hinton (2009). As an alternative, they\nproposed a greedy layer-wise training strategy. This procedure\nconsists in pre-training the layers of the DBM, in much the\nsame way as the Deep Belief Network: i.e. by stacking RBMs\nand training each layer to independently model the output of\nthe previous layer. A \ufb01nal joint \u201c\ufb01ne-tuning\u201d is done following\nthe above SML-based procedure.\n11\nBUILDING-IN INVARIANCE\nIt is well understood that incorporating prior domain knowl-\nedge helps machine learning. Exploring good strategies for\ndoing so is a very important research avenue. However, if we\nare to advance our understanding of core machine learning\nprinciples, it is important that we keep comparisons between\npredictors fair and maintain a clear awareness of the prior\n\n23\ndomain knowledge used by different learning algorithms,\nespecially when comparing their performance on benchmark\nproblems. We have so far only presented algorithms that\nexploited only generic inductive biases for high dimensional\nproblems, thus making them potentially applicable to any\nhigh dimensional problem. The most prevalent approach to\nincorporating prior knowledge is to hand-design better features\nto feed a generic classi\ufb01er, and has been used extensively in\ncomputer vision (e.g. (Lowe, 1999)). Here, we rather focus\non how basic domain knowledge of the input, in particular\nits topological structure (e.g. bitmap images having a 2D\nstructure), may be used to learn better features.\n11.1\nGenerating transformed examples\nGeneralization performance is usually improved by providing\na larger quantity of representative data. This can be achieved\nby generating new examples by applying small random defor-\nmations to the original training examples, using deformations\nthat are known not to change the target variables of interest,\ne.g., an object class is invariant to small transformations of\nimages such as translations, rotations, scaling, or shearing.\nThis old approach (Baird, 1990) has been recently applied with\ngreat success in the work of Ciresan et al. (2010) who used an\nef\ufb01cient GPU implementation (40\u00d7 speedup) to train a stan-\ndard but large deep multilayer Perceptron on deformed MNIST\ndigits. Using both af\ufb01ne and elastic deformations (Simard\net al., 2003), with plain old stochastic gradient descent, they\nreach a record 0.32% classi\ufb01cation error rate.\n11.2\nConvolution and pooling\nAnother powerful approach is based on even more basic\nknowledge of merely the topological structure of the input\ndimensions. By this we mean e.g., the 2D layout of pixels\nin images or audio spectrograms, the 3D structure of videos,\nthe 1D sequential structure of text or of temporal sequences\nin general. Based on such structure, one can de\ufb01ne local\nreceptive \ufb01elds (Hubel and Wiesel, 1959), so that each low-\nlevel feature will be computed from only a subset of the input:\na neighborhood in the topology (e.g. a sub-image at a given\nposition). This topological locality constraint corresponds to a\nlayer having a very sparse weight matrix with non-zeros only\nallowed for topologically local connections. Computing the\nassociated matrix products can of course be made much more\nef\ufb01cient than having to handle a dense matrix, in addition\nto the statistical gain from a much smaller number of free\nparameters. In domains with such topological structure, similar\ninput patterns are likely to appear at different positions, and\nnearby values (e.g. consecutive frames or nearby pixels) are\nlikely to have stronger dependencies that are also important to\nmodel the data. In fact these dependencies can be exploited\nto discover the topology (Le Roux et al., 2008a), i.e. recover\na regular grid of pixels out of a set of vectors without any\norder information, e.g. after the elements have been arbitrarily\nshuf\ufb02ed in the same way for all examples. Thus a same local\nfeature computation is likely to be relevant at all translated po-\nsitions of the receptive \ufb01eld. Hence the idea of sweeping such\na local feature extractor over the topology: this corresponds to\na convolution, and transforms an input into a similarly shaped\nfeature map. Equivalently to sweeping, this may be seen as\nstatic but differently positioned replicated feature extractors\nthat all share the same parameters. This is at the heart of\nconvolutional networks (LeCun et al., 1989, 1998b) which\nhave been applied both to object recognition and to image\nsegmentation (Turaga et al., 2010). Another hallmark of the\nconvolutional architecture is that values computed by the same\nfeature detector applied at several neighboring input locations\nare then summarized through a pooling operation, typically\ntaking their max or their sum. This confers the resulting pooled\nfeature layer some degree of invariance to input translations,\nand this style of architecture (alternating selective feature\nextraction and invariance-creating pooling) has been the ba-\nsis of convolutional networks, the Neocognitron (Fukushima,\n1980) and HMAX (Riesenhuber and Poggio, 1999) models,\nand argued to be the architecture used by mammalian brains\nfor object recognition (Riesenhuber and Poggio, 1999; Serre\net al., 2007; DiCarlo et al., 2012). The output of a pooling\nunit will be the same irrespective of where a speci\ufb01c feature\nis located inside its pooling region. Empirically the use of\npooling seems to contribute signi\ufb01cantly to improved classi-\n\ufb01cation accuracy in object classi\ufb01cation tasks (LeCun et al.,\n1998b; Boureau et al., 2010, 2011). A successful variant of\npooling connected to sparse coding is L2 pooling (Hyv\u00a8arinen\net al., 2009; Kavukcuoglu et al., 2009; Le et al., 2010),\nfor which the pool output is the square root of the possibly\nweighted sum of squares of \ufb01lter outputs. Ideally, we would\nlike to generalize feature-pooling so as to learn what features\nshould be pooled together, e.g. as successfully done in several\npapers (Hyv\u00a8arinen and Hoyer, 2000; Kavukcuoglu et al., 2009;\nLe et al., 2010; Ranzato and Hinton, 2010; Courville et al.,\n2011b; Coates and Ng, 2011b; Gregor et al., 2011). In this\nway, the pool output learns to be invariant to the variations\ncaptured by the span of the features pooled.\nPatch-based training\nThe simplest approach for learning a convolutional layer in an\nunsupervised fashion is patch-based training: simply feeding\na generic unsupervised feature learning algorithm with local\npatches extracted at random positions of the inputs. The\nresulting feature extractor can then be swiped over the input to\nproduce the convolutional feature maps. That map may be used\nas a new input for the next layer, and the operation repeated\nto thus learn and stack several layers. Such an approach\nwas recently used with Independent Subspace Analysis (Le\net al., 2011c) on 3D video blocks, reaching the state-of-the-art\non Hollywood2, UCF, KTH and YouTube action recognition\ndatasets. Similarly (Coates and Ng, 2011a) compared several\nfeature learners with patch-based training and reached state-\nof-the-art results on several classi\ufb01cation benchmarks. Inter-\nestingly, in this work performance was almost as good with\nvery simple k-means clustering as with more sophisticated\nfeature learners. We however conjecture that this is the case\nonly because patches are rather low dimensional (compared\nto the dimension of a whole image). A large dataset might\nprovide suf\ufb01cient coverage of the space of e.g. edges prevalent\nin 6 \u00d7 6 patches, so that a distributed representation is not\nabsolutely necessary. Another plausible explanation for this\n\n24\nsuccess is that the clusters identi\ufb01ed in each image patch are\nthen pooled into a histogram of cluster counts associated with\na larger sub-image. Whereas the output of a regular clustering\nis a one-hot non-distributed code, this histogram is itself a\ndistributed representation, and the \u201csoft\u201d k-means (Coates and\nNg, 2011a) representation allows not only the nearest \ufb01lter but\nalso its neighbors to be active.\nConvolutional and tiled-convolutional training\nIt is possible to directly train large convolutional layers using\nan unsupervised criterion. An early approach (Jain and Seung,\n2008) trained a standard but deep convolutional MLP on\nthe task of denoising images, i.e. as a deep, convolutional,\ndenoising auto-encoder. Convolutional versions of the RBM\nor its extensions have also been developed (Desjardins and\nBengio, 2008; Lee et al., 2009a; Taylor et al., 2010) as well as\na probabilistic max-pooling operation built into Convolutional\nDeep Networks (Lee et al., 2009a,b; Krizhevsky, 2010). Other\nunsupervised feature learning approaches that were adapted to\nthe convolutional setting include PSD (Kavukcuoglu et al.,\n2009, 2010; Jarrett et al., 2009; Henaff et al., 2011), a\nconvolutional version of sparse coding called deconvolutional\nnetworks (Zeiler et al., 2010), Topographic ICA (Le et al.,\n2010), and mPoT that Kivinen and Williams (2012) applied\nto modeling natural textures. Gregor and LeCun (2010a);\nLe et al. (2010) also demonstrated the technique of tiled-\nconvolution, where parameters are shared only between feature\nextractors whose receptive \ufb01elds are k steps away (so the\nones looking at immediate neighbor locations are not shared).\nThis allows pooling units to be invariant to more than just\ntranslations, and is a hybrid between convolutional networks\nand earlier neural networks with local connections but no\nweight sharing (LeCun, 1986, 1989).\nAlternatives to pooling\nAlternatively, one can also use explicit knowledge of the\nexpected invariants expressed mathematically to de\ufb01ne trans-\nformations that are robust to a known family of input defor-\nmations, using so-called scattering operators (Mallat, 2012;\nBruna and Mallat, 2011), which can be computed in a way\ninterestingly analogous to deep convolutional networks and\nwavelets. Like convolutional networks, the scattering operators\nalternate two types of operations: convolution and pooling\n(as a norm). Unlike convolutional networks, the proposed\napproach keeps at each level all of the information about the\ninput (in a way that can be inverted), and automatically yields\na very sparse (but very high-dimensional) representation. An-\nother difference is that the \ufb01lters are not learned but instead\nset so as to guarantee that a priori speci\ufb01ed invariances are\nrobustly achieved. Just a few levels were suf\ufb01cient to achieve\nimpressive results on several benchmark datasets.\n11.3\nTemporal coherence and slow features\nThe principle of identifying slowly moving/changing factors in\ntemporal/spatial data has been investigated by many (Becker\nand Hinton, 1992; Wiskott and Sejnowski, 2002; Hurri and\nHyv\u00a8arinen, 2003; K\u00a8ording et al., 2004; Cadieu and Olshausen,\n2009) as a principle for \ufb01nding useful representations. In\nparticular this idea has been applied to image sequences and as\nan explanation for why V1 simple and complex cells behave\nthe way they do. A good overview can be found in Hurri and\nHyv\u00a8arinen (2003); Berkes and Wiskott (2005).\nMore recently, temporal coherence has been successfully\nexploited in deep architectures to model video (Mobahi et al.,\n2009). It was also found that temporal coherence discov-\nered visual features similar to those obtained by ordinary\nunsupervised feature learning (Bergstra and Bengio, 2009),\nand a temporal coherence penalty has been combined with a\ntraining criterion for unsupervised feature learning (Zou et al.,\n2011), sparse auto-encoders with L1 regularization, in this\ncase, yielding improved classi\ufb01cation performance.\nThe temporal coherence prior can be expressed in several\nways, the simplest being the squared difference between\nfeature values at times t and t + 1. Other plausible tempo-\nral coherence priors include the following. First, instead of\npenalizing the squared change, penalizing the absolute value\n(or a similar sparsity penalty) would state that most of the\ntime the change should be exactly 0, which would intuitively\nmake sense for the real-life factors that surround us. Second,\none would expect that instead of just being slowly changing,\ndifferent factors could be associated with their own different\ntime scale. The speci\ufb01city of their time scale could thus\nbecome a hint to disentangle explanatory factors. Third, one\nwould expect that some factors should really be represented by\na group of numbers (such as x, y, and z position of some object\nin space and the pose parameters of\nHinton et al. (2011))\nrather than by a single scalar, and that these groups tend\nto move together. Structured sparsity penalties (Kavukcuoglu\net al., 2009; Jenatton et al., 2009; Bach et al., 2011; Gregor\net al., 2011) could be used for this purpose.\n11.4\nAlgorithms to Disentangle Factors of Variation\nThe goal of building invariant features is to remove sensitivity\nof the representation to directions of variance in the data that\nare uninformative to the task at hand. However it is often the\ncase that the goal of feature extraction is the disentangling or\nseparation of many distinct but informative factors in the data,\ne.g., in a video of people: subject identity, action performed,\nsubject pose relative to the camera, etc. In this situation,\nthe methods of generating invariant features, such as feature-\npooling, may be inadequate.\nThe process of building invariant features can be seen as\nconsisting of two steps. First, low-level features are recovered\nthat account for the data. Second, subsets of these low level\nfeatures are pooled together to form higher-level invariant\nfeatures, exempli\ufb01ed by the pooling and subsampling layers\nof convolutional neural networks. The invariant representation\nformed by the pooling features offers an incomplete window\non the data as the detailed representation of the lower-level\nfeatures is abstracted away in the pooling procedure. While\nwe would like higher-level features to be more abstract and\nexhibit greater invariance, we have little control over what\ninformation is lost through pooling. What we really would like\nis for a particular feature set to be invariant to the irrelevant\nfeatures and disentangle the relevant features. Unfortunately,\nit is often dif\ufb01cult to determine a priori which set of features\nwill ultimately be relevant to the task at hand.\n\n25\nAn interesting approach to taking advantage of some of\nthe factors of variation known to exist in the data is the\ntransforming auto-encoder (Hinton et al., 2011): instead of a\nscalar pattern detector (e.g,. corresponding to the probability\nof presence of a particular form in the input) one can think\nof the features as organized in groups that include both a\npattern detector and pose parameters that specify attributes\nof the detected pattern. In (Hinton et al., 2011), what is\nassumed a priori is that pairs of examples (or consecutive ones)\nare observed with an associated value for the corresponding\nchange in the pose parameters. For example, an animal that\ncontrols its eyes knows what changes to its ocular motor\nsystem were applied when going from one image on its retina\nto the next. In that work, it is also assumed that the pose\nchanges are the same for all the pattern detectors, and this\nmakes sense for global changes such as image translation and\ncamera geometry changes. Instead, we would like to discover\nthe pose parameters and attributes that should be associated\nwith each feature detector, without having to specify ahead of\ntime what they should be, force them to be the same for all\nfeatures, and having to necessarily observe the changes in all\nof the pose parameters or attributes.\nThe approach taken recently in the Manifold Tangent Clas-\nsi\ufb01er, discussed in section 8.3, is interesting in this respect.\nWithout any supervision or prior knowledge, it \ufb01nds prominent\nlocal factors of variation (tangent vectors to the manifold,\nextracted from a CAE, interpreted as locally valid input \u201ddefor-\nmations\u201d). Higher-level features are subsequently encouraged\nto be invariant to these factors of variation, so that they must\ndepend on other characteristics. In a sense this approach is\ndisentangling valid local deformations along the data manifold\nfrom other, more drastic changes, associated to other factors\nof variation such as those that affect class identity.26\nOne solution to the problem of information loss that would\n\ufb01t within the feature-pooling paradigm, is to consider many\noverlapping pools of features based on the same low-level\nfeature set. Such a structure would have the potential to\nlearn a redundant set of invariant features that may not cause\nsigni\ufb01cant loss of information. However it is not obvious\nwhat learning principle could be applied that can ensure\nthat the features are invariant while maintaining as much\ninformation as possible. While a Deep Belief Network or a\nDeep Boltzmann Machine (as discussed in sections 4 and 10.2\nrespectively) with two hidden layers would, in principle, be\nable to preserve information into the \u201cpooling\u201d second hidden\nlayer, there is no guarantee that the second layer features\nare more invariant than the \u201clow-level\u201d \ufb01rst layer features.\nHowever, there is some empirical evidence that the second\nlayer of the DBN tends to display more invariance than the\n\ufb01rst layer (Erhan et al., 2010a).\nA more principled approach, from the perspective of en-\nsuring a more robust compact feature representation, can\nbe conceived by reconsidering the disentangling of features\nthrough the lens of its generative equivalent \u2013 feature com-\nposition. Since many unsupervised learning algorithms have a\n26. The changes that affect class identity might, in input space, actually be\nof similar magnitude to local deformations, but not follow along the manifold,\ni.e. cross zones of low density.\ngenerative interpretation (or a way to reconstruct inputs from\ntheir high-level representation), the generative perspective can\nprovide insight into how to think about disentangling fac-\ntors. The majority of the models currently used to construct\ninvariant features have the interpretation that their low-level\nfeatures linearly combine to construct the data.27 This is a\nfairly rudimentary form of feature composition with signi\ufb01cant\nlimitations. For example, it is not possible to linearly combine\na feature with a generic transformation (such as translation) to\ngenerate a transformed version of the feature. Nor can we even\nconsider a generic color feature being linearly combined with\na gray-scale stimulus pattern to generate a colored pattern. It\nwould seem that if we are to take the notion of disentangling\nseriously we require a richer interaction of features than that\noffered by simple linear combinations.\n12\nCONCLUSION\nThis review of representation learning and deep learning has\ncovered three major and apparently disconnected approaches:\nthe probabilistic models (both the directed kind such as\nsparse coding and the undirected kind such as Boltzmann\nmachines), the reconstruction-based algorithms related to auto-\nencoders, and the geometrically motivated manifold-learning\napproaches. Drawing connections between these approaches\nis currently a very active area of research and is likely to\ncontinue to produce models and methods that take advantage\nof the relative strengths of each paradigm.\nPractical Concerns and Guidelines. One of the criticisms\naddressed to arti\ufb01cial neural networks and deep learning algo-\nrithms is that they have many hyper-parameters and variants\nand that exploring their con\ufb01gurations and architectures is an\nart. This has motivated an earlier book on the \u201cTricks of the\nTrade\u201d (Orr and Muller, 1998) of which LeCun et al. (1998a)\nis still relevant for training deep architectures, in particular\nwhat concerns initialization, ill-conditioning and stochastic\ngradient descent. A good and more modern compendium of\ngood training practice, particularly adapted to training RBMs,\nis provided in Hinton (2010), while a similar guide oriented\nmore towards deep neural networks can be found in Bengio\n(2013), both of which are part of a novel version of the\nabove book. Recent work on automating hyper-parameter\nsearch (Bergstra and Bengio, 2012; Bergstra et al., 2011;\nSnoek et al., 2012) is also making it more convenient, ef\ufb01cient\nand reproducible.\nIncorporating Generic AI-level Priors. We have covered\nmany high-level generic priors that we believe could bring\nmachine learning closer to AI by improving representation\nlearning. Many of these priors relate to the assumed existence\nof multiple underlying factors of variation, whose variations\nare in some sense orthogonal to each other. They are expected\nto be organized at multiple levels of abstraction, hence the\nneed for deep architectures, which also have statistical advan-\ntages because they allow to re-use parameters in a combi-\nnatorially ef\ufb01cient way. Only a few of these factors would\n27. As an aside, if we are given only the values of the higher-level pooling\nfeatures, we cannot accurately recover the data because we do not know how to\napportion credit for the pooling feature values to the lower-level features. This\nis simply the generative version of the consequences of the loss of information\ncaused by pooling.\n\n26\ntypically be relevant for any particular example, justifying\nsparsity of representation. These factors are expected to be\nrelated to simple (e.g., linear) dependencies, with subsets of\nthese explaining different random variables of interest (inputs,\ntasks) and varying in structured ways in time and space\n(temporal and spatial coherence). We expect future successful\napplications of representation learning to re\ufb01ne and increase\nthat list of priors, and to incorporate most of them instead of\nfocusing on only one. Research in training criteria that better\ntake these priors into account are likely to move us closer to\nthe long-term objective of discovering learning algorithms that\ncan disentangle the underlying explanatory factors.\nInference. We anticipate that methods based on directly\nparametrizing a representation function will incorporate more\nand more of the iterative type of computation one \ufb01nds in the\ninference procedures of probabilistic latent-variable models.\nThere is already movement in the other direction, with prob-\nabilistic latent-variable models exploiting approximate infer-\nence mechanisms that are themselves learned (i.e., producing a\nparametric description of the representation function). A major\nappeal of probabilistic models is that the semantics of the\nlatent variables are clear and this allows a clean separation\nof the problems of modeling (choose the energy function),\ninference (estimating P(h|x)), and learning (optimizing the\nparameters), using generic tools in each case. On the other\nhand, doing approximate inference and not taking that approxi-\nmation into account explicitly in the approximate optimization\nfor learning could have detrimental effects, hence the appeal\nof learning approximate inference. More fundamentally, there\nis the question of the multimodality of the posterior P(h|x).\nIf there are exponentially many probable con\ufb01gurations of\nvalues of the factors hi that can explain x, then we seem\nto be stuck with very poor inference, either focusing on a\nsingle mode (MAP inference), assuming some kind of strong\nfactorization (as in variational inference) or using an MCMC\nthat cannot visit enough modes of P(h|x). What we propose\nas food for thought is the idea of dropping the requirement\nof an explicit representation of the posterior and settle for\nan implicit representation that exploits potential structure in\nP(h|x) in order to represent it compactly: even though P(h|x)\nmay have an exponential number of modes, it may be possible\nto represent it with a small set of numbers. For example,\nconsider computing a deterministic feature representation f(x)\nthat implicitly captures the information about a highly multi-\nmodal P(h|x), in the sense that all the questions (e.g. making\nsome prediction about some target concept) that can be asked\nfrom P(h|x) can also be answered from f(x).\nOptimization. Much remains to be done to better under-\nstand the successes and failures of training deep architectures,\nboth in the supervised case (with many recent successes) and\nthe unsupervised case (where much more work needs to be\ndone). Although regularization effects can be important on\nsmall datasets, the effects that persist on very large datasets\nsuggest some optimization issues are involved. Are they more\ndue to local minima (we now know there are huge numbers\nof them) and the dynamics of the training procedure? Or\nare they due mostly to ill-conditioning and may be handled\nby approximate second-order methods? These basic questions\nremain unanswered and deserve much more study.\nAcknowledgments\nThe author would like to thank David Warde-Farley, Razvan\nPascanu and Ian Goodfellow for useful feedback, as well as\nNSERC, CIFAR and the Canada Research Chairs for funding.\nREFERENCES\nAlain, G. and Bengio, Y. (2012). What regularized auto-encoders\nlearn from the data generating distribution. Technical Report Arxiv\nreport 1211.4246, Universit\u00b4e de Montr\u00b4eal.\nAmari, S. (1998).\nNatural gradient works ef\ufb01ciently in learning.\nNeural Computation, 10(2), 251\u2013276.\nBach, F., Jenatton, R., Mairal, J., and Obozinski, G. (2011). Struc-\ntured sparsity through convex optimization. CoRR, abs/1109.2397.\nBagnell, J. A. and Bradley, D. M. (2009).\nDifferentiable sparse\ncoding. In NIPS\u20192009, pages 113\u2013120.\nBaird, H. (1990). Document image defect models. In IAPR Workshop,\nSyntactic & Structural Patt. Rec., pages 38\u201346.\nBecker, S. and Hinton, G. (1992). A self-organizing neural network\nthat discovers surfaces in random-dot stereograms. Nature, 355,\n161\u2013163.\nBelkin, M. and Niyogi, P. (2003). Laplacian eigenmaps for dimen-\nsionality reduction and data representation. Neural Computation,\n15(6), 1373\u20131396.\nBell, A. and Sejnowski, T. J. (1997). The independent components\nof natural scenes are edge \ufb01lters. Vision Research, 37, 3327\u20133338.\nBengio, Y. (1993). A connectionist approach to speech recognition.\nInternational Journal on Pattern Recognition and Arti\ufb01cial Intel-\nligence, 7(4), 647\u2013668.\nBengio, Y. (2008). Neural net language models. Scholarpedia, 3(1).\nBengio, Y. (2009). Learning deep architectures for AI. Foundations\nand Trends in Machine Learning, 2(1), 1\u2013127. Also published as\na book. Now Publishers, 2009.\nBengio, Y. (2011). Deep learning of representations for unsupervised\nand transfer learning. In JMLR W&CP: Proc. Unsupervised and\nTransfer Learning.\nBengio, Y. (2013).\nPractical recommendations for gradient-based\ntraining of deep architectures. In K.-R. M\u00a8uller, G. Montavon, and\nG. B. Orr, editors, Neural Networks: Tricks of the Trade. Springer.\nBengio, Y. and Delalleau, O. (2009).\nJustifying and generalizing\ncontrastive divergence. Neural Computation, 21(6), 1601\u20131621.\nBengio, Y. and Delalleau, O. (2011). On the expressive power of\ndeep architectures. In ALT\u20192011.\nBengio, Y. and LeCun, Y. (2007). Scaling learning algorithms towards\nAI. In L. Bottou, O. Chapelle, D. DeCoste, and J. Weston, editors,\nLarge Scale Kernel Machines. MIT Press.\nBengio, Y. and Monperrus, M. (2005). Non-local manifold tangent\nlearning. In NIPS\u20192004, pages 129\u2013136. MIT Press.\nBengio, Y., Simard, P., and Frasconi, P. (1994). Learning long-term\ndependencies with gradient descent is dif\ufb01cult. IEEE Transactions\non Neural Networks, 5(2), 157\u2013166.\nBengio, Y., Ducharme, R., Vincent, P., and Jauvin, C. (2003).\nA\nneural probabilistic language model. JMLR, 3, 1137\u20131155.\nBengio, Y., Paiement, J.-F., Vincent, P., Delalleau, O., Le Roux,\nN., and Ouimet, M. (2004). Out-of-sample extensions for LLE,\nIsomap, MDS, Eigenmaps, and Spectral Clustering. In NIPS\u20192003.\nBengio, Y., Delalleau, O., and Le Roux, N. (2006a). The curse of\nhighly variable functions for local kernel machines. In NIPS\u20192005.\nBengio, Y., Larochelle, H., and Vincent, P. (2006b).\nNon-local\nmanifold Parzen windows. In NIPS\u20192005. MIT Press.\nBengio, Y., Lamblin, P., Popovici, D., and Larochelle, H. (2007).\nGreedy layer-wise training of deep networks. In NIPS\u20192006.\nBengio, Y., Louradour, J., Collobert, R., and Weston, J. (2009).\nCurriculum learning. In ICML\u201909.\nBengio, Y., Delalleau, O., and Simard, C. (2010).\nDecision trees\ndo not generalize to new variations. Computational Intelligence,\n26(4), 449\u2013467.\n\n27\nBengio, Y., Alain, G., and Rifai, S. (2012). Implicit density esti-\nmation by local moment matching to sample from auto-encoders.\nTechnical report, arXiv:1207.0057.\nBengio, Y., Mesnil, G., Dauphin, Y., and Rifai, S. (2013). Better\nmixing via deep representations. In ICML\u20192013.\nBergstra, J. and Bengio, Y. (2009). Slow, decorrelated features for\npretraining complex cell-like networks. In NIPS\u20192009.\nBergstra, J. and Bengio, Y. (2012).\nRandom search for hyper-\nparameter optimization. J. Machine Learning Res., 13, 281\u2013305.\nBergstra, J., Bardenet, R., Bengio, Y., and K\u00b4egl, B. (2011). Algo-\nrithms for hyper-parameter optimization. In NIPS\u20192011.\nBerkes, P. and Wiskott, L. (2005). Slow feature analysis yields a\nrich repertoire of complex cell properties. Journal of Vision, 5(6),\n579\u2013602.\nBesag, J. (1975).\nStatistical analysis of non-lattice data.\nThe\nStatistician, 24(3), 179\u2013195.\nBordes, A., Glorot, X., Weston, J., and Bengio, Y. (2012).\nJoint\nlearning of words and meaning representations for open-text\nsemantic parsing. AISTATS\u20192012.\nBoulanger-Lewandowski, N., Bengio, Y., and Vincent, P. (2012).\nModeling temporal dependencies in high-dimensional sequences:\nApplication to polyphonic music generation and transcription. In\nICML\u20192012.\nBoureau, Y., Ponce, J., and LeCun, Y. (2010). A theoretical analysis\nof feature pooling in vision algorithms. In ICML\u201910.\nBoureau, Y., Le Roux, N., Bach, F., Ponce, J., and LeCun, Y. (2011).\nAsk the locals: multi-way local pooling for image recognition. In\nICCV\u201911.\nBourlard, H. and Kamp, Y. (1988). Auto-association by multilayer\nperceptrons and singular value decomposition. Biological Cyber-\nnetics, 59, 291\u2013294.\nBrand, M. (2003). Charting a manifold. In NIPS\u20192002, pages 961\u2013\n968. MIT Press.\nBreuleux, O., Bengio, Y., and Vincent, P. (2011). Quickly generating\nrepresentative samples from an RBM-derived process.\nNeural\nComputation, 23(8), 2053\u20132073.\nBruna, J. and Mallat, S. (2011).\nClassi\ufb01cation with scattering\noperators. In ICPR\u20192011.\nCadieu, C. and Olshausen, B. (2009).\nLearning transformational\ninvariants from natural movies.\nIn NIPS\u20192009, pages 209\u2013216.\nMIT Press.\nCarreira-Perpi\u02dcnan, M. A. and Hinton, G. E. (2005). On contrastive\ndivergence learning. In AISTATS\u20192005, pages 33\u201340.\nChen, M., Xu, Z., Winberger, K. Q., and Sha, F. (2012). Marginalized\ndenoising autoencoders for domain adaptation. In ICML\u20192012.\nCho, K., Raiko, T., and Ilin, A. (2010). Parallel tempering is ef\ufb01cient\nfor learning restricted Boltzmann machines. In IJCNN\u20192010.\nCho, K., Raiko, T., and Ilin, A. (2011).\nEnhanced gradient and\nadaptive learning rate for training restricted Boltzmann machines.\nIn ICML\u20192011, pages 105\u2013112.\nCiresan, D., Meier, U., and Schmidhuber, J. (2012). Multi-column\ndeep neural networks for image classi\ufb01cation. Technical report,\narXiv:1202.2745.\nCiresan, D. C., Meier, U., Gambardella, L. M., and Schmidhuber,\nJ. (2010).\nDeep big simple neural nets for handwritten digit\nrecognition. Neural Computation, 22, 1\u201314.\nCoates, A. and Ng, A. Y. (2011a). The importance of encoding versus\ntraining with sparse coding and vector quantization. In ICML\u20192011.\nCoates, A. and Ng, A. Y. (2011b). Selecting receptive \ufb01elds in deep\nnetworks. In NIPS\u20192011.\nCollobert, R. and Weston, J. (2008).\nA uni\ufb01ed architecture for\nnatural language processing: Deep neural networks with multitask\nlearning. In ICML\u20192008.\nCollobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K.,\nand Kuksa, P. (2011). Natural language processing (almost) from\nscratch. Journal of Machine Learning Research, 12, 2493\u20132537.\nCourville, A., Bergstra, J., and Bengio, Y. (2011a). A spike and slab\nrestricted Boltzmann machine. In AISTATS\u20192011.\nCourville, A., Bergstra, J., and Bengio, Y. (2011b). Unsupervised\nmodels of images by spike-and-slab RBMs. In ICML\u20192011.\nDahl, G. E., Ranzato, M., Mohamed, A., and Hinton, G. E. (2010).\nPhone recognition with the mean-covariance restricted Boltzmann\nmachine. In NIPS\u20192010.\nDahl, G. E., Yu, D., Deng, L., and Acero, A. (2012).\nContext-\ndependent pre-trained deep neural networks for large vocabulary\nspeech recognition.\nIEEE Transactions on Audio, Speech, and\nLanguage Processing, 20(1), 33\u201342.\nDeng, L., Seltzer, M., Yu, D., Acero, A., Mohamed, A., and Hinton,\nG. (2010). Binary coding of speech spectrograms using a deep\nauto-encoder. In Interspeech 2010, Makuhari, Chiba, Japan.\nDesjardins, G. and Bengio, Y. (2008).\nEmpirical evaluation of\nconvolutional RBMs for vision.\nTechnical Report 1327, Dept.\nIRO, U. Montr\u00b4eal.\nDesjardins, G., Courville, A., Bengio, Y., Vincent, P., and Delalleau,\nO. (2010). Tempered Markov chain Monte Carlo for training of\nrestricted Boltzmann machine. In AISTATS\u20192010, volume 9, pages\n145\u2013152.\nDesjardins, G., Courville, A., and Bengio, Y. (2011). On tracking the\npartition function. In NIPS\u20192011.\nDesjardins, G., Courville, A., and Bengio, Y. (2012). On training\ndeep Boltzmann machines. Technical Report arXiv:1203.4416v1,\nUniversit\u00b4e de Montr\u00b4eal.\nDiCarlo, J., Zoccolan, D., and Rust, N. (2012). How does the brain\nsolve visual object recognition? Neuron.\nDonoho, D. L. and Grimes, C. (2003).\nHessian eigenmaps: new\nlocally linear embedding techniques for high-dimensional data.\nTechnical Report 2003-08, Dept. Statistics, Stanford University.\nEisner, J. (2012).\nLearning approximate inference policies for\nfast prediction.\nKeynote talk at ICML Workshop on Inferning:\nInteractions Between Search and Learning.\nErhan, D., Courville, A., and Bengio, Y. (2010a).\nUnderstanding\nrepresentations learned in deep architectures.\nTechnical Report\n1355, Universit\u00b4e de Montr\u00b4eal/DIRO.\nErhan, D., Bengio, Y., Courville, A., Manzagol, P.-A., Vincent, P.,\nand Bengio, S. (2010b). Why does unsupervised pre-training help\ndeep learning? Journal of Machine Learning Research, 11, 625\u2013\n660.\nFreund, Y. and Haussler, D. (1994).\nUnsupervised learning of\ndistributions on binary vectors using two layer networks. Technical\nReport UCSC-CRL-94-25, University of California, Santa Cruz.\nFukushima, K. (1980).\nNeocognitron: A self-organizing neural\nnetwork model for a mechanism of pattern recognition unaffected\nby shift in position. Biological Cybernetics, 36, 193\u2013202.\nGlorot, X. and Bengio, Y. (2010). Understanding the dif\ufb01culty of\ntraining deep feedforward neural networks. In AISTATS\u20192010.\nGlorot, X., Bordes, A., and Bengio, Y. (2011a). Deep sparse recti\ufb01er\nneural networks. In AISTATS\u20192011.\nGlorot, X., Bordes, A., and Bengio, Y. (2011b). Domain adaptation\nfor large-scale sentiment classi\ufb01cation: A deep learning approach.\nIn ICML\u20192011.\nGoodfellow, I., Le, Q., Saxe, A., and Ng, A. (2009).\nMeasuring\ninvariances in deep networks. In NIPS\u20192009, pages 646\u2013654.\nGoodfellow, I., Courville, A., and Bengio, Y. (2011).\nSpike-and-\nslab sparse coding for unsupervised feature discovery. In NIPS\nWorkshop on Challenges in Learning Hierarchical Models.\nGoodfellow, I. J., Courville, A., and Bengio, Y. (2012).\nSpike-\nand-slab\nsparse\ncoding\nfor\nunsupervised\nfeature\ndiscovery.\narXiv:1201.3382.\nGregor, K. and LeCun, Y. (2010a). Emergence of complex-like cells\nin a temporal product network with local receptive \ufb01elds. Technical\nreport, arXiv:1006.0448.\nGregor, K. and LeCun, Y. (2010b). Learning fast approximations of\nsparse coding. In ICML\u20192010.\nGregor, K., Szlam, A., and LeCun, Y. (2011).\nStructured sparse\ncoding via lateral inhibition. In NIPS\u20192011.\nGribonval, R. (2011). Should penalized least squares regression be\ninterpreted as Maximum A Posteriori estimation? IEEE Transac-\ntions on Signal Processing, 59(5), 2405\u20132410.\n\n28\nGrosse, R., Raina, R., Kwong, H., and Ng, A. Y. (2007).\nShift-\ninvariant sparse coding for audio classi\ufb01cation. In UAI\u20192007.\nGrubb, A. and Bagnell, J. A. D. (2010). Boosted backpropagation\nlearning for training deep modular networks. In ICML\u20192010.\nGutmann, M. and Hyvarinen, A. (2010). Noise-contrastive estimation:\nA new estimation principle for unnormalized statistical models. In\nAISTATS\u20192010.\nHamel, P., Lemieux, S., Bengio, Y., and Eck, D. (2011). Temporal\npooling and multiscale learning for automatic annotation and\nranking of music audio. In ISMIR.\nH\u02daastad, J. (1986).\nAlmost optimal lower bounds for small depth\ncircuits. In STOC\u201986, pages 6\u201320.\nH\u02daastad, J. and Goldmann, M. (1991). On the power of small-depth\nthreshold circuits. Computational Complexity, 1, 113\u2013129.\nHenaff, M., Jarrett, K., Kavukcuoglu, K., and LeCun, Y. (2011).\nUnsupervised learning of sparse features for scalable audio clas-\nsi\ufb01cation. In ISMIR\u201911.\nHinton, G., Krizhevsky, A., and Wang, S. (2011). Transforming auto-\nencoders. In ICANN\u20192011.\nHinton, G., Deng, L., Dahl, G. E., Mohamed, A., Jaitly, N., Senior, A.,\nVanhoucke, V., Nguyen, P., Sainath, T., and Kingsbury, B. (2012).\nDeep neural networks for acoustic modeling in speech recognition.\nIEEE Signal Processing Magazine, 29(6), 82\u201397.\nHinton, G. E. (1986). Learning distributed representations of con-\ncepts. In Proc. 8th Conf. Cog. Sc. Society, pages 1\u201312.\nHinton, G. E. (1999). Products of experts. In ICANN\u20191999.\nHinton, G. E. (2000). Training products of experts by minimizing\ncontrastive divergence.\nTechnical Report GCNU TR 2000-004,\nGatsby Unit, University College London.\nHinton, G. E. (2010).\nA practical guide to training restricted\nBoltzmann machines.\nTechnical Report UTML TR 2010-003,\nDepartment of Computer Science, University of Toronto.\nHinton, G. E. and Roweis, S. (2003). Stochastic neighbor embedding.\nIn NIPS\u20192002.\nHinton, G. E. and Salakhutdinov, R. (2006). Reducing the dimension-\nality of data with neural networks. Science, 313(5786), 504\u2013507.\nHinton, G. E. and Zemel, R. S. (1994).\nAutoencoders, minimum\ndescription length, and helmholtz free energy. In NIPS\u20191993.\nHinton, G. E., Osindero, S., and Teh, Y. (2006).\nA fast learning\nalgorithm for deep belief nets. Neural Computation, 18, 1527\u2013\n1554.\nHubel, D. H. and Wiesel, T. N. (1959). Receptive \ufb01elds of single\nneurons in the cat\u2019s striate cortex. Journal of Physiology, 148,\n574\u2013591.\nHurri, J. and Hyv\u00a8arinen, A. (2003).\nTemporal coherence, natural\nimage sequences, and the visual cortex. In NIPS\u20192002.\nHyv\u00a8arinen, A. (2005).\nEstimation of non-normalized statistical\nmodels using score matching. J. Machine Learning Res., 6.\nHyv\u00a8arinen, A. (2007). Some extensions of score matching. Compu-\ntational Statistics and Data Analysis, 51, 2499\u20132512.\nHyv\u00a8arinen, A. (2008). Optimal approximation of signal priors. Neural\nComputation, 20(12), 3087\u20133110.\nHyv\u00a8arinen, A. and Hoyer, P. (2000).\nEmergence of phase and\nshift invariant features by decomposition of natural images into\nindependent feature subspaces. Neural Computation, 12(7).\nHyv\u00a8arinen, A., Karhunen, J., and Oja, E. (2001a).\nIndependent\nComponent Analysis. Wiley-Interscience.\nHyv\u00a8arinen, A., Hoyer, P. O., and Inki, M. (2001b).\nTopographic\nindependent component analysis.\nNeural Computation, 13(7),\n1527\u20131558.\nHyv\u00a8arinen, A., Hurri, J., and Hoyer, P. O. (2009). Natural Image\nStatistics: A probabilistic approach to early computational vision.\nSpringer-Verlag.\nJaeger, H. (2007). Echo state network. Scholarpedia, 2(9), 2330.\nJain, V. and Seung, S. H. (2008).\nNatural image denoising with\nconvolutional networks. In NIPS\u20192008.\nJarrett, K., Kavukcuoglu, K., Ranzato, M., and LeCun, Y. (2009).\nWhat is the best multi-stage architecture for object recognition?\nIn ICCV\u201909.\nJenatton, R., Audibert, J.-Y., and Bach, F. (2009).\nStructured\nvariable selection with sparsity-inducing norms. Technical report,\narXiv:0904.3523.\nJutten, C. and Herault, J. (1991). Blind separation of sources, part I:\nan adaptive algorithm based on neuromimetic architecture. Signal\nProcessing, 24, 1\u201310.\nKavukcuoglu, K., Ranzato, M., and LeCun, Y. (2008). Fast inference\nin sparse coding algorithms with applications to object recognition.\nCBLL-TR-2008-12-01, NYU.\nKavukcuoglu, K., Ranzato, M.-A., Fergus, R., and LeCun, Y. (2009).\nLearning invariant features through topographic \ufb01lter maps.\nIn\nCVPR\u20192009.\nKavukcuoglu, K., Sermanet, P., Boureau, Y.-L., Gregor, K., Math-\nieu, M., and LeCun, Y. (2010).\nLearning convolutional feature\nhierarchies for visual recognition. In NIPS\u20192010.\nKingma, D. and LeCun, Y. (2010). Regularized estimation of image\nstatistics by score matching. In NIPS\u20192010.\nKivinen, J. J. and Williams, C. K. I. (2012).\nMultiple texture\nBoltzmann machines. In AISTATS\u20192012.\nK\u00a8ording, K. P., Kayser, C., Einh\u00a8auser, W., and K\u00a8onig, P. (2004).\nHow are complex cell properties adapted to the statistics of natural\nstimuli? J. Neurophysiology, 91.\nKrizhevsky, A. (2010).\nConvolutional deep belief networks on\nCIFAR-10. Technical report, U. Toronto.\nKrizhevsky, A. and Hinton, G. (2009). Learning multiple layers of\nfeatures from tiny images. Technical report, U. Toronto.\nKrizhevsky, A., Sutskever, I., and Hinton, G. (2012). ImageNet clas-\nsi\ufb01cation with deep convolutional neural networks. In NIPS\u20192012.\nLarochelle, H. and Bengio, Y. (2008). Classi\ufb01cation using discrimi-\nnative restricted Boltzmann machines. In ICML\u20192008.\nLarochelle, H., Bengio, Y., Louradour, J., and Lamblin, P. (2009).\nExploring strategies for training deep neural networks. Journal of\nMachine Learning Research, 10, 1\u201340.\nLazebnik, S., Schmid, C., and Ponce, J. (2006).\nBeyond bags of\nfeatures: Spatial pyramid matching for recognizing natural scene\ncategories. In CVPR\u20192006.\nLe, H.-S., Oparin, I., Allauzen, A., Gauvin, J.-L., and Yvon, F. (2013).\nStructured output layer neural network language models for speech\nrecognition. IEEE Trans. Audio, Speech & Language Processing.\nLe, Q., Ngiam, J., Chen, Z., hao Chia, D. J., Koh, P. W., and Ng, A.\n(2010). Tiled convolutional neural networks. In NIPS\u20192010.\nLe, Q., Ngiam, J., Coates, A., Lahiri, A., Prochnow, B., and Ng,\nA. (2011a).\nOn optimization methods for deep learning.\nIn\nICML\u20192011.\nLe, Q. V., Karpenko, A., Ngiam, J., and Ng, A. Y. (2011b). ICA with\nreconstruction cost for ef\ufb01cient overcomplete feature learning. In\nNIPS\u20192011.\nLe, Q. V., Zou, W. Y., Yeung, S. Y., and Ng, A. Y. (2011c). Learning\nhierarchical spatio-temporal features for action recognition with\nindependent subspace analysis. In CVPR\u20192011.\nLe Roux, N., Bengio, Y., Lamblin, P., Joliveau, M., and Kegl, B.\n(2008a). Learning the 2-D topology of images. In NIPS\u201907.\nLe Roux, N., Manzagol, P.-A., and Bengio, Y. (2008b). Topmoumoute\nonline natural gradient algorithm. In NIPS\u201907.\nLeCun, Y. (1986). Learning processes in an asymmetric threshold\nnetwork.\nIn Disordered Systems and Biological Organization,\npages 233\u2013240. Springer-Verlag.\nLeCun, Y. (1987). Mod`eles connexionistes de l\u2019apprentissage. Ph.D.\nthesis, Universit\u00b4e de Paris VI.\nLeCun, Y. (1989). Generalization and network design strategies. In\nConnectionism in Perspective. Elsevier Publishers.\nLeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E.,\nHubbard, W., and Jackel, L. D. (1989). Backpropagation applied\nto handwritten zip code recognition. Neural Computation.\nLeCun, Y., Bottou, L., Orr, G. B., and M\u00a8uller, K. (1998a). Ef\ufb01cient\nbackprop. In Neural Networks, Tricks of the Trade.\nLeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998b). Gradient\nbased learning applied to document recognition. Proc. IEEE.\nLee, H., Ekanadham, C., and Ng, A. (2008). Sparse deep belief net\n\n29\nmodel for visual area V2. In NIPS\u201907.\nLee, H., Grosse, R., Ranganath, R., and Ng, A. Y. (2009a). Convolu-\ntional deep belief networks for scalable unsupervised learning of\nhierarchical representations. In ICML\u20192009.\nLee, H., Pham, P., Largman, Y., and Ng, A. (2009b). Unsupervised\nfeature learning for audio classi\ufb01cation using convolutional deep\nbelief networks. In NIPS\u20192009.\nLin, Y., Tong, Z., Zhu, S., and Yu, K. (2010). Deep coding network.\nIn NIPS\u20192010.\nLowe, D. (1999).\nObject recognition from local scale invariant\nfeatures. In ICCV\u201999.\nMallat, S. (2012). Group invariant scattering. Communications on\nPure and Applied Mathematics.\nMarlin, B. and de Freitas, N. (2011).\nAsymptotic ef\ufb01ciency of\ndeterministic estimators for discrete energy-based models: Ratio\nmatching and pseudolikelihood. In UAI\u20192011.\nMarlin, B., Swersky, K., Chen, B., and de Freitas, N. (2010).\nInductive principles for restricted Boltzmann machine learning. In\nAISTATS\u20192010, pages 509\u2013516.\nMartens, J. (2010). Deep learning via Hessian-free optimization. In\nICML\u20192010, pages 735\u2013742.\nMartens, J. and Sutskever, I. (2011).\nLearning recurrent neural\nnetworks with Hessian-free optimization. In ICML\u20192011.\nMemisevic, R. and Hinton, G. E. (2010). Learning to represent spatial\ntransformations with factored higher-order Boltzmann machines.\nNeural Comp., 22(6).\nMesnil, G., Dauphin, Y., Glorot, X., Rifai, S., Bengio, Y., Goodfellow,\nI., Lavoie, E., Muller, X., Desjardins, G., Warde-Farley, D., Vin-\ncent, P., Courville, A., and Bergstra, J. (2011). Unsupervised and\ntransfer learning challenge: a deep learning approach. In JMLR\nW&CP: Proc. Unsupervised and Transfer Learning, volume 7.\nMikolov, T., Deoras, A., Kombrink, S., Burget, L., and Cernocky,\nJ. (2011).\nEmpirical evaluation and combination of advanced\nlanguage modeling techniques. In INTERSPEECH\u20192011.\nMobahi, H., Collobert, R., and Weston, J. (2009). Deep learning from\ntemporal coherence in video. In ICML\u20192009.\nMohamed, A., Dahl, G., and Hinton, G. (2012). Acoustic modeling\nusing deep belief networks. IEEE Trans. on Audio, Speech and\nLanguage Processing, 20(1), 14\u201322.\nMontufar, G. F. and Morton, J. (2012).\nWhen does a mixture\nof products contain a product of mixtures?\nTechnical report,\narXiv:1206.0387.\nMurray, I. and Salakhutdinov, R. (2009).\nEvaluating probabilities\nunder high-dimensional latent variable models.\nIn NIPS\u20192008,\npages 1137\u20131144.\nNair, V. and Hinton, G. E. (2010). Recti\ufb01ed linear units improve\nrestricted Boltzmann machines. In ICML\u201910.\nNeal, R. M. (1992).\nConnectionist learning of belief networks.\nArti\ufb01cial Intelligence, 56, 71\u2013113.\nNeal, R. M. (1993).\nProbabilistic inference using Markov chain\nMonte-Carlo methods. Technical Report CRG-TR-93-1, Dept. of\nComputer Science, University of Toronto.\nNgiam, J., Chen, Z., Koh, P., and Ng, A. (2011).\nLearning deep\nenergy models. In Proc. ICML\u20192011. ACM.\nOlshausen, B. A. and Field, D. J. (1996).\nEmergence of simple-\ncell receptive \ufb01eld properties by learning a sparse code for natural\nimages. Nature, 381, 607\u2013609.\nOrr, G. and Muller, K.-R., editors (1998). Neural networks: tricks of\nthe trade. Lect. Notes Comp. Sc. Springer-Verlag.\nPascanu, R. and Bengio, Y. (2013).\nNatural gradient revisited.\nTechnical report, arXiv:1301.3584.\nRaiko, T., Valpola, H., and LeCun, Y. (2012). Deep learning made\neasier by linear transformations in perceptrons. In AISTATS\u20192012.\nRaina, R., Battle, A., Lee, H., Packer, B., and Ng, A. Y. (2007).\nSelf-taught learning: transfer learning from unlabeled data.\nIn\nICML\u20192007.\nRanzato, M. and Hinton, G. H. (2010). Modeling pixel means and\ncovariances using factorized third-order Boltzmann machines. In\nCVPR\u20192010, pages 2551\u20132558.\nRanzato, M., Poultney, C., Chopra, S., and LeCun, Y. (2007). Ef\ufb01cient\nlearning of sparse representations with an energy-based model. In\nNIPS\u20192006.\nRanzato, M., Boureau, Y., and LeCun, Y. (2008).\nSparse feature\nlearning for deep belief networks. In NIPS\u20192007.\nRanzato, M., Krizhevsky, A., and Hinton, G. (2010a). Factored 3-\nway restricted Boltzmann machines for modeling natural images.\nIn AISTATS\u20192010, pages 621\u2013628.\nRanzato, M., Mnih, V., and Hinton, G. (2010b). Generating more\nrealistic images using gated MRF\u2019s. In NIPS\u20192010.\nRanzato, M., Susskind, J., Mnih, V., and Hinton, G. (2011). On deep\ngenerative models with applications to recognition. In CVPR\u20192011.\nRiesenhuber, M. and Poggio, T. (1999). Hierarchical models of object\nrecognition in cortex. Nature Neuroscience.\nRifai, S., Vincent, P., Muller, X., Glorot, X., and Bengio, Y. (2011a).\nContractive auto-encoders: Explicit invariance during feature ex-\ntraction. In ICML\u20192011.\nRifai, S., Mesnil, G., Vincent, P., Muller, X., Bengio, Y., Dauphin,\nY., and Glorot, X. (2011b). Higher order contractive auto-encoder.\nIn ECML PKDD.\nRifai, S., Dauphin, Y., Vincent, P., Bengio, Y., and Muller, X. (2011c).\nThe manifold tangent classi\ufb01er. In NIPS\u20192011.\nRifai, S., Bengio, Y., Dauphin, Y., and Vincent, P. (2012).\nA\ngenerative process for sampling contractive auto-encoders.\nIn\nICML\u20192012.\nRoweis, S. (1997). EM algorithms for PCA and sensible PCA. CNS\nTechnical Report CNS-TR-97-02, Caltech.\nRoweis, S. and Saul, L. K. (2000).\nNonlinear dimensionality\nreduction by locally linear embedding. Science, 290(5500).\nSalakhutdinov, R. (2010a). Learning deep Boltzmann machines using\nadaptive MCMC. In ICML\u20192010.\nSalakhutdinov, R. (2010b). Learning in Markov random \ufb01elds using\ntempered transitions. In NIPS\u20192010.\nSalakhutdinov, R. and Hinton, G. E. (2007). Semantic hashing. In\nSIGIR\u20192007.\nSalakhutdinov, R. and Hinton, G. E. (2009).\nDeep Boltzmann\nmachines. In AISTATS\u20192009, pages 448\u2013455.\nSalakhutdinov, R. and Larochelle, H. (2010). Ef\ufb01cient learning of\ndeep Boltzmann machines. In AISTATS\u20192010.\nSalakhutdinov, R., Mnih, A., and Hinton, G. E. (2007). Restricted\nBoltzmann machines for collaborative \ufb01ltering. In ICML 2007.\nSavard, F. (2011). R\u00b4eseaux de neurones `a relaxation entra\u02c6\u0131n\u00b4es par\ncrit`ere d\u2019autoencodeur d\u00b4ebruitant. Master\u2019s thesis, U. Montr\u00b4eal.\nSchmah, T., Hinton, G. E., Zemel, R., Small, S. L., and Strother,\nS. (2009). Generative versus discriminative training of RBMs for\nclassi\ufb01cation of fMRI images. In NIPS\u20192008, pages 1409\u20131416.\nSch\u00a8olkopf, B., Smola, A., and M\u00a8uller, K.-R. (1998).\nNonlinear\ncomponent analysis as a kernel eigenvalue problem.\nNeural\nComputation, 10, 1299\u20131319.\nSchwenk, H., Rousseau, A., and Attik, M. (2012). Large, pruned or\ncontinuous space language models on a gpu for statistical machine\ntranslation. In Workshop on the future of language modeling for\nHLT.\nSeide, F., Li, G., and Yu, D. (2011a).\nConversational speech\ntranscription using context-dependent deep neural networks.\nIn\nInterspeech 2011, pages 437\u2013440.\nSeide, F., Li, G., and Yu, D. (2011b).\nFeature engineering in\ncontext-dependent deep neural networks for conversational speech\ntranscription. In ASRU\u20192011.\nSerre, T., Wolf, L., Bileschi, S., and Riesenhuber, M. (2007). Robust\nobject recognition with cortex-like mechanisms.\nIEEE Trans.\nPattern Anal. Mach. Intell., 29(3), 411\u2013426.\nSeung, S. H. (1998).\nLearning continuous attractors in recurrent\nnetworks. In NIPS\u20191997.\nSimard, D., Steinkraus, P. Y., and Platt, J. C. (2003). Best practices\nfor convolutional neural networks. In ICDAR\u20192003.\nSimard, P., Victorri, B., LeCun, Y., and Denker, J. (1992). Tangent\nprop - A formalism for specifying selected invariances in an\nadaptive network. In NIPS\u20191991.\n\n30\nSimard, P. Y., LeCun, Y., and Denker, J. (1993). Ef\ufb01cient pattern\nrecognition using a new transformation distance. In NIPS\u201992.\nSmolensky, P. (1986). Information processing in dynamical systems:\nFoundations of harmony theory.\nIn D. E. Rumelhart and J. L.\nMcClelland, editors, Parallel Distributed Processing, volume 1,\nchapter 6, pages 194\u2013281. MIT Press, Cambridge.\nSnoek, J., Larochelle, H., and Adams, R. P. (2012). Practical bayesian\noptimization of machine learning algorithms. In NIPS\u20192012.\nSocher, R., Huang, E. H., Pennington, J., Ng, A. Y., and Manning,\nC. D. (2011a). Dynamic pooling and unfolding recursive autoen-\ncoders for paraphrase detection. In NIPS\u20192011.\nSocher, R., Pennington, J., Huang, E. H., Ng, A. Y., and Manning,\nC. D. (2011b). Semi-supervised recursive autoencoders for pre-\ndicting sentiment distributions. In EMNLP\u20192011.\nSrivastava, N. and Salakhutdinov, R. (2012). Multimodal learning\nwith deep boltzmann machines. In NIPS\u20192012.\nStoyanov, V., Ropson, A., and Eisner, J. (2011).\nEmpirical risk\nminimization of graphical model parameters given approximate\ninference, decoding, and model structure. In AISTATS\u20192011.\nSutskever, I. (2012).\nTraining Recurrent Neural Networks.\nPh.D.\nthesis, Departement of computer science, University of Toronto.\nSutskever, I. and Tieleman, T. (2010). On the Convergence Properties\nof Contrastive Divergence. In AISTATS\u20192010.\nSutskever, I., Hinton, G., and Taylor, G. (2009).\nThe recurrent\ntemporal restricted Boltzmann machine. In NIPS\u20192008.\nSwersky, K. (2010).\nInductive Principles for Learning Restricted\nBoltzmann Machines.\nMaster\u2019s thesis, University of British\nColumbia.\nSwersky, K., Ranzato, M., Buchman, D., Marlin, B., and de Freitas,\nN. (2011).\nOn score matching for energy based models: Gen-\neralizing autoencoders and simplifying deep learning.\nIn Proc.\nICML\u20192011. ACM.\nTaylor, G. and Hinton, G. (2009).\nFactored conditional restricted\nBoltzmann machines for modeling motion style. In ICML\u20192009.\nTaylor, G., Fergus, R., LeCun, Y., and Bregler, C. (2010). Convolu-\ntional learning of spatio-temporal features. In ECCV\u201910.\nTenenbaum, J., de Silva, V., and Langford, J. C. (2000). A global\ngeometric framework for nonlinear dimensionality reduction. Sci-\nence, 290(5500), 2319\u20132323.\nTieleman, T. (2008). Training restricted Boltzmann machines using\napproximations to the likelihood gradient. In ICML\u20192008, pages\n1064\u20131071.\nTieleman, T. and Hinton, G. (2009). Using fast weights to improve\npersistent contrastive divergence. In ICML\u20192009.\nTipping, M. E. and Bishop, C. M. (1999).\nProbabilistic principal\ncomponents analysis. J. Roy. Stat. Soc. B, (3).\nTuraga, S. C., Murray, J. F., Jain, V., Roth, F., Helmstaedter, M.,\nBriggman, K., Denk, W., and Seung, H. S. (2010). Convolutional\nnetworks can learn to generate af\ufb01nity graphs for image segmen-\ntation. Neural Computation, 22, 511\u2013538.\nvan der Maaten, L. (2009).\nLearning a parametric embedding by\npreserving local structure. In AISTATS\u20192009.\nvan der Maaten, L. and Hinton, G. E. (2008). Visualizing data using\nt-SNE. J. Machine Learning Res., 9.\nVincent, P. (2011).\nA connection between score matching and\ndenoising autoencoders. Neural Computation, 23(7).\nVincent, P. and Bengio, Y. (2003). Manifold Parzen windows. In\nNIPS\u20192002. MIT Press.\nVincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.-A. (2008).\nExtracting and composing robust features with denoising autoen-\ncoders. In ICML 2008.\nVincent, P., Larochelle, H., Lajoie, I., Bengio, Y., and Manzagol,\nP.-A. (2010).\nStacked denoising autoencoders: Learning useful\nrepresentations in a deep network with a local denoising criterion.\nJ. Machine Learning Res., 11.\nWeinberger, K. Q. and Saul, L. K. (2004). Unsupervised learning of\nimage manifolds by semide\ufb01nite programming. In CVPR\u20192004,\npages 988\u2013995.\nWelling, M. (2009). Herding dynamic weights for partially observed\nrandom \ufb01eld models. In UAI\u20192009.\nWelling, M., Hinton, G. E., and Osindero, S. (2003). Learning sparse\ntopographic representations with products of Student-t distribu-\ntions. In NIPS\u20192002.\nWeston, J., Ratle, F., and Collobert, R. (2008). Deep learning via\nsemi-supervised embedding. In ICML 2008.\nWeston, J., Bengio, S., and Usunier, N. (2010). Large scale image\nannotation: learning to rank with joint word-image embeddings.\nMachine Learning, 81(1), 21\u201335.\nWiskott, L. and Sejnowski, T. (2002). Slow feature analysis: Un-\nsupervised learning of invariances.\nNeural Computation, 14(4),\n715\u2013770.\nYounes, L. (1999).\nOn the convergence of Markovian stochastic\nalgorithms with rapidly decreasing ergodicity rates. Stochastics\nand Stochastic Reports, 65(3), 177\u2013228.\nYu, D., Wang, S., and Deng, L. (2010). Sequential labeling using\ndeep-structured conditional random \ufb01elds. IEEE Journal of Se-\nlected Topics in Signal Processing.\nYu, K. and Zhang, T. (2010). Improved local coordinate coding using\nlocal tangents. In ICML\u20192010.\nYu, K., Zhang, T., and Gong, Y. (2009). Nonlinear learning using\nlocal coordinate coding. In NIPS\u20192009.\nYu, K., Lin, Y., and Lafferty, J. (2011). Learning image representa-\ntions from the pixel level via hierarchical sparse coding. In CVPR.\nYuille, A. L. (2005). The convergence of contrastive divergences. In\nNIPS\u20192004, pages 1593\u20131600.\nZeiler, M., Krishnan, D., Taylor, G., and Fergus, R. (2010). Decon-\nvolutional networks. In CVPR\u20192010.\nZou, W. Y., Ng, A. Y., and Yu, K. (2011). Unsupervised learning\nof visual invariance with temporal coherence.\nIn NIPS 2011\nWorkshop on Deep Learning and Unsupervised Feature Learning.\n",
    "Tree of Thoughts: Deliberate Problem Solving\nwith Large Language Models\nShunyu Yao\nPrinceton University\nDian Yu\nGoogle DeepMind\nJeffrey Zhao\nGoogle DeepMind\nIzhak Shafran\nGoogle DeepMind\nThomas L. Griffiths\nPrinceton University\nYuan Cao\nGoogle DeepMind\nKarthik Narasimhan\nPrinceton University\nAbstract\nLanguage models are increasingly being deployed for general problem solving\nacross a wide range of tasks, but are still confined to token-level, left-to-right\ndecision-making processes during inference. This means they can fall short in\ntasks that require exploration, strategic lookahead, or where initial decisions play\na pivotal role. To surmount these challenges, we introduce a new framework for\nlanguage model inference, \u201cTree of Thoughts\u201d (ToT), which generalizes over the\npopular \u201cChain of Thought\u201d approach to prompting language models, and enables\nexploration over coherent units of text (\u201cthoughts\u201d) that serve as intermediate steps\ntoward problem solving. ToT allows LMs to perform deliberate decision making\nby considering multiple different reasoning paths and self-evaluating choices to\ndecide the next course of action, as well as looking ahead or backtracking when\nnecessary to make global choices. Our experiments show that ToT significantly\nenhances language models\u2019 problem-solving abilities on three novel tasks requiring\nnon-trivial planning or search: Game of 24, Creative Writing, and Mini Crosswords.\nFor instance, in Game of 24, while GPT-4 with chain-of-thought prompting only\nsolved 4% of tasks, our method achieved a success rate of 74%. Code repo with all\nprompts: https://github.com/princeton-nlp/tree-of-thought-llm.\n1\nIntroduction\nOriginally designed to generate text, scaled-up versions of language models (LMs) such as GPT [25,\n26, 1, 23] and PaLM [5] have been shown to be increasingly capable of performing an ever wider\nrange of tasks requiring mathematical, symbolic, commonsense, and knowledge reasoning. It is\nperhaps surprising that underlying all this progress is still the original autoregressive mechanism for\ngenerating text, which makes token-level decisions one by one and in a left-to-right fashion. Is such\na simple mechanism sufficient for a LM to be built toward a general problem solver? If not, what\nproblems would challenge the current paradigm, and what should be alternative mechanisms?\nThe literature on human cognition provides some clues to answer these questions. Research on \u201cdual\nprocess\u201d models suggests that people have two modes in which they engage with decisions \u2013 a fast,\nautomatic, unconscious mode (\u201cSystem 1\u201d) and a slow, deliberate, conscious mode (\u201cSystem 2\u201d)\n[30, 31, 16, 15]. These two modes have previously been connected to a variety of mathematical\nmodels used in machine learning. For example, research on reinforcement learning in humans and\nother animals has explored the circumstances under which they engage in associative \u201cmodel free\u201d\nlearning or more deliberative \u201cmodel based\u201d planning [7]. The simple associative token-level choices\nof LMs are also reminiscent of \u201cSystem 1\u201d, and thus might benefit from augmentation by a more\ndeliberate \u201cSystem 2\u201d planning process that (1) maintains and explores diverse alternatives for current\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2305.10601v2  [cs.CL]  3 Dec 2023\n\nG\u012e\u0154\u0169\u019c\nj\u0169\u019c\u0154\u0169\u019c\nG\u012e\u0154\u0169\u019c\nj\u0169\u019c\u0154\u0169\u019c\nG\u012e\u0154\u0169\u019c\n\u02e4j\u0169\u019c\u0154\u0169\u019c\n\u02b1\u00e7\u02b2\u02e4\u0092\u00f2\u0126\u0199\u02e4\u001d\u0135\u012e\u015d\u0193\u015d\u0164\u00f2\u012e\u00e7\u0186\u02e4\n\u0180\u0193\u019c\u010e\u02e4\u001d\u0135\u00c9\u02e4\u02b1\u001d\u0135\u00c9\u02c1\u0092\u001d\u02b2\na\u00ca\u0120\u0135\u0157\u0193\u0164\u0186\u02e4\u017f\u0135\u0164\u00f2\nG\u012e\u0154\u0169\u019c\n\u02e4j\u0169\u019c\u0154\u0169\u019c\n\u02b1\u00ed\u02b2\u02e4\u00c9\u0157\u00f2\u00f2\u02e4\u0135\u0199\u02e4\u009a\u010e\u0135\u0169\u0108\u010e\u0164\u015d\u02e4\u02b1\u00c9\u0135\u00c9\u02b2\n\u029f\u029f\n\u029f\u029f\n\u029f\u029f\n\u029f\u029f\n\u029f\u029f\n\u02e4\u02e4\u019b\u010e\u0135\u0169\u0108\u010e\u019c\n\u02b1\u00e7\u02b2\u02e4\u001d\u010e\u00ca\u0111\u012e\u02e4\u0135\u0199\u02e4\u009a\u010e\u0135\u0169\u0108\u010e\u019c\u02e4\n\u0089\u0157\u0135\u012d\u0154\u019c\u0111\u012e\u0108\u02e4\u02b1\u001d\u0135\u00c9\u02b2\n\u02b1\u00ca\u02b2\u02e4G\u012e\u0154\u0169\u019c\u02c1j\u0169\u019c\u0154\u0169\u019c\u02e4\n\u0089\u0157\u0135\u012d\u0154\u019c\u0111\u012e\u0108\u02e4\u02b1Gj\u02b2\nFigure 1: Schematic illustrating various approaches to problem solving with LLMs. Each rectangle\nbox represents a thought, which is a coherent language sequence that serves as an intermediate\nstep toward problem solving. See concrete examples of how thoughts are generated, evaluated, and\nsearched in Figures 2,4,6.\nchoices instead of just picking one, and (2) evaluates its current status and actively looks ahead or\nbacktracks to make more global decisions.\nTo design such a planning process, we return to the origins of artificial intelligence (and cognitive\nscience), drawing inspiration from the planning processes explored by Newell, Shaw, and Simon\nstarting in the 1950s [21, 22]. Newell and colleagues characterized problem solving [21] as search\nthrough a combinatorial problem space, represented as a tree. We thus propose the Tree of Thoughts\n(ToT) framework for general problem solving with language models. As Figure 1 illustrates, while\nexisting methods (detailed below) sample continuous language sequences for problem solving, ToT\nactively maintains a tree of thoughts, where each thought is a coherent language sequence that serves\nas an intermediate step toward problem solving (Table 1). Such a high-level semantic unit allows the\nLM to self-evaluate the progress different intermediate thoughts make towards solving the problem\nthrough a deliberate reasoning process that is also instantiated in language (Figures 2,4,6). This\nimplementation of search heuristics via LM self-evaluation and deliberation is novel, as previous\nsearch heuristics are either programmed or learned. Finally, we combine this language-based\ncapability to generate and evaluate diverse thoughts with search algorithms, such as breadth-first\nsearch (BFS) or depth-first search (DFS), which allow systematic exploration of the tree of thoughts\nwith lookahead and backtracking.\nEmpirically, we propose three new problems that challenge existing LM inference methods even with\nthe state-of-the-art language model, GPT-4 [23]: Game of 24, Creative Writing, and Crosswords\n(Table 1). These tasks require deductive, mathematical, commonsense, lexical reasoning abilities,\nand a way to incorporate systematic planning or search. We show ToT obtains superior results on\nall three tasks by being general and flexible enough to support different levels of thoughts, different\nways to generate and evaluate thoughts, and different search algorithms that adapt to the nature of\ndifferent problems. We also analyze how such choices affect model performances via systematic\nablations and discuss future directions to better train and use LMs.\n2\nBackground\nWe first formalize some existing methods that use large language models for problem-solving,\nwhich our approach is inspired by and later compared with. We use p\u03b8 to denote a pre-trained LM\nwith parameters \u03b8, and lowercase letters x, y, z, s, \u00b7 \u00b7 \u00b7 to denote a language sequence, i.e. x =\n(x[1], \u00b7 \u00b7 \u00b7 , x[n]) where each x[i] is a token, so that p\u03b8(x) = Qn\ni=1 p\u03b8(x[i]|x[1...i]). We use uppercase\nletters S, \u00b7 \u00b7 \u00b7 to denote a collection of language sequences.\nInput-output (IO) prompting is the most common way to turn a problem input x into output\ny with LM: y \u223cp\u03b8(y|promptIO(x)), where promptIO(x) wraps input x with task instructions\nand/or few-shot input-output examples. For simplicity, let us denote pprompt\n\u03b8\n(output | input) =\np\u03b8(output | prompt(input)), so that IO prompting can be formulated as y \u223cpIO\n\u03b8 (y|x).\n2\n\nChain-of-thought (CoT) prompting [38] was proposed to address cases where the mapping of\ninput x to output y is non-trivial (e.g. when x is a math question and y is the final numerical answer).\nThe key idea is to introduce a chain of thoughts z1, \u00b7 \u00b7 \u00b7 , zn to bridge x and y, where each zi is a\ncoherent language sequence that serves as a meaningful intermediate step toward problem solving\n(e.g. zi could be an intermediate equation for math QA). To solve problems with CoT, each thought\nzi \u223cpCoT\n\u03b8\n(zi | x, z1\u00b7\u00b7\u00b7i\u22121) is sampled sequentially, then the output y \u223cpCoT\n\u03b8\n(y|x, z1\u00b7\u00b7\u00b7n). In\npractice, [z1\u00b7\u00b7\u00b7n, y] \u223cpCoT\n\u03b8\n(z1\u00b7\u00b7\u00b7n, y|x) is sampled as a continuous language sequence, and the\ndecomposition of thoughts (e.g. is each zi a phrase, a sentence, or a paragraph) is left ambiguous.\nSelf-consistency with CoT (CoT-SC) [36] is an ensemble approach that samples k i.i.d. chains\nof thought: [z(i)\n1\u00b7\u00b7\u00b7n, y(i)] \u223cpCoT\n\u03b8\n(z1\u00b7\u00b7\u00b7n, y|x) (i = 1 \u00b7 \u00b7 \u00b7 k), then returns the most frequent output:\narg maxy #{i | y(i) = y}. CoT-SC improves upon CoT, because there are generally different\nthought processes for the same problem (e.g. different ways to prove the same theorem), and the\noutput decision can be more faithful by exploring a richer set of thoughts. However, within each\nchain there is no local exploration of different thought steps, and the \u201cmost frequent\u201d heuristic only\napplies when the output space is limited (e.g. multi-choice QA).\n3\nTree of Thoughts: Deliberate Problem Solving with LM\nA genuine problem-solving process involves the repeated use of available informa-\ntion to initiate exploration, which discloses, in turn, more information until a way\nto attain the solution is finally discovered.\u2014\u2014 Newell et al. [21]\nResearch on human problem-solving suggests that people search through a combinatorial problem-\nspace \u2013 a tree where the nodes represent partial solutions, and the branches correspond to operators\nthat modify them [21, 22]. Which branch to take is determined by heuristics that help to navigate the\nproblem-space and guide the problem-solver towards a solution. This perspective highlights two key\nshortcomings of existing approaches that use LMs to solve general problems: 1) Locally, they do not\nexplore different continuations within a thought process \u2013 the branches of the tree. 2) Globally, they\ndo not incorporate any type of planning, lookahead, or backtracking to help evaluate these different\noptions \u2013 the kind of heuristic-guided search that seems characteristic of human problem-solving.\nTo address these shortcomings, we introduce Tree of Thoughts (ToT), a paradigm that allows LMs to\nexplore multiple reasoning paths over thoughts (Figure 1(c)). ToT frames any problem as a search\nover a tree, where each node is a state s = [x, z1\u00b7\u00b7\u00b7i] representing a partial solution with the input and\nthe sequence of thoughts so far. A specific instantiation of ToT involves answering four questions:\n1. How to decompose the intermediate process into thought steps; 2. How to generate potential\nthoughts from each state; 3. How to heuristically evaluate states; 4. What search algorithm to use.\n1. Thought decomposition. While CoT samples thoughts coherently without explicit decomposition,\nToT leverages problem properties to design and decompose intermediate thought steps. As Table 1\nshows, depending on different problems, a thought could be a couple of words (Crosswords), a line of\nequation (Game of 24), or a whole paragraph of writing plan (Creative Writing). In general, a thought\nshould be \u201csmall\u201d enough so that LMs can generate promising and diverse samples (e.g. generating\na whole book is usually too \u201cbig\u201d to be coherent), yet \u201cbig\u201d enough so that LMs can evaluate its\nprospect toward problem solving (e.g. generating one token is usually too \u201csmall\u201d to evaluate).\n2. Thought generator G(p\u03b8, s, k). Given a tree state s = [x, z1\u00b7\u00b7\u00b7i], we consider two strategies to\ngenerate k candidates for the next thought step:\n(a) Sample i.i.d. thoughts from a CoT prompt (Creative Writing, Figure 4):\nz(j)\n\u223c\npCoT\n\u03b8\n(zi+1|s) = pCoT\n\u03b8\n(zi+1|x, z1\u00b7\u00b7\u00b7i) (j = 1 \u00b7 \u00b7 \u00b7 k). This works better when the thought\nspace is rich (e.g. each thought is a paragraph), and i.i.d. samples lead to diversity;\n(b) Propose thoughts sequentially using a \u201cpropose prompt\u201d (Game of 24, Figure 2; Crosswords,\nFigure 6): [z(1), \u00b7 \u00b7 \u00b7 , z(k)] \u223cppropose\n\u03b8\n(z(1\u00b7\u00b7\u00b7k)\ni+1\n| s). This works better when the thought\nspace is more constrained (e.g. each thought is just a word or a line), so proposing different\nthoughts in the same context avoids duplication.\n3. State evaluator V (p\u03b8, S). Given a frontier of different states, the state evaluator evaluates the\nprogress they make towards solving the problem, serving as a heuristic for the search algorithm\nto determine which states to keep exploring and in which order. While heuristics are a standard\napproach to solving search problems, they are typically either programmed (e.g. DeepBlue [3]) or\n3\n\nlearned (e.g. AlphaGo [29]). We propose a third alternative, by using the LM to deliberately reason\nabout states. When applicable, such a deliberate heuristic can be more flexible than programmed\nrules, and more sample-efficient than learned models. Similar to the thought generator, we consider\ntwo strategies to evaluate states either independently or together:\n(a) Value each state independently: V (p\u03b8, S)(s) \u223cpvalue\n\u03b8\n(v|s) \u2200s \u2208S, where a value\nprompt reasons about the state s to generate a scalar value v (e.g. 1-10) or a classifica-\ntion (e.g. sure/likely/impossible) that could be heuristically turned into a value. The basis\nof such evaluative reasoning can vary across problems and thought steps. In this work, we\nexplore evaluation via few lookahead simulations (e.g. quickly confirm that 5, 5, 14 can\nreach 24 via 5 + 5 + 14, or \u201chot l\u201d can mean \u201cinn\u201d via filling \u201ce\u201d in \u201c \u201d) plus commonsense\n(e.g. 1 2 3 are too small to reach 24, or no word can start with \u201ctzxc\u201d). While the former\nmight promote \u201cgood\u201d states, the latter could help eliminate \u201cbad\u201d states. Such valuations\ndo not need to be perfect, and only need to be approximately helpful for decision making.\n(b) Vote across states: V (p\u03b8, S)(s) = 1[s = s\u2217], where a \u201cgood\u201d state s\u2217\u223cpvote\n\u03b8\n(s\u2217|S) is\nvoted out based on deliberately comparing different states in S in a vote prompt. When\nproblem success is harder to directly value (e.g. passage coherency), it is natural to to instead\ncompare different partial solutions and vote for the most promising one. This is similar\nin spirit to a \u201cstep-wise\u201d self-consistency strategy, i.e. cast \u201cwhich state to explore\u201d as a\nmulti-choice QA, and use LM samples to vote for it.\nFor both strategies, we could prompt the LM multiple times to aggregate the value or vote results to\ntrade time/resource/cost for more faithful/robust heuristics.\nAlgorithm 1 ToT-BFS(x, p\u03b8, G, k, V, T, b)\nRequire: Input x, LM p\u03b8, thought generator G()\n& size limit k, states evaluator V (), step limit T,\nbreadth limit b.\nS0 \u2190{x}\nfor t = 1, \u00b7 \u00b7 \u00b7 , T do\nS\u2032\nt \u2190{[s, z] | s \u2208St\u22121, zt \u2208G(p\u03b8, s, k)}\nVt \u2190V (p\u03b8, S\u2032\nt)\nSt \u2190arg maxS\u2282S\u2032\nt,|S|=b\nP\ns\u2208S Vt(s)\nend for\nreturn G(p\u03b8, arg maxs\u2208ST VT (s), 1)\nAlgorithm 2 ToT-DFS(s, t, p\u03b8, G, k, V, T, vth)\nRequire: Current state s, step t, LM p\u03b8, thought\ngenerator G() and size limit k, states evaluator\nV (), step limit T, threshold vth\nif t > T then record output G(p\u03b8, s, 1)\nend if\nfor s\u2032 \u2208G(p\u03b8, s, k) do\n\u25b7sorted candidates\nif V (p\u03b8, {s\u2032})(s) > vthres then \u25b7pruning\nDFS(s\u2032, t + 1)\nend if\nend for\n4. Search algorithm. Finally, within the ToT framework, one can plug and play different search\nalgorithms depending on the tree structure. We explore two relatively simple search algorithms and\nleave more advanced ones (e.g. A* [11], MCTS [2]) for future work:\n(a) Breadth-first search (BFS) (Algorithm 1) maintains a set of the b most promising states\nper step. This is used for Game of 24 and Creative Writing where the tree depth is limit\n(T \u22643), and initial thought steps can be evaluated and pruned to a small set (b \u22645).\n(b) Depth-first search (DFS) (Algorithm 2) explores the most promising state first, until the\nfinal output is reached (t > T), or the state evaluator deems it impossible to solve the\nproblem from the current s (V (p\u03b8, {s})(s) \u2264vth for a value threshold vth). In the latter\ncase, the subtree from s is pruned to trade exploration for exploitation. In both cases, DFS\nbacktracks to the parent state of s to continue exploration.\nConceptually, ToT has several benefits as a method for general problem-solving with LMs: (1) Gener-\nality. IO, CoT, CoT-SC, and self-refinement can be seen as special cases of ToT (i.e. trees of limited\ndepth and breadth; Figure 1). (2) Modularity. The base LM, as well as the thought decomposition,\ngeneration, evaluation, and search procedures can all be varied independently. (3) Adaptability.\nDifferent problem properties, LM capabilities, and resource constraints can be accommodated. (4)\nConvenience. No extra training is needed, just a pre-trained LM is sufficient. The next section will\nshow how these conceptual benefits translate to strong empirical performance in different problems.\n4\nExperiments\nWe propose three tasks that are hard even when sampling from the state-of-the-art language model,\nGPT-4 [23], using standard IO prompting or chain-of-thought (CoT) prompting. We show how\n4\n\nGame of 24\nCreative Writing\n5x5 Crosswords\nInput\n4 numbers (4 9 10 13)\n4 random sentences\n10 clues (h1. presented;..)\nOutput\nAn equation to reach 24\n(13-9)*(10-4)=24\nA passage of 4 paragraphs\nending in the 4 sentences\n5x5 letters:\nSHOWN;\nWIRRA; AVAIL; ...\nThoughts\n3 intermediate equations\n(13-9=4 (left 4,4,10); 10-\n4=6 (left 4,6); 4*6=24)\nA\nshort\nwriting\nplan\n(1. Introduce a book that\nconnects...)\nWords to fill in for clues:\n(h1. shown; v5. naled; ...)\n#ToT steps\n3\n1\n5-10 (variable)\nTable 1: Task overview. Input, output, thought examples are in blue.\ndeliberate search in trees of thoughts (ToT) produces better results, and more importantly, interesting\nand promising new ways to use language models to solve problems requiring search or planning.\nUnless otherwise stated, we perform experiments using a Chat Completion mode GPT-41 with a\nsampling temperature of 0.7.\n4.1\nGame of 24\nGame of 24 is a mathematical reasoning challenge, where the goal is to use 4 numbers and basic\narithmetic operations (+-*/) to obtain 24. For example, given input \u201c4 9 10 13\u201d, a solution output\ncould be \u201c(10 - 4) * (13 - 9) = 24\u201d.\n\u02b3\u0135\u012e\u00f2\u02e4\u00f2\u0185\u00ca\u012d\u0154\u0126\u00f2\u02b4\u02e4\nG\u012e\u0154\u0169\u019c\u029d\u02e4\u0281\u02e4\u0286\u02e4\u027e\u027d\u02e4\u027e\u0280\n\u0089\u0135\u015d\u015d\u0111\u00e6\u0126\u00f2\u02e4\u012e\u00f2\u0185\u019c\u02e4\u015d\u0164\u00f2\u0154\u015d\u029d\u02e4\u02e4\u02e4\n\u000bD\f\u00033URSRVH\u00033URPSW\n\u0281\u02e4\u030c\u02e4\u0286\u02e4\u0310\u02e4\u027e\u0280\u02e4\u02b1\u0126\u00f2\u0199\u019c\u029d\u02e4\u027e\u027d\u02e4\u027e\u0280\u02e4\u027e\u0280\u02b2\n\u027e\u027d\u02e4\u02ca\u02e4\u0281\u02e4\u0310\u02e4\u0283\u02e4\u02b1\u0126\u00f2\u0199\u019c\u029d\u02e4\u0283\u02e4\u0286\u02e4\u027e\u0280\u02b2\n\u02b3\u029b\u029b\u029b\u012d\u0135\u0157\u00f2\u02e4\u0126\u0111\u012e\u00f2\u015d\u029f\u02b4\n7KRXJKW\u0003*HQHUDWLRQ\n/0\n)\u017f\u00ca\u0126\u0169\u00ca\u0164\u00f2\u02e4\u0111\u0199\u02e4\u0108\u0193\u017f\u00f2\u012e\u02e4\u012e\u0169\u012d\u00e6\u00f2\u0157\u015d\u02e4\u00e7\u00ca\u012e\u02e4\n\u0157\u00f2\u00ca\u00e7\u010e\u02e4\u027f\u0281\u02e4\u02b1\u015d\u0169\u0157\u00f2\u02ab\u0126\u0111\u0123\u00f2\u0126\u0186\u02ab\u0111\u012d\u0154\u0135\u015d\u015d\u0111\u00e6\u0126\u00f2\u02b2\n\u027e\u027d\u02e4\u027e\u0281\u029d\u02e4\u027e\u027d\u02e4\u030c\u02e4\u027e\u0281\u02e4\u0310\u02e4\u027f\u0281\u029b\u02e4\u015d\u0169\u0157\u00f2\n\u02b3\u012d\u0135\u0157\u00f2\u02e4\u00f2\u0185\u00ca\u012d\u0154\u0126\u00f2\u015d\u02b4\n\u027e\u027d\u02e4\u027e\u0280\u02e4\u027e\u0280\n\u000bE\f\u00039DOXH\u00033URPSW\n\u02b1\u027e\u0280\u02e4\u02ca\u02e4\u027e\u027d\u02b2\u02e4\u02a6\u02e4\u027e\u0280\u02e4\u0310\u02e4\u0280\u02e4\u02a6\u02e4\u027e\u0280\u02e4\u0310\u02e4\u0280\u0286\n\u027e\u027d\u02e4\u030c\u02e4\u027e\u0280\u02e4\u030c\u02e4\u027e\u0280\u02e4\u0310\u02e4\u0280\u0283\u02e4\u009a\u010e\u00f2\u0157\u00f2\u02e4\u0193\u015d\u02e4\u012e\u0135\u02e4\u0180\u00ca\u0186\u02e4\n\u0198\u0135\u02e4\u0135\u00e6\u0164\u00ca\u0111\u012e\u02e4\u027f\u0281\u02e4\u0180\u0193\u019c\u010e\u02e4\u019b\u010e\u00f2\u015d\u00f2\u02e4\u00e6\u0193\u0108\u02e4\n\u012e\u0169\u012d\u00e6\u00f2\u0157\u015d\u029b\u02e4\u0111\u012d\u0154\u0135\u015d\u015d\u0111\u00e6\u0126\u00f2\n7KRXJKW\u0003(YDOXDWLRQ\n/0\nG\u012e\u0154\u0169\u019c\u029d\u02e4\u0281\u02e4\u0286\u02e4\u027e\u027d\u02e4\u027e\u0280\n\u0281\u030c\u0286\u0310\u027e\u0280\n\u02b1\u0126\u00f2\u0199\u019c\u029d\u02e4\u027e\u027d\u02e4\u027e\u0280\u02e4\u027e\u0280\u02b2\n\u027e\u027d\u02c1\u0281\u0310\u0283\n\u02b1\u0126\u00f2\u0199\u019c\u029d\u02e4\u0283\u02e4\u0286\u02e4\u027e\u0280\u02b2\n\u029f\u029f\n\u027e\u0280\u02c1\u0283\u0310\u0284\n\u02b1\u0126\u00f2\u0199\u019c\u029d\u02e4\u0284\u02e4\u0286\u02b2\n\u027e\u0280\u02c1\u0286\u0310\u0281\n\u02b1\u0126\u00f2\u0199\u019c\u029d\u02e4\u0281\u02e4\u0283\u02b2\n\u029f\u029f\n\u0281\u02a6\u0283\u0310\u027f\u0281\n\u02b1\u0126\u00f2\u0199\u019c\u029d\u02e4\u027f\u0281\u02b2\n\u0281\u030c\u0283\u0310\u027e\u027d\n\u02b1\u0126\u00f2\u0199\u019c\u029d\u02e4\u027e\u027d\u02b2\n\u029f\u029f\nFigure 2: ToT in a game of 24. The LM is prompted for (a) thought generation and (b) valuation.\nTask Setup. We scrape data from 4nums.com, which has 1,362 games that are sorted from easy to\nhard by human solving time, and use a subset of relatively hard games indexed 901-1,000 for testing.\nFor each task, we consider the output as success if it is a valid equation that equals 24 and uses the\ninput numbers each exactly once. We report the success rate across 100 games as the metric.\nBaselines. We use a standard input-output (IO) prompt with 5 in-context examples. For chain-of-\nthought (CoT) prompting, we augment each input-output pair with 3 intermediate equations, each\noperating on two remaining numbers. For example, given input \u201c4 9 10 13\u201d, the thoughts could be\n\u201c13 - 9 = 4 (left: 4 4 10); 10 - 4 = 6 (left: 4 6); 4 * 6 = 24 (left: 24)\u201d. For each game, we sample IO\nand CoT prompting for 100 times for average performance. We also consider a CoT self-consistency\nbaseline, which takes the majority output from 100 CoT samples, and an iterative-refine approach on\ntop of an IO sample for at most 10 iterations. At each iteration, the LM is conditioned on all previous\nhistory to \u201creflect on your mistakes and generate a refined answer\u201d if the output is incorrect. Note\nthat it uses groundtruth feedback signals about equation correctness.\nToT Setup. To frame Game of 24 into ToT, it is natural to decompose the thoughts into 3 steps,\neach an intermediate equation. As shown in Figure 2(a), at each tree node, we exact the remaining\nnumbers and prompt the LM to propose some possible next steps. The same \u201cpropose prompt\u201d is\nused for all 3 thought steps, though it only has one example with 4 input numbers. We perform a\nbreadth-first search (BFS) in ToT, where at each step we keep the best b = 5 candidates. To perform\ndeliberate BFS in ToT, as shown in Figure 2(b), we prompt LM to evaluate each thought candidate as\n\u201csure/maybe/impossible\u201d with regard to reaching 24. The aim is to promote correct partial solutions\nthat can be verdicted within few lookahead trials, and eliminate impossible partial solutions based on\n\u201ctoo big/small\u201d commonsense, and keep the rest \u201cmaybe\u201d. We sample values 3 times for each thought.\n1Experiments were done between May 5-16, 2023.\n5\n\nMethod\nSuccess\nIO prompt\n7.3%\nCoT prompt\n4.0%\nCoT-SC (k=100)\n9.0%\nToT (ours) (b=1)\n45%\nToT (ours) (b=5)\n74%\nIO + Refine (k=10)\n27%\nIO (best of 100)\n33%\nCoT (best of 100)\n49%\nTable 2: Game of 24 Results.\n0\n25\n50\n75\n100\n0.2\n0.4\n0.6\n(a) Success rate with nodes visited\nIO (best of k)\nCoT (best of k)\nToT (b=1...5)\n1\n2\n3\n4\nCorrect\n0.0\n0.2\n0.4\n0.6\n(b) Samples failed at each step\nCoT\nToT (b=5)\nFigure 3: Game of 24 (a) scale analysis & (b) error analysis.\nResults. As shown in Table 2, IO, CoT, and CoT-SC prompting methods perform badly on the task,\nachieving only 7.3%, 4.0%, and 9.0% success rates. In contrast, ToT with a breadth of b = 1 already\nachieves a success rate of 45%, while b = 5 achieves 74%. We also consider an oracle setup for\nIO/CoT, by calculating the success rate using best of k samples (1 \u2264k \u2264100). To compare IO/CoT\n(best of k) with ToT, we consider calculating the tree nodes visited per task in ToT across b = 1 \u00b7 \u00b7 \u00b7 5,\nand map the 5 success rates in Figure 3(a), treating IO/CoT (best of k) as visiting k nodes in a bandit.\nNot surprisingly, CoT scales better than IO, and best of 100 CoT samples achieve a success rate of\n49%, but still much worse than exploring more nodes in ToT (b > 1).\nError analysis. Figure 3(b) breaks down at which step CoT and ToT samples fail the task, i.e. the\nthought (in CoT) or all b thoughts (in ToT) are invalid or impossible to reach 24. Notably, around\n60% of CoT samples already failed the task after generating the first step, or equivalently, the first\nthree words (e.g. \u201c4 + 9\u201d). This highlights the issues with direct left-to-right decoding.\n4.2\nCreative writing\nNext, we invent a creative writing task where the input is 4 random sentences and the output should\nbe a coherent passage with 4 paragraphs that end in the 4 input sentences respectively. Such a task is\nopen-ended and exploratory, and challenges creative thinking as well as high-level planning.\nTask setup. We sample random sentences from randomwordgenerator.com to form 100 inputs, and\nthere is no groundtruth passage for each input constraint. As we find that GPT-4 can follow the\ninput constraints most of the time, we focus on evaluating passage coherency in two ways: using a\nGPT-4 zero-shot prompt to provide a 1-10 scalar score, or using human judgments to compare pairs\nof outputs from different methods. For the former, we sample 5 scores and average them for each task\noutput, and we find these 5 scores usually consistent, with a standard deviation of around 0.56 on\naverage across outputs. For the latter, we employ a subset of the authors in a blind study to compare\nthe coherency of CoT vs. ToT generated passage pairs, where the order of passages is random flipped\nover 100 inputs.\nBaselines. Given the creative nature of the task, both IO and CoT prompts are zero-shot. While the\nformer prompts the LM to directly generate a coherent passage given input constraints, the latter\nprompts the LM to first make a brief plan then write the passage, i.e. the plan serves as the intermediate\nthought step. We generate 10 IO and CoT samples per task. We also consider an iterative-refine\n(k \u22645) method on top of a random IO sample for each task, where the LM is conditioned on input\nconstraints and the last generated passage to decide if the passage is already \u201cperfectly coherent\u201d,\nand if not generate a refined one.\nToT setup. We build a ToT with depth 2 (and only 1 intermediate thought step) \u2014 the LM first\ngenerates k = 5 plans and votes for the best one (Figure 4), then similarly generate k = 5 passages\nbased on the best plan then vote for the best one. Here the breadth limit b = 1, as only one choice is\nkept per step. A simple zero-shot vote prompt (\u201canalyze choices below, then conclude which is most\npromising for the instruction\u201d) is used to sample 5 votes at both steps.\nResults. Figure 5(a) shows average GPT-4 scores across 100 tasks, where ToT (7.56) is deemed to\ngenerate more coherent passages than IO (6.19) and CoT (6.93) on average. While such an automatic\nmetric might be noisy, Figure 5(b) confirms the finding by showing that humans prefer ToT over\nCoT in 41 out of 100 passage pairs, while only prefer CoT over ToT in 21 (other 38 pairs are found\n\u201csimilarly coherent\u201d). Lastly, iterative-refine is more effective on this natural language task, where\n6\n\n\u00b5\u0157\u0193\u0164\u00f2\u02e4\u00ca\u02e4\u00e7\u0135\u010e\u00f2\u0157\u00f2\u012e\u019c\u02e4\u0154\u00ca\u015d\u015d\u00ca\u0108\u00f2\u02e4\u0135\u0199\u02e4\u0281\u02e4\u015d\u010e\u0135\u0157\u019c\u02e4\u0154\u00ca\u0157\u00ca\u0108\u0157\u00ca\u0154\u010e\u015d\u029b\u02e4\u009a\u010e\u00f2\u02e4\u00f2\u012e\u00ed\u02e4\u015d\u00f2\u012e\u0164\u00f2\u012e\u00e7\u00f2\u02e4\u0135\u0199\u02e4\u00f2\u00ca\u00e7\u010e\u02e4\u0154\u00ca\u0157\u00ca\u0108\u0157\u00ca\u0154\u010e\u02e4\u012d\u0169\u015d\u019c\u02e4\u00e6\u00f2\u029d\u02e4\u027e\u029b\u02e4G\u019c\u02e4\u0193\u015d\u012e\u02d9\u019b\u02e4\n\u00ed\u0111\u0199\u0199\u0193\u00e7\u0169\u0126\u019c\u02e4\u0198\u0135\u02e4\u00ed\u0135\u02e4\u00ca\u02e4\u010e\u00ca\u012e\u00ed\u015d\u0164\u00ca\u012e\u00ed\u02e4\u0111\u0199\u02e4\u0186\u0135\u0169\u02e4\u0120\u0169\u015d\u019c\u02e4\u015d\u0164\u00ca\u012e\u00ed\u02e4\u0135\u012e\u02e4\u0186\u0135\u0169\u0157\u02e4\u010e\u00ca\u012e\u00ed\u015d\u029b\u02e4\u027f\u029b\u02e4G\u019c\u02e4\u00e7\u00ca\u0169\u0108\u010e\u019c\u02e4\u010e\u0111\u012d\u02e4\u0135\u0199\u0199\u02e4\u0108\u0169\u00ca\u0157\u00ed\u02e4\u019b\u010e\u00ca\u019c\u02e4\u015d\u0154\u00ca\u00e7\u00f2\u02e4\u015d\u012d\u00f2\u0126\u0126\u00f2\u00ed\u02e4\u0135\u0199\u02e4\n\u015d\u00f2\u00ca\u0157\u00f2\u00ed\u02e4\u015d\u0164\u00f2\u00ca\u0123\u029b\u02e4\u0280\u029b\u02e4\u00b5\u010e\u00f2\u012e\u02e4\u015d\u010e\u00f2\u02e4\u00ed\u0193\u00ed\u012e\u02d2\u019b\u02e4\u0126\u0111\u0123\u00f2\u02e4\u00ca\u02e4\u0108\u0169\u0186\u02e4\u0180\u010e\u0135\u02e4\u0180\u00ca\u015d\u02e4\u019b\u0157\u0186\u0111\u012e\u0108\u02e4\u0198\u0135\u02e4\u0154\u0193\u00e7\u0123\u02e4\u010e\u00f2\u0157\u02e4\u0169\u0154\u029c\u02e4\u015d\u010e\u00f2\u02e4\u015d\u0164\u00ca\u0157\u0164\u00f2\u00ed\u02e4\u0169\u015d\u0111\u012e\u0108\u02e4\u015d\u0193\u0108\u012e\u02e4\u0126\u00ca\u012e\u0108\u0169\u00ca\u0108\u00f2\u029b\u02e4\u0281\u029b\u02e4\n)\u00ca\u00e7\u010e\u02e4\u0154\u00f2\u0157\u015d\u0135\u012e\u02e4\u0180\u010e\u0135\u02e4\u0123\u012e\u0135\u0180\u015d\u02e4\u0186\u0135\u0169\u02e4\u010e\u00ca\u015d\u02e4\u00ca\u02e4\u00ed\u0111\u0199\u0107\u00f2\u0157\u00f2\u012e\u019c\u02e4\u0154\u00f2\u0157\u00e7\u00f2\u0154\u019c\u0193\u0135\u012e\u02e4\u0135\u0199\u02e4\u0180\u010e\u0135\u02e4\u0186\u0135\u0169\u02e4\u00ca\u0157\u00f2\u029b\u02e4\u02e4\n\u027e\u029b\u02e4G\u012e\u019c\u0157\u0135\u00ed\u0169\u00e7\u00f2\u02e4\u00ca\u012e\u00ed\u02e4\u00f2\u0185\u0154\u0126\u00ca\u0111\u012e\u02e4\u019b\u010e\u00f2\u02e4\u0198\u00f2\u00e7\u010e\u012e\u0193\u0156\u0169\u00f2\u02e4\n\u0135\u0199\u02e4\u00ed\u0135\u0111\u012e\u0108\u02e4\u00ca\u02e4\u010e\u00ca\u012e\u00ed\u015d\u0164\u00ca\u012e\u00ed\u02e4\u027f\u029b\u02e4\u0092\u0180\u0193\u0164\u00e7\u010e\u02e4\u0198\u0135\u02e4\u00ca\u02e4\n\u015d\u0164\u0135\u0157\u0186\u02e4\u00ca\u00e6\u0135\u0169\u019c\u02e4\u00ca\u012e\u02e4\u00ca\u015d\u019c\u0157\u0135\u012e\u00ca\u0169\u019c\u02d9\u015d\u02e4\u019a\u0111\u0157\u015d\u019c\u02e4\u019b\u0111\u012d\u00f2\u02e4\u0111\u012e\u02e4\n\u015d\u0154\u00ca\u00e7\u00f2\u02e4\u0280\u029b\u02e4#\u00f2\u015d\u00e7\u0157\u0111\u00e6\u00f2\u02e4\u00ca\u02e4\u015d\u0193\u019c\u0169\u00ca\u019c\u0193\u0135\u012e\u02e4\u0180\u010e\u00f2\u0157\u00f2\u02e4\u00ca\u02e4\n\u0180\u0135\u012d\u00ca\u012e\u02e4\u0169\u015d\u00f2\u015d\u02e4\u015d\u0193\u0108\u012e\u02e4\u0126\u00ca\u012e\u0108\u0169\u00ca\u0108\u00f2\u02e4\u0198\u0135\u02e4\u00ca\u017f\u0135\u0193\u00ed\u02e4\n\u0169\u012e\u0180\u00ca\u012e\u0164\u00f2\u00ed\u02e4\u00ca\u019c\u0164\u00f2\u012e\u019c\u0193\u0135\u012e\u02e4\u0281\u029b\u02e4\u009a\u010e\u00f2\u02e4\u019a\u0111\u012e\u00ca\u0126\u02e4\n\u0154\u00ca\u0157\u00ca\u0108\u0157\u00ca\u0154\u010e\u02e4\u00f2\u0185\u0154\u0126\u00ca\u0111\u012e\u015d\u02e4\u010e\u0135\u0180\u02e4\u00f2\u017f\u00f2\u0157\u0186\u0135\u012e\u00f2\u02e4\u010e\u00ca\u015d\u02e4\n\u00ed\u0111\u0199\u0107\u00f2\u0157\u00f2\u012e\u019c\u02e4\u0154\u00f2\u0157\u00e7\u00f2\u0154\u019c\u0193\u0135\u012e\u015d\u02e4\u0135\u0199\u02e4\u0135\u019c\u010e\u00f2\u0157\u015d\n\u027e\u029b\u02e4G\u012e\u019c\u0157\u0135\u00ed\u0169\u00e7\u019c\u0193\u0135\u012e\u02e4\u0198\u0135\u02e4\u00ca\u012e\u02e4\u0169\u012e\u0169\u015d\u0169\u00ca\u0126\u02e4\u015d\u00f2\u0126\u0199\u02ca\u010e\u00f2\u0126\u0154\u02e4\u00e6\u0135\u0135\u0123\u029c\u02e4\n\u012d\u00f2\u012e\u019c\u0193\u0135\u012e\u0111\u012e\u0108\u02e4\u00ca\u02e4\u010e\u00ca\u012e\u00ed\u015d\u0164\u00ca\u012e\u00ed\u02e4\u00ca\u015d\u02e4\u00ca\u02e4\u012d\u00f2\u0164\u00ca\u0154\u010e\u0135\u0157\u02e4\u0197\u0135\u0157\u02e4\n\u00f2\u012d\u00e6\u0157\u00ca\u00e7\u0111\u012e\u0108\u02e4\u00e7\u010e\u00ca\u0126\u0126\u00f2\u012e\u0108\u00f2\u015d\u029b\u02e4\u027f\u029b\u02e4#\u0193\u015d\u00e7\u0169\u015d\u015d\u02e4\u019b\u010e\u00f2\u02e4\u0169\u012e\u00f2\u0185\u0154\u00f2\u00e7\u0164\u00f2\u00ed\u02e4\n\u019b\u010e\u0111\u012e\u0108\u015d\u02e4\u0126\u00f2\u00ca\u0157\u012e\u00f2\u00ed\u02e4\u019a\u0157\u0135\u012d\u02e4\u00ca\u015d\u019c\u0157\u0135\u012e\u00ca\u0169\u0164\u015d\u029c\u02e4\u0111\u012e\u00e7\u0126\u0169\u00ed\u0111\u012e\u0108\u02e4\u019b\u010e\u00f2\u02e4\u015d\u012d\u00f2\u0126\u0126\u02e4\u0135\u0199\u02e4\n\u015d\u0154\u00ca\u00e7\u00f2\u029b\u02e4\u0280\u029b\u02e4#\u00f2\u015d\u00e7\u0157\u0111\u00e6\u00f2\u02e4\u00ca\u02e4\u0180\u0135\u012d\u00ca\u012e\u02d9\u015d\u02e4\u00e7\u0126\u00f2\u017f\u00f2\u0157\u02e4\u0198\u00ca\u00e7\u019c\u0193\u00e7\u02e4\u0197\u0135\u0157\u02e4\u00ca\u017f\u0135\u0193\u00ed\u0111\u012e\u0108\u02e4\n\u0169\u012e\u0180\u00ca\u012e\u0164\u00f2\u00ed\u02e4\u00ca\u019c\u0164\u00f2\u012e\u019c\u0193\u0135\u012e\u02e4\u00ca\u019c\u02e4\u00ca\u02e4\u00e6\u00ca\u0157\u029b\u02e4\u0281\u029b\u02e4\u001d\u0135\u012e\u0164\u00f2\u012d\u0154\u0126\u00ca\u0164\u00f2\u02e4\u010e\u0135\u0180\u02e4\n\u00ed\u0111\u0199\u0107\u00f2\u0157\u00f2\u012e\u019c\u02e4\u0154\u00f2\u0157\u00e7\u00f2\u0154\u019c\u0193\u0135\u012e\u015d\u02e4\u0135\u0199\u02e4\u0135\u012e\u00f2\u015d\u00f2\u0126\u0199\u02e4\u00e7\u00ca\u012e\u02e4\u015d\u010e\u00ca\u0154\u00f2\u02e4\u0135\u012e\u00f2\u02d9\u015d\u02e4\n\u0193\u00ed\u00f2\u012e\u019c\u0193\u0164\u0186\u029b\n\u02b1\u00ca\u02b2\u02e4\nG\u012e\u0154\u0169\u019c\n\u02b1\u00e6\u02b2\u02e4\n\u0089\u0126\u00ca\u012e\u015d\n\u02b1\u00e7\u02b2\u02e4\n\u00b4\u0135\u0164\u00f2\u015d\n\u0001\u012e\u00ca\u0126\u0186\u018f\u0111\u012e\u0108\u02e4\u00f2\u00ca\u00e7\u010e\u02e4\u00e7\u010e\u0135\u0193\u00e7\u00f2\u02e4\u0111\u012e\u02e4\u00ed\u00f2\u0164\u00ca\u0193\u0126\u029d\u02e4\u02e4\u001d\u010e\u0135\u0193\u00e7\u00f2\u02e4\u027e\u029c\u02e4\u0180\u010e\u0193\u0126\u00f2\u02e4\u0111\u012e\u00e7\u0135\u0157\u0154\u0135\u0157\u00ca\u019c\u0111\u012e\u0108\u02e4\u019b\u010e\u00f2\u02e4\u0157\u00f2\u0156\u0169\u0111\u0157\u00f2\u00ed\u02e4\u00f2\u012e\u00ed\u02e4\u015d\u00f2\u012e\u0164\u00f2\u012e\u00e7\u00f2\u015d\u029c\u02e4\u015d\u00f2\u00f2\u012d\u015d\u02e4\u0198\u0135\u02e4\u0126\u00ca\u00e7\u0123\u02e4\u00ca\u02e4\n\u00e7\u0126\u00f2\u00ca\u0157\u02e4\u00e7\u0135\u012e\u012e\u00f2\u00e7\u019c\u0193\u0135\u012e\u02e4\u00e6\u00f2\u0164\u0180\u00f2\u00f2\u012e\u02e4\u019b\u010e\u00f2\u02e4\u0154\u00ca\u0157\u00ca\u0108\u0157\u00ca\u0154\u010e\u015d\u02e4\u02b3\u029b\u029b\u029b\u02b4\u02e4\u001d\u010e\u0135\u0193\u00e7\u00f2\u02e4\u027f\u02e4\u0135\u0199\u0107\u00f2\u0157\u015d\u02e4\u00ca\u012e\u02e4\u0111\u012e\u0164\u00f2\u0157\u00f2\u015d\u019c\u0111\u012e\u0108\u02e4\u0154\u00f2\u0157\u015d\u0154\u00f2\u00e7\u019c\u0193\u017f\u00f2\u02e4\u00e6\u0186\u02e4\u0169\u015d\u0111\u012e\u0108\u02e4\u019b\u010e\u00f2\u02e4\n\u0157\u00f2\u0156\u0169\u0111\u0157\u00f2\u00ed\u02e4\u00f2\u012e\u00ed\u02e4\u015d\u00f2\u012e\u0164\u00f2\u012e\u00e7\u00f2\u015d\u02e4\u0198\u0135\u02e4\u0154\u0157\u00f2\u015d\u00f2\u012e\u019c\u02e4\u00ca\u02e4\u015d\u00f2\u0126\u0199\u02ca\u010e\u00f2\u0126\u0154\u02e4\u00e6\u0135\u0135\u0123\u02d9\u015d\u02e4\u00e7\u0135\u012e\u0164\u00f2\u012e\u019c\u029b\u02e4G\u019c\u02e4\u00e7\u0135\u012e\u012e\u00f2\u00e7\u0164\u015d\u02e4\u019b\u010e\u00f2\u02e4\u0154\u00ca\u0157\u00ca\u0108\u0157\u00ca\u0154\u010e\u015d\u02e4\u0180\u0193\u019c\u010e\u02e4\u019b\u010e\u00f2\u02e4\u019b\u010e\u00f2\u012d\u00f2\u02e4\u0135\u0199\u02e4\n\u015d\u00f2\u0126\u0199\u02ca\u0111\u012d\u0154\u0157\u0135\u017f\u00f2\u012d\u00f2\u012e\u019c\u02e4\u00ca\u012e\u00ed\u02e4\u00f2\u012d\u00e6\u0157\u00ca\u00e7\u0111\u012e\u0108\u02e4\u00e7\u010e\u00ca\u0126\u0126\u00f2\u012e\u0108\u00f2\u015d\u029c\u02e4\u012d\u00ca\u0123\u0111\u012e\u0108\u02e4\u0197\u0135\u0157\u02e4\u00ca\u02e4\u00e7\u0135\u010e\u00f2\u0157\u00f2\u012e\u019c\u02e4\u0154\u00ca\u015d\u015d\u00ca\u0108\u00f2\u029b\u02e4\u02e4\u02b3\u029b\u029b\u029b\u02b4\u02e4\u009a\u010e\u00f2\u02e4\u00e6\u00f2\u015d\u019c\u02e4\u00e7\u010e\u0135\u0193\u00e7\u00f2\u02e4\u0193\u015d\u02e4\u027f\u029b\nG\u012e\u0154\u0169\u019c\n\u0089\u0126\u00ca\u012e\u02e4\u027e\n\u02e4\u0089\u0126\u00ca\u012e\u02e4\u027f\u02e4\u02e4 \u029f\u029f\n\u0089\u00ca\u015d\u015d\u00ca\u0108\u00f2\n\u027e\n\u0089\u00ca\u015d\u015d\u00ca\u0108\u00f2\n\u027f\n\u029f\u029f\n\u027d\u02ab\u0282\u02e4\u017f\u0135\u0164\u00f2\u015d\n\u0089\u0126\u00ca\u012e\u02e4\u027e\u02e4\u02e4\u02e4\n\u029f\u029b\n\u029f\u029b\n\u027e\u029f\u029b\n\u027f\u029f\n\u029f\n\u0280\u02ab\u0282\u02e4\u017f\u0135\u0164\u00f2\u015d\n\u0089\u0126\u00ca\u012e\u02e4\u0280\u02c1\u0282\u02e4\u02e4\u02e4\n\u012e\u02ab\u0282\u02e4\u017f\u0135\u0164\u00f2\u015d\n\u0089\u0126\u00ca\u012e\u02e4\u027f\u02e4\u02e4\u02e4\n\u0001\u012e\u00ca\u0126\u0186\u018f\u0111\u012e\u0108\u02e4\u00f2\u00ca\u00e7\u010e\u02e4\u00e7\u010e\u0135\u0193\u00e7\u00f2\u02e4\u0111\u012e\u02e4\u00ed\u00f2\u0164\u00ca\u0193\u0126\u029d\u02e4\u02e4\u001d\u010e\u0135\u0193\u00e7\u00f2\u02e4\u027e\u029c\u02e4\u0180\u010e\u0193\u0126\u00f2\u02e4\u0111\u012e\u00e7\u0135\u0157\u0154\u0135\u0157\u00ca\u019c\u0111\u012e\u0108\u02e4\u019b\u010e\u00f2\u02e4\u0157\u00f2\u0156\u0169\u0111\u0157\u00f2\u00ed\u02e4\u00f2\u012e\u00ed\u02e4\u015d\u00f2\u012e\u0164\u00f2\u012e\u00e7\u00f2\u015d\u029c\u02e4\u015d\u00f2\u00f2\u012d\u015d\u02e4\u0198\u0135\u02e4\u0126\u00ca\u00e7\u0123\u02e4\u00ca\u02e4\n\u00e7\u0126\u00f2\u00ca\u0157\u02e4\u00e7\u0135\u012e\u012e\u00f2\u00e7\u019c\u0193\u0135\u012e\u02e4\u00e6\u00f2\u0164\u0180\u00f2\u00f2\u012e\u02e4\u019b\u010e\u00f2\u02e4\u0154\u00ca\u0157\u00ca\u0108\u0157\u00ca\u0154\u010e\u015d\u02e4\u02b3\u029b\u029b\u029b\u02b4\u02e4\u001d\u010e\u0135\u0193\u00e7\u00f2\u02e4\u027f\u02e4\u0135\u0199\u0107\u00f2\u0157\u015d\u02e4\u00ca\u012e\u02e4\u0111\u012e\u0164\u00f2\u0157\u00f2\u015d\u019c\u0111\u012e\u0108\u02e4\u0154\u00f2\u0157\u015d\u0154\u00f2\u00e7\u019c\u0193\u017f\u00f2\u02e4\u00e6\u0186\u02e4\u0169\u015d\u0111\u012e\u0108\u02e4\u019b\u010e\u00f2\u02e4\n\u0157\u00f2\u0156\u0169\u0111\u0157\u00f2\u00ed\u02e4\u00f2\u012e\u00ed\u02e4\u015d\u00f2\u012e\u0164\u00f2\u012e\u00e7\u00f2\u015d\u02e4\u0198\u0135\u02e4\u0154\u0157\u00f2\u015d\u00f2\u012e\u019c\u02e4\u00ca\u02e4\u015d\u00f2\u0126\u0199\u02ca\u010e\u00f2\u0126\u0154\u02e4\u00e6\u0135\u0135\u0123\u02d9\u015d\u02e4\u00e7\u0135\u012e\u0164\u00f2\u012e\u019c\u029b\u02e4G\u019c\u02e4\u00e7\u0135\u012e\u012e\u00f2\u00e7\u0164\u015d\u02e4\u019b\u010e\u00f2\u02e4\u0154\u00ca\u0157\u00ca\u0108\u0157\u00ca\u0154\u010e\u015d\u02e4\u0180\u0193\u019c\u010e\u02e4\u019b\u010e\u00f2\u02e4\u019b\u010e\u00f2\u012d\u00f2\u02e4\u0135\u0199\u02e4\n\u015d\u00f2\u0126\u0199\u02ca\u0111\u012d\u0154\u0157\u0135\u017f\u00f2\u012d\u00f2\u012e\u019c\u02e4\u00ca\u012e\u00ed\u02e4\u00f2\u012d\u00e6\u0157\u00ca\u00e7\u0111\u012e\u0108\u02e4\u00e7\u010e\u00ca\u0126\u0126\u00f2\u012e\u0108\u00f2\u015d\u029c\u02e4\u012d\u00ca\u0123\u0111\u012e\u0108\u02e4\u0197\u0135\u0157\u02e4\u00ca\u02e4\u00e7\u0135\u010e\u00f2\u0157\u00f2\u012e\u019c\u02e4\u0154\u00ca\u015d\u015d\u00ca\u0108\u00f2\u029b\u02e4\u02e4\u02b3\u029b\u029b\u029b\u02b4\u02e4\u009a\u010e\u00f2\u02e4\u00e6\u00f2\u015d\u019c\u02e4\u00e7\u010e\u0135\u0193\u00e7\u00f2\u02e4\u0193\u015d\u02e4\u027f\u029b\n\u0001\u012e\u00ca\u0126\u0186\u018f\u0111\u012e\u0108\u02e4\u00f2\u00ca\u00e7\u010e\u02e4\u00e7\u010e\u0135\u0193\u00e7\u00f2\u02e4\u0111\u012e\u02e4\u00ed\u00f2\u0164\u00ca\u0193\u0126\u029d\u02e4\u02e4\u001d\u010e\u0135\u0193\u00e7\u00f2\u02e4\u027e\u029c\u02e4\u0180\u010e\u0193\u0126\u00f2\u02e4\u0111\u012e\u00e7\u0135\u0157\u0154\u0135\u0157\u00ca\u019c\u0111\u012e\u0108\u02e4\u019b\u010e\u00f2\u02e4\u0157\u00f2\u0156\u0169\u0111\u0157\u00f2\u00ed\u02e4\u00f2\u012e\u00ed\u02e4\u015d\u00f2\u012e\u0164\u00f2\u012e\u00e7\u00f2\u015d\u029c\u02e4\u015d\u00f2\u00f2\u012d\u015d\u02e4\u0198\u0135\u02e4\u0126\u00ca\u00e7\u0123\u02e4\u00ca\u02e4\n\u00e7\u0126\u00f2\u00ca\u0157\u02e4\u00e7\u0135\u012e\u012e\u00f2\u00e7\u019c\u0193\u0135\u012e\u02e4\u00e6\u00f2\u0164\u0180\u00f2\u00f2\u012e\u02e4\u019b\u010e\u00f2\u02e4\u0154\u00ca\u0157\u00ca\u0108\u0157\u00ca\u0154\u010e\u015d\u02e4\u02b3\u029b\u029b\u029b\u02b4\u02e4\u001d\u010e\u0135\u0193\u00e7\u00f2\u02e4\u027f\u02e4\u0135\u0199\u0107\u00f2\u0157\u015d\u02e4\u00ca\u012e\u02e4\u0111\u012e\u0164\u00f2\u0157\u00f2\u015d\u019c\u0111\u012e\u0108\u02e4\u0154\u00f2\u0157\u015d\u0154\u00f2\u00e7\u019c\u0193\u017f\u00f2\u02e4\u00e6\u0186\u02e4\u0169\u015d\u0111\u012e\u0108\u02e4\u019b\u010e\u00f2\u02e4\n\u0157\u00f2\u0156\u0169\u0111\u0157\u00f2\u00ed\u02e4\u00f2\u012e\u00ed\u02e4\u015d\u00f2\u012e\u0164\u00f2\u012e\u00e7\u00f2\u015d\u02e4\u0198\u0135\u02e4\u0154\u0157\u00f2\u015d\u00f2\u012e\u019c\u02e4\u00ca\u02e4\u015d\u00f2\u0126\u0199\u02ca\u010e\u00f2\u0126\u0154\u02e4\u00e6\u0135\u0135\u0123\u02d9\u015d\u02e4\u00e7\u0135\u012e\u0164\u00f2\u012e\u019c\u029b\u02e4G\u019c\u02e4\u00e7\u0135\u012e\u012e\u00f2\u00e7\u0164\u015d\u02e4\u019b\u010e\u00f2\u02e4\u0154\u00ca\u0157\u00ca\u0108\u0157\u00ca\u0154\u010e\u015d\u02e4\u0180\u0193\u019c\u010e\u02e4\u019b\u010e\u00f2\u02e4\u019b\u010e\u00f2\u012d\u00f2\u02e4\u0135\u0199\u02e4\n\u015d\u00f2\u0126\u0199\u02ca\u0111\u012d\u0154\u0157\u0135\u017f\u00f2\u012d\u00f2\u012e\u019c\u02e4\u00ca\u012e\u00ed\u02e4\u00f2\u012d\u00e6\u0157\u00ca\u00e7\u0111\u012e\u0108\u02e4\u00e7\u010e\u00ca\u0126\u0126\u00f2\u012e\u0108\u00f2\u015d\u029c\u02e4\u012d\u00ca\u0123\u0111\u012e\u0108\u02e4\u0197\u0135\u0157\u02e4\u00ca\u02e4\u00e7\u0135\u010e\u00f2\u0157\u00f2\u012e\u019c\u02e4\u0154\u00ca\u015d\u015d\u00ca\u0108\u00f2\u029b\u02e4\u02e4\u02b3\u029b\u029b\u029b\u02b4\u02e4\u009a\u010e\u00f2\u02e4\u00e6\u00f2\u015d\u019c\u02e4\u00e7\u010e\u0135\u0193\u00e7\u00f2\u02e4\u0193\u015d\u02e4\u027f\u029b\n\u0001\u012e\u00ca\u0126\u0186\u018f\u0111\u012e\u0108\u02e4\u00f2\u00ca\u00e7\u010e\u02e4\u00e7\u010e\u0135\u0193\u00e7\u00f2\u02e4\u0111\u012e\u02e4\u00ed\u00f2\u0164\u00ca\u0193\u0126\u029d\u02e4\u02e4\u001d\u010e\u0135\u0193\u00e7\u00f2\u02e4\u027e\u029c\u02e4\u0180\u010e\u0193\u0126\u00f2\u02e4\u0111\u012e\u00e7\u0135\u0157\u0154\u0135\u0157\u00ca\u019c\u0111\u012e\u0108\u02e4\u019b\u010e\u00f2\u02e4\u0157\u00f2\u0156\u0169\u0111\u0157\u00f2\u00ed\u02e4\u00f2\u012e\u00ed\u02e4\u015d\u00f2\u012e\u0164\u00f2\u012e\u00e7\u00f2\u015d\u029c\u02e4\u015d\u00f2\u00f2\u012d\u015d\u02e4\u0198\u0135\u02e4\u0126\u00ca\u00e7\u0123\u02e4\u00ca\u02e4\n\u00e7\u0126\u00f2\u00ca\u0157\u02e4\u00e7\u0135\u012e\u012e\u00f2\u00e7\u019c\u0193\u0135\u012e\u02e4\u00e6\u00f2\u0164\u0180\u00f2\u00f2\u012e\u02e4\u019b\u010e\u00f2\u02e4\u0154\u00ca\u0157\u00ca\u0108\u0157\u00ca\u0154\u010e\u015d\u02e4\u02b3\u029b\u029b\u029b\u02b4\u02e4\u001d\u010e\u0135\u0193\u00e7\u00f2\u02e4\u027f\u02e4\u0135\u0199\u0107\u00f2\u0157\u015d\u02e4\u00ca\u012e\u02e4\u0111\u012e\u0164\u00f2\u0157\u00f2\u015d\u019c\u0111\u012e\u0108\u02e4\u0154\u00f2\u0157\u015d\u0154\u00f2\u00e7\u019c\u0193\u017f\u00f2\u02e4\u00e6\u0186\u02e4\u0169\u015d\u0111\u012e\u0108\u02e4\u019b\u010e\u00f2\u02e4\n\u0157\u00f2\u0156\u0169\u0111\u0157\u00f2\u00ed\u02e4\u00f2\u012e\u00ed\u02e4\u015d\u00f2\u012e\u0164\u00f2\u012e\u00e7\u00f2\u015d\u02e4\u0198\u0135\u02e4\u0154\u0157\u00f2\u015d\u00f2\u012e\u019c\u02e4\u00ca\u02e4\u015d\u00f2\u0126\u0199\u02ca\u010e\u00f2\u0126\u0154\u02e4\u00e6\u0135\u0135\u0123\u02d9\u015d\u02e4\u00e7\u0135\u012e\u0164\u00f2\u012e\u019c\u029b\u02e4G\u019c\u02e4\u00e7\u0135\u012e\u012e\u00f2\u00e7\u0164\u015d\u02e4\u019b\u010e\u00f2\u02e4\u0154\u00ca\u0157\u00ca\u0108\u0157\u00ca\u0154\u010e\u015d\u02e4\u0180\u0193\u019c\u010e\u02e4\u019b\u010e\u00f2\u02e4\u019b\u010e\u00f2\u012d\u00f2\u02e4\u0135\u0199\u02e4\n\u015d\u00f2\u0126\u0199\u02ca\u0111\u012d\u0154\u0157\u0135\u017f\u00f2\u012d\u00f2\u012e\u019c\u02e4\u00ca\u012e\u00ed\u02e4\u00f2\u012d\u00e6\u0157\u00ca\u00e7\u0111\u012e\u0108\u02e4\u00e7\u010e\u00ca\u0126\u0126\u00f2\u012e\u0108\u00f2\u015d\u029c\u02e4\u012d\u00ca\u0123\u0111\u012e\u0108\u02e4\u0197\u0135\u0157\u02e4\u00ca\u02e4\u00e7\u0135\u010e\u00f2\u0157\u00f2\u012e\u019c\u02e4\u0154\u00ca\u015d\u015d\u00ca\u0108\u00f2\u029b\u02e4\u02e4\u02b3\u029b\u029b\u029b\u02b4\u02e4\u009a\u010e\u00f2\u02e4\u00e6\u00f2\u015d\u019c\u02e4\u00e7\u010e\u0135\u0193\u00e7\u00f2\u02e4\u0193\u015d\u02e4\u027f\u029b\n\u0001\u012e\u00ca\u0126\u0186\u018f\u0111\u012e\u0108\u02e4\u00f2\u00ca\u00e7\u010e\u02e4\u00e7\u010e\u0135\u0193\u00e7\u00f2\u02e4\u0111\u012e\u02e4\u00ed\u00f2\u0164\u00ca\u0193\u0126\u029d\u02e4\u02e4\u001d\u010e\u0135\u0193\u00e7\u00f2\u02e4\u027e\u029c\u02e4\u0180\u010e\u0193\u0126\u00f2\u02e4\u0111\u012e\u00e7\u0135\u0157\u0154\u0135\u0157\u00ca\u019c\u0111\u012e\u0108\u02e4\u019b\u010e\u00f2\u02e4\u0157\u00f2\u0156\u0169\u0111\u0157\u00f2\u00ed\u02e4\u00f2\u012e\u00ed\u02e4\u015d\u00f2\u012e\u0164\u00f2\u012e\u00e7\u00f2\u015d\u029c\u02e4\u015d\u00f2\u00f2\u012d\u015d\u02e4\u0198\u0135\u02e4\u0126\u00ca\u00e7\u0123\u02e4\u00ca\u02e4\n\u00e7\u0126\u00f2\u00ca\u0157\u02e4\u00e7\u0135\u012e\u012e\u00f2\u00e7\u019c\u0193\u0135\u012e\u02e4\u00e6\u00f2\u0164\u0180\u00f2\u00f2\u012e\u02e4\u019b\u010e\u00f2\u02e4\u0154\u00ca\u0157\u00ca\u0108\u0157\u00ca\u0154\u010e\u015d\u02e4\u02b3\u029b\u029b\u029b\u02b4\u02e4\u001d\u010e\u0135\u0193\u00e7\u00f2\u02e4\u027f\u02e4\u0135\u0199\u0107\u00f2\u0157\u015d\u02e4\u00ca\u012e\u02e4\u0111\u012e\u0164\u00f2\u0157\u00f2\u015d\u019c\u0111\u012e\u0108\u02e4\u0154\u00f2\u0157\u015d\u0154\u00f2\u00e7\u019c\u0193\u017f\u00f2\u02e4\u00e6\u0186\u02e4\u0169\u015d\u0111\u012e\u0108\u02e4\u019b\u010e\u00f2\u02e4\n\u0157\u00f2\u0156\u0169\u0111\u0157\u00f2\u00ed\u02e4\u00f2\u012e\u00ed\u02e4\u015d\u00f2\u012e\u0164\u00f2\u012e\u00e7\u00f2\u015d\u02e4\u0198\u0135\u02e4\u0154\u0157\u00f2\u015d\u00f2\u012e\u019c\u02e4\u00ca\u02e4\u015d\u00f2\u0126\u0199\u02ca\u010e\u00f2\u0126\u0154\u02e4\u00e6\u0135\u0135\u0123\u02d9\u015d\u02e4\u00e7\u0135\u012e\u0164\u00f2\u012e\u019c\u029b\u02e4G\u019c\u02e4\u00e7\u0135\u012e\u012e\u00f2\u00e7\u0164\u015d\u02e4\u019b\u010e\u00f2\u02e4\u0154\u00ca\u0157\u00ca\u0108\u0157\u00ca\u0154\u010e\u015d\u02e4\u0180\u0193\u019c\u010e\u02e4\u019b\u010e\u00f2\u02e4\u019b\u010e\u00f2\u012d\u00f2\u02e4\u0135\u0199\u02e4\n\u015d\u00f2\u0126\u0199\u02ca\u0111\u012d\u0154\u0157\u0135\u017f\u00f2\u012d\u00f2\u012e\u019c\u02e4\u00ca\u012e\u00ed\u02e4\u00f2\u012d\u00e6\u0157\u00ca\u00e7\u0111\u012e\u0108\u02e4\u00e7\u010e\u00ca\u0126\u0126\u00f2\u012e\u0108\u00f2\u015d\u029c\u02e4\u012d\u00ca\u0123\u0111\u012e\u0108\u02e4\u0197\u0135\u0157\u02e4\u00ca\u02e4\u00e7\u0135\u010e\u00f2\u0157\u00f2\u012e\u019c\u02e4\u0154\u00ca\u015d\u015d\u00ca\u0108\u00f2\u029b\u02e4\u02e4\u02b3\u029b\u029b\u029b\u02b4\u02e4\u009a\u010e\u00f2\u02e4\u00e6\u00f2\u015d\u019c\u02e4\u00e7\u010e\u0135\u0193\u00e7\u00f2\u02e4\u0193\u015d\u02e4\u027f\u029b\nFigure 4: A step of deliberate search in a randomly picked Creative Writing task. Given the input, the\nLM samples 5 different plans, then votes 5 times to decide which plan is best. The majority choice is\nused to consequently write the output passage with the same sample-vote procedure.\nIO\nCoT\nToT\nIO\n+refine\nToT\n+refine\n4\n6\n8\n(a) GPT-4 coherency scores\nCoT > ToT Similar ToT > CoT\n0\n10\n20\n30\n40\n21\n38\n41\n(b) Human coherency comparison\nFigure 5: Creative Writing results.\nMethod\nSuccess Rate (%)\nLetter Word Game\nIO\n38.7\n14\n0\nCoT\n40.6\n15.6\n1\nToT (ours)\n78\n60\n20\n+best state\n82.4\n67.5\n35\n-prune\n65.4\n41.5\n5\n-backtrack\n54.6\n20\n5\nTable 3: Mini Crosswords results.\nit improves IO coherency score from 6.19 to 7.67, and ToT coherency score from 7.56 to 7.91. We\nbelieve it could be thought of as a third approach to thought generation in the ToT framework, where\nnew thoughts can arise from refining old thoughts instead of i.i.d. or sequentially generated.\n4.3\nMini crosswords\nIn Game of 24 and Creative Writing, ToT is relatively shallow \u2014 at most 3 thought steps are needed\nto reach the final output. Here we explore 5\u00d75 mini crosswords as a harder search problem involving\nnatural language. Again, the goal is not just to solve the task, as more general crosswords can be\nreadily solved with specialized NLP pipelines [34] that leverages large-scale retrieval instead of LM.\nRather, we aim to explore the limit of LM as a general problem solver that explores its own thoughts\nand guides its own exploration with deliberate reasoning as heuristics.\nTask setup. We scrape data from GooBix, which contains 156 games of 5 \u00d7 5 mini crosswords. As\nwe observe adjacent games contain similar clues, we use 20 games with indices 1, 6, \u00b7 \u00b7 \u00b7 , 91, 96 for\ntesting, and games 136, 141, 146, 151, 156 for prompting. For each task, the input describes the 5\nhorizontal clues and 5 vertical clues, and the output should be a board of 5 \u00d7 5 = 25 letters to solve\nthe crosswords. For evaluation, we consider three levels of success: the portion of correct letters (25\nper game), words (10 per game), and games.\nBaselines. We provide 5 example input-output pairs in the IO prompt, and in the CoT prompt\nadditionally include intermediate words in the order h1..5 then v1..5. We run each prompt for 10\nsamples and average the results.\nToT setup. We leverage a depth-first search (Algorithm 2) that keeps exploring the most promising\nsubsequent word clue until the state is no longer promising, then backtrack to the parent state to\nexplore alternative thoughts. To make search tractable, subsequent thoughts are constrained not to\nchange any filled words or letters, so that the ToT has at most 10 intermediate steps. For thought\ngeneration, at each state we translate all existing thoughts (e.g. \u201ch2.motor; h1.tasks\u201d for the state\nin Figure 6(a)) into letter constraints for remaining clues (e.g. \u201cv1.To heap: tm\n;...\u201d) and prompt\na proposal prompt 5 times to come up with candidates for where and what to fill in the next word.\nImportantly, we also prompt the LM to give a confidence level for different thoughts, and aggregate\n7\n\nG\u012e\u0154\u0169\u019c\u02e4\u001d\u0126\u0169\u00f2\u015d\n\u010e\u027f\u029b\u012d\u0135\u0164\u0135\u0157\n\u010e\u027e\u029b\u0198\u00ca\u015d\u0123\u015d\n\u010e\u0281\u029b\u015d\u00ca\u0126\u0135\u012e\n\u010e\u0281\u029b\u02e4\u015d\u00ca\u0126\u0135\u012e\u02e4\u02b1\u015d\u0169\u0157\u00f2\u02b2\n\u017f\u0282\u029b\u02e4\u015d\u0157\u00ed\u0157\u0186\u02e4\u02b1\u0126\u0135\u0180\u02b2\n\u017f\u0280\u029b\u02e4\u015d\u019c\u0157\u0111\u012e\u0108\u02e4\u02b1\u010e\u0193\u0108\u010e\u02b2\n\u029f\u029f\n7KRXJKW\u00033URSRVDOV\n\u00ca\u0108\u0108\u0157\u00f2\u0108\u00ca\u0164\u00f2\n\u017f\u0280\u029b\u02e4\u0089\u0157\u00f2\u0164\u00f2\u012e\u019c\u0193\u0135\u0169\u015d\u029e\u02e4\u019a\u0126\u0135\u0180\u00f2\u0157\u0186\u029d\u02e4\u02c8\u02c8\u02c8\u02c8\u02c8\u02e4\u015d\u0169\u0157\u00f2\n6WDWH\u0003(YDOXDWRU\u0003\u000bRYHU\u0003HDFK\u0003FOXH\f\n\u017f\u027e\u029b\u02e4\u00c9\u0135\u02e4\u010e\u00f2\u00ca\u0154\u029d\u02e4\u019b\u012d\u02c8\u015d\u02c8\u02e4\u02b3\u029b\u029b\u029b\u02b4\u02e4\u0111\u012d\u0154\u0135\u015d\u015d\u0111\u00e6\u0126\u00f2\n\u017f\u0282\u029b\u02e4#\u00f2\u015d\u0193\u00e7\u00e7\u00ca\u0164\u0135\u0157\u029e\u02e4\u012d\u0135\u0157\u00f2\u02e4\u00ed\u0157\u0186\u029d\u02e4\u015d\u0157\u02c8\u012e\u02c8\u02e4\u02b3\u029b\u029b\u029b\u02b4\u02e4\u012d\u00ca\u0186\u00e6\u00f2\n\u029f\u029f\n\u02b1\u00e6\u00ca\u00e7\u0123\u019c\u0157\u00ca\u00e7\u0123\u02b2\n\u010e\u0280\u029b\u0108\u0157\u00ca\u012e\u00ed\n\u029f\u029f\n\u02b1\u015d\u0169\u00e6\u019c\u0157\u00f2\u00f2\u02e4\u0154\u0157\u0169\u012e\u00f2\u00ed\u02b2\n\u010e\u0281\u029b\u02e4\u015d\u00ca\u0126\u0135\u012e\n\u010e\u0280\u029b\u02e4\u0108\u0157\u00ca\u012e\u00ed\n\u017f\u0280\u029b\u02e4\u015d\u019c\u0157\u0111\u012e\u0108\n\u029f\u029f\n')6\u0003\n2UGHU\n\u010e\u0281\u029b\u02e4\u015d\u00ca\u0126\u0135\u012e\u02e4\u02b1\u015d\u0169\u0157\u00f2\u02b2\n\u017f\u0282\u029b\u02e4\u015d\u0157\u00ed\u0157\u0186\u02e4\u02b1\u0126\u0135\u0180\u02b2\n\u017f\u0280\u029b\u02e4\u015d\u019c\u0157\u0111\u012e\u0108\u02e4\u02b1\u010e\u0193\u0108\u010e\u02b2\n\u029f\u029f\n7KRXJKW\u00033URSRVDOV\n\u010e\u0281\u029b\u02e4\u015d\u00ca\u0126\u0135\u012e\u02e4\u02b1\u015d\u0169\u0157\u00f2\u02b2\n\u017f\u0282\u029b\u02e4\u015d\u0157\u00ed\u0157\u0186\u02e4\u02b1\u0126\u0135\u0180\u02b2\n\u017f\u0280\u029b\u02e4\u015d\u019c\u0157\u0111\u012e\u0108\u02e4\u02b1\u010e\u0193\u0108\u010e\u02b2\n\u029f\u029f\n7KRXJKW\u00033URSRVDOV\n\u010e\u0281\u029b\u02e4\u015d\u00ca\u0126\u0135\u012e\u02e4\u02b1\u015d\u0169\u0157\u00f2\u02b2\n\u017f\u0282\u029b\u02e4\u015d\u0157\u00ed\u0157\u0186\u02e4\u02b1\u0126\u0135\u0180\u02b2\n\u017f\u0280\u029b\u02e4\u015d\u019c\u0157\u0111\u012e\u0108\u02e4\u02b1\u010e\u0193\u0108\u010e\u02b2\n\u029f\u029f\n7KRXJKW\u00033URSRVDOV\n\u010e\u0281\u029b\u02e4\u015d\u00ca\u0126\u0135\u012e\u02e4\u02b1\u015d\u0169\u0157\u00f2\u02b2\n\u017f\u0282\u029b\u02e4\u015d\u0157\u00ed\u0157\u0186\u02e4\u02b1\u0126\u0135\u0180\u02b2\n\u017f\u0280\u029b\u02e4\u015d\u019c\u0157\u0111\u012e\u0108\u02e4\u02b1\u010e\u0193\u0108\u010e\u02b2\n\u029f\u029f\n7KRXJKW\u00033URSRVDOV\n\u02b1\u00ca\u02b2\n\u02b1\u00e6\u02b2\n\u019b\u02e4\u00ca\u02e4\u015d\u02e4\u0123\u02e4\u015d\n\u012d\u02e4\u0135\u02e4\u019b\u02e4\u0135\u02e4\u0157\n\u02c8\u02e4\u02c8\u02e4\u02c8\u02e4\u02c8\u02e4\u02c8\n\u015d\u02e4\u00ca\u02e4\u0126\u02e4\u0135\u02e4\u012e\n\u02c8\u02e4\u02c8\u02e4\u02c8\u02e4\u02c8\u02e4\u02c8\nFigure 6: In Mini Crosswords, (a) how thoughts are proposed and aggregated in a priority queue\nfor depth-first search (DFS), and (b) how a state is evaluated based on the possibility of filling in\neach remaining word clue, and pruned if any remaining clue is deemed not possible to fill by the LM.\nThen DFS backtracks to the parent state and explore the next promising thought for clue.\nthese across proposals to obtain a sorted list of next thoughts to explore (Figure 6(a)). For state\nevaluations, we similarly translate each state into letter constraints for remaining clues, then evaluate\nfor each clue if it is possible to fill given the constraints. If any remaining clue is deemed \u201cimpossible\u201d\nto fill in (e.g. \u201cv1. To heap: tm s \u201d), then the exploration of the state\u2019s subtree is pruned and DFS\nbacktracks to its parent to explore the next promising thought. We limit DFS search steps to 100, and\nsimply render the deepest explored state (the first explored one if multiple) into the final output.\nResults. As shown in Table 3, IO and CoT prompting methods perform poorly with a word-level\nsuccess rate less than 16%, while ToT significantly improves all metrics, achieving a word-level\nsuccess rate of 60% and solving 4 out of 20 games. Such an improvement is not surprising, given IO\nand CoT lack mechanisms to try different clues, make changes to decisions, or backtrack.\nOracle and ablation studies. When outputting from the oracle best DFS state (instead of the\nheuristically determined best state) per task, ToT performance is even higher and actually solves\n7/20 games (Table 3, \u201c+best state\u201d), indicating our simple output heuristics can be readily improved.\nInterestingly, sometimes when the crosswords game is actually solved, the state evaluator might still\ndeem some words as \u201cimpossible\u201d and prune \u2014 possibly because 5 \u00d7 5 crosswords by design have\nsome rare or obselete words that GPT-4 cannot recognize2. Given the state evaluation as a pruning\nheuristic is imperfect, we also explore ablating the pruning, and find the performance generally worse\n(Table 3, \u201c-prune\u201d). However, it could actually find the correct solution for 4/20 games (though only\noutputting 1 via heuristic), 3 of which are games ToT+pruning cannot solve within 100 steps. Thus,\nbetter heuristics for DFS pruning are critical for problem solving in this case. Lastly, we confirm the\nimportance of backtracking by running an ablation that keeps filling the most promising clue for at\nmost 20 steps, allowing overwrites. This is similar to a \u201cgreedy\u201d BFS search with breadth limit of\nb = 1, and performs poorly with a word level success of only 20% (Table 3, \u201c-backtrack\u201d).\n5\nRelated Work\nPlanning and decision making. Smart planning and decision making are critical to achieving\npredefined goals. As they are trained on vast amount of world knowledge and human examples, LMs\nare known to have already absorbed rich commonsense that makes it possible to propose reasonable\nplans conditioned on problem setting and environmental states [12, 42, 37, 13, 35, 41, 40]. Our\nproposed ToT approach extends existing planning formulations by considering multiple potentially\nfeasible plans simultaneously at each problem-solving step, and proceeding with the most promising\nones. The integration between thought sampling and value feedback organically integrates planning\nand decision-making mechanisms, enabling effective search inside a solution tree. On the other hand,\ntraditional decision-making procedures usually require training dedicated reward and policy models\nas in reinforcement learning (for example CHAI [33]), whereas we use the LM itself to provide\nthe value estimates for decision making. RAP [9] is a concurrent work that treats language model\n2For example, \u201cagend\u201d is an obsolete form of \u201cagendum\u201d, but GPT-4 deems it a typo for \u201cagenda\u201d. External\nretrieval or web interaction could augment LM for problem solving under knowledge uncertainty.\n8\n\nreasoning as planning with its internal world model, and proposes a MCTS-based method similar to\nToT. However, its tasks are simpler than ours, and its framework lacks the modularity to incorporate\ndifferent tree search algorithms.\nSelf-reflection. Using LLMs to assess the viability of their own predictions is becoming an in-\ncreasingly important procedure in problem solving. [28, 20, 24] introduced the \u201cself-reflection\u201d\nmechanism, in which LMs provide feedback to their generation candidates. [4] improves LMs code\ngeneration accuracy by injecting feedback messages generated by the LM itself based on its code\nexecution results. Similarly, [17] also introduces \u201ccritic\u201d or review steps over the actions and states,\ndeciding the next action to take in solving computer operation tasks. Another recent work very\nrelevant to ours is \u201cself-eval guided decoding\u201d [39]. Similar to our method, self-eval decoding\nalso follows a tree-search procedure with leaves sampled from stochastic beam search decoding,\nwhich are then evaluated by LLM itself with carefully prepared self-eval prompts. Their approach\nhowever, uses the PAL formulation [8] which represents thoughts as codes, which makes it difficult\nto tackle challenging tasks like creative writing which we consider in this paper. Our Tree-of-Thought\nformulation is thus more versatile and handles challenging tasks on which GPT-4 only achieves very\nlow accuracy with standard prompts.\nProgram-guided LLM generation. Our proposal is also related to recent advancements that organize\nLM\u2019s behavior with systematic procedures [14, 44, 6, 43] or symbolic program guidance. For example,\nSchlag et al. [27] embeds LMs in an algorithmic search procedure to help solve problems like question\nanswering step-by-step, in which the search trees are expanded by relevant paragraphs that might\nprovide answers. This approach however differs from ours in that trees are expanded by sampling\nexternal paragraphs instead of the LM\u2019s own thoughts, and there is no reflection or voting steps.\nAnother approach, LLM+P [18], goes one step further and delegates the actual planning process to a\nclassical planner.\nClassical search methods. Last but not least, our approach can be treated as a modern rendition\nof classical search methods for problem solving. For example it can be considered as a heuristic\nsearch algorithm like A* [10], in which the heuristic at each search node is provided by the LM\u2019s self-\nassessment. From this perspective, our method is also related to NeuroLogic A*esque decoding [19],\nwhich is inspired by A* search but introduces look-ahead heuristics that are efficient for LMs to\nimprove the beam-search or top-k sampling decoding. This method however is constrained to\nsentence generation tasks, whereas our framework are designed for complex, multi-step problem\nsolving guarded by value feedback.\n6\nDiscussion\nLimitations and future directions. Deliberate search such as ToT might not be necessary for many\nexisting tasks that GPT-4 already excels at (see Appendix B.1), and as an initial step this work only\nexplores three relatively simple tasks that challenges GPT-4 (see Appendix B.2 for some GPT-3.5\nexperiment results) and calls of better search and planning abilities incorporated with LMs. However,\nas we begin to deploy LMs for more real-world decision making applications (e.g. coding, data\nanalysis, robotics, etc.), more complex tasks could emerge and present new opportunities to study\nthese research questions. Also, search methods like ToT requires more resources (e.g. GPT-4 API\ncost) than sampling methods in order to improve task performances, but the modular flexibility of\nToT allows users to customize such performance-cost tradeoffs, and ongoing open-source efforts [32]\nshould readily reduce such costs in the near future. More details about cost and efficiency are in\nAppendix B.3. Lastly, this work focuses on using an off-the-shelf LM, and fine-tuning LMs using\na ToT-style high-level counterfactual decision making (e.g. deliberating over potential choices for\nthe next paragraph, instead of predicting the next token) might present opportunities to enhance the\nproblem-solving capabilities of LMs.\nConclusion. The associative \u201cSystem 1\u201d of LMs can be beneficially augmented by a \u201cSystem 2\u201d\nbased on searching a tree of possible paths to the solution to a problem. The Tree of Thoughts\nframework provides a way to translate classical insights about problem-solving into actionable\nmethods for contemporary LMs. At the same time, LMs address a weakness of these classical\nmethods, providing a way to solve complex problems that are not easily formalized, such as creative\nwriting. We see this intersection of LMs with classical approaches to AI as an exciting direction.\n9\n\nBroader Impact\nToT is a framework that empowers LMs to more autonomously and intelligently make decisions\nand solve problems. While current tasks are limited to reasoning and search problems, future\napplications involving interaction with external environments or humans could bring potential danger,\ne.g. facilitating harmful uses of LMs. On the other hand, ToT also improves the interpretability\nof model decisions and the opportunity for human alignment, as the resulting representations are\nreadable, high-level language reasoning instead of implicit, low-level token values.\nAcknowledgements\nSY and KN acknowledge support from an Oracle Collaborative Research award and the National\nScience Foundation under Grant No. 2239363. Any opinions, findings, conclusions, or recommenda-\ntions expressed in this material are those of the author(s) and do not necessarily reflect the views of\nthe National Science Foundation. SY is also supported by the Harold W. Dodds Fellowship from\nPrinceton.\nReferences\n[1] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam,\nG. Sastry, A. Askell, et al. Language models are few-shot learners. Advances in neural\ninformation processing systems, 33:1877\u20131901, 2020.\n[2] C. Browne, E. J. Powley, D. Whitehouse, S. M. M. Lucas, P. I. Cowling, P. Rohlfshagen,\nS. Tavener, D. P. Liebana, S. Samothrakis, and S. Colton. A survey of monte carlo tree search\nmethods. IEEE Transactions on Computational Intelligence and AI in Games, 4:1\u201343, 2012.\n[3] M. Campbell, A. J. Hoane Jr, and F.-h. Hsu. Deep blue. Artificial intelligence, 134(1-2):57\u201383,\n2002.\n[4] X. Chen, M. Lin, N. Sch\u00a8arli, and D. Zhou. Teaching large language models to self-debug, 2023.\n[5] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W.\nChung, C. Sutton, S. Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv\npreprint arXiv:2204.02311, 2022.\n[6] A. Creswell and M. Shanahan. Faithful reasoning using large language models. arXiv preprint\narXiv:2208.14271, 2022.\n[7] N. D. Daw, Y. Niv, and P. Dayan. Uncertainty-based competition between prefrontal and\ndorsolateral striatal systems for behavioral control. Nature neuroscience, 8(12):1704\u20131711,\n2005.\n[8] L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan, and G. Neubig. Pal: Program-\naided language models, 2023.\n[9] S. Hao, Y. Gu, H. Ma, J. J. Hong, Z. Wang, D. Z. Wang, and Z. Hu. Reasoning with language\nmodel is planning with world model. arXiv preprint arXiv:2305.14992, 2023.\n[10] P. E. Hart, N. J. Nilsson, and B. Raphael. A formal basis for the heuristic determination of\nminimum cost paths. IEEE Transactions on Systems Science and Cybernetics, 4(2):100\u2013107,\n1968. doi: 10.1109/TSSC.1968.300136.\n[11] P. E. Hart, N. J. Nilsson, and B. Raphael. A formal basis for the heuristic determination of\nminimum cost paths. IEEE transactions on Systems Science and Cybernetics, 4(2):100\u2013107,\n1968.\n[12] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch. Language models as zero-shot planners:\nExtracting actionable knowledge for embodied agents, 2022.\n[13] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch,\nY. Chebotar, et al. Inner monologue: Embodied reasoning through planning with language\nmodels. arXiv preprint arXiv:2207.05608, 2022.\n10\n\n[14] J. Jung, L. Qin, S. Welleck, F. Brahman, C. Bhagavatula, R. L. Bras, and Y. Choi. Maieu-\ntic prompting: Logically consistent reasoning with recursive explanations. arXiv preprint\narXiv:2205.11822, 2022.\n[15] D. Kahneman. Thinking, fast and slow. Macmillan, 2011.\n[16] D. Kahneman, S. Frederick, et al. Representativeness revisited: Attribute substitution in intuitive\njudgment. Heuristics and biases: The psychology of intuitive judgment, 49(49-81):74, 2002.\n[17] G. Kim, P. Baldi, and S. McAleer. Language models can solve computer tasks, 2023.\n[18] B. Liu, Y. Jiang, X. Zhang, Q. Liu, S. Zhang, J. Biswas, and P. Stone. Llm+p: Empowering\nlarge language models with optimal planning proficiency, 2023.\n[19] X. Lu, S. Welleck, P. West, L. Jiang, J. Kasai, D. Khashabi, R. L. Bras, L. Qin, Y. Yu,\nR. Zellers, N. A. Smith, and Y. Choi. Neurologic a*esque decoding: Constrained text generation\nwith lookahead heuristics. In North American Chapter of the Association for Computational\nLinguistics, 2021.\n[20] A. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri,\nS. Prabhumoye, Y. Yang, S. Welleck, B. P. Majumder, S. Gupta, A. Yazdanbakhsh, and P. Clark.\nSelf-refine: Iterative refinement with self-feedback, 2023.\n[21] A. Newell, J. C. Shaw, and H. A. Simon. Report on a general problem solving program. In IFIP\ncongress, volume 256, page 64. Pittsburgh, PA, 1959.\n[22] A. Newell, H. A. Simon, et al. Human problem solving. Prentice-Hall, 1972.\n[23] OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.\n[24] D. Paul, M. Ismayilzada, M. Peyrard, B. Borges, A. Bosselut, R. West, and B. Faltings. Refiner:\nReasoning feedback on intermediate representations, 2023.\n[25] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al. Improving language understanding\nby generative pre-training. OpenAI blog, 2018.\n[26] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are\nunsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n[27] I. Schlag, S. Sukhbaatar, A. Celikyilmaz, W. tau Yih, J. Weston, J. Schmidhuber, and X. Li.\nLarge language model programs, 2023.\n[28] N. Shinn, B. Labash, and A. Gopinath. Reflexion: an autonomous agent with dynamic memory\nand self-reflection, 2023.\n[29] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A. Guez, T. Hubert, L. Baker,\nM. Lai, A. Bolton, et al. Mastering the game of go without human knowledge. nature, 550\n(7676):354\u2013359, 2017.\n[30] S. A. Sloman. The empirical case for two systems of reasoning. Psychological bulletin, 119(1):\n3, 1996.\n[31] K. E. Stanovich. Who is rational? Studies of individual differences in reasoning. Psychology\nPress, 1999.\n[32] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi`ere, N. Goyal,\nE. Hambro, F. Azhar, et al. Llama: Open and efficient foundation language models. arXiv\npreprint arXiv:2302.13971, 2023.\n[33] S. Verma, J. Fu, S. Yang, and S. Levine. Chai: A chatbot ai for task-oriented dialogue with\noffline reinforcement learning. In Proceedings of the 2022 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies,\npages 4471\u20134491, 2022.\n11\n\n[34] E. Wallace, N. Tomlin, A. Xu, K. Yang, E. Pathak, M. Ginsberg, and D. Klein. Automated\ncrossword solving. arXiv preprint arXiv:2205.09665, 2022.\n[35] L. Wang, W. Xu, Y. Lan, Z. Hu, Y. Lan, R. K.-W. Lee, and E.-P. Lim. Plan-and-solve prompting:\nImproving zero-shot chain-of-thought reasoning by large language models, 2023.\n[36] X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, and D. Zhou. Self-consistency improves chain\nof thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.\n[37] Z. Wang, S. Cai, A. Liu, X. Ma, and Y. Liang. Describe, explain, plan and select: Interactive\nplanning with large language models enables open-world multi-task agents, 2023.\n[38] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought\nprompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.\n[39] Y. Xie, K. Kawaguchi, Y. Zhao, X. Zhao, M.-Y. Kan, J. He, and Q. Xie. Decomposition\nenhances reasoning via self-evaluation guided decoding, 2023.\n[40] S. Yang, O. Nachum, Y. Du, J. Wei, P. Abbeel, and D. Schuurmans. Foundation models for\ndecision making: Problems, methods, and opportunities, 2023.\n[41] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao. ReAct: Synergizing\nreasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022.\n[42] S. Zhang, Z. Chen, Y. Shen, M. Ding, J. B. Tenenbaum, and C. Gan. Planning with large\nlanguage models for code generation. In The Eleventh International Conference on Learning\nRepresentations, 2023. URL https://openreview.net/forum?id=Lr8cOOtYbfL.\n[43] D. Zhou, N. Sch\u00a8arli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schuurmans, C. Cui, O. Bousquet,\nQ. Le, et al. Least-to-most prompting enables complex reasoning in large language models.\narXiv preprint arXiv:2205.10625, 2022.\n[44] X. Zhu, J. Wang, L. Zhang, Y. Zhang, R. Gan, J. Zhang, and Y. Yang. Solving math word\nproblem via cooperative reasoning induced language models. arXiv preprint arXiv:2210.16257,\n2022.\n12\n\nA\nCode, Prompts, Trajectories\nAll code is available at https://github.com/princeton-nlp/tree-of-thought-llm.\nAll prompts are available at https://github.com/princeton-nlp/tree-of-thought-llm/\ntree/master/src/tot/prompts.\nTrajectories are available at https://github.com/princeton-nlp/tree-of-thought-llm/\ntree/master/logs.\nB\nAdditional Experiment Results\nGiven the motivation of exploring and extending the capability frontier of language models, our\nexperiments in the main paper have focused on a setup with the state-of-the-art language model\n(GPT-4), and three hard tasks invented to challenge it. Here, we report additional experiments with\nweaker LLM or easier tasks, and discuss cost and efficiency.\nGSM8K\nStrategyQA\nIO\n51\n73\nCoT\n86\n82\nToT\n90\n83\nTable 4: New tasks with\nzero-shot ToT and GPT-4.\nGPT-4\nGPT-3.5\nIO\n7.3%\n6%\nCoT\n4.0%\n3%\nToT\n74%\n19%\nTable 5: Game of 24 with\nGPT-4 vs GPT-3.5.\nGPT-4\nGPT-3.5\nIO\n6.19\n4.47\nCoT\n6.93\n5.16\nToT\n7.56\n6.62\nTable 6: Creative Writing with\nGPT-4 vs. GPT-3.5.\nB.1\nExtension to new tasks (GSM8k, StrategyQA) with zero-shot ToT\nWhile more common NLP tasks might be too easy for GPT-4 and do not require ToT (which is why\nwe considered harder new tasks), we believe applying ToT to new tasks could be straightforward.\nFor example, we implemented a simple and generic zero-shot ToT-BFS similar to creative writing\n(sample 5 problem solving strategies then vote for the best one; then sample 5 solutions based on the\nbest strategy then vote for the best one) for GSM8K and StrategyQA with few extra lines of code:\n# define the answer format of new tasks\ngsm8k_format = \u2018\"the answer is n\" where n is a number\u2019\nstrategyqa_format = \u2018either \"the answer is yes\" or \"the answer is no\"\u2019\n# define zero-shot io prompting\nstandard_prompt = \u2018Answer the following question with {format}: {input}\u2019\n# define thought format for zero-shot cot and zero-shot tot\ncot_prompt = \u2018\u2018\u2018Answer the following question: {input}\nMake a strategy then write. Your output should be of the following format:\nStrategy:\nYour strategy about how to answer the question.\nAnswer:\nYour answer to the question. It should end with {format}.\n\u2019\u2019\u2019\n# define zero-shot voting used for zero-shot tot\nvote_prompt = \u2018\u2018\u2018Given an instruction and several choices,\ndecide which choice is most promising.\nAnalyze each choice in detail, then conclude in the last line\n\"The best choice is {s}\", where s the integer id of the choice.\n\u2019\u2019\u2019\n13\n\nWe evaluated on a subset of 100 random GSM8K test and StrategyQA dev questions. As shown\nin Table 4 and as expected, ToT improves over CoT on both tasks (but only slightly, given GPT-4\n+ CoT is already very good on such tasks, and StrategyQA\u2019s bottleneck is external knowledge, not\nreasoning). Considering computational costs, it is more suitable to try smaller LLMs + ToT for\ntraditional NLP tasks, or GPT-4 + ToT for hard tasks that challenge GPT-4 + CoT\u2019s reasoning.\nB.2\nExtension to new LMs (GPT-3.5)\nTo understand how ToT works with other LLMs, we also ran GPT-3.5-turbo for Creative Writing\n(Table 6) and Game of 24 (Table 5). On both tasks, \u201cToT > CoT > IO\u201d remains true for GPT-3.5. On\nCreative Writing, we find GPT-3.5+ToT outperform GPT-4+IO, and similar to GPT-4+CoT, which\nsuggests ToT could also work well on weaker language models.\nOn Game of 24 (we changed 1-shot proposal prompt to 3-shot to make it work), GPT-3.5+ToT\u2019s\n19% is far worse than GPT-4+ToT\u2019s 74%. To further understand the importance of generation\nvs. evaluation, we ran GPT-4 generation + GPT-3.5 evaluation (64%) and GPT-3.5 generation +\nGPT-4 evaluation (31%). This suggests the game\u2019s bottleneck is thought generation, and different\ngeneration/evaluation language models might attain decent results while reducing costs.\nB.3\nCost and efficiency\nRunning ToT requires significantly more computations than IO or CoT prompting. For example, in\nGame of 24 (Table 7 below), solving a problem with ToT requires 5.5k completion tokens, close to\n100 CoT trials (6.7k tokens). But the performance of ToT is better than best of 100 independent CoT\ntrials.\nGame of 24\nGenerate/Prompt tokens\nCost per case\nSuccess\nIO (best of 100)\n1.8k / 1.0k\n$0.13\n33%\nCoT (best of 100)\n6.7k / 2.2k\n$0.47\n49%\nToT\n5.5k / 1.4k\n$0.74\n74%\nTable 7: Cost analysis on Game of 24.\nOn Creative Writing (Table 8 below), we found ToT takes around 5x completion tokens and money\ncost, which is intuitive as b = 5 and most tokens are generated passages.\nCreative Writing\nGenerate/Prompt tokens\nCost per case\nIO\n0.9k / 0.4k\n$0.06\nCoT\n0.9k / 0.4k\n$0.07\nToT\n4k / 2.9k\n$0.32\nTable 8: Cost analysis on Game of 24.\nSo completing Game of 24 and Creative Writing\u2019s main ToT experiments cost around 0.74 \u00d7 100 +\n0.32 \u00d7 100 = 106 dollars. Crosswords\u2019 DFS experiments should be also within 100 dollars. In\ngeneral, cost and efficiency of ToT highly depend on the prompts and search algorithms used, and\ncould require 5-100 times more generated tokens than CoT. Some actionable insights:\n\u2022 We recommend using ToT on tasks requiring deliberate reasoning, on which CoT struggles.\n\u2022 Flexibility of ToT allows some performance-cost tradeoff, e.g., change beam size or vote\nnumber in BFS, few-shot vs. zero-shot prompting, GPT-3.5 vs. GPT-4, etc. One could\nconfigure the setup based on some resource constraints or performance goal.\n\u2022 There is much space for improving efficiency, e.g., BFS could early stop when solution is\nfound, or trim down beam size to when some thoughts are \u201dimpossible\u201d.\n\u2022 We believe that more computation is indeed required in order for the model to achieve\nstronger intelligence, and this should not become a blocking issue as in the long run, (open-\nsource) LMs will become much cheaper and more efficient. It is also a great direction how\nto better train/finetune LMs for thought generation and/or evaluation.\n14\n",
    "AlphaMaze: Enhancing Large Language Models\u2019 Spatial\nIntelligence via GRPO\nAlan Dao (Gia Tuan Dao)1, Dinh Bach Vu1\nMenlo Research\nalan@menlo.ai, bach@menlo.ai\n1Equal contribution.\nFebruary 20, 2025\nAbstract\nLarge Language Models (LLMs) have demonstrated im-\npressive capabilities in language processing, yet they of-\nten struggle with tasks requiring genuine visual spatial\nreasoning.\nIn this paper, we introduce a novel two-\nstage training framework designed to equip standard\nLLMs with visual reasoning abilities for maze naviga-\ntion. First, we leverage Supervised Fine-Tuning (SFT)\non a curated dataset of tokenized maze representations\nto teach the model to predict step-by-step movement\ncommands.\nNext, we apply Group Relative Policy\nOptimization (GRPO)\u2014a technique used in DeepSeek-\nR1\u2014with a carefully crafted reward function to refine the\nmodel\u2019s sequential decision-making and encourage emer-\ngent chain-of-thought behaviors. Experimental results on\nsynthetically generated mazes show that while a base-\nline model fails to navigate the maze, the SFT-trained\nmodel achieves 86% accuracy, and further GRPO fine-\ntuning boosts accuracy to 93%. Qualitative analyses re-\nveal that GRPO fosters more robust and self-corrective\nreasoning, highlighting the potential of our approach to\nbridge the gap between language models and visual spa-\ntial tasks.\nThese findings offer promising implications\nfor applications in robotics, autonomous navigation, and\nother domains that require integrated visual and sequen-\ntial reasoning.\n1\nIntroduction\nThe ability to reason about visual information, partic-\nularly in spatial contexts, is a hallmark of intelligent\nsystems. From navigating physical environments to in-\nterpreting complex diagrams, visual spatial reasoning is\ncrucial for a wide range of tasks. While Large Language\nModels (LLMs) have achieved impressive performance in\nnatural language processing and code generation, their\ncapacity for genuine visual reasoning, especially spatial\nunderstanding and sequential decision-making in visual\nenvironments, remains a significant open question [Zhang\net al., 2024, Ma et al., 2024]. Current Vision-Language\nModels (VLMs) often excel at pattern recognition and ob-\nject identification but may struggle with tasks requiring\ndeeper spatial inference and step-by-step planning\n0\n200\n400\n600\n800\n1000\n1200\n1400\n1600\n1800\n2000\n80\n85\n90\n95\nGRPO steps\nMazeBench\nMazeBench scores with GRPO steps\nMazeBench\nTrendline (Linear Regression)\n\u00b11 Std Dev\nFigure 1: MazeBench scores over GRPO steps with a\nlinear regression trendline and its \u00b11 standard deviation\nbounds.\nin visual domains [Ma et al., 2024]. Bridging this gap\nand endowing standard LLMs with robust visual reason-\ning capabilities is a critical step towards more versatile\nand human-like AI.\nIn this paper, we address the challenge of teaching vi-\nsual spatial reasoning to a standard LLM, focusing on\nthe task of maze navigation.\nWe hypothesize that by\nproviding an LLM with a tokenized visual representation\nof a maze, we can train it to learn step-by-step move-\nment commands to navigate from a designated origin to\na target. The core of our approach lies in a two-stage\ntraining framework. First, we employ Supervised Fine-\nTuning (SFT) to equip the LLM with the foundational\nskill of predicting movement tokens based on the visual\nmaze input. Subsequently, we apply Group Relative Pol-\nicy Optimization (GRPO), drawing inspiration from re-\ncent advancements in reinforcement learning for reason-\ning in LLMs, such as DeepSeek-R1 [Guo et al., 2025].\nDeepSeek-R1 demonstrated that Reinforcement Learning\n(RL) can elicit emergent reasoning behaviors, including\nchain-of-thought, even without prior SFT. We adapt and\nextend these RL strategies, combined with carefully de-\nsigned reward functions, to refine our model\u2019s visual rea-\nsoning process for maze navigation.\nTo systematically evaluate LLM\u2019s ability to solve maze,\n1\narXiv:2502.14669v3  [cs.CL]  25 Feb 2025\n\nwe introduce MazeBench\u2014a comprehensive benchmark\non solving maze. MazeBench provides a controlled yet\ndiverse environment that spans a range of maze sizes and\ncomplexities. By evaluating our model on MazeBench,\nwe can rigorously measure both its maze-solving accuracy\nand the sophistication of its emergent reasoning behavior.\nOur key contributions are as follows:\n\u2022 We present a novel training framework that combines\nSupervised Fine-Tuning and Group Relative Policy\nOptimization to enhance visual reasoning in stan-\ndard LLMs, specifically for spatial tasks.\n\u2022 We empirically demonstrate that this framework, us-\ning a tokenized visual maze representation, enables\nan LLM to achieve improved maze navigation accu-\nracy and exhibit emergent chain-of-thought reason-\ning in generating movement sequences.\n\u2022 We provide a detailed analysis of the design and im-\npact of reward functions within the GRPO stage,\nhighlighting their crucial role in shaping the model\u2019s\nvisual reasoning performance.\n\u2022 We draw comparisons with insights from state-of-\nthe-art reasoning models like DeepSeek-R1, both in\nterms of methodology and observed emergent behav-\niors, positioning our work within the context of cur-\nrent advancements in LLM reasoning.\n\u2022 We present MazeBench, a benchmark for visual maze\nnavigation that captures a wide spectrum of spatial\nchallenges.\n2\nRelated Work\n2.1\nChain-of-Thought Reasoning in Language\nModels\nChain-of-Thought (CoT) prompting has emerged as a\npowerful technique to elicit complex reasoning from Large\nLanguage Models [Wei et al., 2022b].\nBy prompting\nLLMs to \u201dthink step by step,\u201d CoT encourages the gen-\neration of intermediate reasoning steps, leading to im-\nproved performance on tasks requiring multi-step infer-\nence. Prior research, including Wei et al. [2022b], Wei\net al. [2023], Wang et al. [2023] prompting significantly\nenhances LLM performance on arithmetic, commonsense\nreasoning, and symbolic reasoning tasks. Our work builds\nupon the concept of CoT reasoning, aiming to induce a\nsimilar step-by-step thought process in LLMs, but within\nthe domain of visual spatial reasoning for maze naviga-\ntion.\n2.2\nSupervised Fine-Tuning for Visual and Spa-\ntial Tasks\nSupervised Fine-Tuning (SFT) is a widely adopted tech-\nnique for adapting pre-trained LLMs to specific down-\nstream tasks [Wei et al., 2022a].\nBy training on task-\nspecific datasets, SFT allows LLMs to acquire special-\nized skills and improve performance in targeted domains.\nJiang et al. [2024] recently highlighted the effectiveness\nof SFT in enhancing visual foundation models, demon-\nstrating its utility in visual tasks.\nIn our research, we\nleverage SFT as the initial stage of our training pipeline,\nusing it to equip the LLM with the basic capability of\nprocessing tokenized visual maze inputs and predicting\nmovement tokens.\nThis SFT phase serves as a crucial\nfoundation upon which we build more sophisticated rea-\nsoning through reinforcement learning.\n2.3\nReinforcement Learning and GRPO for Rea-\nsoning and Reward Shaping\nReinforcement Learning from Human Feedback (RLHF)\nand its variants have demonstrated significant efficacy\nin aligning Large Language Models (LLMs) with human\npreferences and enhancing their reasoning capabilities.\nHowever, RLHF faces substantial scalability challenges\ndue to its resource-intensive nature and reliance on hu-\nman feedback data. As an alternative approach, recent\nmethodologies like Group Relative Policy Optimization\n(GRPO)[Kwon et al., 2023a] and Self-Play fIne-tuNing\n(SPIN) leverage self-play mechanisms, where models au-\ntonomously generate training signals and iteratively im-\nprove through self-competition Chen et al. [2024]. These\nself-play approaches show promise in achieving human-\nlevel performance without the need for extensive human\nfeedback, potentially offering a more scalable solution to\nthe alignment challenge. GRPO, as described by Shao\net al. [2024] and implemented in DeepSeek-R1 [Guo et al.,\n2025], offers a computationally efficient approach to re-\ninforcement learning by estimating advantages based on\ngroup scores, eliminating the need for a separate critic\nnetwork. Reward function design is paramount in RLHF\nand GRPO, as it directly guides the model\u2019s learning pro-\ncess. Carefully crafted reward functions can incentivize\ndesired behaviors and shape the model\u2019s policy towards\noptimal performance. Our work draws inspiration from\nthe reward shaping strategies used in DeepSeek-R1 and\nadapts them to the context of visual maze navigation, de-\nsigning reward components to encourage accuracy, valid\nmovement sequences, and proper output formatting.\n2.4\nDeepSeek-R1\nand\nEmergent\nReasoning\nthrough RL\nThe DeepSeek-R1 model [Guo et al., 2025] represents a\nsignificant advancement in using reinforcement learning\nto elicit sophisticated reasoning capabilities in LLMs. A\nkey finding of DeepSeek-R1 is the demonstration that\npure RL, specifically GRPO, can lead to the emer-\ngent development of chain-of-thought reasoning and even\n\u201daha moments,\u201d where the model re-evaluates previ-\nous steps and corrects its reasoning process.\nFurther-\nmore, DeepSeek-R1 highlights the benefits of a multi-\nstage training pipeline, combining initial RL training with\nsubsequent supervised fine-tuning to refine language co-\nherence and readability. We directly adapt the GRPO\noptimization strategy and multi-stage training insights\nfrom DeepSeek-R1 to our visual maze navigation task.\n2\n\nFigure 2: Visual of the Example Maze\nWe hypothesize that similar RL techniques can drive the\nemergence of visual spatial reasoning in standard LLMs,\nenabling them to solve mazes through a step-by-step, self-\ncorrective process.\n2.5\nVisual Reasoning and Maze Solving in AI\nMaze solving has long been a benchmark task in Ar-\ntificial Intelligence, serving as a testbed for various\nproblem-solving and search algorithms [Janamian and\nAlam, 2023].\nTraditional approaches include graph\nsearch algorithms like Depth-First Search, Breadth-First\nSearch, and A* [Lester, 2014-2024].\nMore recently, AI\ntechniques, particularly reinforcement learning and neu-\nral networks, have been applied to maze navigation\n[Zafrany, 2020]. While Chain-of-Thought (CoT) prompt-\ning has significantly enhanced complex reasoning capabil-\nities in Large Language Models (LLMs) and Multimodal\nLLMs, it shows limitations in complex spatial reasoning\ntasks. Recent work by Microsoft introduces Multimodal\nVisualization-of-Thought (MVoT), which enables models\nto generate visual representations during their reasoning\nprocess, similar to human visual thinking [Li et al., 2025].\nThis breakthrough demonstrates the potential of combin-\ning verbal and visual reasoning in AI systems.\nOur research builds upon these advances, focusing\non teaching visual maze reasoning to standard language\nmodels through a tokenized visual representation and a\ncombination of SFT and GRPO. This approach differs\nfrom traditional maze solvers by leveraging the inher-\nent reasoning capabilities of LLMs and adapting them\nto process and reason about visual spatial information.\nFurthermore, research in neural-symbolic visual reason-\ning [Mao et al., 2023] explores combining neural networks\nwith symbolic AI for visual tasks, offering a complemen-\ntary perspective on integrating reasoning and visual pro-\ncessing.\n3\nMethodology\n3.1\nTokenized Visual Maze Representation\nTo enable the LLM to process maze information visually,\nwe designed a tokenized input format that represents\nthe maze grid, walls, origin, and target locations. Each\ncell in the maze is represented by a coordinate token\n<|row-col|>, e.g., <|0-0|> for the top-left cell.\nWall\ninformation for each cell is encoded using tokens such\nas\n<|no wall|>, <|up wall|>, <|up down wall|>,\n<|up down left right wall|>, ....\nThe origin and\ntarget\nlocations\nare\nmarked\nwith\n<|origin|>\nand\n<|target|> tokens, respectively.\nEmpty spaces within\nthe maze representation are filled with <|blank|> tokens\nfor consistent grid structure. This tokenization scheme\nprovides a visual representation by explicitly encoding\nthe spatial relationships between cells and the presence\nof walls, allowing the LLM to \u201csee\u201d the maze structure\nin a symbolic, tokenized form.\nExample Maze Tokenization 2:\n<|0-0|><| up_left_wall |><|blank |><|0-1|><|\nup_down_wall |><|blank |><|0-2|><|\nup_down_wall |><|blank |><|0-3|><|\nup_down_right_wall |><|blank |><|0-4|><|\nup_left_right_wall |><|blank|>\n<|1-0|><| down_left_wall |><|blank |><|1-1|><|\nup_down_wall |><|blank |><|1-2|><|\nup_right_wall |><|blank |><|1-3|><|\nup_down_left_wall |><|blank |><|1-4|><|\nright_wall |><|blank|>\n<|2-0|><| up_left_wall |><|blank |><|2-1|><|\nup_down_right_wall |><| origin |><|2-2|><|\nleft_right_wall |><|blank |><|2-3|><|\nup_left_wall |><|blank |><|2-4|><|\nright_wall |><|blank|>\n<|3-0|><| left_wall |><|blank |><|3-1|><|\nup_right_wall |><|blank |><|3-2|><|\ndown_left_wall |><|blank |><|3-3|><|\ndown_right_wall |><|blank |><|3-4|><|\nleft_right_wall |><|blank|>\n<|4-0|><| down_left_right_wall |><|blank\n|><|4-1|><| down_left_wall |><|blank\n|><|4-2|><| up_down_wall |><| target\n|><|4-3|><| up_down_wall |><|blank\n|><|4-4|><| down_right_wall |><|blank|>\n3.2\nBaseline Models\nTo establish performance benchmarks for our approach,\nwe employed three distinct baseline models, leveraging\nthe DeepSeek-R1 [Guo et al., 2025] Distill-Qwen family\nof language models.\nWe evaluate two distilled models:\nDeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-\nQwen-1.5B. Additionally, a Direct Prediction baseline\nwas established using a Supervised Fine-Tuning (SFT)\napproach on the DeepSeek-R1-Distill-Qwen-1.5B archi-\ntecture. This model was trained to directly predict the\ncomplete sequence of movement tokens representing the\nsolution path through a given maze. The training objec-\ntive was the minimization of cross-entropy loss between\nthe predicted token sequence and the ground truth solu-\ntion. This baseline assesses the performance of a stan-\ndard language model trained to generate complete solu-\ntions without intermediate reasoning steps or reinforce-\nment learning techniques.\nWe include these three baselines to provide a compre-\nhensive comparison, examining the influence of model\n3\n\nFigure 3: Visualization of AlphaMaze\u2019s step-by-step rea-\nsoning process while solving a maze.\nsize (7B vs. 1.5B) and the effectiveness of direct predic-\ntion versus our proposed step-by-step and reinforcement\nlearning approaches.\nThe subsequent sections will pri-\nmarily focus on the customized direct prediction model\nand its enhancements through SFT for step-by-step rea-\nsoning and GRPO.\n3.3\nSupervised Fine-Tuning (SFT) for Step-by-\nStep Reasoning\nFor the SFT stage, we curated a training dataset. Mazes\nwere synthetically generated with fixed sizes (5x5) and\nvaried complexity level. The Qwen 1.5B SFT model was\nthen trained on this dataset. The training objective was\nto predict the next movement token at each step, condi-\ntioned on the maze input and the preceding movement\ntokens in the sequence as visually illustrated in Figure\n3.\nThis step-by-step prediction approach was designed\nto encourage the model to learn sequential reasoning for\nmaze navigation.\n3.4\nGroup\nRelative\nPolicy\nOptimization\n(GRPO) for Enhanced Reasoning\nFollowing SFT, we applied Group Relative Policy Opti-\nmization (GRPO) to further enhance the model\u2019s maze-\nsolving capabilities and encourage more robust reasoning.\nThe GRPO training utilized a smaller set of data than\nSFT state. We designed a reward function 3 components.\nCorrectness Reward (+0.2 per solution step):\nThis reward is scaled according to the number of steps\nin the maze solution.\nEach valid movement step adds\n0.2 points to the total score.\nFor example, a solution\nrequiring 4 steps earns a reward of 0.2 \u00d7 4 = 0.8 points,\nincentivizing both accuracy and efficiency in navigation.\nIntegrity Reward (+0.5):\nThis reward is given\nfor each valid movement token (<|up|>,\n<|down|>,\n<|left|>, <|right|>) in the predicted sequence, encour-\naging the generation of meaningful and valid movement\nsteps.\nThinking Reward (+0.25): This reward is given for\ncorrectly using the <think> tag in the output, ensuring\ncompleteness and consistency in the reasoning format.\nThese reward components were weighted to prioritize\ncorrectness while also encouraging valid movement se-\nquences and proper reasoning formatting with <think>\ntag. We adapted the Group Relative Policy Optimization\n(GRPO) algorithm, as employed in DeepSeek-R1 [Guo\net al., 2025], to perform reinforcement learning. GRPO\nestimates advantages based on relative group scores, of-\nfering computational efficiency compared to critic-based\nmethods.\n3.5\nTraining Procedure and Pipeline\nOur training pipeline consisted of two stages. First, Su-\npervised Fine-Tuning (SFT) was performed on the\nQwen 1.5B model using a curated maze dataset for 10\nepochs to learn step-by-step movement prediction for\nmaze navigation. This phase established a strong initial\npolicy, ensuring that the model could effectively interpret\nand respond to sequential movement tasks.\nFollowing SFT, Group Relative Policy Optimiza-\ntion (GRPO) was applied to refine the model\u2019s perfor-\nmance. The SFT-trained model was further fine-tuned\nusing LoRA Hu et al. [2021] with the GRPO method, im-\nplemented in Unsloth Daniel Han and team [2023] with\nVLLM Kwon et al. [2023b] for efficient inference.\nA\ncarefully designed reward function guided the optimiza-\ntion process, and model checkpoints were saved every 200\nsteps to track improvements.\nThis two-stage pipeline mirrors the multi-stage train-\ning approach employed in DeepSeek-R1 [Guo et al., 2025],\nwhere initial RL training is followed by supervised fine-\ntuning for refinement. In our case, SFT provided a robust\nstarting point for reinforcement learning (RL), allowing\nGRPO to focus on refining reasoning capabilities and en-\nhancing task-specific performance.\n4\nExperiments and Results\n4.1\nDataset Details\nThe dataset is constructed through a multi-stage process\ninvolving generation, refinement, and augmentation. The\nprocess begins with the creation of a large initial pool of\n530,000 synthetic mazes. These mazes are generated us-\ning the maze-dataset framework Ivanitskiy et al. [2023],\nwhich employs a randomized depth-first search algorithm.\nThis algorithm ensures that every generated maze has a\nguaranteed solution path connecting the designated ori-\ngin and target locations.\nFurther details of algorithm\nused can be found at Appendix 1. All mazes within the\ndataset have a fixed size of 5x5 grids. From this extensive\ninitial pool, a subset of 30,000 mazes is randomly selected\nand reserved as a held-out test set. This separation guar-\nantees that the training and evaluation data are entirely\ndistinct, preventing data leakage and enabling a robust\nassessment of model generalization.\n4\n\nThe remaining 500,000 mazes form the basis for the\nvarious training datasets used in this work. This pool of\nmazes undergoes a multi-stage processing and augmenta-\ntion procedure to create datasets specifically tailored for\ndifferent training objectives.\nReset Dataset Creation: A significant portion of the\ntraining data focuses on teaching the model to recover\nfrom errors.\nTo this end, a \u201dreset\u201d dataset is created.\nThis dataset is generated by algorithmically producing\nincorrect solution paths for the mazes. These incorrect\npaths are designed to be plausible but ultimately unsuc-\ncessful, either leading to dead ends or deviating from the\ncorrect solution. Importantly, they adhere to constraints:\nthey do not revisit locations already visited within the\nincorrect attempt, and they avoid portions of the known\ncorrect solution path.\nAssociated with each incorrect path, a textual \u201dRE-\nSET\u201d message is generated, simulating the feedback a\nsystem might provide upon encountering an error. The\ncontent of this message depends on whether the incorrect\npath terminates at a dead end (three surrounding walls)\nor simply deviates from the wrong route. These incor-\nrect paths, along with their reset messages and the cor-\nrect solution\u2019s Chain-of-Thought (COT) reasoning, are\ncombined. This process results in approximately 400,000\ntraining examples where the model is presented with sce-\nnarios of initial failure(s) followed by a successful attempt\nafter a \u201dreset.\u201d The intent is to train the model to recog-\nnize incorrect trajectories and adapt its strategy. Illustra-\ntive examples of reset samples are provided in Appendix.\nSFT Training Data Construction: The final Su-\npervised Fine-Tuning (SFT) training dataset is a bal-\nanced combination of \u201dstraight success\u201d examples and\n\u201dretry\u201d examples:\n\u2022 Straight Success Data (250,000 mazes): This portion\nconsists of mazes where the model is expected to gen-\nerate the correct solution path on the first attempt,\nwithout any resets.\n\u2022 Retry Data (250,000 mazes): This portion is drawn\nfrom the \u201dreset\u201d dataset described above, provid-\ning examples where the model learns from incor-\nrect attempts and subsequent resets. This combined\n500,000-maze SFT set, encompassing success and er-\nror recovery, enables robust learning.\nGRPO Training Data:\nThe remaining 150,000 mazes\nfrom the original \u201dstraight success\u201d data pool are used to\ncreate a dataset for GRPO stage.\n4.2\nMazeBench\nTo rigorously evaluate the spatial reasoning and planning\ncapabilities of large language models (LLMs), we intro-\nduce MazeBench, a novel benchmark consisting of a cu-\nrated collection of 100 maze-solving challenges.\nWhile\nexisting benchmarks often assess logical reasoning or com-\nmonsense knowledge, MazeBench specifically targets the\nability of LLMs to understand spatial relationships, plan\nmulti-step paths, and execute sequential actions within a\nconstrained environment. This capacity is crucial for ap-\nplications ranging from robotics and navigation to game\nplaying and virtual agent control.\nMazeBench is a collection of 100 unique mazes, ran-\ndomly selected from a larger test set containing 30,000\nmazes. It is designed to evaluate the performance of large\nlanguage models (LLMs) by categorizing mazes into dif-\nferent difficulty levels. Each maze requires the model to\ndetermine an optimal path from the starting point to the\ntarget, with difficulty primarily based on the number of\nsteps needed to reach the goal.\nThe benchmark is structured into three levels: Easy,\nMedium, and Hard, ensuring a progressive assessment of\nan LLM\u2019s pathfinding and problem-solving abilities. The\ncomponents are described in Table 1.\nTable 1: Maze Configuration by Difficulty Level\nCategory\nNumber of Mazes\nSteps\nEasy\n50\n1 \u2013 4\nMedium\n40\n5 \u2013 8\nHard\n10\n9 \u2013 13\nTotal\n100\n1 \u2013 13\nThe Easy category consists of 50 mazes, each requir-\ning between 1 and 4 steps to solve. These simpler mazes\nestablish a baseline for evaluating fundamental navigation\nskills.\nThe Medium category includes 40 mazes that de-\nmand solution paths of 5 to 8 steps.\nThese mazes in-\ntroduce a higher level of complexity, requiring more ad-\nvanced planning and spatial reasoning. Successfully solv-\ning them indicates an LLM\u2019s ability to manage moder-\nately intricate environments.\nThe Hard category comprises 10 mazes, each neces-\nsitating 9 to 13 steps to reach the target. These mazes\npresent the greatest challenge, testing an LLM\u2019s capacity\nto handle long-range dependencies and navigate complex\nspatial structures. Performance on this level reflects the\nmodel\u2019s ability to process and reason over extended solu-\ntion paths.\nAs mentioned previously, the mazes are presented to\nthe LLM in a tokenized input format; the full details of\nthis representation, including examples, are provided in\nSection 3.1. The LLM is expected to produce sequence of\nmovement tokens. During evaluation, we will parse the\nLLM\u2019s output to extract these tokens. The order of these\ntokens is crucial. The presence of extraneous characters,\nwhitespace, or other tokens will not automatically inval-\nidate the solution, provided that the correct sequence of\nmovement tokens can be extracted.\nA solution is con-\nsidered incorrect if the extracted sequence of movement\ntokens does not lead to the target or leads to an invalid\nstate (e.g., attempting to move into a wall) is considered\nincorrect. The evaluation metric is the success rate: the\npercentage of mazes solved correctly.\n5\n\n4.3\nQuantitative Results\n4.3.1\nModel Performance on MazeBench\nAs shown in Table 2, the baseline model, trained for di-\nrect path prediction without explicit reasoning, achieved\n0% accuracy on MazeBench. This highlights the neces-\nsity of step-by-step reasoning for the task. The SFT-only\nmodel reached a baseline of 86.0%, demonstrating the ef-\nfectiveness of supervised fine-tuning for learning step-by-\nstep maze navigation. Further enhancement with GRPO\nled to significant improvement, reaching 93.0% after 1600\nsteps of GRPO training.\nTable 2: Maze Solving Accuracy on MazeBench\nModel\nSFT\nGRPO\nScore\nBaseline-1.5B\n\u2717\n\u2717\n0.0\nBaseline-7B\n\u2717\n\u2717\n0.0\nBaseline-1.5B (SFT)\n\u2713\n\u2717\n0.0\nAlphaMaze-SFT\n\u2713\n\u2717\n86.0\nAlphaMaze\n\u2713\n\u2713\n93.0\n4.3.2\nModel Evolution During GRPO\nFigure 1 displays the MazeBench scores (blue crosses)\nover GRPO steps along with a linear regression trend-\nline (red dashed line) and its \u00b11 standard deviation\nbounds. The steady increase in the trendline indicates\nthat GRPO effectively guides the model towards im-\nproved maze-solving policies.\n4.4\nQualitative Results\nQualitative analysis of model outputs revealed notable\ndifferences in reasoning behavior.\nThe baseline model\noften produced nonsensical or incomplete movement se-\nquences, frequently failing to reach the target and ex-\nhibiting \u201dhallucinations\u201d by predicting movements invalid\nwithin the maze structure. The AlphaMaze-SFT model\ndemonstrated improved coherence and step-by-step pro-\ngression, but still struggled with longer or more complex\nmazes, sometimes becoming trapped in loops or making\nincorrect turns in later stages of the solution path.\nIn contrast, the AlphaMaze-SFT+GRPO model\nexhibited the most sophisticated reasoning.\nIn many\ninstances, emergent chain-of-thought patterns were ob-\nserved, with AlphaMaze (two-stage) appearing to explic-\nitly consider wall constraints and spatial relationships at\neach step before predicting the next movement. Further-\nmore, outputs occasionally displayed instances reminis-\ncent of the \u201daha moments\u201d reported in prior work on\nDeepSeek-R1. For example, in some complex mazes, Al-\nphaMaze (two-stage) would initially begin along one path,\nthen appear to \u201dre-evaluate\u201d its trajectory mid-sequence,\ncorrecting its course to find a more efficient or correct\nsolution. Error analysis indicated that AlphaMaze (two-\nstage) made fewer invalid moves and was more robust\nto long-context reasoning challenges compared to the\nAlphaMaze-SFT model. However, limitations remained,\nparticularly in mazes requiring backtracking or complex\nspatial planning beyond the immediate next step.\n5\nDiscussion\n5.1\nAnalysis of GRPO\u2019s Impact on Visual Maze\nReasoning\nOur results clearly demonstrate the incremental benefit of\nGroup Relative Policy Optimization (GRPO) in enhanc-\ning visual maze reasoning within Large Language Models.\nWhile Supervised Fine-Tuning (SFT) establishes a strong\nfoundation, enabling the model to achieve a 86% accu-\nracy on MazeBench, the application of GRPO further\nelevates performance to 93% after 1600 training steps.\nThis improvement, albeit seemingly modest in percent-\nage points, is significant considering the already strong\nbaseline established by SFT. It suggests that GRPO is\neffectively refining the model\u2019s policy, leading to more ro-\nbust and accurate maze navigation.\nThe qualitative analysis provides further insight into\nthe nature of this improvement.\nThe AlphaMaze-\nSFT+GRPO model exhibited more pronounced chain-\nof-thought reasoning patterns and instances of self-\ncorrection, indicating that GRPO is not merely fine-\ntuning the existing SFT policy, but rather encourag-\ning more sophisticated reasoning processes. The reward\nfunction, designed to incentivize correctness, valid move-\nments, and structured output, likely plays a crucial role\nin shaping this behavior. By rewarding successful naviga-\ntion and penalizing invalid steps, GRPO encourages the\nmodel to learn more deliberate and considered movement\nstrategies.\n5.2\nComparison with DeepSeek-R1 and RL for\nReasoning\nIt is important to note that the base DeepSeek-R1 model,\nwhen operating with an extremely long context win-\ndow, demonstrates emergent visual reasoning capabili-\nties. However, our experiments reveal that the distilled\nvariants (DeepSeek-R1 Distill-Qwen models) do not carry\nover these spatial reasoning abilities, as evidenced by their\n0% accuracy on MazeBench. This suggests that the dis-\ntillation process into Qwen or other smaller models is in-\nsufficient to preserve the emergent ability of visual spatial\nreasoning observed in the base model.\nIn\ncontrast,\nour\ntwo-stage\ntraining\nap-\nproach\u2014combining\nSupervised\nFine-Tuning\n(SFT)\nto establish foundational step-by-step reasoning with\nGroup Relative Policy Optimization (GRPO) for further\nrefinement\u2014effectively equips the distilled model with\nrobust visual maze-solving skills. Even with only 1600\nGRPO steps, the model achieves a notable improvement,\nreaching 93% accuracy and exhibiting clear chain-of-\nthought\nbehaviors\nalong\nwith\nself-correction\nduring\nnavigation.\nThese findings underscore the necessity of specialized\ntraining to recover or enhance spatial reasoning in dis-\ntilled models, highlighting that while the base DeepSeek-\n6\n\nR1 model is capable of visual reasoning with sufficient\ncontext, additional training stages are crucial to maintain\nor induce this capability in smaller, distilled variants.\n5.3\nLimitations\nDespite the encouraging results, our study is not without\nlimitations. Firstly, the performance gain from GRPO,\nwhile statistically significant, is small (7% accuracy im-\nprovement in our reported experiment). Further inves-\ntigation is needed to explore whether more extensive\nGRPO training, or modifications to the reward func-\ntion, could lead to more substantial performance gains.\nIt is possible that the current reward function, while ef-\nfective, could be further optimized to better incentivize\nmore complex reasoning strategies, such as backtracking\nor more proactive exploration of alternative paths.\nSecondly, our evaluation, while including qualitative\nanalysis, is primarily based on maze-solving accuracy.\nThis metric, while important, provides a somewhat lim-\nited view of the model\u2019s reasoning capabilities. Future\nwork could benefit from more nuanced evaluation met-\nrics that assess the efficiency of the generated paths, the\nrobustness of the model to maze complexity variations,\nand the interpretability of the model\u2019s internal reasoning\nprocess. Furthermore, while we observed qualitative signs\nof chain-of-thought reasoning, a more rigorous analysis,\nperhaps using techniques from interpretability research, is\nneeded to definitively characterize the nature and depth\nof the model\u2019s reasoning process.\nFinally, our experiments are limited to synthetically\ngenerated mazes.\nWhile these mazes were designed to\nvary in size and complexity, they may not fully capture\nthe intricacies and variability of real-world visual spatial\nreasoning tasks. Future research should explore the gen-\neralizability of our approach to more diverse and ecolog-\nically valid visual environments and tasks.\n6\nConclusion\nThis paper introduced AlphaMaze, a novel approach to\nenhance Large Language Models\u2019 spatial intelligence, fo-\ncusing on maze navigation.\nWe demonstrated the effi-\ncacy of a two-stage training framework, leveraging Su-\npervised Fine-Tuning (SFT) followed by Group Relative\nPolicy Optimization (GRPO). While initial pre-trained\nLLMs exhibited 0% accuracy on MazeBench, highlight-\ning the need for task-specific adaptation, our approach\nsuccessfully imbued a distilled LLM with robust spatial\nreasoning capabilities.\nSFT provided a crucial founda-\ntion by teaching step-by-step movement prediction from\ntokenized maze inputs, reaching 86% accuracy. This un-\nderscores the importance of structured input and targeted\ntraining for LLMs to effectively engage with visual spatial\ninformation.\nCrucially, we adapted and applied the two-stage train-\ning methodology pioneered by DeepSeek-R1, demonstrat-\ning its generalizability beyond language-centric reasoning\ntasks. Following SFT, GRPO fine-tuning further elevated\nperformance to 93% on MazeBench after 1600 training\nsteps, showcasing the power of reinforcement learning to\nrefine reasoning processes in a novel domain. Qualitative\nanalysis revealed that GRPO fostered more sophisticated\nand self-corrective reasoning strategies, including emer-\ngent chain-of-thought patterns, mirroring observations in\nDeepSeek-R1 and suggesting a common mechanism for\nenhanced reasoning through RL.\nOur work contributes to the broader effort of expand-\ning LLMs\u2019 reasoning abilities beyond natural language,\ndemonstrating the potential of a two-stage approach for\nvisually grounded tasks. The success of GRPO, inspired\nby DeepSeek-R1\u2019s advancements in language reasoning,\nhighlights the transferability of these techniques to spa-\ntial domains. This suggests that carefully designed re-\ninforcement learning, following an initial phase of super-\nvised task learning, can be a powerful method to unlock\nand refine sophisticated reasoning capabilities in LLMs\nacross diverse problem spaces. The implications extend\nbeyond maze navigation to a wide array of applications\ndemanding spatial understanding and sequential decision-\nmaking.\nFuture research will focus on further validating this\ntwo-stage GRPO approach across various reasoning do-\nmains beyond spatial tasks, exploring its potential to en-\nhance LLMs\u2019 capabilities in areas such as symbolic rea-\nsoning, logical deduction, and planning. Investigating the\noptimal configurations of SFT and GRPO stages, diver-\nsifying training data to encompass richer and more com-\nplex reasoning scenarios, and developing more refined re-\nward functions tailored to different reasoning challenges\nare critical next steps. By pursuing these directions, we\naim to establish the broader applicability of this two-stage\ntraining paradigm for imbuing standard LLMs with ro-\nbust and versatile reasoning abilities, paving the way for\nmore capable and generally intelligent language models.\nReferences\nZixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji,\nand Quanquan Gu. Self-play fine-tuning converts weak\nlanguage models to strong language models, 2024. URL\nhttps://arxiv.org/abs/2401.01335.\nMichael Han Daniel Han and Unsloth team.\nUnsloth,\n2023. URL http://github.com/unslothai/unsloth.\nDuyu Guo, Guoxin Xu, Yuchen Chen, Chen Tang, and\nOthers. Deepseek-r1: Incentivizing reasoning capabil-\nity in llms via reinforcement learning. arXiv preprint\narXiv:2501.12948, 2025.\nURL https://arxiv.org/\nabs/2501.12948.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu\nChen.\nLora: Low-rank adaptation of large language\nmodels, 2021.\nURL https://arxiv.org/abs/2106.\n09685.\nMichael Igorevich Ivanitskiy, Rusheb Shah, Alex F.\nSpies, Tilman R\u00a8auker, Dan Valentine, Can Rager, Lu-\n7\n\ncia Quirke, Chris Mathwin, Guillaume Corlouer, Ce-\ncilia Diniz Behn, and Samy Wu Fung. A configurable\nlibrary for generating and manipulating maze datasets,\n2023. URL https://arxiv.org/abs/2309.10498.\nSaba\nJanamian\nand\nMD\nSahabul\nAlam.\nMaze\nsolver\nrobot\nusing\na*\nalgorithm,\n2023.\nURL\nhttps://scholarworks.calstate.edu/concern/\ntheses/0c483r787. ScholarWorks@CSUN.\nXiaohu Jiang, Yixiao Ge, Yuying Ge, Dachuan Shi, Chun\nYuan, and Ying Shan. Supervised fine-tuning in turn\nimproves visual foundation models, 2024. URL https:\n//arxiv.org/abs/2401.10222.\nMinae Kwon, Sang Michael Xie, Kalesha Bullard, and\nDorsa Sadigh. Reward design with language models,\n2023a. URL https://arxiv.org/abs/2303.00001.\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng,\nLianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez,\nHao Zhang, and Ion Stoica.\nEfficient memory man-\nagement for large language model serving with page-\ndattention. In Proceedings of the ACM SIGOPS 29th\nSymposium on Operating Systems Principles, 2023b.\nPatrick Lester. Pathfinding algorithms, 2014-2024. URL\nhttps://www.redblobgames.com/pathfinding/.\nRed Blob Games.\nChengzu Li, Wenshan Wu, Huanyu Zhang, Yan Xia,\nShaoguang Mao,\nLi Dong,\nIvan Vuli\u00b4c,\nand Furu\nWei.\nImagine while reasoning in space: Multimodal\nvisualization-of-thought, 2025. URL https://arxiv.\norg/abs/2501.07542.\nYueen Ma, Zixing Song, Yuzheng Zhuang, Jianye Hao,\nand Irwin King.\nA survey on vision-language-action\nmodels for embodied ai, 2024. URL https://arxiv.\norg/abs/2405.14093.\nJiajun Mao, Chuang Gan, Fan Zhang, and Others.\nNeural-symbolic visual reasoning: A survey. 2023. URL\nhttps://arxiv.org/abs/2302.07200.\nZhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junx-\niao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang,\nY. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Push-\ning the limits of mathematical reasoning in open lan-\nguage models, 2024. URL https://arxiv.org/abs/\n2402.03300.\nBoshi Wang, Sewon Min, Xiang Deng, Jiaming Shen, You\nWu, Luke Zettlemoyer, and Huan Sun. Towards un-\nderstanding chain-of-thought prompting: An empiri-\ncal study of what matters.\nIn Anna Rogers, Jordan\nBoyd-Graber, and Naoaki Okazaki, editors, Proceed-\nings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 2717\u20132739, Toronto, Canada, July 2023. Associ-\nation for Computational Linguistics. doi: 10.18653/v1/\n2023.acl-long.153. URL https://aclanthology.org/\n2023.acl-long.153/.\nJason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M. Dai, and Quoc V. Le.\nFinetuned language\nmodels are zero-shot learners, 2022a.\nURL https:\n//arxiv.org/abs/2109.01652.\nJason Wei, Denny Zhou, Quoc Le, Denny Zhou, Quoc\nLe, and Others.\nChain-of-thought prompting elic-\nits reasoning in large language models.\nAdvances\nin\nNeural\nInformation\nProcessing\nSystems,\n35:\n24824\u201324837, 2022b.\nURL https://proceedings.\nneurips.cc/paper_files/paper/2022/hash/\n9d8fc0533c2250385321d99c6a3f2f2c-Abstract-Conference.\nhtml.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and\nDenny Zhou. Chain-of-thought prompting elicits rea-\nsoning in large language models, 2023. URL https:\n//arxiv.org/abs/2201.11903.\nSamy Zafrany. Deep reinforcement learning for maze solv-\ning, 2020.\nURL https://www.samyzaf.com/ML/rl/\nqmaze.html. samyzaf.com.\nJingyi Zhang, Jiaxing Huang, Sheng Jin, and Shijian\nLu. Vision-language models for vision tasks: A survey,\n2024. URL https://arxiv.org/abs/2304.00685.\n8\n\nA\nAlgorithm\nThis appendix details the algorithm used to generate the maze reasoning dataset with reset demonstrations. The\nalgorithm processes a base dataset of maze navigation problems and augments it with demonstration of incorrect\nattempts followed by resets and correct solutions.\nAlgorithm 1 Maze Reasoning Reset Data Generation - Main Process\nRequire: Base dataset D containing maze problems with:\n1:\n- Adjacency list representation of 5 \u00d7 5 maze grid\n2:\n- Origin and target coordinates\n3:\n- Correct solution path\nEnsure: Augmented dataset with reset demonstrations\n4: Initialize empty datasets D1 and D2\n5: for all example e \u2208D do\n6:\nExtract adjacency list A, origin O, target T, and path P from e\n7:\nCount walls W around origin O\n8:\nif W = 1 then\n9:\nAdd e to D1\n10:\nCall ProcessOrder1(e) {See Algorithm 2}\n11:\nelse if W = 2 then\n12:\nAdd e to D2\n13:\nCall ProcessOrder2(e) {See Algorithm 3}\n14:\nend if\n15: end for\n16: Combine processed examples from D1 and D2 into the final dataset\nAlgorithm 2 Order-1 Processing (1 wall at origin)\n1: Procedure ProcessOrder1(example)\n2: WP \u2190\u2205{Initialize wrong paths set}\n3: for all adjacent node N to origin O do\n4:\nif N /\u2208correct path P then\n5:\nfor n steps from max n steps down to 1 do\n6:\nAttempt to extend path from N until a dead end or n steps are reached.\n7:\nif path length = n steps or a dead end is reached then\n8:\nWP \u2190WP \u222a{path}\n9:\nbreak\n10:\nend if\n11:\nend for\n12:\nend if\n13: end for\n14: for all path p \u2208WP do\n15:\nGenerate chain-of-thought steps for path p.\n16:\nAdd \u201cHeading in wrong direction\u201d message.\n17:\nAdd RESET marker.\n18: end for\n19: Append original correct solution (path P).\n20: Format as conversation pairs.\n21: End Procedure\n9\n\nAlgorithm 3 Order-2 Processing (2 walls at origin)\n1: Procedure ProcessOrder2(example)\n2: for n steps from max n steps down to 1 do\n3:\nGenerate wrong path WP of length n steps starting from O.\n4:\nif a valid path WP is found then\n5:\nGenerate chain-of-thought for WP.\n6:\nif WP ends at a dead end (3 walls) then\n7:\nAdd \u201cHit a dead end\u201d message.\n8:\nelse\n9:\nAdd \u201cHeading in wrong direction\u201d message.\n10:\nend if\n11:\nAdd RESET marker.\n12:\nbreak\n13:\nend if\n14: end for\n15: Append original correct solution (path P).\n16: Format as conversation pairs.\n17: End Procedure\n10\n"
  ]
}